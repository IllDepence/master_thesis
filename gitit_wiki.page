# orga

### big TODOs

* switch from arXiv IDs to MAG IDs
* implement document wise splitting for evaluation
* generate a complete dataset (instead of just one month)

### recent notes

* match to MAG IDs instead of arXiv IDs
* mby treat "co-appearing" citations as valid (e.g. "some stuff super important [1][2][3]" <- all three count)
* general idea: high weight on smaller local context, then also consider (with some lower weight) larger context
* do doc wise split (train/test)
* do pre-selection before cosine sim (or similar) -> how?
* (NEL) annotate documents, *then* extract citation contexts (b/c annotators use large context -> better annotation results)

### overall

* thema verstehen → expose (mail mit details kommt noch) schreiben → thesis anemlden
* genaue richtung noch nicht so fest wie vermutet

# links / notes

* arXiv
    * 1.4 M documents → processing each for **0.42 seconds** takes **~1 week**
    * [sources](https://arxiv.org/year/cs/18) (single dl)
    * [IDs over time](https://arxiv.org/help/arxiv_identifier)
    * availability of DOIs (in CS only dump from Dec. 2017)
        * ```
        >>> len(tree.xpath("//dc:identifier[starts-with(text(),'doi')]", namespaces={'dc':'http://purl.org/dc/elements/1.1/'}))
        29739
        ```
        * → 28775 of which are contained in MAG
        * ```
        >>> len(tree.xpath("//ListRecords/record"))
        155308
        ```
        * → 19%
* MAG
    * [schema](https://microsoftdocs.github.io/microsoft-academic/microsoft-academic-graph/reference/data-schema.html)
    * [blog post w/ stats](https://www.microsoft.com/en-us/research/?post_type=msr-blog-post&p=480543&preview=true)
    * [oldish examples on quora](https://www.quora.com/Where-can-I-obtain-a-schema-description-of-Microsoft-Academic-Graph-data/answer/Milind-Gokhale-1)
    * availability of DOIs
        * ```
        saiert@shetland:~$ cut -f3 -d$'\t' /vol1/mag/data/2018-07-19/dumps/Papers.txt | sed '/^\s*$/d' | wc -l
        75266339
        ```
        * ```
        saiert@shetland:~$ cat /vol1/mag/data/2018-07-19/dumps/Papers.txt | wc -l
        206252196
        ```
        * → 36.5%

# questions

* entity based citerec
    * go-to example/paper? (candidates for similar task see below)
* general (still somewhat far away)
    * not a single of the citerec approaches in CireRecSurvey uses the Microsoft Academic Graph (or arXiv for that matter) as dataset, right? (→existence of approaches to directly compare to?)
    * similar to ^ baseline to start from?
    * other approaches often are a large ensemble of techniques/features/prior work reused — should I rather build/try out "from scratch" or  recreate/assemble existing things? in both cases: is trying for improvement just enough or do choices need to be justified further?
    * ^ pick/engineer vs "brute force" (i.e. just try a lot of different stuff and see what contributes most to a good result)

# structured notes

### general

* task
    * mail
        * <small>**keinen komplett neuen Ansatz** zu citation recommendation entwerfen, sondern eher ein gutes lauffähiges System zu citation recommendation bauen. Bereits die Nachimplementierung eines "state-of-the-art"-Systems wäre eine contribution</small>
        * <small>Wahrscheinlich werden wir uns eher **semantisch-strukturierte Repräsentationsformen (entities, claims, arguments) für citation recommendation** widmen.</small>
        * <small>(Ashwath ... Indizierung des großen Microsoft Academic Graphs (MAG) und um die Verwendung eines state-of-the-art approaches für citation recommendation)</small>
        * <small>in Ihrer Arbeit eher mit **semantischen und pragmatischen Ansätzen für citation recommendation** beschäftigen. Diesbezüglich gibt es nämlich **in der Literatur noch recht wenig**</small>
        * <small>An sich ist der Microsoft Academic Graph als Datensatz für citation recommendation geeignet, allerdings enthält er nur einen Satz als citation context -- und ein größerer Kontext wäre oft angebracht, besonders, wenn wir nicht nur Entitäten zitieren, sondern ganze Argumente. Ganze Papers (fulltext) sind an sich nicht im MAG verfügbar. Die Lösung läge darin, stattdessen die Paper von arXiv.org als Subset zu verwenden. Diese sind m.E. auch im MAG. Außerdem können diese von arXiv heruntergeladen werden und liegen im Format TeX (+bibtex) vor, was den Vorteil hat, besser an den Reintext und an die Zitate zu kommen. Allerdings müsste ein TeX parser geschrieben werden, der den Rohtext usw. extrahiert. In Haskell wurde ein solch einfacher Parser bereits entwickelt [1], allerdings ohne Berücksichtigung von section headers etc.</small>
        * <small>Zitate kann man differenzieren dahingehend, ob eine Entität (z.B. ein Konzept oder ein Datensatz oder eine Methode) zitiert wird, oder eine Behauptung (claim), oder eine Behauptung/Argument im Zusammenspiel einer Argumentationskette. Demzufolge kann man für jede dieser Möglichkeiten geeignete Repräsentationsformen (aus der Computerlinguistik) für ein besseres citation recommendation verwenden. **Konkret kann man z.B. mentions im Content des Papers zu Entitäten in Wikipedia oder Wikidata verlinken (entity linking bzw. text annotation), sowie claims oder ganze Argumentationsketten extrahieren und entsprechend formal repräsentieren** (einige Beispiele diesbzgl. sind in [2-5]; die genauen Ansätze, die wir verwenden werden, können wir später festlegen). Insgesamt ginge es also hier um die **Anwendung von NLP-Komponenten auf wissenschaftliche Texte**.</small>
        * <small>Im Rahmen der Masterarbeit würde man **für jedes Unterthema (z.B. entity, claim, argumentation) einen Ansatz für citation recommendation entwickeln**. Für eine Einschätzung der Schwierigkeiten bei der Ansatzentwicklung und dann später für eine Evaluation der Ansätze müssen wohl einige Texte auch manuell annotiert werden. Außerdem würde man die Implementierungen der einzelnen Systeme zur recommendation in das Gesamtsystem von Ashwath integrieren.</small>
    * gist
        * starting point
            * MAG has much going for it, but only one sentence citation context
            * arXiv has paper sources (TeX)
        * use intersection of MAG and arXiv
        * get plain text from arXiv source
        * match MAG citation contexts to arXiv plain texts (? if so, plain text of arXiv would be sufficient (no need to retain citations))
        * → MAG with arbitrary large citation contexts
        * ———
        * in large citation contexts, find and formally represent "semantic NLP components" (using *all* the buzzwords)
        * ML
        * ???
        * profit 
* defs
    * argument = claim + one or more premises justifying the claim
        * [premises] [step(s) of deduction] [claim]
        * x (and y), therefore z

### start out w/ MAG or arXiv

* MAG
    * `+` matching MAG IDs to arXiv IDs should be quite reliable (could even use citation contexts to match to arXiv plain text)
    * `+` citation network is already given
    * `-` (blind?) reliance on MS / less transparent
    * `-` citation markers might not be *that* reliable (need to match arXiv source bibitem strings to MAG IDs)
    * `o` might enable just using latexpand+detex for TeX processing
* argument for arXiv
    * `-` matching arXiv source bibitem strings to arXiv IDs might not be *that* reliable
    * `+` more transparent
    * `+` citation markers are (albeit dependent on initial matching) certain
    * `+` (if we don't add in the MAG:) only rely on one source → cleaner approach (?)

questions to be answered:

* how hard is matching arXiv bibitem strings to arXiv IDs?
* if starting from arXiv, what is gained from adding in the MAG?
* ...

### tex parser

* grabcite[/test-tex](https://github.com/agrafix/grabcite/tree/master/test-tex) mby "edge cases"? (download source by ID, don't just use .tex file from repo)

##### existing software

* ~~[plastex](https://github.com/tiarno/plastex) — largish, last commit 1y ago, python~~
    * in addition to requirements.txt also `$ pip install Pillow Unidecode` for less noisy output
    * TEST: neat output when it works, **but** failed on all tested arXiv papers and it's own documentation
* [TexSoup](https://github.com/alvinwan/texsoup) — smallish, recent commits, python
    * doesn't handle definitions (`\dev`) [source](https://stackoverflow.com/a/50151171)
    * [stackoverflow question](https://stackoverflow.com/questions/49779853/) specifically asking about arXiv TeX (by someone using TeXSoup)
    * TEST: works on most tested; fails with e.g. `1607.00138`
    * find citations
        ```
        soup = TexSoup(tex_str)
        c = soup.find_all(name='cite')
        ```
    * output text
        ```
        soup = TexSoup(tex_str)
        for cntnt in soup.document.contents:
            if type(cntnt) == str:
                print(cntnt)
        ```
    * **TESTED** on larger sample: 32% fails
        * 54 of 169 failed — can be reduced to 50 by replacing preamble w/ a clean one
        * → mby do 70% TexSoup + 30% fallback sth. (e.g. detex + python script to put cites back in?)
* [grabcite](https://github.com/agrafix/grabcite)
    * installation
        * required additional packages `libpcre++-dev`, `libpq-dev`, `libghc-hdbc-odbc-dev` (+500 MB of dependencies)
    * unpack arXiv dump
        * file given for param `--arxiv-meta-xml` has do be in cwd (giving a path results in `grabcite-datagen: InvalidRelFile "..."`)
        * arXiv: *"Note: Many of the formats above are served gzipped (Content-Encoding: x-gzip). Your browser may silently uncompress after downloading so the files you see saved may appear uncompressed."*
        * grabcite:
            * expects arXiv sources with `.gz` extension in input folder
            * has `gzHandler` and `tarGzHandler` (see `src/GrabCite/Arxiv.hs`)
            * → arXiv sources manually downloaded are mere `tar` archives w/o file extension → need to be gzipped and renamed to have `.gz` file extension (*not* `.tar.gz`)
    * generate data set
        * tries to connect to papergrep.com (expired namecheap.com registration that probably once hosted [this](https://github.com/agrafix/papergrep))
* [opendetex](https://github.com/pkubowicz/opendetex) / [detex](https://www.freebsd.org/cgi/man.cgi?query=detex) — ?, recent commits, compiled
    * specifically for getting plain text
    * `-c` flag is supposed to preserve `\cite`s, but breaks output completely for first tested paper `1010.2903`
    * TEST: opendetex seems to leave in more control sequences than on system detex
* [LaTeXML](https://github.com/brucemiller/LaTeXML) — *very* mature, recent commits, perl
    * LaTeX→XML
    * existing (dead?) [project on on arXiv data](https://kwarc.info/projects/arXMLiv/) ([active LaTeXML fork](https://github.com/KWARC/LaTeXML))
    * ~~install from source~~
        * `sudo apt install cpanminus libxml-libxml-perl libxml-libxslt-perl libxml2-dev libimage-size-perl`
        * `sudo cpanm XML::LibXML`
        * `sudo cpanm Parse::RecDescent`
        * `perl Makefile.PL`
        * `make`
        * `make test` → **fails**
        * `make install`
    * install:
        * `sudo apt install libtext-unidecode-perl`
        * `sudo apt install latexml`
    * misc notes:
        * xpath matches in output require namespace. e.g.:
            * `tree.xpath('//LaTeXML:Math', namespaces={'LaTeXML':'http://dlmf.nist.gov/LaTeXML'})`
            * `etree.strip_elements(tree, '{http://dlmf.nist.gov/LaTeXML}Math', with_tail=False)`
* [Tralics](https://www-sop.inria.fr/marelle/tralics/) — ?, 2015, C++
    * LaTeX→XML
    * [apparently](https://jblevins.org/log/xml-tools) *fast*
* [flap](https://github.com/fchauvel/flap)
    * can be run from inside python by copying `flap/ui.py` and replacing bottom (click) part with `Controller(OSFileSystem(), Display(sys.stdout, False)).run('some.tex', 'out_folder')`
    * somewhat slow?
    * → try out [alternatives](https://tex.stackexchange.com/questions/21838/replace-inputfilex-by-the-content-of-filex-automatically)
    * → latexpand looks good
        * has `--expand-usepackage` flag
        * has `--expand-bbl FILE` flag (mby useful)

##### arXiv

* IDs
    * metadata record: `oai:arXiv.org:0704.0046`　　(xPath: `/ListRecords/record/header/identifier[text()='<id>']`)
    * web: `https://arxiv.org/abs/0704.0046`
    * data dump: `0704.0046.gz`
* only *some* metadata records have DOIs
* `\cite` in text, then `\bibitem` lower down or in `.bib` file
* problems
    * sometimes a paper's [v2](https://arxiv.org/format/1306.0555v2) gives empty source while it's [v1](https://arxiv.org/format/1306.0555v1) has proper source
    * [cases](https://arxiv.org/format/1607.00145) where "source" is just a single `\includepdf`
    * `physics0001026`: html instead of tex
    * `physics0001060`: postscript instead of tex
    * `astro-ph0001005`: using file extension .ltx for latex
    * `hep-th0001079`: few sentences of plaintext
    * `hep-th0001205`, `quant-ph0001041`, ...: TeX (not LaTeX)
    * `ph0001041`: weird LaTeX?

##### arXiv source dump

* lots of tar archives like `arXiv_src_0001_001.tar` and a manifest file `arXiv_src_manifest.xml` w/ some metadata
* top level archive content examples:
    * `cs0001012.gz` → `arXiv:cs/0001012`
    * `gr-qc0001036.pdf` → `arXiv:gr-qc/0001036`
    * `1502.00318.gz` → `arXiv:1502.00318`
* 流れ
    * step 1: normalize (separate steps to allow for separate evaluation of results)
        * check for file ending (pdf/gz)
        * gz: unzip and check if tar archive or tex file (`tarfile.is_tarfile()`)
        * tar: extract contents, identify main tex file (`\begin{document}` mby?), flatten with [flap](https://github.com/fchauvel/flap), also check for `\includepdf`
        * save tex[+bib] / pdf in output folder
    * step 2: get plain text
        * pdf: pdf2text (citation markers byebye)
        * tex: TeXSoup
        * save metadata from XML (mby in JSON)
        * tex: replace local citation markers w/ globally unique one (UUID, save UUID→original bibitem text, save UUID→citing paper ID index)
    * step 3: networking
        * try to match citation UUIDs to arXiv IDs (and mby at one point other identifiers)

**test w/ 2364 input docs**  
**step 1**
```
saiert@shetland:~/arxiv_test$ time python3 normalize_arxiv_dump.py /home/saiert/arxiv_test/0001_001/0001/ /home/saiert/arxiv_test/0001_001/normalized

real    3m37.049s
user    3m16.520s
sys     0m20.866s
```
(~0.09 sec per doc)  
→ all of arXiv in 37 hours  
→ 2364m→2250 95% success

**step 2**  
on shetland, 50min for 392 docs (~7 sec per doc); 289 min for 2364 docs (~7.7 sec per doc)  
→ all of arXiv in 128 days (>4 months)  
→ 2218\*→2138 96% success (90% overall)  
\*32 PDFs, therefore not 2250

##### arXiv processing speed up

* math pre removal
    * time: 3m53s (~0.09s/doc) | 122m (3.3s/doc)
    * coverage: unchanged | 2066 **93%**

```
saiert@shetland:~/arxiv_test$ time python3 normalize_arxiv_dump.py /home/saiert/arxiv_test/0001_001/0001/ /home/saiert/arxiv_test/0001_001/normalized

real    3m53.446s
user    3m33.779s
sys     0m19.952s

output: 2218 docs

saiert@shetland:~/arxiv_test$ time python3 parse_latex_latexml.py 0001_001/normalized/ 0001_001/text

real    122m26.729s
user    113m3.402s
sys     8m43.531s

output 2066 docs
```

* ーネ申 **tralics** ネ申ー
    * on shinobu: 2218 docs in 53s → 0.024s/doc
    * on shinobu w/ SQLite DB instead of separate text files for metadata: 2m13s
    * in: 2216, out: 2199 (30 messed up) → 98% (91.8% overall) (latexml only 2138)
    * a bit more preamble noise (already in the XML in `<p>`s, so no straightforward way to deal with it)
* hep-lat0001017 is creating new commands for marking math environments, *with a newly created command for newcommand*, ***outside the preamble***

**tralics problem docs with latexml**  
(messed up output (but not skip b/c error) for 0001_001; for which latexml outputs 2138, tralics 2199)
```
tralics problem         latexml out                 latexml better
astro-ph0001027.txt     none                        (x)
astro-ph0001100.txt     none                        (x)
astro-ph0001144.txt     also messed up
astro-ph0001152.txt     also messed up
astro-ph0001153.txt     also messed up
(astro-ph0001462.txt)   fine (more content)         x
astro-ph0001540.txt     none                        (x)
astro-ph0001546.txt     none                        (x)
(cond-mat0001187.txt)   same
(cond-mat0001222.txt)   no weird "aaaaaaaaa"
cs0001010.txt           same
gr-qc0001071.txt        fine                        x
hep-lat0001018.txt      fine                        x
hep-ph0001051.txt       also messed up
hep-ph0001203.txt       fine                        x
(hep-th0001048.txt)     same
hep-th0001109.txt       fine                        x
hep-th0001139.txt       same
hep-th0001146.txt       same
hep-th0001181.txt       fine                        x
hep-th0001199.txt       fine                        x
math0001022.txt         same
math0001029.txt         same
math0001070.txt         same
math0001103.txt         same
math0001164.txt         fine                        x
math0001166.txt         same
nucl-th0001065.txt      fine                        x
```

```
python3 parse_latex_tralics.py ~/dl/foo/0001_normalized ~/dl/foo/0001_text_new 70,28s user 20,82s system 68% cpu 2:13,90 total
```

**tralics citation/bibitem parsing** (tested on Dec 2017 complete dump)    
```
Citations: 488725    *(295710 unique)
Unmatched citations: 93315
```

→ Tralics is supposed to be able to understand [commands from a lot of packages](https://www-sop.inria.fr/marelle/tralics/packages.html) (esp. natbib is relevant here) but apparently doesn't  
→ fixing `\citep`s, `\bibitem[bla{problem}bla]{key}`s etc. helps with a lot of citations (see regex and results below)  
→ still a problem are [nmras](https://www-sop.inria.fr/marelle/tralics/packages.html) style bibliographies (predictable format so mby. #TODO)

```
NATBIB_PATT = re.compile((r'\\cite(t|p|alt|alp|author|year|yearpar)\s*?\*?\s*?'                                         
                           '(\[[^\]]*?\]\s*?)*?\s*?\*?\s*?\{([^\}]+?)\}'),                                              
                         re.I)
cntnt = NATBIB_PATT.sub(r'\\cite{\3}', cntnt)

BIBOPT_PATT = re.compile(r'\\bibitem\s*?\[[^]]*?\]', re.I|re.M)
cntnt = BIBOPT_PATT.sub(r'\\bibitem', cntnt)
```

```
Citations: 565613 (not unique)                                                  
Unmatched citations: 28372
```

**→ increase from 70% to 95%** (assuming above 565613 are 100%)

impact on example data set:

```
$ cat items_1712_100w_2mincont_not_stemmed.csv | wc -l
17640
$ cat items_1712_100w_2mincont_not_stemmed_NATBIB_FIX.csv | wc -l
28673

```

##### bibitem matching

* arxiv.org web API test (use unrealistic b/c time)
    * match for e.g. first bibitem in `1711.00002`
        * original: `L. J. Ba and R. Caruana. Do deep nets really need to be deep? In Proceedings of NIPS, 2014.`
        * query: `http://export.arxiv.org/api/query?search_query=all:Caruana%20deep%20nets%20really%20need%20deep%20Proceedings%20NIPS%202014` (words longer 2 letters)
* source given links/arXiv IDs (tested w/ 377 docs from 2017)  
```
sqlite> select count(*) from bibitem;
10270
sqlite> select count(distinct uuid) from bibitemlinkmap;
1615
sqlite> select count(distinct uuid) from bibitemarxividmap;
476
```
* Solr test

```
$ time python3 solrize.py
no metadata in 1008.3138 in arxiv-physics-oai_dc-cursor147000.xml
no metadata in 1008.3138 in arxiv-physics:quant-ph-oai_dc-cursor13000.xml
no metadata in 1005.0836 in arxiv-math-oai_dc-cursor50000.xml
python3 solrize.py  430,84s user 8,54s system 98% cpu 7:27,43 total
```

```
$ time ./bin/post -c arxiv_meta ~/dl/foo/newArxivMetaHarvesting201712/solrized
[...]
2643 files indexed.
COMMITting Solr index changes to http://localhost:8983/solr/arxiv_meta/update...
Time spent: 0:19:19.727
./bin/post -c arxiv_meta ~/dl/foo/newArxivMetaHarvesting201712/solrized  10,20s user 28,05s system 3% cpu 19:20,09 total
```
* bibitem matching w/ Solr

```
- - - - - using title_query_split_heuristic - - - - -
total: 10270
matches: 612
checked: 476
false negatives: 463
false positives: 0
python3 match_bibitems.py path ~/dl/foo/1712_text  35,91s user 1,67s system 74% cpu 50,206 total
```

```
- - - - - using title_author_query_words - - - - -
$ python3 match_bibitems.py path ~/dl/foo/1712_text
total: 10270
matches: 1703
checked: 476
false negatives: 315
false positives: 1
python3 match_bibitems.py path ~/dl/foo/1712_text  38,76s user 1,87s system 43% cpu 1:33,12 total
```

**NOTE**: bibitems that include IDs might be more likely to not include a title. In a sample of 250 of above 476 checked bibitems, only 117 included a proper title.

**optimistic extrapolation**  
1703 + 315 out of 10270 → 19.6%  
assuming 1.45 M papers each 27 citations → 7.6 M citations

~~**(!!!) somewhere along the line duplicates found there way in arXiv ID map(!!!)** (05.11.2017)~~ (fixed)  
→ add constraints to DB, rerun pipleline for Dec. 2017  
```
sqlite> select count(*) from bibitemarxividmap;
70353
sqlite> delete from bibitemarxividmap where id not in (select min(id) from bibitemarxividmap group by uuid, arxiv_id);
sqlite> select count(*) from bibitemarxividmap;
62232
```

##### experiments

1712_001.tar.gz probably way too small:  
```
sqlite> select count(distinct uuid) from bibitemarxividmap;
476
sqlite> select count(distinct arxiv_id) from bibitemarxividmap;
463
```  
→ only one citation context per cited document  
→ huge risk of overfitting

0001_001.tar.gz  
```
sqlite> select count(*) from bibitemarxividmap;
2135
sqlite> select count(distinct arxiv_id) from bibitemarxividmap;
1778
sqlite> select count(distinct arxiv_id) from bibitemarxividmap where arxiv_id in (select arxiv_id from bibitemarxividmap group by arxiv_id having count(arxiv_id) > 1);
227    <- citations w/ more than one context
```

**get all citations that have more than one context**  
`sqlite> select uuid, arxiv_id from bibitemarxividmap where arxiv_id in (select arxiv_id from bibitemarxividmap group by arxiv_id having count(arxiv_id) > 1) order by arxiv_id desc;`  
with `in_doc`:  
`sqlite> select in_doc, bibitem.uuid, arxiv_id from bibitemarxividmap join bibitem on bibitemarxividmap.uuid = bibitem.uuid where arxiv_id in (select arxiv_id from bibitemarxividmap group by arxiv_id having count(arxiv_id) > 1) order by arxiv_id desc;`

##### december 2017 dump complete test

* on shetland
* w/ sqlite DB (probably not the most performant) 
* solr instance on shinobu (→ latency)

```
$ time python3 prepare.py /home/saiert/transit/ /home/saiert/arxiv_test_2017_12_all

total: 295710
matches: 52800
checked: 17329
false negatives: 9141
false positives: 159

real    293m9.947s
user    101m10.978s
sys     12m8.748s
```

(parse_latex_tralics (run individually later) is only 19m4.407s of above)

```
$ du -hs metadata.db 
80M	metadata.db
```

**re-run w/ natbib fixes**  
```
$ python3 match_bibitems.py path /home/saiert/arxiv_test_2017_12_all_tralics_preprocessing/all_text
total: 317010
matches: 59277
checked: 21294
false negatives: 10364
false positives: 197
```

(complete) **re-run w/ throwing out tables and figures**  
```
total: 312536
matches: 58613
checked: 20784
false negatives: 10110
false positives: 188
```

### citation recommendation baseline

* idea: only syntax (mby +metadata) based
* misc notes:
    * [python code example using gensim](https://github.com/charles-vdulac/japanese-content-engine) (from [this presentation](https://www.youtube.com/watch?v=-xVeMQXMEcg))
    * [*simple* python code example using only pandas and scikit-learn](http://blog.untrod.com/2016/06/simple-similar-products-recommendation-engine-in-python.html)
    * [graph database](https://www.kernix.com/blog/an-efficient-recommender-system-based-on-graph-database_p9) [based thingy](https://www.kernix.com/blog/recommender-system-based-on-natural-language-processing_p10)

**train/test data generation stuff**  
```
$ cat items_1712_100w_4mincont_not_stemmed_oneperdoc.csv | wc -l    ← only one context per uuid per doc
11613
$ cat items_1712_100w_4mincont_not_stemmed.csv | wc -l              ← notice there's multiple contexts per doc
77121
$ cat items_1712_100w_4mincont_2mindoc_not_stemmed.csv | wc -l      ← notice you should do per doc splitting (require >1 docs)
41359
```

**TFIDF baseline test w/ gensim**  
(many fails, below a lucky nice example)  
```
$ python3 recommend.py items_1712_100w_4mincont.csv
similar to: math/9904022 (6768)
correct: [6186, 9728, 9771]
- - - - - - - -
✔ 0.309774786233902: 9770
✔ 0.179429829120636: 6186
  0.1443936824798584: 10067
  0.13785989582538605: 7983
  0.13637538254261017: 4082
  0.1355295479297638: 3858
  0.13414070010185242: 9969
  0.1162891760468483: 1831
  0.11208947747945786: 4716
  0.11141400039196014: 3620
  0.10494373738765717: 6270
```

**TFIDF baseline test w/ gensim** (80/20 split, then combine training vectors for same cited doc)  
```
$ time python3 recommend.py items_1712_100w_4mincont_stemmed.csv
- - - - - 1404/1404 - - - - -
#1: 199
in top 5: 663
in top 10: 954
avg: 14.554843304843304
ndcg: 0.4466429864293447
map: 0.2992083662933575
ndcg@5: 0.3093782646226242
map@5: 0.25600664767331355

real    15m13.260s
user    15m14.722s
sys     0m14.319s
```

not stemmed  
```
$ time python3 recommend.py items_1712_100w_4mincont_not_stemmed.csv
- - - - - 1404/1404 - - - - -
#1: 254
in top 5: 753
in top 10: 980
avg: 18.957264957264957
ndcg: 0.48227821110408237
map: 0.3448019509227324
ndcg@5: 0.3660963359894245
map@5: 0.3098765432098757

real    15m10.916s
user    15m12.664s
sys     0m13.642s
```

minimum of 2 citation contexts  
```
$ time python3 recommend.py items_1712_100w_2mincont_not_stemmed.csv
- - - - - 6843/6843 - - - - -
#1: 596
in top 5: 1895
in top 10: 2690
avg: 333.87315504895514
ndcg: 0.3212211934067741
map: 0.1825363717938699
ndcg@5: 0.18340135284347936
map@5: 0.15270836377807132

real    234m12.826s
user    233m30.086s
sys     0m54.815s
```

more data (natbib fix)  
```
$ python3 recommend.py items_1712_100w_2mincont_not_stemmed_NATBIB_FIX.csv
- - - - - 10497/10497 - - - - -
#1: 699
in top 5: 2382
in top 10: 3600
avg: 481.9152138706297
ndcg: 0.2925079165753687
map: 0.15257636645208303
ndcg@5: 0.14781602404672087
map@5: 0.12187450382649058
```

currently running evals: test difference between one per doc (2.7k test) and many per doc (18k)

one per doc:  
```
$ time python3 recommend.py ~/items_1712_100w_4mincont_not_stemmed_oneperdoc.csv
- - - - - 2790/2790 - - - - -
#1: 366
in top 5: 1305
in top 10: 1769
avg: 30.940501792114695
ndcg: 0.43196490690778483
map: 0.28601776529361
ndcg@5: 0.3027048501650121
map@5: 0.24845280764635663

real    61m20.370s
user    61m13.365s
sys     0m21.400s
```

many per doc *without* per doc splitting:  
```
$ python3 recommend.py ~/items_1712_100w_4mincont_not_stemmed.csv
- - - - - 18438/18438 - - - - -
#1: 6201
in top 5: 12608
in top 10: 14513
avg: 37.909643128321946
ndcg: 0.597700200736707
map: 0.48999624579362355
ndcg@5: 0.5218584144074571
map@5: 0.46789872365042323
```

##### notes on evaluation

* considerations when train/test splitting
    * always ensure a "stratified" split (i.e. don't just split 80/20 along all items in some order, split each *grouped per cited doc*-mini-package 80/20)
    * because an author's word choice and a paper's content can be a strong signal, it would be ideal to not have citations from one paper split (some in test, some in training)
        * just like typhoon/sequence wise splits in pyphoon
        * → require at least 2 *containing docs* for evaluation, then do a document wise train/test split
* minimum threshold for citation contexts (2 and 4 tested) has large impact on performance


### citation recommendation based on entities

* no clue of argumentative structure yet → chose fixed citation context size
* mby equivalent [1](https://link.springer.com/content/pdf/10.1007%2F978-3-319-30671-1_3.pdf), [2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.8098&rep=rep1&type=pdf), [3](http://l3s.de/~fetahu/publications/cikm2016.pdf), [4](http://www.l3s.de/~fetahu/publications/fetahu_cikm2015.pdf) (recommend news articles for wikipedia articles referencing past events)

##### named entity linking

* find tool for linking stuff to wiki-/dbpedia
* `curl -d '<item><text>Convolutional neural networks have been used for artificial intelligence.</text></item>' -X POST http://132.230.150.127:8080/text-annotation-with-offset-Nov14/`
* citations + POS tags (only in part involving named entities):
    * `[] (+ sub clause) + verb` (e.g. *[] show that* or *[], cited a gazillion times, found that*)
    * `proper Noun (+ et al.) + []` (e.g. *the work of ____ []*)
    * `proper Noun (+ "dataset") + []` (e.g. *DBpedia []*) (mentioned in Abu-Jbara2013 (there: noun phrase + []))
    * `preposition (+ Authors) + []` (most general case; e.g. *in []*, *by []*, *from []*)
    * `[].` (sentence end; could be anything)
* spotlight docker test
    * `$ docker pull dbpedia/spotlight-english`
    * `$ docker run -i -p 2222:80 dbpedia/spotlight-english:latest spotlight.sh`
    * `$ time python3 spotlight_annotate.py /home/saiert/arxiv_test_2017_12_all_181110/all_text/`  
        ```
        real    40m35.011s
        user    3m25.997s
        sys     0m5.937s
        ```  
        ```
        $ ls arxiv_test_2017_12_all_181110/all_text/ | grep txt | wc -l
        8996
        $ ls arxiv_test_2017_12_all_181110/all_text/ | grep json | wc -l
        8821
        ```

##### ~~named entity recognition~~ (probably not so important)

* [NLTK](https://www.nltk.org/)
    * simple example:  
        ```
        import nltk
        s = 'Some sentence with named entities like Bill Gates.'
        sent = nltk.word_tokenize(s)
        sent = nltk.pos_tag(s)
        print(nltk.ne_chunk(sent, binary=True))
        ```
* spaCy
    * apparently faster than NLTK but opinionated (offers one implementation per task, not multiple to choose from)
* gensim
    * "most commonly used for topic modeling and similarity detection"

### citation recommendation based on claims

* some clue of argumentative structure (?) → mby variable citation context size
* [ClausIE](https://gate.d5.mpi-inf.mpg.de/ClausIEGate/ClausIEGate/)

### citation recommendation based on arguments

* "detection of context should be related to the identification of the argument around the citation" (HERNANDEZ-ALVAREZ2016)

##### michael notes

* definition in paper Argumentation Mining: The Detection, Classification and Structure of Arguments in Text
    * challenges/goals of argumentation mining:
        * extract arguments ("Component identification" sometimes)
        * extract and represent the internal structure of arguments, i.e., the propositions and their relations (an argument consists of at least two propositions, so the assumption here; "Component classification" sometimes)
        * extract the argumentation structure, i.e., the interaction between arguments ("structure identification" sometimes).
* good survey: [Parsing Argumentation Structures in Persuasive Essays](https://arxiv.org/pdf/1604.07370.pdf)
* good survey: [Argumentation Mining in User-Generated Web Discourse](https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00276)
    * dort u.a. Erwähnung/Verlinkung von confirmation bias: Link zu *Recognizing the Absence of Opposing Arguments in Persuasive Essays* (darin: "tendency to ignore opposing arguments is known as *myside bias* or *confirmation bias* (Stanovich et al., 2013). It has been shown that guiding students to include opposing arguments in their writings significantly improves the argumentation quality, the precision of claims and the elaboration of reasons (Wolfe and Britt, 2009). Therefore, it is likely that a system which automatically recognizes the absence of opposing arguments effectively guides students to improve thier argumentation".
* Weitere aktuelle Arbeiten (z.B. gute und schlechte Ansätze) im ["Argument Mining"-Workshop](http://aclweb.org/anthology/W17-51) (z.B. "What works and what does not: Classifier and feature analysis for argument mining")
* Auch noch leicht verwandt: [SemEval-2018 Task12: Argument Reasoning Comprehension](https://competitions.codalab.org/competitions/17327)
* further resources
    * Sehr nützlich: ["Transition words"](https://msu.edu/~jdowell/135/transw.html) benutzt in Recognizing the Absence of Opposing Arguments in Persuasive Essays.
* also
    * Discourse Theory, e.g. Discourse Representation Theory, bzw. Discourse Analysis:
        * "Similar  to  the  identification  of  argumentation structures,  discourse  analysis  aims  at identifying elementary discourse units and discourse relations between them. Existing approaches on discourse analysis mainly differ in the employed discourse theory." [https://arxiv.org/pdf/1604.07370.pdf](https://arxiv.org/pdf/1604.07370.pdf)
    * Argumentation theory

# misc notes

* [Daniel Hershcovich](http://www.cs.huji.ac.il/~danielh/)
* python
    * [scikit-learn](http://scikit-learn.org/)
        * `fit-predict` pattern
        * classification: `train_test_split` function has useful param `stratify` to ensure balanced distribution over classes
    * [pandas](https://pandas.pydata.org/)
        * classification: `pd.plotting.scatter_matrix` in case of not so many features
* scipy random forest works column wise and therefore is more efficient on csc (not csr) sparse matrices
* probably not useful but:
    * [pybtex.org](https://pybtex.org/)
    * [pycld2 (language detection)](https://github.com/aboSamoor/pycld2)