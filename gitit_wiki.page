# orga

### actionable *now*

* create arXiv extractor (identify tex and bib file(s))
* think of how to evaluate tex2text methods
* try building tex2text method w/ python TeXSoup
* get (larger) MAG and arXiv samples

### overall

* thema verstehen → expose (mail mit details kommt noch) schreiben → thesis anemlden
* genaue richtung noch nicht so fest wie vermutet

# links / notes

* arXiv
    * [sources](https://arxiv.org/year/cs/18) (single dl)
    * [IDs over time](https://arxiv.org/help/arxiv_identifier)
    * availability of DOIs
        * ```
        >>> len(tree.xpath("//dc:identifier[starts-with(text(),'doi')]", namespaces={'dc':'http://purl.org/dc/elements/1.1/'}))
        29739
        ```
        * → 28775 of which are contained in MAG
        * ```
        >>> len(tree.xpath("//ListRecords/record"))
        155308
        ```
        * → 19%
* MAG
    * [schema](https://microsoftdocs.github.io/microsoft-academic/microsoft-academic-graph/reference/data-schema.html)
    * [blog post w/ stats](https://www.microsoft.com/en-us/research/?post_type=msr-blog-post&p=480543&preview=true)
    * [oldish examples on quora](https://www.quora.com/Where-can-I-obtain-a-schema-description-of-Microsoft-Academic-Graph-data/answer/Milind-Gokhale-1)
    * availability of DOIs (in CS only dump from Dec. 2017)
        * ```
        saiert@shetland:~$ cut -f3 -d$'\t' /vol1/mag/data/2018-07-19/dumps/Papers.txt | sed '/^\s*$/d' | wc -l
        75266339
        ```
        * ```
        saiert@shetland:~$ cat /vol1/mag/data/2018-07-19/dumps/Papers.txt | wc -l
        206252196
        ```
        * → 36.5%

# questions

* entity based citerec
    * go-to example/paper? (candidates for similar task see below)
* general (still somewhat far away)
    * not a single of the citerec approaches in CireRecSurvey uses the Microsoft Academic Graph (or arXiv for that matter) as dataset, right? (→existence of approaches to directly compare to?)
    * similar to ^ baseline to start from?
    * other approaches often are a large ensemble of techniques/features/prior work reused — should I rather build/try out "from scratch" or  recreate/assemble existing things? in both cases: is trying for improvement just enough or do choices need to be justified further?
    * ^ pick/engineer vs "brute force" (i.e. just try a lot of different stuff and see what contributes most to a good result)

# structured notes

### general

* task
    * mail
        * <small>**keinen komplett neuen Ansatz** zu citation recommendation entwerfen, sondern eher ein gutes lauffähiges System zu citation recommendation bauen. Bereits die Nachimplementierung eines "state-of-the-art"-Systems wäre eine contribution</small>
        * <small>Wahrscheinlich werden wir uns eher **semantisch-strukturierte Repräsentationsformen (entities, claims, arguments) für citation recommendation** widmen.</small>
        * <small>(Ashwath ... Indizierung des großen Microsoft Academic Graphs (MAG) und um die Verwendung eines state-of-the-art approaches für citation recommendation)</small>
        * <small>in Ihrer Arbeit eher mit **semantischen und pragmatischen Ansätzen für citation recommendation** beschäftigen. Diesbezüglich gibt es nämlich **in der Literatur noch recht wenig**</small>
        * <small>An sich ist der Microsoft Academic Graph als Datensatz für citation recommendation geeignet, allerdings enthält er nur einen Satz als citation context -- und ein größerer Kontext wäre oft angebracht, besonders, wenn wir nicht nur Entitäten zitieren, sondern ganze Argumente. Ganze Papers (fulltext) sind an sich nicht im MAG verfügbar. Die Lösung läge darin, stattdessen die Paper von arXiv.org als Subset zu verwenden. Diese sind m.E. auch im MAG. Außerdem können diese von arXiv heruntergeladen werden und liegen im Format TeX (+bibtex) vor, was den Vorteil hat, besser an den Reintext und an die Zitate zu kommen. Allerdings müsste ein TeX parser geschrieben werden, der den Rohtext usw. extrahiert. In Haskell wurde ein solch einfacher Parser bereits entwickelt [1], allerdings ohne Berücksichtigung von section headers etc.</small>
        * <small>Zitate kann man differenzieren dahingehend, ob eine Entität (z.B. ein Konzept oder ein Datensatz oder eine Methode) zitiert wird, oder eine Behauptung (claim), oder eine Behauptung/Argument im Zusammenspiel einer Argumentationskette. Demzufolge kann man für jede dieser Möglichkeiten geeignete Repräsentationsformen (aus der Computerlinguistik) für ein besseres citation recommendation verwenden. **Konkret kann man z.B. mentions im Content des Papers zu Entitäten in Wikipedia oder Wikidata verlinken (entity linking bzw. text annotation), sowie claims oder ganze Argumentationsketten extrahieren und entsprechend formal repräsentieren** (einige Beispiele diesbzgl. sind in [2-5]; die genauen Ansätze, die wir verwenden werden, können wir später festlegen). Insgesamt ginge es also hier um die **Anwendung von NLP-Komponenten auf wissenschaftliche Texte**.</small>
        * <small>Im Rahmen der Masterarbeit würde man **für jedes Unterthema (z.B. entity, claim, argumentation) einen Ansatz für citation recommendation entwickeln**. Für eine Einschätzung der Schwierigkeiten bei der Ansatzentwicklung und dann später für eine Evaluation der Ansätze müssen wohl einige Texte auch manuell annotiert werden. Außerdem würde man die Implementierungen der einzelnen Systeme zur recommendation in das Gesamtsystem von Ashwath integrieren.</small>
    * gist
        * starting point
            * MAG has much going for it, but only one sentence citation context
            * arXiv has paper sources (TeX)
        * use intersection of MAG and arXiv
        * get plain text from arXiv source
        * match MAG citation contexts to arXiv plain texts (? if so, plain text of arXiv would be sufficient (no need to retain citations))
        * → MAG with arbitrary large citation contexts
        * ———
        * in large citation contexts, find and formally represent "semantic NLP components" (using *all* the buzzwords)
        * ML
        * ???
        * profit 
* defs
    * argument = claim + one or more premises justifying the claim
        * [premises] [step(s) of deduction] [claim]
        * x (and y), therefore z

### start out w/ MAG or arXiv

* MAG
    * `+` matching MAG IDs to arXiv IDs should be quite reliable (could even use citation contexts to match to arXiv plain text)
    * `+` citation network is already given
    * `-` (blind?) reliance on MS / less transparent
    * `-` citation markers might not be *that* reliable (need to match arXiv source bibitem strings to MAG IDs)
    * `o` might enable just using latexpand+detex for TeX processing
* argument for arXiv
    * `-` matching arXiv source bibitem strings to arXiv IDs might not be *that* reliable
    * `+` more transparent
    * `+` citation markers are (albeit dependent on initial matching) certain
    * `+` (if we don't add in the MAG:) only rely on one source → cleaner approach (?)

questions to be answered:

* how hard is matching arXiv bibitem strings to arXiv IDs?
* if starting from arXiv, what is gained from adding in the MAG?
* ...

### tex parser

* grabcite[/test-tex](https://github.com/agrafix/grabcite/tree/master/test-tex) mby "edge cases"? (download source by ID, don't just use .tex file from repo)

##### existing software

* ~~[plastex](https://github.com/tiarno/plastex) — largish, last commit 1y ago, python~~
    * in addition to requirements.txt also `$ pip install Pillow Unidecode` for less noisy output
    * TEST: neat output when it works, **but** failed on all tested arXiv papers and it's own documentation
* [TexSoup](https://github.com/alvinwan/texsoup) — smallish, recent commits, python
    * doesn't handle definitions (`\dev`) [source](https://stackoverflow.com/a/50151171)
    * [stackoverflow question](https://stackoverflow.com/questions/49779853/) specifically asking about arXiv TeX (by someone using TeXSoup)
    * TEST: works on most tested; fails with e.g. `1607.00138`
    * find citations
        ```
        soup = TexSoup(tex_str)
        c = soup.find_all(name='cite')
        ```
    * output text
        ```
        soup = TexSoup(tex_str)
        for cntnt in soup.document.contents:
            if type(cntnt) == str:
                print(cntnt)
        ```
* [grabcite](https://github.com/agrafix/grabcite)
    * installation
        * required additional packages `libpcre++-dev`, `libpq-dev`, `libghc-hdbc-odbc-dev` (+500 MB of dependencies)
    * unpack arXiv dump
        * file given for param `--arxiv-meta-xml` has do be in cwd (giving a path results in `grabcite-datagen: InvalidRelFile "..."`)
        * arXiv: *"Note: Many of the formats above are served gzipped (Content-Encoding: x-gzip). Your browser may silently uncompress after downloading so the files you see saved may appear uncompressed."*
        * grabcite:
            * expects arXiv sources with `.gz` extension in input folder
            * has `gzHandler` and `tarGzHandler` (see `src/GrabCite/Arxiv.hs`)
            * → arXiv sources manually downloaded are mere `tar` archives w/o file extension → need to be gzipped and renamed to have `.gz` file extension (*not* `.tar.gz`)
    * generate data set
        * tries to connect to papergrep.com (expired namecheap.com registration that probably once hosted [this](https://github.com/agrafix/papergrep))
* [opendetex](https://github.com/pkubowicz/opendetex) / [detex](https://www.freebsd.org/cgi/man.cgi?query=detex) — ?, recent commits, compiled
    * specifically for getting plain text
    * `-c` flag is supposed to preserve `\cite`s, but breaks output completely for first tested paper `1010.2903`
    * TEST: opendetex seems to leave in more control sequences than on system detex
* [LaTeXML](https://github.com/brucemiller/LaTeXML) — *very* mature, recent commits, perl
    * LaTeX→XML
    * existing (dead?) [project on on arXiv data](https://kwarc.info/projects/arXMLiv/) ([active LaTeXML fork](https://github.com/KWARC/LaTeXML))
* [Tralics](https://www-sop.inria.fr/marelle/tralics/) — ?, 2015, C++
    * LaTeX→XML
    * [apparently](https://jblevins.org/log/xml-tools) *fast*
* [flap](https://github.com/fchauvel/flap)
    * can be run from inside python by copying `flap/ui.py` and replacing bottom (click) part with `Controller(OSFileSystem(), Display(sys.stdout, False)).run('some.tex', 'out_folder')`
    * somewhat slow?
    * → try out [alternatives](https://tex.stackexchange.com/questions/21838/replace-inputfilex-by-the-content-of-filex-automatically)
    * → latexpand looks good
        * has `--expand-usepackage` flag
        * has `--expand-bbl FILE` flag (mby useful)

##### arXiv

* IDs
    * metadata record: `oai:arXiv.org:0704.0046`　　(xPath: `/ListRecords/record/header/identifier[text()='<id>']`)
    * web: `https://arxiv.org/abs/0704.0046`
    * data dump: `0704.0046.gz`
* only *some* metadata records have DOIs
* `\cite` in text, then `\bibitem` lower down or in `.bib` file
* problems
    * sometimes a paper's [v2](https://arxiv.org/format/1306.0555v2) gives empty source while it's [v1](https://arxiv.org/format/1306.0555v1) has proper source
    * [cases](https://arxiv.org/format/1607.00145) where "source" is just a single `\includepdf`

##### arXiv source dump

* lots of tar archives like `arXiv_src_0001_001.tar` and a manifest file `arXiv_src_manifest.xml` w/ some metadata
* top level archive content examples:
    * `cs0001012.gz` → `arXiv:cs/0001012`
    * `gr-qc0001036.pdf` → `arXiv:gr-qc/0001036`
    * `1502.00318.gz` → `arXiv:1502.00318`
* 流れ
    * step 1: normalize (separate steps to allow for separate evaluation of results)
        * check for file ending (pdf/gz)
        * gz: unzip and check if tar archive or tex file (`tarfile.is_tarfile()`)
        * tar: extract contents, identify main tex file (`\begin{document}` mby?), flatten with [flap](https://github.com/fchauvel/flap), also check for `\includepdf`
        * save tex[+bib] / pdf in output folder
    * step 2: get plain text
        * pdf: pdf2text (citation markers byebye)
        * tex: TeXSoup
        * save metadata from XML (mby in JSON)
        * tex: replace local citation markers w/ globally unique one (UUID, save UUID→original bibitem text, save UUID→citing paper ID index)
    * step 3: networking
        * try to match citation UUIDs to arXiv IDs (and mby at one point other identifiers)

### citation recommendation based on entities

* mby equivalent [1](https://link.springer.com/content/pdf/10.1007%2F978-3-319-30671-1_3.pdf), [2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.8098&rep=rep1&type=pdf), [3](http://l3s.de/~fetahu/publications/cikm2016.pdf), [4](http://www.l3s.de/~fetahu/publications/fetahu_cikm2015.pdf) (recommend news articles for wikipedia articles referencing past events)

### citation recommendation based on claims

asd

### citation recommendation based on arguments

asd

# misc notes

* [Daniel Hershcovich](http://www.cs.huji.ac.il/~danielh/)
* python
    * [scikit-learn](http://scikit-learn.org/)
    * [pandas](https://pandas.pydata.org/)
* scipy random forest works column wise and therefore is more efficient on csc (not csr) sparse matrices
* probably not useful for the TeX parser but: [pybtex.org](https://pybtex.org/)