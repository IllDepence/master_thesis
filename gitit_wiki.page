# links

* arXiv
    * [sources](https://arxiv.org/year/cs/18) (single dl)
    * [IDs over time](https://arxiv.org/help/arxiv_identifier)
* MAG
    * [schema](https://microsoftdocs.github.io/microsoft-academic/microsoft-academic-graph/reference/data-schema.html)
    * [blog post w/ stats](https://www.microsoft.com/en-us/research/?post_type=msr-blog-post&p=480543&preview=true)
    * [oldish examples on quora](https://www.quora.com/Where-can-I-obtain-a-schema-description-of-Microsoft-Academic-Graph-data/answer/Milind-Gokhale-1)

# questions

* MAG + arXiv task
    * TeX parsing subtask
        * is speed or accuracy/coverage a higher priority?
        * is only plain text output needed? or properly separated sentences? or citation markings/info?
    * mapping of IDs?
    * general gist (see below)
* entity based citerec
    * go-to example/paper? (candidates for similar task see below)
* general (still somewhat far away)
    * not a single of the citerec approaches in CireRecSurvey uses the Microsoft Academic Graph (or arXiv for that matter) as dataset, right? (→existence of approaches to directly compare to?)
    * similar to ^ baseline to start from?
    * other approaches often are a large ensemble of techniques/features/prior work reused — should I rather build/try out "from scratch" or  recreate/assemble existing things? in both cases: is trying for improvement just enough or do choices need to be justified further?
    * ^ pick/engineer vs "brute force" (i.e. just try a lot of different stuff and see what contributes most to a good result)

# structured notes

### general

* task
    * mail
        * <small>An sich ist der Microsoft Academic Graph als Datensatz für citation recommendation geeignet, allerdings enthält er nur einen Satz als citation context -- und ein größerer Kontext wäre oft angebracht, besonders, wenn wir nicht nur Entitäten zitieren, sondern ganze Argumente. Ganze Papers (fulltext) sind an sich nicht im MAG verfügbar. Die Lösung läge darin, stattdessen die Paper von arXiv.org als Subset zu verwenden. Diese sind m.E. auch im MAG. Außerdem können diese von arXiv heruntergeladen werden und liegen im Format TeX (+bibtex) vor, was den Vorteil hat, besser an den Reintext und an die Zitate zu kommen. Allerdings müsste ein TeX parser geschrieben werden, der den Rohtext usw. extrahiert. In Haskell wurde ein solch einfacher Parser bereits entwickelt [1], allerdings ohne Berücksichtigung von section headers etc.</small>
        * <small>Zitate kann man differenzieren dahingehend, ob eine Entität (z.B. ein Konzept oder ein Datensatz oder eine Methode) zitiert wird, oder eine Behauptung (claim), oder eine Behauptung/Argument im Zusammenspiel einer Argumentationskette. Demzufolge kann man für jede dieser Möglichkeiten geeignete Repräsentationsformen (aus der Computerlinguistik) für ein besseres citation recommendation verwenden. Konkret kann man z.B. mentions im Content des Papers zu Entitäten in Wikipedia oder Wikidata verlinken (entity linking bzw. text annotation), sowie claims oder ganze Argumentationsketten extrahieren und entsprechend formal repräsentieren (einige Beispiele diesbzgl. sind in [2-5]; die genauen Ansätze, die wir verwenden werden, können wir später festlegen). Insgesamt ginge es also hier um die Anwendung von NLP-Komponenten auf wissenschaftliche Texte.</small>
    * gist
        * starting point
            * MAG has much going for it, but only one sentence citation context
            * arXiv has paper sources (TeX)
        * use intersection of MAG and arXiv
        * get plain text from arXiv source
        * match MAG citation contexts to arXiv plain texts (? if so, plain text of arXiv would be sufficient (no need to retain citations))
        * → MAG with arbitrary large citation contexts
* defs
    * argument = claim + one or more premises justifying the claim
        * [premises] [step(s) of deduction] [claim]
        * x (and y), therefore z

### tex parser

* grabcite[/test-tex](https://github.com/agrafix/grabcite/tree/master/test-tex) mby "edge cases"? (download source by ID, don't just use .tex file from repo)

##### existing software

* ~~[plastex](https://github.com/tiarno/plastex) — largish, last commit 1y ago, python~~
    * in addition to requirements.txt also `$ pip install Pillow Unidecode` for less noisy output
    * TEST: neat output when it works, **but** failed on all tested arXiv papers and it's own documentation
* [TexSoup](https://github.com/alvinwan/texsoup) — smallish, recent commits, python
    * doesn't handle definitions (`\dev`) [source](https://stackoverflow.com/a/50151171)
    * [stackoverflow question](https://stackoverflow.com/questions/49779853/) specifically asking about arXiv TeX (by someone using TeXSoup)
    * TEST: works on most tested; fails with e.g. `1607.00138`
    * find citations
        ```
        soup = TexSoup(tex_str)
        c = soup.find_all(name='cite')
        ```
    * output text
        ```
        soup = TexSoup(tex_str)
        for cntnt in soup.document.contents:
            if type(cntnt) == str:
                print(cntnt)
        ```
* [grabcite](https://github.com/agrafix/grabcite)
    * installation
        * required additional packages `libpcre++-dev`, `libpq-dev`, `libghc-hdbc-odbc-dev` (+500 MB of dependencies)
    * unpack arXiv dump
        * file given for param `--arxiv-meta-xml` has do be in cwd (giving a path results in `grabcite-datagen: InvalidRelFile "..."`)
        * expects arXiv srouces with `.gz` extension in input folder
        * has `gzHandler` and `tarGzHandler` (see `src/GrabCite/Arxiv.hs`)
        * → arXiv files are `tar` archives w/o file extension → need to be gzipped and then renamed to have `.gz` file extension (*not* `.tar.gz`)
    * generate data set
        * tries to connect to papergrep.com (expired namecheap.com registration that probably once hosted [this](https://github.com/agrafix/papergrep))
* [opendetex](https://github.com/pkubowicz/opendetex) — ?, recent commits, compiled
    * specifically for getting plain text
    * TEST: seems to leave in more control sequences than on system detex
* [LaTeXML](https://github.com/brucemiller/LaTeXML) — *very* mature, recent commits, perl
    * LaTeX→XML
    * existing (dead?) [project on on arXiv data](https://kwarc.info/projects/arXMLiv/) ([active LaTeXML fork](https://github.com/KWARC/LaTeXML))
* [Tralics](https://jblevins.org/log/xml-tools) — ?, 2015, C++
    * LaTeX→XML
    * apparently *fast*

##### arXiv

* IDs
    * metadata record example: `<identifier>oai:arXiv.org:0704.0046</identifier>`
    * equivalent web: `https://arxiv.org/abs/0704.0046`
* only *some* metadata records have DOIs
* `\cite` in text, then `\bibitem` lower down or in `.bib` file
* problems
    * sometimes a paper's [v2](https://arxiv.org/format/1306.0555v2) gives empty source while it's [v1](https://arxiv.org/format/1306.0555v1) has proper source
    * [cases](https://arxiv.org/format/1607.00145) where "source" is just a single `\includepdf`

### citation recommendation based on entities

* mby equivalent [1](https://link.springer.com/content/pdf/10.1007%2F978-3-319-30671-1_3.pdf), [2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.8098&rep=rep1&type=pdf), [3](http://l3s.de/~fetahu/publications/cikm2016.pdf), [4](http://www.l3s.de/~fetahu/publications/fetahu_cikm2015.pdf) (recommend news articles for wikipedia articles referencing past events)

### citation recommendation based on claims

asd

### citation recommendation based on arguments

asd

# misc notes

* [Daniel Hershcovich](http://www.cs.huji.ac.il/~danielh/)
* python
    * [scikit-learn](http://scikit-learn.org/)
    * [pandas](https://pandas.pydata.org/)
* scipy random forest works column wise and therefore is more efficient on csc (not csr) sparse matrices
* probably not useful for the TeX parser but: [pybtex.org](https://pybtex.org/)