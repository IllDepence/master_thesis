# orga

### actionable *now*

* Exposé pre feedback:
    * be more explicit in 3.2:
        * 3 approaches (entity, claim, argument), also name related works (show stuff was already read)
        * explain larger example, from beginning to end, with illustration
    * for related works
        * table: input, output, dataset, ml approach
    * arguments agains PMC (for arXiv): (mby) always new data, has CS

### overall

* thema verstehen → expose (mail mit details kommt noch) schreiben → thesis anemlden
* genaue richtung noch nicht so fest wie vermutet

# links / notes

* arXiv
    * 1.4 M documents → processing each for **0.42 seconds** takes **~1 week**
    * [sources](https://arxiv.org/year/cs/18) (single dl)
    * [IDs over time](https://arxiv.org/help/arxiv_identifier)
    * availability of DOIs (in CS only dump from Dec. 2017)
        * ```
        >>> len(tree.xpath("//dc:identifier[starts-with(text(),'doi')]", namespaces={'dc':'http://purl.org/dc/elements/1.1/'}))
        29739
        ```
        * → 28775 of which are contained in MAG
        * ```
        >>> len(tree.xpath("//ListRecords/record"))
        155308
        ```
        * → 19%
* MAG
    * [schema](https://microsoftdocs.github.io/microsoft-academic/microsoft-academic-graph/reference/data-schema.html)
    * [blog post w/ stats](https://www.microsoft.com/en-us/research/?post_type=msr-blog-post&p=480543&preview=true)
    * [oldish examples on quora](https://www.quora.com/Where-can-I-obtain-a-schema-description-of-Microsoft-Academic-Graph-data/answer/Milind-Gokhale-1)
    * availability of DOIs
        * ```
        saiert@shetland:~$ cut -f3 -d$'\t' /vol1/mag/data/2018-07-19/dumps/Papers.txt | sed '/^\s*$/d' | wc -l
        75266339
        ```
        * ```
        saiert@shetland:~$ cat /vol1/mag/data/2018-07-19/dumps/Papers.txt | wc -l
        206252196
        ```
        * → 36.5%

# questions

* entity based citerec
    * go-to example/paper? (candidates for similar task see below)
* general (still somewhat far away)
    * not a single of the citerec approaches in CireRecSurvey uses the Microsoft Academic Graph (or arXiv for that matter) as dataset, right? (→existence of approaches to directly compare to?)
    * similar to ^ baseline to start from?
    * other approaches often are a large ensemble of techniques/features/prior work reused — should I rather build/try out "from scratch" or  recreate/assemble existing things? in both cases: is trying for improvement just enough or do choices need to be justified further?
    * ^ pick/engineer vs "brute force" (i.e. just try a lot of different stuff and see what contributes most to a good result)

# structured notes

### general

* task
    * mail
        * <small>**keinen komplett neuen Ansatz** zu citation recommendation entwerfen, sondern eher ein gutes lauffähiges System zu citation recommendation bauen. Bereits die Nachimplementierung eines "state-of-the-art"-Systems wäre eine contribution</small>
        * <small>Wahrscheinlich werden wir uns eher **semantisch-strukturierte Repräsentationsformen (entities, claims, arguments) für citation recommendation** widmen.</small>
        * <small>(Ashwath ... Indizierung des großen Microsoft Academic Graphs (MAG) und um die Verwendung eines state-of-the-art approaches für citation recommendation)</small>
        * <small>in Ihrer Arbeit eher mit **semantischen und pragmatischen Ansätzen für citation recommendation** beschäftigen. Diesbezüglich gibt es nämlich **in der Literatur noch recht wenig**</small>
        * <small>An sich ist der Microsoft Academic Graph als Datensatz für citation recommendation geeignet, allerdings enthält er nur einen Satz als citation context -- und ein größerer Kontext wäre oft angebracht, besonders, wenn wir nicht nur Entitäten zitieren, sondern ganze Argumente. Ganze Papers (fulltext) sind an sich nicht im MAG verfügbar. Die Lösung läge darin, stattdessen die Paper von arXiv.org als Subset zu verwenden. Diese sind m.E. auch im MAG. Außerdem können diese von arXiv heruntergeladen werden und liegen im Format TeX (+bibtex) vor, was den Vorteil hat, besser an den Reintext und an die Zitate zu kommen. Allerdings müsste ein TeX parser geschrieben werden, der den Rohtext usw. extrahiert. In Haskell wurde ein solch einfacher Parser bereits entwickelt [1], allerdings ohne Berücksichtigung von section headers etc.</small>
        * <small>Zitate kann man differenzieren dahingehend, ob eine Entität (z.B. ein Konzept oder ein Datensatz oder eine Methode) zitiert wird, oder eine Behauptung (claim), oder eine Behauptung/Argument im Zusammenspiel einer Argumentationskette. Demzufolge kann man für jede dieser Möglichkeiten geeignete Repräsentationsformen (aus der Computerlinguistik) für ein besseres citation recommendation verwenden. **Konkret kann man z.B. mentions im Content des Papers zu Entitäten in Wikipedia oder Wikidata verlinken (entity linking bzw. text annotation), sowie claims oder ganze Argumentationsketten extrahieren und entsprechend formal repräsentieren** (einige Beispiele diesbzgl. sind in [2-5]; die genauen Ansätze, die wir verwenden werden, können wir später festlegen). Insgesamt ginge es also hier um die **Anwendung von NLP-Komponenten auf wissenschaftliche Texte**.</small>
        * <small>Im Rahmen der Masterarbeit würde man **für jedes Unterthema (z.B. entity, claim, argumentation) einen Ansatz für citation recommendation entwickeln**. Für eine Einschätzung der Schwierigkeiten bei der Ansatzentwicklung und dann später für eine Evaluation der Ansätze müssen wohl einige Texte auch manuell annotiert werden. Außerdem würde man die Implementierungen der einzelnen Systeme zur recommendation in das Gesamtsystem von Ashwath integrieren.</small>
    * gist
        * starting point
            * MAG has much going for it, but only one sentence citation context
            * arXiv has paper sources (TeX)
        * use intersection of MAG and arXiv
        * get plain text from arXiv source
        * match MAG citation contexts to arXiv plain texts (? if so, plain text of arXiv would be sufficient (no need to retain citations))
        * → MAG with arbitrary large citation contexts
        * ———
        * in large citation contexts, find and formally represent "semantic NLP components" (using *all* the buzzwords)
        * ML
        * ???
        * profit 
* defs
    * argument = claim + one or more premises justifying the claim
        * [premises] [step(s) of deduction] [claim]
        * x (and y), therefore z

### start out w/ MAG or arXiv

* MAG
    * `+` matching MAG IDs to arXiv IDs should be quite reliable (could even use citation contexts to match to arXiv plain text)
    * `+` citation network is already given
    * `-` (blind?) reliance on MS / less transparent
    * `-` citation markers might not be *that* reliable (need to match arXiv source bibitem strings to MAG IDs)
    * `o` might enable just using latexpand+detex for TeX processing
* argument for arXiv
    * `-` matching arXiv source bibitem strings to arXiv IDs might not be *that* reliable
    * `+` more transparent
    * `+` citation markers are (albeit dependent on initial matching) certain
    * `+` (if we don't add in the MAG:) only rely on one source → cleaner approach (?)

questions to be answered:

* how hard is matching arXiv bibitem strings to arXiv IDs?
* if starting from arXiv, what is gained from adding in the MAG?
* ...

### tex parser

* grabcite[/test-tex](https://github.com/agrafix/grabcite/tree/master/test-tex) mby "edge cases"? (download source by ID, don't just use .tex file from repo)

##### existing software

* ~~[plastex](https://github.com/tiarno/plastex) — largish, last commit 1y ago, python~~
    * in addition to requirements.txt also `$ pip install Pillow Unidecode` for less noisy output
    * TEST: neat output when it works, **but** failed on all tested arXiv papers and it's own documentation
* [TexSoup](https://github.com/alvinwan/texsoup) — smallish, recent commits, python
    * doesn't handle definitions (`\dev`) [source](https://stackoverflow.com/a/50151171)
    * [stackoverflow question](https://stackoverflow.com/questions/49779853/) specifically asking about arXiv TeX (by someone using TeXSoup)
    * TEST: works on most tested; fails with e.g. `1607.00138`
    * find citations
        ```
        soup = TexSoup(tex_str)
        c = soup.find_all(name='cite')
        ```
    * output text
        ```
        soup = TexSoup(tex_str)
        for cntnt in soup.document.contents:
            if type(cntnt) == str:
                print(cntnt)
        ```
    * **TESTED** on larger sample: 32% fails
        * 54 of 169 failed — can be reduced to 50 by replacing preamble w/ a clean one
        * → mby do 70% TexSoup + 30% fallback sth. (e.g. detex + python script to put cites back in?)
* [grabcite](https://github.com/agrafix/grabcite)
    * installation
        * required additional packages `libpcre++-dev`, `libpq-dev`, `libghc-hdbc-odbc-dev` (+500 MB of dependencies)
    * unpack arXiv dump
        * file given for param `--arxiv-meta-xml` has do be in cwd (giving a path results in `grabcite-datagen: InvalidRelFile "..."`)
        * arXiv: *"Note: Many of the formats above are served gzipped (Content-Encoding: x-gzip). Your browser may silently uncompress after downloading so the files you see saved may appear uncompressed."*
        * grabcite:
            * expects arXiv sources with `.gz` extension in input folder
            * has `gzHandler` and `tarGzHandler` (see `src/GrabCite/Arxiv.hs`)
            * → arXiv sources manually downloaded are mere `tar` archives w/o file extension → need to be gzipped and renamed to have `.gz` file extension (*not* `.tar.gz`)
    * generate data set
        * tries to connect to papergrep.com (expired namecheap.com registration that probably once hosted [this](https://github.com/agrafix/papergrep))
* [opendetex](https://github.com/pkubowicz/opendetex) / [detex](https://www.freebsd.org/cgi/man.cgi?query=detex) — ?, recent commits, compiled
    * specifically for getting plain text
    * `-c` flag is supposed to preserve `\cite`s, but breaks output completely for first tested paper `1010.2903`
    * TEST: opendetex seems to leave in more control sequences than on system detex
* [LaTeXML](https://github.com/brucemiller/LaTeXML) — *very* mature, recent commits, perl
    * LaTeX→XML
    * existing (dead?) [project on on arXiv data](https://kwarc.info/projects/arXMLiv/) ([active LaTeXML fork](https://github.com/KWARC/LaTeXML))
    * ~~install from source~~
        * `sudo apt install cpanminus libxml-libxml-perl libxml-libxslt-perl libxml2-dev libimage-size-perl`
        * `sudo cpanm XML::LibXML`
        * `sudo cpanm Parse::RecDescent`
        * `perl Makefile.PL`
        * `make`
        * `make test` → **fails**
        * `make install`
    * install:
        * `sudo apt install libtext-unidecode-perl`
        * `sudo apt install latexml`
    * misc notes:
        * xpath matches in output require namespace. e.g.:
            * `tree.xpath('//LaTeXML:Math', namespaces={'LaTeXML':'http://dlmf.nist.gov/LaTeXML'})`
            * `etree.strip_elements(tree, '{http://dlmf.nist.gov/LaTeXML}Math', with_tail=False)`
* [Tralics](https://www-sop.inria.fr/marelle/tralics/) — ?, 2015, C++
    * LaTeX→XML
    * [apparently](https://jblevins.org/log/xml-tools) *fast*
* [flap](https://github.com/fchauvel/flap)
    * can be run from inside python by copying `flap/ui.py` and replacing bottom (click) part with `Controller(OSFileSystem(), Display(sys.stdout, False)).run('some.tex', 'out_folder')`
    * somewhat slow?
    * → try out [alternatives](https://tex.stackexchange.com/questions/21838/replace-inputfilex-by-the-content-of-filex-automatically)
    * → latexpand looks good
        * has `--expand-usepackage` flag
        * has `--expand-bbl FILE` flag (mby useful)

##### arXiv

* IDs
    * metadata record: `oai:arXiv.org:0704.0046`　　(xPath: `/ListRecords/record/header/identifier[text()='<id>']`)
    * web: `https://arxiv.org/abs/0704.0046`
    * data dump: `0704.0046.gz`
* only *some* metadata records have DOIs
* `\cite` in text, then `\bibitem` lower down or in `.bib` file
* problems
    * sometimes a paper's [v2](https://arxiv.org/format/1306.0555v2) gives empty source while it's [v1](https://arxiv.org/format/1306.0555v1) has proper source
    * [cases](https://arxiv.org/format/1607.00145) where "source" is just a single `\includepdf`
    * `physics0001026`: html instead of tex
    * `physics0001060`: postscript instead of tex
    * `astro-ph0001005`: using file extension .ltx for latex
    * `hep-th0001079`: few sentences of plaintext
    * `hep-th0001205`, `quant-ph0001041`, ...: TeX (not LaTeX)
    * `ph0001041`: weird LaTeX?

##### arXiv source dump

* lots of tar archives like `arXiv_src_0001_001.tar` and a manifest file `arXiv_src_manifest.xml` w/ some metadata
* top level archive content examples:
    * `cs0001012.gz` → `arXiv:cs/0001012`
    * `gr-qc0001036.pdf` → `arXiv:gr-qc/0001036`
    * `1502.00318.gz` → `arXiv:1502.00318`
* 流れ
    * step 1: normalize (separate steps to allow for separate evaluation of results)
        * check for file ending (pdf/gz)
        * gz: unzip and check if tar archive or tex file (`tarfile.is_tarfile()`)
        * tar: extract contents, identify main tex file (`\begin{document}` mby?), flatten with [flap](https://github.com/fchauvel/flap), also check for `\includepdf`
        * save tex[+bib] / pdf in output folder
    * step 2: get plain text
        * pdf: pdf2text (citation markers byebye)
        * tex: TeXSoup
        * save metadata from XML (mby in JSON)
        * tex: replace local citation markers w/ globally unique one (UUID, save UUID→original bibitem text, save UUID→citing paper ID index)
    * step 3: networking
        * try to match citation UUIDs to arXiv IDs (and mby at one point other identifiers)

**test w/ 2364 input docs**  
**step 1**
```
saiert@shetland:~/arxiv_test$ time python3 normalize_arxiv_dump.py /home/saiert/arxiv_test/0001_001/0001/ /home/saiert/arxiv_test/0001_001/normalized

real    3m37.049s
user    3m16.520s
sys     0m20.866s
```
(~0.09 sec per doc)  
→ all of arXiv in 37 hours  
→ 2364m→2250 95% success

**step 2**  
on shetland, 50min for 392 docs (~7 sec per doc); 289 min for 2364 docs (~7.7 sec per doc)  
→ all of arXiv in 128 days (>4 months)  
→ 2218\*→2138 96% success (90% overall)  
\*32 PDFs, therefore not 2250

##### arXiv processing speed up

* math pre removal
    * time: 3m53s (~0.09s/doc) | 122m (3.3s/doc)
    * coverage: unchanged | 2066 **93%**

```
saiert@shetland:~/arxiv_test$ time python3 normalize_arxiv_dump.py /home/saiert/arxiv_test/0001_001/0001/ /home/saiert/arxiv_test/0001_001/normalized

real    3m53.446s
user    3m33.779s
sys     0m19.952s

output: 2218 docs

saiert@shetland:~/arxiv_test$ time python3 parse_latex_latexml.py 0001_001/normalized/ 0001_001/text

real    122m26.729s
user    113m3.402s
sys     8m43.531s

output 2066 docs
```

* ーネ申 **tralics** ネ申ー
    * on shinobu: 2218 docs in 53s → 0.024s/doc
    * on shinobu w/ SQLite DB instead of separate text files for metadata: 2m13s
    * in: 2216, out: 2199 (30 messed up) → 98% (91.8% overall) (latexml only 2138)
    * a bit more preamble noise (already in the XML in `<p>`s, so no straightforward way to deal with it)
* hep-lat0001017 is creating new commands for marking math environments, *with a newly created command for newcommand*, ***outside the preamble***

**tralics problem docs with latexml**  
(messed up output (but not skip b/c error) for 0001_001; for which latexml outputs 2138, tralics 2199)
```
tralics problem         latexml out                 latexml better
astro-ph0001027.txt     none                        (x)
astro-ph0001100.txt     none                        (x)
astro-ph0001144.txt     also messed up
astro-ph0001152.txt     also messed up
astro-ph0001153.txt     also messed up
(astro-ph0001462.txt)   fine (more content)         x
astro-ph0001540.txt     none                        (x)
astro-ph0001546.txt     none                        (x)
(cond-mat0001187.txt)   same
(cond-mat0001222.txt)   no weird "aaaaaaaaa"
cs0001010.txt           same
gr-qc0001071.txt        fine                        x
hep-lat0001018.txt      fine                        x
hep-ph0001051.txt       also messed up
hep-ph0001203.txt       fine                        x
(hep-th0001048.txt)     same
hep-th0001109.txt       fine                        x
hep-th0001139.txt       same
hep-th0001146.txt       same
hep-th0001181.txt       fine                        x
hep-th0001199.txt       fine                        x
math0001022.txt         same
math0001029.txt         same
math0001070.txt         same
math0001103.txt         same
math0001164.txt         fine                        x
math0001166.txt         same
nucl-th0001065.txt      fine                        x
```

```
python3 parse_latex_tralics.py ~/dl/foo/0001_normalized ~/dl/foo/0001_text_new 70,28s user 20,82s system 68% cpu 2:13,90 total
```

##### bibitem matching

* arxiv.org web API test (use unrealistic b/c time)
    * match for e.g. first bibitem in `1711.00002`
        * original: `L. J. Ba and R. Caruana. Do deep nets really need to be deep? In Proceedings of NIPS, 2014.`
        * query: `http://export.arxiv.org/api/query?search_query=all:Caruana%20deep%20nets%20really%20need%20deep%20Proceedings%20NIPS%202014` (words longer 2 letters)
* source given links/arXiv IDs (tested w/ 377 docs from 2017)  
```
sqlite> select count(*) from bibitem;
10270
sqlite> select count(distinct uuid) from bibitemlinkmap;
1615
sqlite> select count(distinct uuid) from bibitemarxividmap;
476
```
* Solr test

```
$ time python3 solrize.py
no metadata in 1008.3138 in arxiv-physics-oai_dc-cursor147000.xml
no metadata in 1008.3138 in arxiv-physics:quant-ph-oai_dc-cursor13000.xml
no metadata in 1005.0836 in arxiv-math-oai_dc-cursor50000.xml
python3 solrize.py  430,84s user 8,54s system 98% cpu 7:27,43 total
```

```
$ time ./bin/post -c arxiv_meta ~/dl/foo/newArxivMetaHarvesting201712/solrized
[...]
2643 files indexed.
COMMITting Solr index changes to http://localhost:8983/solr/arxiv_meta/update...
Time spent: 0:19:19.727
./bin/post -c arxiv_meta ~/dl/foo/newArxivMetaHarvesting201712/solrized  10,20s user 28,05s system 3% cpu 19:20,09 total
```
* bibitem matching w/ Solr

```
- - - - - using title_query_split_heuristic - - - - -
total: 10270
matches: 612
checked: 476
false negatives: 463
false positives: 0
python3 match_bibitems.py path ~/dl/foo/1712_text  35,91s user 1,67s system 74% cpu 50,206 total
```

```
- - - - - using title_author_query_words - - - - -
$ python3 match_bibitems.py path ~/dl/foo/1712_text
total: 10270
matches: 1703
checked: 476
false negatives: 315
false positives: 1
python3 match_bibitems.py path ~/dl/foo/1712_text  38,76s user 1,87s system 43% cpu 1:33,12 total
```

**NOTE**: bibitems that include IDs might be more likely to not include a title. In a sample of 250 of above 476 checked bibitems, only 117 included a proper title.

**optimistic extrapolation**  
1703 + 315 out of 10270 → 19.6%  
assuming 1.45 M papers each 27 citations → 7.6 M citations


### citation recommendation based on entities

* mby equivalent [1](https://link.springer.com/content/pdf/10.1007%2F978-3-319-30671-1_3.pdf), [2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.8098&rep=rep1&type=pdf), [3](http://l3s.de/~fetahu/publications/cikm2016.pdf), [4](http://www.l3s.de/~fetahu/publications/fetahu_cikm2015.pdf) (recommend news articles for wikipedia articles referencing past events)

##### experiments

1712_001.tar.gz probably way too small:  
```
sqlite> select count(distinct uuid) from bibitemarxividmap;
476
sqlite> select count(distinct arxiv_id) from bibitemarxividmap;
463
```  
→ only one citation context per cited document  
→ huge risk of overfitting

##### name entity linking

* find tool for linking stuff to wiki-/dbpedia

##### ~~named entity recognition~~ (probably not so important)

* [NLTK](https://www.nltk.org/)
    * simple example:  
        ```
        import nltk
        s = 'Some sentence with named entities like Bill Gates.'
        sent = nltk.word_tokenize(s)
        sent = nltk.pos_tag(s)
        print(nltk.ne_chunk(sent, binary=True))
        ```
* spaCy
    * apparently faster than NLTK but opinionated (offers one implementation per task, not multiple to choose from)
* gensim
    * "most commonly used for topic modeling and similarity detection"

### citation recommendation based on claims

* [ClausIE](https://gate.d5.mpi-inf.mpg.de/ClausIEGate/ClausIEGate/)

### citation recommendation based on arguments

##### michael notes

* definition in paper Argumentation Mining: The Detection, Classification and Structure of Arguments in Text
    * challenges/goals of argumentation mining:
        * extract arguments ("Component identification" sometimes)
        * extract and represent the internal structure of arguments, i.e., the propositions and their relations (an argument consists of at least two propositions, so the assumption here; "Component classification" sometimes)
        * extract the argumentation structure, i.e., the interaction between arguments ("structure identification" sometimes).
* good survey: [Parsing Argumentation Structures in Persuasive Essays](https://arxiv.org/pdf/1604.07370.pdf)
* good survey: [Argumentation Mining in User-Generated Web Discourse](https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00276)
    * dort u.a. Erwähnung/Verlinkung von confirmation bias: Link zu *Recognizing the Absence of Opposing Arguments in Persuasive Essays* (darin: "tendency to ignore opposing arguments is known as *myside bias* or *confirmation bias* (Stanovich et al., 2013). It has been shown that guiding students to include opposing arguments in their writings significantly improves the argumentation quality, the precision of claims and the elaboration of reasons (Wolfe and Britt, 2009). Therefore, it is likely that a system which automatically recognizes the absence of opposing arguments effectively guides students to improve thier argumentation".
* Weitere aktuelle Arbeiten (z.B. gute und schlechte Ansätze) im ["Argument Mining"-Workshop](http://aclweb.org/anthology/W17-51) (z.B. "What works and what does not: Classifier and feature analysis for argument mining")
* Auch noch leicht verwandt: [SemEval-2018 Task12: Argument Reasoning Comprehension](https://competitions.codalab.org/competitions/17327)
* further resources
    * Sehr nützlich: ["Transition words"](https://msu.edu/~jdowell/135/transw.html) benutzt in Recognizing the Absence of Opposing Arguments in Persuasive Essays.
* also
    * Discourse Theory, e.g. Discourse Representation Theory, bzw. Discourse Analysis:
        * "Similar  to  the  identification  of  argumentation structures,  discourse  analysis  aims  at identifying elementary discourse units and discourse relations between them. Existing approaches on discourse analysis mainly differ in the employed discourse theory." [https://arxiv.org/pdf/1604.07370.pdf](https://arxiv.org/pdf/1604.07370.pdf)
    * Argumentation theory

# misc notes

* [Daniel Hershcovich](http://www.cs.huji.ac.il/~danielh/)
* python
    * [scikit-learn](http://scikit-learn.org/)
        * `fit-predict` pattern
        * classification: `train_test_split` function has useful param `stratify` to ensure balanced distribution over classes
    * [pandas](https://pandas.pydata.org/)
        * classification: `pd.plotting.scatter_matrix` in case of not so many features
* scipy random forest works column wise and therefore is more efficient on csc (not csr) sparse matrices
* probably not useful for the TeX parser but: [pybtex.org](https://pybtex.org/)
