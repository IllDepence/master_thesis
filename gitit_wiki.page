# questions

* goto example/paper for entity based citerec? (mby. in MAG papers)
* classical ML, therefore feature engineering?
* other approaches often are a large ensemble of techniques/features/prior work reused — should I rather build from scratch or try to recreate/assemble existing things? in both cases: is trying for improvement just enough or do choices need to be justified further?
* ^ pick/engineer vs "brute force"
* not a single of the citerec approaches in CireRecSurvey use the Microsoft Academic Graph (oder arXiv) as dataset, right? (→existence of approaches to directly compare to?) (mby. in MAG papers)

# structured notes

## general

* argument = claim + one or more premises justifying the claim
    * [premises] [step(s) of deduction] [claim]
    * x (and y), therefore z

## citation recommendation based on entities

* mby equivalent [1](https://link.springer.com/content/pdf/10.1007%2F978-3-319-30671-1_3.pdf), [2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.8098&rep=rep1&type=pdf), [3](http://l3s.de/~fetahu/publications/cikm2016.pdf), [4](http://www.l3s.de/~fetahu/publications/fetahu_cikm2015.pdf) (recommend news articles for wikipedia articles referencing past events)

## citation recommendation based on claims

asd

## citation recommendation based on arguments

asd

# misc notes

* [Daniel Hershcovich](http://www.cs.huji.ac.il/~danielh/)
* python
    * [scikit-learn](http://scikit-learn.org/)
    * [pandas](https://pandas.pydata.org/)
* scipy random forest works column wise and therefore is more efficient on csc (not csr) sparse matrices