
Multiview [Deep Learning](deep learning) for Predicting Twitter Users' [Location](location)
Tien Huu Do,
Duc Minh§ Nguyen, 
Evaggelia Tsiligianni, 
Bruno Cornelis, 
and Nikos Deligiannis, Member, IEEE
T. H. Do, D. M. Nguyen, E. Tsiligianni, B. Cornelis and N. Deligiannis are with the Department of [Electronics](electronics) and [Informatics](informatics) (ETRO), Vrije Universiteit Brussel, Brussels B-1050, Belgium, and also with IMEC, Leuven B-3001, Belgium (e-mail: thdo@etrovub.be, mdnguyen@etrovub.be, etsiligi@etrovub.be, bcorneli@etrovub.be, ndeligia@etrovub.be).
The problem of predicting the [location](l[location](l[location](l[location](location) of users on large social networks like Twitter has emerged from real-life applications such as social [unrest](unrest) detection and online [marketing](marketing). Twitter user [geolocation](g[geolocation](g[geolocation](g[geolocation](geolocation) is a difficult and active research topic with a vast [literature](literature). Most of the proposed methods follow either a content-based or a network-based approach. The former exploits user-generated content while the latter utilizes the connection or [interaction](i[interaction](interaction) between Twitter users. In this paper, we introduce a novel [method](method) combining the strength of both approaches. Concretely, we propose a multi-entry neural network [architecture](architecture) named MENET leveraging the advances in deep learning and multi[view](view) learning. The generalizability of MENET enables the integration of multiple [data](d[data](d[data](data) representations. In the context of Twitter user geo[location](location), we realize MENET with textual, network, and [metadata](metadata) features. Considering the natural distribution of Twitter users across the concerned geo[graph](graph)ical area, we subdivide the surface of the earth into multi-scale [cell](cell)s and train MENET with the labels of the cells.
We show that our method outperforms the state of the art by a large margin on three benchmark datasets.
Twitter user [geolocation](geolocation), deep learning, [feature learning](feature learning), multiview learning, big [data](data).
Introduction
Over the last ten years, social networks have grown and engaged a massive amount of users. Among them, Twitter is one of the most popular, reaching over 300 million users by the 4th quarter of 2017 {{cite:5e3b477c-9502-4291-bb91-52552ddbe67c}}. On Twitter, users publish short messages of 140 characters or less called tweets, which can be seen by followers or by the public.
Tweets can also be re-published by users who have seen the tweets, a process known as retweeting.
This way, information can be spread quickly and widely throughout the whole Twitter network. Twitter can even be considered as a human-powered sensing network, with a lot of useful information, yet, in an unstructured form. For this reason, automatic mining and extracting meaningful information from the massive amount of Twitter data is of great significance {{cite:3a607474-7342-4067-b79f-0b06ab8888f4}}, {{cite:2fe8a1e4-ef27-4456-96ce-67fbeaa6a948}}.
A very useful piece of information on Twitter is user [location](location), which enables several applications including event detection {{cite:3493223c-6661-4c31-bb8e-33bc244e49f8}}, [online community](online community) analysis {{cite:4cae300e-9f66-47f3-9fcf-1608a61f040c}}, social [unrest](unrest) forecasting {{cite:36afecd6-ccc2-4397-a3b2-9c1ea2bedb4c}} and [location](location)-based recommendation {{cite:ea23a5b8-fac1-47d0-b526-175564906815}}, {{cite:6f4db547-f1f0-483c-a574-abb653048f05}}. As another example, user location information can be useful for online marketers and governments to understand trends and patterns [ranging](ranging) from customer and citizen feedback {{cite:9053b015-8d3a-4e12-be01-2da7af46e16c}} to the mapping of epidemics in concerned geo[graph](graph)ical areas {{cite:61729f37-eebf-46f9-b0a1-2e3cbf3425c6}}. In 2009, Twitter enabled a geo-tagging feature, with which users can choose to geo-tag their tweets while posting. However, the majority of tweets are not geo-tagged by the users {{cite:a4b14ca2-2451-461b-853e-90f9678d245e}}. Alternatively, users' location might be available via their profile data. Nonetheless, not many users disclose their location via their Twitter profile, or the provided information is often unreliable. For example, a user might share vague or non-existent places such as "Everywhere" and "Small town, RW Texas". This results in a quest for [geolocation](geolocation) [algorithm](a[algorithm](algorithm)s that can automatically analyze and infer the [location](l[location](location) of Twitter users.
The Twitter geo[location](location) problem can be addressed at two different levels, namely, the tweet level and the user level. The former aims at predicting the location of single tweets, while the latter aims at inferring the location of a user from the [data](data) generated by that user.
The [geolocation](geolocation) of single tweets is extremely difficult due to the [limited availability](limited availability) of information. Research on single tweet [geolocation](geolocation) has been conducted {{cite:b2763692-d8ad-40b9-a175-4faaa039efe7}}, {{cite:d7fba818-cf0f-49f6-a90e-9c76734128a3}}, but a good accuracy can be achieved only under specific constraints, which are normally not applicable in real-life situations. On the other hand, the Twitter [geolocation](geolocation) at user level, also refered to as Twitter user [geolocation](geolocation), is more common, with plenty of methods described in the literature {{cite:3fd1b51d-971d-4e13-a1f2-4905261fb57e}}. In this paper, we focus on the [geolocation](geolocation) problem at user level instead of tweet level.
The Twitter user [geolocation](geolocation) problem can be formulated under a classification or a [regression](regression) setting. Under the classification setting, one can predict the [location](location) of users in terms of geographical regions, such as countries, states and cities. Under the [regression](regression) setting, the task is to estimate the exact geocoordinates of the users. Both prediction settings are considered in this paper. It is worth mentioning that we address the [regression](regression) problem from a classification point of [view](view). Towards this [end](end), we employ a [map](map) partitioning technique to divide the concerned geographical area into small regions corresponding to classes. The exact geocoordinates of Twitter users can be estimated using the classes' centroids.
In the Twitter user [geolocation](geolocation) [literature](literature), most of the existing algorithms follow either a content-based approach or a network-based approach. Content-based methods extract information from the textual contents of tweets to predict user locations {{cite:a4b14ca2-2451-461b-853e-90f9678d245e}}, {{cite:afb09f17-553c-44f5-bad3-8805564aa848}}, {{cite:7634b1f9-eb34-461c-b165-0a8301ab6cac}}. Network-based methods, on the other hand, employ connections between users for [geolocation](geolocation) {{cite:6fe6376d-66b1-4637-a476-540c8b4ea617}}, {{cite:a9814f63-a7f3-443f-8520-dd63a68cb471}}, {{cite:5631bbe7-4c93-4bfc-85ab-35cad0b5a502}}. Both approaches have achieved good geolocation accuracy {{cite:a4b14ca2-2451-461b-853e-90f9678d245e}}, {{cite:c5aaa5fa-acdc-4eb9-82b1-5cd63ab57a64}}.
This paper explores a more generic approach, which inherits the advantages of both content-based and network-based strategies.
Our approach leverages recent advances in deep neural networks (i.e., [deep learning](deep learning)) and multiview learning.
Deep neural networks {{cite:321a9557-472b-4d92-94cc-115fc4f4d800}}, have been proven to be very effective in many domains including image classification {{cite:7e8e301c-a823-4b51-a5c7-1fe08f9d5a37}}, [machine](machine) translation {{cite:cc339504-a636-486d-a206-3c86888e721e}}, and compressive sensing {{cite:6bc2de6a-2d90-4797-8903-e3cf977d59e4}}.
On the other hand, multiview learning is an emerging paradigm encompassing methods that learn from examples with [multiple](multiple) representations {{cite:4e11a20e-b371-442c-bb81-026db2d6e4a6}} showing a great progress recently {{cite:229d5ace-9186-4076-a6c3-41105d409e1d}}, {{cite:784c7593-5d53-4720-b06a-4f94b3a56b17}}. In Twitter user [geolocation](geolocation), the views can be different types of information available on Twitter such as text and [metadata](metadata), or even features extracted from the tweets themselves.
Our contributions in this work are as follows:

We propose a generic multiview neural [network architecture](network a[architecture](architecture), named multi-entry neural network (MENET), for Twitter user geolocation.
MENET is capable of combining multiview features into a unified model to infer users' location.

We propose to incorporate four specific types of features to realize MENET for Twitter user geolocation. These features capture the textual information (TF-IDF, doc2vec {{cite:96558a04-9599-45b4-ab46-fc21cc965bfa}}), the user [interaction network](interaction network) structure (node2vec {{cite:1798db00-1f71-43de-8458-7a5635c703e6}}), and the time-related user behavior.

We show the effectiveness of using [map](map) partitioning techniques in Twitter user [geolocation](geolocation), especially with Google's S2 partitioning libraryhttps://code.google.com/archive/p/s2-[geometry](geometry)-[library](library)/. We have achieved state-of-the-art results on several popular datasets with these partitioning techniques.

We show a thorough analysis on the importance of input features and the [impact](impact) of partitioning strategies on the performance of MENET.

The [remainder](remainder) of this paper is organized as follows. In Section , we review related works. Section  describes our [method](method) in details, including the model [architecture](architecture), [feature learning](feature learning), [feature extraction](feature extraction) and how we improve our model with the density-driven [map](map) partitioning technique. Section  describes the performance criteria, the pre-processing procedures and details the [parameter](parameter) setting of our [method](method). The results of our experiments are also presented in this section. Finally, we draw the conclusion and discuss future work in Section .

Related Work
Most current approaches for predicting the [location](location) of Twitter users are based either on user-generated content or on the social ties. The first approach, which has been investigated thoroughly, uses textual features from tweets to build [location](location) predictive models. The latter arises from an observation that a user often interacts with people in nearby areas {{cite:6fe6376d-66b1-4637-a476-540c8b4ea617}}, and exploits the network connections of users. This section will bring a closer look on recently published works for both approaches.
Plenty of content-based methods have been proposed for Twitter user [geolocation](geolocation). Geographical topic models {{cite:f7a1e2c5-24eb-45db-b391-02e90a33b625}}, {{cite:47121970-999b-43f6-8ebc-22930037554a}} consider tweets and locations as the outputs of a generative process incorporating topics and regions as latent [variables](variables), thus geo-locating users by seeking to recover these [variables](variables). An alternative approach is using geographical [Gaussian](gaussian) Mixture Models (GMMs) {{cite:7634b1f9-eb34-461c-b165-0a8301ab6cac}} to model the distribution of terms of tweets across geographical areas. By calculating a weighted sum of corresponding GMMs on terms of tweets, a geographical density function can be found, revealing the [location](location) at the single tweet level. A smilar approach, making use of GMMs, is introduced by Chang et al. {{cite:00b6ffd4-f607-4350-aabf-ad9acaeb42ef}}, where a GMM model is fit to the [conditional probability](conditional probability) of a certain city, given a term. [Char](char) et al. {{cite:a4b14ca2-2451-461b-853e-90f9678d245e}} estimate [location](location) by exploiting the expressiveness of sparse coding and the advances in dictionary learning to obtain the state of the [art](art) on a benchmark dataset named GeoText {{cite:47121970-999b-43f6-8ebc-22930037554a}}. Recently, several methods have addressed the Twitter user [geolocation](geolocation) problem using [deep learning](d[deep learning](deep learning). For example, Liu and Inkpen [train](train) stacked denoising autoencoders for predicting regions, states, and geographical coordinates {{cite:afb09f17-553c-44f5-bad3-8805564aa848}}. These vanilla models obtain quite good results with a pre-training [procedure](procedure).
These methods, however, do not take into account the natural distribution of Twitter users in the considered datasets over the different regions of [interest](i[interest](interest). Concretely, the density of Twitter users is much higher in inner-city areas than countrysides. To [exploit](exploit) this attribute, grid-based [geolocation](geolocation) methods are introduced in {{cite:8f65203f-6702-4b68-857b-108b6e996a7c}}, {{cite:711e514a-fe0d-4e56-a234-323ba1933cfc}}, {{cite:b8f31317-067b-4a88-b4db-c5596d97b7c8}}, {{cite:335aa4cf-8b51-4a56-8852-898a75f47fec}}, where adaptive or uniform grids are created to partition the datasets into geographical [cell](c[cell](cell)s at different levels. The prediction of geographical coordinates is then converted to a classification problem using the [cell](cell)s as classes, and off-the-shelf classifiers can be applied directly. This [strategy](strategy) is also used in our [method](method) but with a different spliting scheme and with a novel model [architecture](architecture).
Recent works have shown a [correlation](c[correlation](c[correlation](correlation) between the likelihood of [friendship](friendship) of two [social network](social network) users and the geographical [distance](d[distance](distance) between them {{cite:6fe6376d-66b1-4637-a476-540c8b4ea617}}. Using this correlation, the [location](location) of users can be estimated using their friends' [location](location). This is the key idea behind the network-based approach. By leveraging the social [interaction](interaction)s like bi-directional followingTwitter users follow other people to see their latest updates. Bi-directional following means two users follow each other. and bi-directional mentioningTwitter users can mention other people in their tweets by [typing](typing) @username. Bi-directional mentioning is the two-way [interaction](interaction) which happens when two users have mentioned each other., one can establish graphs of Twitter users where a [label](label) propagation algorithm {{cite:325f957a-12b0-453a-80c0-8ef14d84762c}} or its variants {{cite:e61247da-4302-46f5-8872-9f35cc1e43d6}}, {{cite:20de8a75-55a2-4ce4-b698-7dcb271ce525}} are used to identify locations of unlabeled users {{cite:22296e6d-2371-452c-aaf9-29906460f643}}, {{cite:a9814f63-a7f3-443f-8520-dd63a68cb471}}, {{cite:5631bbe7-4c93-4bfc-85ab-35cad0b5a502}}, {{cite:42b4a216-f3bb-4b92-8700-4449c5bdab6d}}. The network-based approach has several advantages over the content-based counterpart, including language independence. Also, it does not require training, which is a very resource intensive and time-consuming process on [big data](big data)sets.
However, the inherent [weakness](weakness) of this approach is that it cannot propagate labels (locations) to users that are not connected to the [graph](graph). As a result, isolated users remain unlabeled.
To address the problem of isolated users in the network-based approach, unified text and network methods are proposed in {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}, {{cite:c5aaa5fa-acdc-4eb9-82b1-5cd63ab57a64}}, which leverage both the discriminative power of textual information and the representativeness of the users' [graph](graph). In particular, the textual information is used to predict [label](label)s for disconnected users before running label propagation algorithms. Additionally, the [novelty](novelty) of the works {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}, {{cite:c5aaa5fa-acdc-4eb9-82b1-5cd63ab57a64}} lies in [building](building) a densely undirected [graph](graph) based on the mentioning of users. This makes a significant improvement in the [location](location) prediction.
Following {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}, {{cite:c5aaa5fa-acdc-4eb9-82b1-5cd63ab57a64}}, models combining text, [metadata](metadata) and user network features have been introduced {{cite:a34e2638-152d-4fdd-ade1-40602d367026}}, {{cite:d7fba818-cf0f-49f6-a90e-9c76734128a3}}. These models have to rely on [user profile](user profile) information including user [location](location), user timezone and user [UTC offset](utc offset). These types of information should be considered unvailable in the Twitter user [geolocation](g[geolocation](geolocation) context.
That is the [reason](reason) why the three benchmark datasets considered in this paper do not provide the Twitter profile information.
Our [method](method) does not rely on the Twitter [user profile](user profile) information.
It employs a similar [graph](graph) of Twitter users derived from tweets as in {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}; however, instead of propagating labels through the [graph](graph), our [method](method) trains an [embedding](embedding) mapping function to capture the [graph](graph)'s structure. The graph feature is then integrated with all other features in a neural [network architecture](network a[architecture](architecture). Our architecture is simpler as it does not require designing a specific architecture for each type of feature like in {{cite:a34e2638-152d-4fdd-ade1-40602d367026}}, {{cite:d7fba818-cf0f-49f6-a90e-9c76734128a3}}, thus easier and less resource intensive to train.

Multi-entry Neural Network for Twitter User Geolocation
FIGURE 
In Twitter user [geolocation](geolocation), we [wish](wish) to predict the [location](location) of a user using textual information and [metadata](metadata), obtained from a corpus of tweets sent by the user, as well as information extracted from the user's network. Using this information, we predict either the area ([alias](alias), region), where the user most probably resides, or even the [location](location) of the user by means of geocoordinates.
Our [method](method) addresses this problem as a classification problem. Concretely, for each considered dataset, we subdivide Twitter users into discrete geographical regions, which correspond to classes. We define the [centroid](centroid) of a region by the [median](median) value of the geocoordinates of all training users in that region. Once a test user is classified to a certain region, we consider the [centroid](centroid) of that region as the predicted geocoordinates.
We propose a generic neural [network model](network model) to learn from [multiple](m[multiple](m[multiple](multiple) views of [data](d[data](data) for Twitter user [geolocation](g[geolocation](geolocation).
We [coin](coin) the proposed model MENET.
The [advantage](advantage) of this model is the [capability](capability) of exploiting both content-based and network-based features, as well as other available features concurrently.
In this work, we realize MENET with different types of features. These features capture not only the tweets' content, but also the user network structure and time information.
It is worth mentioning that except the time information, all other features are extracted from the tweets' content. Hence, MENET works even in case tweets' [metadata](m[metadata](metadata) is not available.
Integrating all features into MENET results in a powerful [method](method) for [geolocation](geolocation).
Combining this [method](method) with the Google S2 [map](map) p[art](art)itioning technique, we achieve state-of-the-art results in several Twitter user [geolocation](geolocation) benchmarks.
This section presents our MENET model and the different types of employed features in detail.
Model [Architecture](architecture)
[Architecture](architecture)
Our MENET [architecture](architecture) is illustrated in Fig. REF .
The model leverages different features extracted from the tweets' content and [metadata](metadata).
Each corresponds to one [view](view) of the network.
In Fig. REF , FORMULA  features are put into FORMULA  individual [branch](branch)es. Each branch can contain [multiple](multiple) hidden [layers](layers) allowing to learn higher order features.
Given [multiple](multiple) views of the input [data](data), a straightforward approach to combine them is to use vector [concatenation](concatenation).
Nevertheless, we argue that our [architecture](architecture) is more effective. Simple vector [concatenation](concatenation) often does not fully utilize the power of [multiple](multiple) features. In MENET,
each [view](view) is the input to one network [branch](branch), which comprises of a number of fully connected hidden [layers](layers).
In order to learn a non-linear transformation function for each [branch](branch), we employ the ReLU {{cite:bb4d75a6-cdad-43df-9e70-8e05e2e674af}} [activation function](a[activation function](activation function) after each hidden layer.
The ReLU function is efficient for [backpropagation](backpropagation) and less prone to the vanishing gradient problem {{cite:dd655b83-9449-4b82-9814-fd9c054aa20b}} than the tanh and sigmoid activation functions, hence,
has been used widely in [deep learning](deep learning) literature {{cite:19312136-ddf8-4a8a-840e-9a00e78ca7db}}, {{cite:149b5268-e40a-4365-8695-dd3fe7ef7160}}. The outputs of these branches are concatenated making a combined hidden layer. More fully connected [layers](layers) can be added after this [concatenation](concatenation) layer to [gain](gain) more nonlinearity (see component Post-combined Hidden [Layers](layers) in Fig. REF ). Again, ReLU is used to activate these [layers](layers). At the [end](end), we employ a softmax layer {{cite:1448fe88-b32c-48fb-bbfc-057fa6326a27}} to obtain the output probabilities.
We employ the cross-entropy loss as the objective function. Let FORMULA  be the number of examples and FORMULA  be the number of classes, then the cross-entropy loss is defined by:
FORMULA 
where FORMULA , FORMULA  is the ground-truth vector, FORMULA  is the predicted [probability vector](probability vector), namely, FORMULA  is the probability that user FORMULA  resides in region FORMULA .

Training MENET
We [train](t[train](train) MENET using the stochastic [gradient descent](gradient descent) (SGD) algorithm {{cite:3bc3ac09-f9ad-4471-b3ae-00f525386b18}}, which optimizes the objective function in (REF ).
In order to avoid overfitting, we use FORMULA  regularization and [early stopping](early stopping) techniques.
The FORMULA  regularization adds an additional term to the objective function, penalizing [weight](weight)s with big absolute [values](values).
Even though it is common practice to regularize weights in all [layers](layers), we empirically found that regularizing only
the final output layer still effectively avoids [overfitting](overfitting), and does not affect the model's [capability](capability). This, eventually, results in better classification results.
The parameters of MENET are fine-tuned using a sepa[rate](rate)d set of examples, namely the development set. During training, the classification accuracy of the model on the development set is continuously monitored. If this metric does not improve for a pre-defined amount of consecutive steps FORMULA , the training process is stopped. By using the same mechanism, the learning rate is also annealed when the training proceeds.

Testing MENET
To predict the [location](location) of users from the [test set](test set), we use the trained MENET model to classify these users into pre-defined classes (regions).
The exact geocoordinates of a user is given by the [centroid](centroid) of the respective region.
The performance of the MENET model is measured by either the accuracy in case of regional classification or [distance](d[distance](distance) error [metrics](metrics) (see Section REF ) in case of geographical coordinates prediction.

Multiview Features
Figure REF  shows the [capability](capability) of MENET in exploiting [data](data) from [multiple](multiple) sources.
In the context of Twitter user [geolocation](geolocation), we realize MENET by leveraging features from textual information (Term [Frequency](frequency) - [Inverse](inverse) Document Frequency {{cite:26ec4e49-111a-4e4f-bbc0-8f69592ac3c8}}, doc2vec {{cite:fc8b565d-9c86-47cf-9e5c-285a72e4af4c}}), user [interaction](interaction) network (node2vec {{cite:1798db00-1f71-43de-8458-7a5635c703e6}}) and metadata ([timestamp](t[timestamp](timestamp)). These features are all extracted from tweets provided they are available. The rest of this section will describe these features and how they are computed.
The Term [Frequency](frequency) - [Inverse](inverse) Document [Frequency](frequency) Feature
The Term [Frequency](frequency) - [Inverse](inverse) Document [Frequency](frequency) (TF-IDF) is a statistical measure used to evaluate how important a term is to a document in a [collection](collection) or corpus.
The importance increases proportionally to the number of times the term appears in the document but is offset by the [frequency](frequency) of the term in the corpus.
TF-IDF is composed of two components, presented next.
Term [Frequency](frequency) (TF): It measures how often a term occurs in a document.
The simplest choice is the raw [frequency](frequency) of the term in a document
FORMULA 
where FORMULA  is the [frequency](frequency) of term FORMULA  in the document FORMULA .
[Inverse](inverse) Document [Frequency](frequency) (IDF): It measures the informative quantity a term brings across documents.
Concretely, a common term across [multiple](multiple) documents will be given a [low](low) [weight](weight) while a rare term will have a higher [weight](weight).
The IDF is defined as
FORMULA 
with FORMULA  denoting the whole set of documents.
Then, the TF-IDF is defined by:
FORMULA 
The output from (REF ) is normalized with the FORMULA  norm to have unit length.
In fact, there are many variants for the definition of TF-IDF, and selecting one form depends on the specific situation.
We use the formulations (REF ) and (REF ) following the existing [implementation](implementation) in the well-established [library](l[library](library) scikit-learnhttp://scikit-learn.org/stable/ {{cite:31b16714-76c1-47f8-809d-cdaee973462a}}.

The Context Feature
The context feature is a mapping from a variable length block of text (e.g. [sentence](sentence), [paragraph](paragraph), or entire document)
to a fixed-length continuous valued vector.
It provides a numerical representation capturing the context of the document.
Originally proposed in {{cite:96558a04-9599-45b4-ab46-fc21cc965bfa}},
the context feature is also referred to as doc2vec or Distributed Representation of Sentences,
and it is an extension of the broadly used [word2vec](word2vec) model {{cite:fc8b565d-9c86-47cf-9e5c-285a72e4af4c}}.
The [intuition](intuition) of doc2vec is that a certain context is more likely to produce some sets of words than other contexts. Doc2vec trains an [embedding](embedding) capable of expressing the relation between the context and the corresponding words. To achieve this goal, it employs a simple neural [network architecture](network a[architecture](architecture) consisting of one hidden layer without an activation function.
A text window samples some nearby words in a document;
some of these words are used as inputs to the network and some as outputs.
Moreover, an additional input for the document is added to the network bringing the document's context.
The training process is totally unsupervised. After training, the fixed representaion of the document input will capture the context of the whole document.
Two [architecture](architecture)s were proposed in {{cite:96558a04-9599-45b4-ab46-fc21cc965bfa}} to learn a document's representation, namely, Distributed Bag of Words (PV-DBOW) and [Distributed Memory](distributed memory) (PV-DM) versions of [Paragraph](paragraph) Vector.
Athough PV-DBOW is a simpler [architecture](a[architecture](architecture), it has been claimed that PV-DBOW performs robustly if [train](train)ed on large datasets {{cite:13ec3d0b-ba92-4997-a26b-3b16da0d46df}}.
Therefore, we [select](select) PV-DBOW model to extract the context feature.
In this paper, we [train](train) PV-DBOW models using the tweets from the training sets.
Later, we extract the context feature vectors for the training,
the development and the test sets.
Our [implementation](implementation) is based on gensimhttps://radimrehurek.com/gensim/ {{cite:75e87f3b-0e63-437a-8e53-fd01b1c8dbdc}}.

The Node2vec Feature
Node2vec is a [method](m[method](method) proposed in {{cite:1798db00-1f71-43de-8458-7a5635c703e6}}
to learn continuous feature representations (embeddings) for nodes in graphs.
The low-dimensional [feature vector](feature vector) represents the network neighborhoods of a node.
Let FORMULA  be the set of nodes of a [graph](graph).
Node2vec learns a mapping function FORMULA 
that captures the [connectivity](connectivity) patterns observed in the [graph](graph).
Here, FORMULA  is a [parameter](parameter) specifying the dimensionality of the feature representation,
and FORMULA  is a matrix of [size](size) FORMULA .
For every source node FORMULA , a set of neighborhood nodes FORMULA  is generated through a neighborhood sampling [strategy](strategy) FORMULA .
Then, FORMULA  is obtained by maximizing the log-probability of observing the neighborhood FORMULA , that is,
FORMULA 
Node2vec employs a sampling [method](method) referred to as biased Random Walk {{cite:1798db00-1f71-43de-8458-7a5635c703e6}}, which samples nodes belonging to the neighborhood of node FORMULA ,
according to discrete transition probabilities between the current node FORMULA  and the next node FORMULA .
These probabilities depend on the [distance](distance) between the previous node FORMULA  and the next node FORMULA .
Denote by FORMULA  the [distance](distance) in terms of number of edges from node FORMULA  to node FORMULA ,
if the next node coincides with the previous node, then FORMULA .
If the next node has a direct connection to the previous node, then FORMULA ,
and if the next node is not connected to the previous node, then FORMULA .
The transition probabilities are defined as follows {{cite:1798db00-1f71-43de-8458-7a5635c703e6}}:
FORMULA 
where the parameters FORMULA  and FORMULA  are small positive numbers.
The [random walk](random walk) sampling runs on nodes to obtain a [list](list) of walks.
Later, the node's embeddings are found from the set of walks using the [stochastic gradient descent](stochastic g[gradient descent](gradient descent) procedure.
In the context of Twitter user geo[location](location), each node corresponds to a user, while an edge is the connection between two users.
We can define these connections by several criteria depending on the availability of data.
For example, we may consider that two users are connected when actions such as following, mentioning or retweeting are detected.
In this paper, the content of tweet messages is used to build graph connections.
Similar to {{cite:c5aaa5fa-acdc-4eb9-82b1-5cd63ab57a64}}, {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}, we
construct an undirected user graph by employing mention connections.
First, we create a unique set FORMULA  with all the users of [interest](interest).
If a user mentions directly another user and both of them belong to FORMULA , we create an edge reflecting this interaction.
The edge is assigned a [weight](weight) equal to the number of mentions.
To avoid sparsity of the connections, if two users of interest mention a third user, who does not belong to FORMULA ,
we create an edge between these two users.
Again, the weight of this edge is the sum of mentions between the third user and the two others.
Furthermore, we define a [list](list) of so-called celebrities consisting of users that have a number of unique connections exceeding a threshold FORMULA .
We remove all connections to these celebrities since the celebrities are often mentioned by plenty of people all over the world. Mentioning a celebrity, therefore, might not be a good indication of geo[graph](graph)ical relation.
The graph [building](building) [procedure](procedure) is depicted in Fig. REF .
FIGURE 
A shortcoming of this [method](method) is that it can only produce an [embedding](embedding) for a node
if that node has at least one connection to another node.
Nodes without an edge can not be represented.
Therefore, for an isolated node, we consider an all-zero vector as its [embedding](embedding).
Moreover, whenever a new node [joins](joins) the [graph](graph),
the [algorithm](algorithm) needs to run again to learn feature vectors for all the nodes of the [graph](graph),
making our [method](method) inherently transductive.
There are some existing efforts addressing this problem.
In {{cite:4b47edfa-c88e-422c-996c-5f20aabac1f0}}, the authors consider a node's [embedding](embedding) as a function of its natural feature;
in this case the [embedding](embedding) could be a function of either the TF-IDF or doc2vec feature.
A similar approach presented in {{cite:42bb6e28-9afe-4591-88fc-40562106ea68}}
generates a node's [embedding](embedding) by sampling and aggregating features from the node's local neighborhood.
These inductive approaches will be considered in our future work.

The [Timestamp](timestamp) Feature
In many commonly used Twitter databases like GeoText {{cite:47121970-999b-43f6-8ebc-22930037554a}} and UTGeo2011 {{cite:8f65203f-6702-4b68-857b-108b6e996a7c}},
the posting time of all tweets is available in UTC value (Coordinated [Universal Time](universal time)).
This allows us to leverage another view of the data.
In {{cite:57efba00-c6c6-4ea6-879f-df106164163b}}, it was shown that there exists a correlation between time and place in a Twitter [stream](stream) of [data](data).
In fact, it is less likely that people tweet late at night than at any other time, which implies a drift in [longitude](longitude).
Therefore, the [timestamp](timestamp) could be an indication for a [time zone](time zone).
We obtain the [timestamp](timestamp) feature for a given user as follows.
First, we extract the timestamps from all the tweets of that user
and convert them to the standard format to extract the hour value.
Then, a 24-dimensional vector is created corresponding to 24 hours in a day;
the FORMULA -th element of this vector equals the number of messages posted by the user at the FORMULA -th hour.
This feature is FORMULA  normalized to a [unit vector](unit vector) before feeding it to our neural [network model](network model).

Improvements with S2 adaptive [grid](g[grid](grid)
When addressing the prediction of users' [location](location) as a classification problem,
the geographical coordinate assigned to a user with unknown [location](location)
equals the [centroid](centroid) of the class, which has been predicted for the user. A straightforward way to form the classes is taking administrative boundaries such as states, regions or countries.
Such an approach brings large [distance](distance) errors if the respective areas are large.
Intuitively, the prediction accuracy could be improved
if we increase the [granularity](granularity) level by defining classes that correspond to smaller areas.
The tiling should also consider the distribution of users;
very imbalanced [custom](custom) classes should be avoided,
otherwise, the training process will not be efficient.
Therefore, finding an appropriate way to subdivide users
into [custom](custom) small geographical areas is critical.
An early work of Roller et al. {{cite:8f65203f-6702-4b68-857b-108b6e996a7c}}
has built an adaptive [grid](grid) using a k-d tree to p[art](a[art](art)ition [data](d[data](data) into [custom](custom) classes.
This partitioning, though considers the distribution of users, does not necessarily produce uniform cells at the same level.
Here, we [split](split) the Twitter users in the [training set](training set) into small areas called S2 cells,
using Google's S2 [geometry](geometry) [library](library).
This [library](library) is a powerful tool for partitioning the earth's surface.
Considering the earth as a sphere, the [library](library) hierarchically subdivides the sphere's surface by projecting it on an enclosing [cube](cube).
On each surface of the [cube](cube), a hierarchical partition is made using a spatial [data](data) structure named quad-tree.
Each node on the tree represents an S2 cell, which corresponds to an area on the earth's surface.
The quad-tree used in the Google S2 geometry library
has a depth of 30; the root cell is assigned the lowest level of zero and the leaf cells are assigned the highest level of 30.
The library outputs mostly uniform cells at the same level.
For instance, the minimum area of level-12 cells is FORMULA  km2
and the maximum area of these cells is FORMULA  km2.
In this work, we build an S2 adaptive grid, aiming at a balanced tiling,
meaning that the defined cells (geo[graph](graph)ical areas) contain a similar number of users.
For this [reason](reason), we specify a threshold FORMULA , as the maximum allowed number of users per [cell](cell).
We build the adaptive [grid](grid) from bottom to top. First, we identify the leaves corresponding to given geocoordinates.
As long as the total number of users in children nodes ([cell](cell)s) is smaller than FORMULA ,
we merge these nodes together; the children nodes' users are assigned to the parent cell, i.e.,
a larger geographical area.
We [climb](climb) the tree gradually repeating this process.
If we reach a specific level, FORMULA , we stop the [climb](climb)
in order to avoid defining cells that correspond to large geographical areas;
otherwise, the prediction error would increase.
Figures REF  and REF  show the [subdivision](subdivision) of users in S2 cells for the considered datasets.
FIGURE 
FIGURE 

Experiments
[Data](data)sets
In our experiments, we employ the following three datasets, which contain tweets coming from the United States (GeoText {{cite:47121970-999b-43f6-8ebc-22930037554a}}, UTGeo2011 {{cite:8f65203f-6702-4b68-857b-108b6e996a7c}}) and all over the world (TwitterWorld {{cite:d17d7a52-20c6-4249-9e52-3996cc835af1}}).
GeoText: This is a small dataset containing more than FORMULA  tweets posted by 9475 unique users from 48 contiguous states and Washington D.C. during the first week of March, 2010.
Tweets were filtered carefully before being put into the dataset to make sure that only relevant tweets are kept.
In this dataset, the geospatial coordinates of the first message of users were used as their primary [location](location).
This was done originally by the author in {{cite:47121970-999b-43f6-8ebc-22930037554a}} and followed by other authors {{cite:8f65203f-6702-4b68-857b-108b6e996a7c}}, {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}.
The dataset was already [split](split) into the training, development and testing sets with 7580, 1895 and 1895 users, respectively.
For the downstream tasks, tweets from a user are concatenated making a tweet document.
UTGeo2011: This is a larger dataset which was created by the authors of {{cite:8f65203f-6702-4b68-857b-108b6e996a7c}}. The dataset is also referred to as TwitterUS in many Twitter user [geolocation](g[geolocation](geolocation) publications {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}, {{cite:c5aaa5fa-acdc-4eb9-82b1-5cd63ab57a64}}, {{cite:335aa4cf-8b51-4a56-8852-898a75f47fec}}.
The dataset contains approximately 38 [million](m[million](million) tweets sent by FORMULA  users from the US.
In contrast to GeoText, this dataset is noisier, namely
many tweets have no [location](location) information.
To treat it similarly to GeoText, all the tweets from a specific user are concatenated into a single document;
a primary [location](location) is defined as the earliest valid coordinate of the tweets.
Ten thousand users are selected randomly to make the development set,
and the same amount is reserved for the evaluation set.
The remaining users form the [training set](training set).
TwitterWorld: This is the dataset created by the authors of {{cite:d17d7a52-20c6-4249-9e52-3996cc835af1}}. The dataset contains 12 [million](million) tweets sent by FORMULA  [million](million) users from different countries in the world, of which ten thousand users are kept for each the development set and the testing set. Moreover, only tweets that are in English and close to a city are retained. The primary [location](location) of a user in this dataset is assigned the [centre](centre) of the city where most of his tweets were sent. Different from GeoText and UTGeo2011, this dataset provides purely textual information; the timestamps of messages are not available.

The [location](location) of a user is indicated by a pair of real numbers, namely, [latitude](latitude) and [longitude](longitude).
However, classification models need discrete labels. For the datasets collected from the US, we follow {{cite:47121970-999b-43f6-8ebc-22930037554a}}, {{cite:8f65203f-6702-4b68-857b-108b6e996a7c}} to employ administrative boundaries to create the class labels. By doing so, we can consider the tasks of regional and state classification as in {{cite:47121970-999b-43f6-8ebc-22930037554a}}, {{cite:afb09f17-553c-44f5-bad3-8805564aa848}}, {{cite:a4b14ca2-2451-461b-853e-90f9678d245e}}.
We rely on the [Ray](ray) [Casting](casting) [algorithm](algorithm) of {{cite:60accd7d-83db-4eef-a40b-71b03b188388}} to decide if a location is inside a region or state's boundary.
For the region and state boundaries, we use information from [Census](census) Divisionshttps://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf.
Also, we have employed the Google S2 [library](library), k-means and k-d tree clusterings to partition all the geospatial datasets, making other sets of labels. This supports the task of predicting the geocoordinates. More details on the settings of the partitioning schemes and their [impact](impact)s will follow in the next section.

Performance Criteria and [Experiment](experiment) Design
The proposed model for [geolocation](geolocation) of Twitter users addresses the following tasks:
(i) four-way classification of US regions including Northeast, Midwest, West and South,
(ii) fifty-way classification to predict the states of users,
and (iii) [estimation](e[estimation](estimation) of the real-valued coordinates of users, i.e., [latitude](latitude) and [longitude](longitude).
For the region and state classification tasks,
we compare the performance of our model with existing methods,
by calculating the percentage of correctly classified users, which is the accuracy.
Considering the [estimation](estimation) of the user coordinates,
we measure the [distance](d[distance](distance) between the predicted and the actual geocoordinates
and calculate the mean and the [median](m[median](median) [values](values) over the testing dataset.
The [distance](d[distance](distance) between the predicted and the [ground](ground) truth coordinates is computed using the Haversine formula {{cite:3b627b6d-7008-44cb-8a69-1efa63272771}}.
Another common way to measure the success of coordinate estimation
is to calculate the percentage of estimations with accuracy better than 161 km;
this metric, known as @161161 km FORMULA  100 [mile](mile), has been used in many works {{cite:8f65203f-6702-4b68-857b-108b6e996a7c}}, {{cite:711e514a-fe0d-4e56-a234-323ba1933cfc}}, {{cite:b8f31317-067b-4a88-b4db-c5596d97b7c8}}, {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}, {{cite:c5aaa5fa-acdc-4eb9-82b1-5cd63ab57a64}}, {{cite:afb09f17-553c-44f5-bad3-8805564aa848}}.
It is worth noting that for the classification accuracy and the accuracy @161 [metrics](m[metrics](metrics), the higher [values](values) indicate a good prediction. Conversely, achieving lower [values](values) for the mean and [median](median) [distance](distance) errors is desired.
Concerning the first two classification tasks, we conduct experiments on the US Twitter datasets, namely GeoText and UTGeo2011. For predicting Twitter users' geocoordinates, experiments are performed on the three datasets. Furthermore, it should be noted that the experiments for geo[graph](graph)ical coordinate prediction use different sets of labels created by S2, FORMULA -d tree and FORMULA -means partitioning. Also, the administrative boundaries used in task (ii) are exploited for exact geocoordinate [estimation](estimation).

[Data](data) Pre-processing and Normalization
Before computing node2vec and TF-IDF features, a simple pre-processing phase is required.
First, we tokenize the tweets and remove [stop words](stop words) using nltkhttp://www.nltk.org/ {{cite:17d8b9b3-1bb3-41ab-b4d8-188933ee2d36}},
a dedicated [library](library) for [natural language](natural language) [processing](processing).
Then, we replace URLs and [punctuation](punctuation) by special characters,
which results in reducing the [size](size) of the [vocabulary](vocabulary) without affecting the [semantics](semantics) of tweets.
Again, nltk is used for [stemming](stemming) in the last stage of pre-processing.
Normalization is a common step to pre-process [data](data) before applying [machine](machine) learning algorithms.
Data can be normalized by removing the mean and dividing by the standard [deviation](deviation).
Alternatively, samples can be scaled into a small range of FORMULA  or FORMULA .
The less common way is to scale the samples so that their [module](module) is equal to 1, also known as FORMULA  normalisation.
In our case, the TF-IDF, node [embedding](embedding) and context features are already scaled to the range [0,1]. We apply FORMULA  normalization for the [timestamp](timestamp) feature only.

[Parameter](parameter) Settings
Our framework considers four different features and each feature requires some parameters for extraction. Extracting TF-IDF using scikit-learn requires a minimum term [frequency](frequency) across documents min_df. For the GeoText [data](data)set, we choose min_df=40. For the UTGeo2011 and TwitterWorld datasets, because of the sheer volume of data, we set min_df=500 and min_df=400, respectively. Concerning doc2vec, we [select](select) an [embedding](embedding) [size](size) equal to 300. The [size](size) of the sampling window is set to 10.
We have built the Twitter users' [graph](graph)s for the three datasets using mentions extracted from tweet messages only as discussed in Section REF . Following {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}, we set the celebrity connection thresholds FORMULA  to 5, 15 and 5 for GeoText, UTGeo2011 and TwitterWorld, respectively. Table REF  shows graph [statistics](statistics) for all three datasets.
We use the code provided by the authors of {{cite:1798db00-1f71-43de-8458-7a5635c703e6}} to obtain the node2vec feature. We choose an [embedding](embedding) [size](size) equal to 300. When training the embeddings, we [select](select) the weighted [graph](graph) option, which takes into account the weights of edges. Other parameters are set to [default](default) [values](values), namely the walk length FORMULA , transition parameters FORMULA , FORMULA . The sampling window [size](size) is set to 5.
TABLE 
TABLE 
Choosing the right [hyperparameter](hyperparameter)s for neural networks, which are the number and [size](size) of hidden [layers](layers), is always a challenge. In our experiments, these parameters are set empirically.
We set the number of hidden [layers](layers) on each individual [branch](branch) to 1, namely we use hidden [layers](layers) FORMULA , FORMULA , FORMULA  and FORMULA  for features TF-IDF, node2vec, doc2vec, and [timestamp](timestamp), respectively. Also, we connect the combination layer with the softmax layer directly without adding any layer in between.
All hyperparameters can be found in Table REF . We use a small value for the learning [rate](rate) FORMULA  and regularize the weights right before the output layer only. The regularization [parameter](parameter) FORMULA  is set to FORMULA . The training [procedure](procedure) is performed using [stochastic gradient descent](stochastic g[gradient descent](gradient descent) with the optimization algorithm ADAM {{cite:d0ec2bac-2a41-432b-8e6a-ea4439128504}} as the updating rule. The consecutively non-improving performance threshold FORMULA  is set to 10 for GeoText and 6 for both UTGeo2011 and TwitterWorld datasets.
Creating S2 grids requires setting the minimum cell level FORMULA  and maximum number of users per cell FORMULA . We have experimented with different settings and reported the best result in Table REF  with FORMULA , FORMULA  for GeoText, FORMULA , FORMULA  for UTGeo2011 and FORMULA , FORMULA  for TwitterWorld.

Results
After experimenting with different parameters,
normalization techniques and feature combination strategies,
we report here the best obtained results.
Table REF  presents results for regional and state [geolocation](geolocation) for GeoText and UTGeo2011,
while for the prediction of user geographical coordinates, results are presented in Table REF .
Concerning the classification tasks, our model significantly outperforms all previous works.
Successful regional classification is achieved for FORMULA  of users,
while for state classification the result is FORMULA .
By leveraging the classification strength of multiple features,
the improvement in regional accuracy is FORMULA  compared to the work in {{cite:a4b14ca2-2451-461b-853e-90f9678d245e}}.
Concerning the accuracy in state classification,
we achieve a greater improvement that rises to FORMULA 
compared to the state of the art presented in {{cite:a4b14ca2-2451-461b-853e-90f9678d245e}}.
TABLE 
TABLE 
The estimation of geographical coordinates of Twitter users
involves experiments with two types of labels, thus two sets of experiments.
In the first set of experiments, we use classes corresponding to the fifty states of the US.
In the second set of experiments, we employ the S2 classes described in Section .
As can be seen in Table REF ,
concerning the results obtained with state labels,
the mean distance error obtained with MENET is smaller than with other methods.
Likewise, the median distance error and the @161 accuracy are better on GeoText. However, our result with these metrics is worse on UTGeo2011.
The [reason](r[reason](reason) being that the state boundaries ignore the geographical distribution of users.
The performance of MENET is improved significantly over all criteria with S2 labels,
when the definition of regions takes into account the distribution of users.
In this case, Table REF  shows that
the proposed [method](method) outperforms existing methods
in terms of mean, [median](median) [distance](distance) error and @161 accuracy on GeoText and UTGeo2011. On TwitterWorld, the [median](median) [distance](distance) error is reduced more than FORMULA  compared to the result in {{cite:c5aaa5fa-acdc-4eb9-82b1-5cd63ab57a64}} while the result for the other [metrics](metrics) is comparable to the state of the [art](art).
At this point, we would like to [underline](underline) that
the number of employed classes is critical for the performance of our [method](method).
A larger number of classes results in smaller geographical areas,
which may improve the geocoordinate prediction.
However, training a model with more classes may be more difficult,
thus, the classification may perform worse.
[Granularity](granularity) Analysis
TABLE 
TABLE 
As explained in Section REF , an S2 adaptive [grid](grid) is built using two parameters: the minimum S2 [cell](cell) level FORMULA  and the maximum number of users per [cell](cell) (region, class) FORMULA . As an example, the [geolocation](g[geolocation](geolocation) result with S2 labels presented in Table REF  for GeoText is associated with the minimum [cell](cell) level of 6 and the user threshold of 500. The number of cells and their area (FORMULA ) will vary depending on these parameters. One may [wonder](wonder) if this setting is optimal or not. In this section, we present an analysis of the performance of MENET with regard to different S2 [parameter](parameter) settings. Concretely, we run experiments using the same [hyperparameter](hyperparameter) setting of MENET on GeoText with different S2 [label](label) sets. The [label](label) sets are created by either varying the minimum S2 [cell](cell) level FORMULA  or the user threshold FORMULA . The results of these experiments are shown in Tables REF  and  REF .
We can see a clear trend in the [median](median) of the [distance](d[distance](distance) error from the experiments with varying FORMULA . When FORMULA  increases, meaning more regions are generated, the [median](m[median](median) of the [distance](d[distance](distance) error decreases monotonically to a very small value (i.e., 28 km). The [reason](reason) for this is very intuitive. S2 cells at a higher level have smaller area, and if the classification performance of MENET does not get significantly worse with more classes, a predicted [location](location) will be more likely closer to the [ground truth](g[ground](ground)truth) location. This also explains the increasing trend in accuracy within 161 km. There is no clear trend in the mean of distance error. This could be explained by the sensitivity of the mean with regard to the outliers. Even if the classification accuracy of MENET goes down slightly, it may bring huge distance errors from large area cells. This has a large [impact](impact) on the mean value. On the other hand, the impact of these outliers is small on the [median](m[median](median) value.
Table REF  shows that when the maximum number of users per [cell](cell) FORMULA  increases, fewer regions are created. The decreasing trend in the mean [distance](d[distance](distance) errors can be explained by the better classification performance when using less classes. Moreover, the [median](median) and @161 remain stable within the range of FORMULA  for FORMULA . The [reason](reason) being that the classification accuracy in this range does not change significantly. The [median](median) and the @161, however, are much worse with FORMULA  set to 100 even when the corresponding number of regions is limited. The [reason](reason) being, again, that the classification performance drops dramatically. The question arises: why is the classification accuracy so [low](low)? The [reason](reason) is that [splitting](splitting) with this setting ignores the geographically natural distribution of [data](d[data](d[data](data). In fact, an S2 [cell](cell) at level 6 is a good fit with the area of cities, where most of the tweets originate. If we lower the user threshold FORMULA  in a [cell](cell), the [splitting](splitting) [algorithm](algorithm) will stop at much higher [cell](cell) levels for cities where the tweet density is high, thus dividing the city area into [multiple](multiple) smaller regions. That explains why the classification performance is very [low](low). Figures REF  and  REF  show the subvidision of GeoText at level 6 with different [values](values) of FORMULA .
FIGURE 
FIGURE 

Feature Analysis
The MENET [architecture](architecture) has the [capability](capability) of exploiting [multiple](multiple) features according to the multiview learning paradigm. In this paper, we realize the model using four features: TF-IDF, node2vec, doc2vec and [timestamp](timestamp). The question, then, arises: which feature contributes the most to the discriminative strength of the model?
To answer this question, we conduct additional experiments with different combinations of features.
Concretely, we eliminate one type of feature from the feature set and perform experiments with the rest.
This can be done by temporarily removing a [branch](branch) in MENET just before the [concatenation](concatenation) layer (see Fig. REF ). For a fair comparison, we use the same [parameter](parameter) setting for MENET as in the experiments with the full feature set. The results from the experiments on the GeoText dataset are presented in Table REF .
Compared to the results in Table REF , it is clear that the node2vec feature is the most important. Removing this feature results in a significant reduction of MENET's performance in terms of mean [distance](distance) error (894 km), [median](median) [distance](distance) error (480 km) and accuracy within 161 km (FORMULA  %). The contribution of the doc2vec feature is also noticeable, indicating an increase of more than 100 km in terms of mean [distance](distance) error, compared to the full feature set. The other features help to improve the performance slightly as removing them results in a marginal decrease in the three performance criteria.
TABLE 
TABLE 

Performance of MENET with regard to Observed Partitioning
In Table REF , we have a notable improvement in [geolocation](geol[location](location) result with MENET by using S2 labels. Using Google's S2 [geometry](geometry) [library](library) is one of many ways to create [label](label) sets for our classification problem. A similar partitioning [strategy](strategy) to Google's S2 [library](library) is called Hierarchical Equal Area isoLatitude [Pixelization](pixelization) of a sphere (HEALPix) {{cite:449fe36f-a247-487f-ac45-c242c6cd22a1}}. Like the S2 [library](library), it is able to partition the sphere into a uniform [grid](grid), and has appeared in several papers for Twitter user [geolocation](geolocation) such as {{cite:335aa4cf-8b51-4a56-8852-898a75f47fec}}. Other examples include the use of FORMULA -d tree {{cite:cff8bf03-2b43-40c2-ae48-cbdac4380511}} and FORMULA -means {{cite:58846419-5d86-4fce-9a5e-86668751d75e}} clustering algorithms for grouping users, thus making [label](label)s as in {{cite:c5aaa5fa-acdc-4eb9-82b1-5cd63ab57a64}}, {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}. In this section, we aim at investigating the performance of MENET with respect to two label creation strategies, namely FORMULA -d tree and FORMULA -means subdivisions. Our experiments are again conducted on the GeoText dataset.
Following {{cite:c5aaa5fa-acdc-4eb9-82b1-5cd63ab57a64}}, we create groups of users using either FORMULA -d tree or FORMULA -means partitioning. The clustering of users is based on geographical coordinates, namely [latitude](latitude) and [longitude](longitude).
For FORMULA -d tree [subdivision](subdivision), we make the root node with the bounding box that contains all user coordinates. Then, the tree is made by recursively [splitting](splitting) nodes, which correspond to boxes, into children nodes with straight dividing lines. The [splitting](splitting) takes into account the larger dimension of a node, then tries to divide all users in that node into two groups evenly. Note that we use only leaves to store users, which corresponds to classes. Therefore, the dividing lines must not go through any user's point. The recursive [splitting](splitting) process stops if the number of users in a [cell](cell) falls below a given threshold. Following {{cite:5d2f0dab-126a-4303-810b-7a0a7cf914fb}}, we set the theshold to 300 resulting in 32 geographical cells (i.e., classes). When using FORMULA -means for making classes, the number of clusters is set to 32, and the Euclidean [distance](distance) metric is used. The same hyperparameter settings are kept for MENET in these experiments.
The geolocation results on the GeoText dataset with FORMULA -d tree and FORMULA -means partitionings are shown in Table REF .
It is clear that FORMULA -means is better than FORMULA -d tree in partitioning Twitter users, in the sense that it can mitigate the geolocation errors.
Concretely, using FORMULA -means labels reduces the mean distance error with more than 30 km. The median distance error reduces by 50% while the accuracy within 161 km improves by roughly 7%.
On the other hand, the performance of MENET using S2 labels is better for all the concerned performance criteria. Also, it is worth mentioning that the performance of MENET with the FORMULA -means labels is close to that of S2 labels. However, the S2 partitioning is more flexible in controlling the median distance error and it is stable in creating labels compared with FORMULA -means.

Conclusion and Future Work
Noisy and sparse [labeled data](labeled data) make the prediction of Twitter user locations a challenging task.
While plenty approaches have been proposed, no [method](method) has attained a very high accuracy.
Following the multiview learning paradigm,
this paper shows the effectiveness of combining knowledge
from both user-generated content and network-based relationships.
In particular, we propose a generic neural [network model](network model), referred to as MENET,
that uses words, [paragraph](paragraph) [semantics](semantics), network [topology](topology) and timestamp information,
to infer users' location.
The proposed model provides more accurate results compared to the state of the art,
and it can be extended to leverage other types of available information, besides the types of data considered in this paper.
The performance of our model heavily depends on user graph features. The node2vec algorithm used in this paper is transductive, meaning the graph is built on all users.
In our future work, we will focus on making the model truly inductive, meaning able to generalize to never seen users.
