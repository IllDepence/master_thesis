
px
Computer Vision and Image Understanding
PuRe: Robust pupil detection for real-time pervasive eye tracking
[1]Thiago Santinicor1fn1
[cor1]Corresponding author:
[1]Wolfgang Fuhlfn1
[fn1]Authors contributed equally and should be considered co-first authors.
thiago.santini@uni-tuebingen.de
[1]Enkelejda Kasneci
[1]University of Tübingen, Sand 14, Tübingen – 72076, Germany
1 May 2013
10 May 2013
13 May 2013
15 May 2013
S. Sarkar
Real-time, accurate, and robust pupil detection is an essential prerequisite to
enable pervasive eye-tracking and its applications – e.g., gaze-based human
computer interaction, health monitoring, foveated rendering, and advanced
driver assistance.
However, automated pupil detection has proved to be an intricate task in
real-world scenarios due to a large mixture of challenges such as quickly
changing illumination and occlusions.
In this paper, we introduce the Pupil
Reconstructor (PuRe), a method for pupil detection in pervasive
scenarios based on a novel edge segment selection and conditional segment
combination schemes; the method also includes a confidence measure for the
detected pupil.
The proposed method was evaluated on over 316,000 images acquired with four
distinct head-mounted eye tracking devices.
Results show a pupil detection rate improvement of over 10 percentage points
w.r.t. state-of-the-art algorithms in the two most challenging data
sets (6.46 for all data sets), further pushing the envelope for pupil detection.
Moreover, we advance the evaluation protocol of pupil detection algorithms by
also considering eye images in which pupils are not present.
In this aspect, PuRe improved precision and specificity w.r.t.
state-of-the-art algorithms by 25.05 and 10.94 percentage points, respectively,
demonstrating the meaningfulness of PuRe's confidence measure.
PuRe operates in real-time for modern eye trackers (at 120 fps).
Pupil DetectionPervasiveEye TrackingEmbedded
Introduction
Head-mounted video-based eye trackers are becoming increasingly more accessible
and prevalent.
For instance, such eye trackers are now available as low-cost devices (e.g.,
 {{cite:8ec9b22a-2290-416f-bfe7-297fa52e2fea}}) or integrated into wearables such as Google Glasses,
Microsoft Hololens, and the Oculus
Rift {{cite:d7ff38a9-9922-4e4a-94bd-567bde3c94d2}}, {{cite:ee168619-7fd1-4840-9372-15f33ed05c32}}, {{cite:7e1f3c3c-2d84-4286-9551-41de7a4ba6ea}}.
As a consequence, eye trackers are no longer constrained to their origins as
research instruments but are developing into fully fledged pervasive devices.
Therefore, guaranteeing that these devices are able to seamlessly operate in
out-of-the-lab scenarios is not only pertinent to the research of human
perception, but also to enable further applications such as pervasive gaze-based
human-computer interaction {{cite:aa7550e9-4912-4a74-ac75-9b16137666b2}}, health
monitoring {{cite:19f2f583-d867-4125-bc79-d92e1db8b068}}, foveated
rendering {{cite:b28dbe2a-0a2a-425c-a571-798d6dfe93fb}}, and conditionally automated
driving {{cite:ed77ebd0-af8d-41c0-b404-44fd0aefe147}}.
Pupil detection is the fundamental layer in the eye-tracking stack since most
other layers rely on the signal generated by this layer – e.g., for gaze
estimation {{cite:5c73f033-4468-4354-b540-db2a8c37d50e}}, and automatic identification of eye
movements {{cite:5c3923c5-081c-435d-8abd-84ec99c2cb99}}.
Thus, errors in the pupil detection layer propagate to other layers,
systematically degrading eye-tracking performance.
Unfortunately, robust real-time pupil detection in natural environments has
remained an elusive challenge.
This elusiveness is evidenced by several reports of difficulties and low pupil
detection rates in natural environments such as
driving {{cite:cf46fedb-603a-47a3-a26e-e8a8317a4ad1}}, {{cite:de195f8b-74b6-448c-a03c-0188f72e8cef}}, {{cite:3682b257-413a-47e2-8713-76396fbf9a75}}, {{cite:d5117a81-c18c-4ab7-baf8-dcec8708c99c}}, {{cite:dfc165ce-160d-4957-927d-6ea732c0c88c}}, {{cite:474e4101-53fc-480b-9bf3-1c5789f30752}}, {{cite:5c5e2d17-ed78-4e28-9c31-5a864429d72b}},
shopping {{cite:9a57b108-7c58-4340-96f3-aa23fcb4de5f}},
walking {{cite:887391df-e469-4e24-8cbd-21904e670e02}}, {{cite:456bce87-d4b4-4718-80c7-1c78347be3e4}}, and in an operating
room {{cite:b865ddb3-685e-40ab-8e81-17a0b90cdc44}}.
These difficulties in pupil detection stems from multiple factors; for instance,
reflections (Fig. REF ), occlusions (Fig. REF ),
complex illuminations (Fig. REF ), and physiological
irregularities (Fig. REF ) {{cite:8cd1d300-9406-4a73-9685-2da0c7cf3599}}, {{cite:cd7323a4-f198-43e6-b26f-8f020f784c18}}, {{cite:452e6fc4-f123-4f23-93be-bdfa827e8258}}, {{cite:871af6a9-887c-4988-a3d0-9393dceb9a7a}}.
FIGURE 
In this paper, we introduce the Pupil
Reconstructor (PuRe), a method for pupil detection in pervasive
scenarios based on a novel edge segment selection and conditional segment
combination schemes; the proposed method also includes a meaningful confidence
measure for the detected pupil.
Previous work in the field of pupil detection is presented in
Section , and the proposed method is described in
Section .
Previous work usually focuses on evaluating pupil detection algorithms based
solely on the detection rate. Similarly, we contrast the proposed method to
previous work in Section REF .
Moreover, we go one step further and introduce novel metrics to evaluate
these algorithms in terms of incorrect pupil detection rates
(Section REF ) as well as dynamic signal properties
(Section REF ). Run time considerations are discussed in
Section REF , and Section  presents final remarks and
future work.

Related Work
While there is a large gamma of previous work for pupil detection, most methods
are not suitable for out-of-the-lab scenarios.
For an extensive appraisal of state-of-the-art pupil detection methods, we refer
the reader to the works by {{cite:8cd1d300-9406-4a73-9685-2da0c7cf3599}} and  {{cite:ce2b4324-9c48-4caf-bc14-189fecf6e2d5}}
for head-mounted eye trackers as well as  {{cite:73fa2ed6-fb3c-4bb2-a94a-21bf0f96b1e1}} for remote
eye trackers.
In this work, we focus solely on methods that have been shown to be relatively
robust enough for deployment in pervasive scenarios, namely
ElSe {{cite:2a938884-c851-428e-ab48-31673fc45f2d}}, ExCuSe {{cite:2ee599cd-9fe1-401a-97c7-a73a1e941495}}, and
Świrski {{cite:bda1cbf9-eb1d-4f88-9f23-eed96f0b7b2a}}.
ElSe
consists of two approaches.
First, a Canny edge detector is applied, and the resulting edges are filtered
through morphological operations{{cite:2a938884-c851-428e-ab48-31673fc45f2d}} also describe an
algorithmic approach to edge filtering producing similar results; however the
morphological approach is preferred because it requires less computing power..
Afterwards, ellipses are fit to the remaining edges, edges are removed based on
emprirically defined heuristics, and one ellipse is selected as pupil based on its
roundness and enclosed intensity value.
If this method fails to produce a pupil, a second approach that combines a mean
and a center surround filter to find a coarse pupil estimate is employed; an
area around this coarse estimate is then thresholded with an adaptive parameter,
and the center of mass of pixels below the threshold is returned as pupil center
estimate {{cite:2a938884-c851-428e-ab48-31673fc45f2d}}.
ExCuSe
first analyzes the input images w.r.t. reflections based on
peaks in the intensity histogram.
If the image is determined to be reflection free, the image is thresholded with
an adaptive parameter, and a coarse pupil position is estimated through an
angular integral projection function {{cite:5c93b885-6966-4a09-af18-6bf4b672a019}}; this position
is then refined based on surrounding intensity values.
If a reflection is detected, a Canny edge detector is applied, and the resulting
edges are filtered with morphological operations; ellipses are fit to the
remaining edges, and the pupil is then selected as the ellipse with the darkest
enclosed intensity {{cite:2ee599cd-9fe1-401a-97c7-a73a1e941495}}.
Świrski
starts with a coarse positioning using Haar-like features. The
intensity histogram of an area around the coarse position is clustered using
k-means clustering, followed by a modified RANSAC-based
ellipse fit {{cite:bda1cbf9-eb1d-4f88-9f23-eed96f0b7b2a}}.

From these algorithms, ElSe has shown a significantly better performance over
multiple data sets {{cite:8cd1d300-9406-4a73-9685-2da0c7cf3599}}.
Moreover, it is worth noticing that these algorithms employ multiple parameters
that were empirically defined, albeit there is usually no need to tune these
parameters.
It is worth dedicating part of this section to discuss machine-learning
approaches in contrast to the algorithmic ones, particularly convolutional
neural networks (CNN).
Similarly to other computer vision problems, from a solely pupil detection stand
point, deep CNNs will likely outperform human-crafted pupil detection approaches
given enough training data – with incremental improvements appearing as more
data becomes available and finer network tuning.
Besides labeled data availability, which might be alleviated with
developments of unsupervised learning methods, there are other impediments to
the use of CNNs in pervasive scenarios – i.e., in embedded systems.
For instance, computation time and power consumption. While these impediments
might be lessened with specialized hardware – e.g.,
cuDNN {{cite:bb2d5407-4077-47de-8558-004d30178399}}, Tensilica Vision
DSP {{cite:740b4d11-fc0f-49f3-a21f-0188d602dacd}}, such hardware might not always be available or
incur prohibitive additional production costs.
Finally, CNN-based approaches might be an interesting solution from an
engineering point of view, but remain a black box from the scientific
one.
To date, we are aware of two previous works that employ CNNs for pupil
detections:
1) PupilNet {{cite:ad3f8ee7-8571-45e0-a805-a1976826e0a9}}, which aims at a computationally inexpensive
solution in the absence of hardware support, and
2) Vera-Olmos {{cite:4f532262-85e7-4844-8bc0-8923af8564ff}}, which consists of two very deep CNNs –
a coarse estimation stage (with 35 convolution plus 7 max-pooling layers for
encoding and 10 convolution plus 7 deconvolution layers for decoding), and a
fine estimation stage (with 14 convolution plus 5 max-pooling layers for
encoding and 7 convolution plus 5 deconvolution layers).

PuRe: The Pupil Reconstructor
Similarly to related work, the proposed method was designed for
near-infraredThis is the standard image format for head-mounted
eye trackers and can be compactly represented as a grayscale image. eye images
acquired by head-mounted eye trackers.
Our method only makes two uncomplicated assumptions to constrain the valid pupil
dimension space without requiring empirically defined values: 1) the eye canthi
lay within the image, and 2) the eye canthi cover at least two-thirds of the
image diagonal.
It is worth noticing that these are soft assumptions – i.e., the
proposed method still operates satisfactorily if the assumptions are not
significantly violated.
Fig. REF  illustrates these concepts.
Furthermore, these assumptions are in accordance to eye tracker placement
typically suggested by eye tracker vendor's guidelines to capture the full
range of eye movements.
FIGURE 
PuRe works purely based on edges, selecting curved edge segments that
are likely to be significant parts of the pupil outline.
These selected segments are then conditionally combined to construct further
candidates that may represent a reconstructed pupil outline.
An ellipse is fit to each candidate, and the candidate is evaluated
based on its ellipse aspect ratio, the angular spread of its edges relative to
the ellipse, and the ratio of ellipse outline points that support the hypothesis
of it being a pupil.
This evaluation yields a confidence measure for each candidate to be the pupil,
and the candidate with the highest confidence measure is then selected as pupil.
The remainder of this section describes the proposed method in detail.
Preprocessing
Prior to processing, if required, the input image is downscaled to the
working size FORMULA  through bilinear
interpolation, where FORMULA  and FORMULA  are the working
width and height, respectively.
The original aspect ratio is respected during downscaling.
Afterwards, the resulting image is linearly normalized using a
Min-Max approach.

Edge Detection and Morphological Manipulation
PuRe's first step is to perform edge detection using a Canny edge
operator {{cite:ff62563f-36d7-4cd4-8f94-0b2966cd27d7}}.
The resulting edge image is then manipulated with a morphological approach to
thin and straighten edges as well as to break up orthogonal connections
following the procedure described by {{cite:2a938884-c851-428e-ab48-31673fc45f2d}}.
The result of this step is an image with unconnected and thinned edge segments.

Edge Segment Selection
Each edge segment is first approximated by a set of dominant points FORMULA 
following the k-cosine chain approximation method described by
{{cite:30cd6a3a-7bbd-4e98-804a-8879da4da3f6}}.
This approximation reduces the computational requirements for our approach and
typically results in a better ellipse fit in cases where a pupil segment has not
been properly separated from surrounding edges.
After approximation, multiple heuristics are applied to discard edge segments
that are not likely to be part of the pupil outline:

Given the general conic equation FORMULA , at least
five points are required to fit an ellipse in a least-squares sense.
Therefore, we exclude segments in which FORMULA 's cardinality is smaller
than five. This heuristic discards plain shapes such as small segments
and substantially straight lines.

Based on the assumptions highlighted in the beginning of this section,
it is possible to establish the maximal and minimal distance between the
lateral and medial eye canthus in pixels when
frontally imaged as
FORMULA 
These estimates can then be used to infer rough values for the maximal
(Fig. REF ) and minimal
(Fig. REF ) pupil diameter bounds
(FORMULA  and FORMULA ) based on the human physiology.
We approximate the eye canthi distance through the palpebral fissure
width as 27.6 {{cite:df9795b4-e81c-47f3-b457-251ccf06e84e}};
similarly, the maximal and minimal pupil diameter are approximated as
8 and 2,
respectively {{cite:ad96409a-81dc-40a3-9995-e62c132934bf}}.
Therefore,
FORMULA 
Note that whereas maximal values hold independent of camera rotation and
translation w.r.t. the eye, minimal values might not hold due to
perspective projection distortions and corneal refractions.
Nonetheless, FORMULA  already represents a minute part (FORMULA ) of the image diagonal, and we opted to retain this lower bound
– for reference, see Fig. REF .
For each candidate, we approximate the segment's diameter by the largest
gap between two of its points.
Candidates with a diameter outside of the range FORMULA 
violate bounds and are thus discarded.

To estimate a segment's curvature, first the minimum rectangle
containing FORMULA  is calculated using the rotating calipers
method {{cite:6c34cc67-d00b-4712-8fe8-1bf59c995794}}. The curvature is then estimated
based on the ratio between this rectangle's smallest and largest
sides. The straighter the candidate is, the smaller the ratio.
The cut-off threshold for this ratio is based on the ratio between the
minor and major axes of an ellipsis with axes extremities inscribed in
45 of a circle, which evaluates to FORMULA . This
heuristic servers to discard relatively linear candidates.

At this stage, an ellipse FORMULA  is fit to the points in FORMULA  following
the least-squares method described in {{cite:21588e27-ef00-4a5f-8dc3-e99bed15c9a5}}. A segment is
discarded if: I) FORMULA 's center lays outside of the image boundaries,
which violates PuRe's assumptions, or II) the ratio between FORMULA 's minor
and major axes is smaller than FORMULA , which assumes that the camera
pose relative to the eye can only distort the pupil round shape to a
certain extend.

Seldom, the ellipse fit will not produce a proper fit. We identify and
discard most such cases inexpensivly if the mean point from FORMULA  does not lay
within the polygon defined by the extremities of FORMULA 's axes.


Confidence Measure
For each remaining candidate, PuRe takes into account three distinct metrics to
determine a confidence measure FORMULA  that the candidate is a pupil:
Ellipse Aspect Ratio (FORMULA ):
measures the roundness of FORMULA . This
metric favors rounder ellipses (that typically result due to the eye
camera placement w.r.t. the eye) and is evaluated as the ratio between
FORMULA 's minor and major axis.
Angular Edge Spread (FORMULA ):
measures the angular spread of the
points in FORMULA  relative to FORMULA , assuming that the better distributed the
edges are, the more likely it is that the edges originated from a
clearly defined elliptical shape (i.e., a pupil's shape).
This metric is roughly approximated as the ratio of FORMULA  centered quadrants
that contain a point from FORMULA .
Ellipse Outline Contrast (FORMULA ):
measures the ratio of the FORMULA 's
outline that supports the hypothesis of a darker region surrounded by a
brighter region (i.e., a pupil's appearance).
This metric is approximated by selecting FORMULA 's outline points with a
stride of ten degrees.
For each point, the linear equation passing through the point and the
FORMULA 's center is calculated, which is used to define a line segment
with length proportional to FORMULA 's minor axis and centered at the
outline point.
If the mean intensity of the inner segment is lower than the mean
intensity of the outer one, the point supports the pupil-appearance
hypothesisIf a bright pupil eye tracker is used, the inverse
holds.

If the candidate's ellipse outline is invalid – i.e., violates PuRe's size
assumptions or less than half of the outline contrast FORMULA  supports the
candidate – the confidence metric is set to zero.
Otherwise, the aforementioned metrics are averaged when determining the
resulting confidence. In other words,
FORMULA 
It is worth noticing that the range of all three metrics (and consequently
FORMULA ) is [0,1].

Conditional Segment Combination
The segments that remain as candidates are combined pairwise to generate
additional candidates. This procedure attempts to reconstruct the pupil outline
based on nearby segment pairs since the pupil outline is often broken up due to
occlusions from, for example, reflections or eye lashes.
Let FORMULA  and FORMULA  be the set of dominant points for two segments and FORMULA  and
FORMULA  the set of points contained by the up-right squares bounding FORMULA  and
FORMULA , respectively.
The segments are combined if these bounding squares intersect but are not
fully contained into one another – i.e., FORMULA .
The resulting merged segment is then validated according to
Section REF , and its confidence measure evaluated according to
Section REF .
Since this procedure is likely to produce candidates with high aspect ratio
FORMULA  and angular spread FORMULA  values, the new candidate is only added to
the candidate list if its outline contrast FORMULA  improves on the FORMULA 
from the original segments.
After conditional combination, the candidate with highest confidence FORMULA  is
selected as the initial pupil.
Note that the inner intensities relative to other candidates do not contribute
to the pupil selection.
Thus, the iris might be selected since it exhibits properties similar
to the pupil – e.g., roundness, inner-outer contrast, and size range.
For this reason, the inside of the initial pupil is searched for a roughly
cocentered candidate with adequate size and strong inner-outer outline contrast.
This is achieved through a circular search area centered at the center of the
initial pupil with radius equal to the initial semi-major axis – i.e.,
representing a circular iris.
Candidates 1) lying inside this area, 2) with major axis smaller than the search
radius, and 3) with atleast three thirds of the outline contrast (FORMULA )
valid are collected.
The collected candidate with highest confidence is then choosen as new
pupil estimate.
If no candidate is collected in this procedure, the initial pupil remains as the
pupil estimate.
As output, PuRe returns not only a pupil center, but also its outline and a
confidence metric.

Experimental Evaluation
As previously mentioned, we evaluate PuRe only against robust
state-of-the-art pupil detection methods, namely ElSeWith
morphological split and validity threshold., ExCuSe, and Świrski.
All algorithms were evaluated using their open-source C++ implementations;
default parameters were employed unless specified otherwise.
For ExCuSe, the input images were downscaled to 240p (i.e.,
[product-units=single]320 x 240) as there is evidence that this is a
favorable input size detection-rate-wise {{cite:ce2b4324-9c48-4caf-bc14-189fecf6e2d5}}.
Similarly, the working size for PuRe (FORMULA , Section REF )
was set to 240p as well to keep run time compatible with state-of-the-art
head-mounted eye trackers (see Section REF ).
ElSe provides an embedded downscaling and border cropping mechanism,
effectively operating with a resolution of [product-units=single]346 x
260.
Notice that whenever the input images are downscaled, the results must be
upscaled to be compared with the ground truth.
No preprocessing downscaling was performed for Świrski since evidence suggests it
degrades performance for this method {{cite:ce2b4324-9c48-4caf-bc14-189fecf6e2d5}}.
Additionally, we juxtapose our results to the ones from
PupilNet {{cite:ad3f8ee7-8571-45e0-a805-a1976826e0a9}} and Vera-Olmos {{cite:4f532262-85e7-4844-8bc0-8923af8564ff}} whenever
possible.
In this work, we use the term use case to refer to each individual
eye video. For instance, the LPW data set contains 22 subjects with three
recordings per subject in distinct conditions (e.g., indoors, outdoors),
resulting in 66 distinct use cases.
Furthermore, we often compare PuRe with the rival, meaning the
best performant from the other algorithms for the metric in question.
For instance, for the aggregated detection rate, ElSe performs better than
ExCuSe and Świrski and is, therefore, the rival.
Pupil Detection Rate
A pupil is considered detected if the algorithm's pupil center estimate lies
within a radius of FORMULA  pixels from the ground-truth pupil center.
Similar to previous work, we focus on an error up to five pixels to account for
small deviations in the ground-truth labeling process – e.g., human inaccuracy
{{cite:2ee599cd-9fe1-401a-97c7-a73a1e941495}}, {{cite:2a938884-c851-428e-ab48-31673fc45f2d}}, {{cite:ce2b4324-9c48-4caf-bc14-189fecf6e2d5}}, {{cite:4f532262-85e7-4844-8bc0-8923af8564ff}}.
This error magnitude is illustrated in Fig. REF .
For this evaluation, we employed five data sets totaling 266,786 realistic and
challenging images acquired with three distinct head-mounted eye tracking
devices, namely, the
Świrski {{cite:bda1cbf9-eb1d-4f88-9f23-eed96f0b7b2a}},
ExCuSe {{cite:2ee599cd-9fe1-401a-97c7-a73a1e941495}},
ElSe {{cite:2a938884-c851-428e-ab48-31673fc45f2d}},
LPW {{cite:ce2b4324-9c48-4caf-bc14-189fecf6e2d5}},
and PupilNet {{cite:ad3f8ee7-8571-45e0-a805-a1976826e0a9}} data sets.
In total, these data sets encompass 99 distinct use cases.
It is worth noticing that we corrected a disparity of one frame in the
ground truth for five use cases of the ElSe data set and for the whole
PupilNet data set, which increased the detection rate of all algorithms (by
FORMULA  on average).
FIGURE 
Fig. REF  shows the cumulative detection rate per pixel error of the
evaluated algorithms for the aggregated 266,786 images as well as the detection
rate distribution per use case at five pixels.
As can be seen, PuRe outperforms all algorithmic competitors for all pixel
errors.
In particular, PuRe achieved a detection rate of FORMULA  at the five pixel
error mark, further advancing the state-of-the-art detection rate by a
significant margin of 6.46 percentage points when compared to the rival.
Moreover, the proposed method estimated the pupil center correctly FORMULA  of the
time for the majority of use cases, attesting for PuRe's comprehensive
applicability in realistic scenarios.
FIGURE 
It is worth noticing that the aggregated detection rate does not account for
differences in data set sizes.
As a consequence, this metric is dominated by the ExCuSe and
LPW datasets, which together represent FORMULA  of the data.
Since these two data sets are not the most challenging onesAs
evidenced by higher detection rates for all algorithms in Fig.  REF, the
algorithms tend to perform better on them, and differences between the
algorithms are less pronounced.
Inspecting the detection rates per data set in Fig. REF  gives a better
overview of the real differences between the algorithms and data sets, revealing that
PuRe improves the detection rate by more than 10 percentage points w.r.t. the
rival for the most challenging data sets (i.e., ElSe and PupilNet).
To allow for a more fine-grained appreciation of the method's performance
relative to the other algorithms, Fig. REF  presents PuRe's detection
rate at five pixels relative to the rival for each use
case. In FORMULA  of all use cases, PuRe outperformed all contenders.
In particular, for the two most challenging data sets, PuRe surpassed the
competition in FORMULA  of the use cases.
In contrast, the rivals noticeably outperformed PuRe in five use cases:
Swirski/p1-right, ExCuSe/data-set-II, LPW/4/12, LPW/9/17,
and LPW/10/11, from which representative frames are shown in
Fig. REF .
These five use cases also highlight some of PuRe's imperfections.
For instance, Swirski/p1-right and ExCuSe/data-set-II have weak and
broken pupil edges due to inferior illumination and occlusions due to eye
lashes/corneal reflections; ElSe compensates this lack of edges with its
second step.
LPW/4/12 contains large pupils that violate PuRe's assumptions; in fact,
relaxing the maximum pupil size by only ten percent increases PuRe's detection
rate from FORMULA  to FORMULA  (or FORMULA  w.r.t. the rival).
LPW/9/17 often has parts of the pupil outline occluded by eye lashes and
reflections, whereas LPW/10/11 contains pupils in extremely off-axial
positions combined with occlusions caused by reflections. However, visually
inspecting the latter two use cases, we did not find any particular
reason for Świrski to outperform the other algorithms.
FIGURE 
FIGURE 
FIGURE 
Regarding CNN-based approachesThese approaches used the uncorrected ElSe
and PupilNet data sets, which might slightly affect the detection rate.:
1) For PupilNet, {{cite:ad3f8ee7-8571-45e0-a805-a1976826e0a9}} report a detection rate of FORMULA  at the
five pixel error range when trained in half of the data from the ExCuSe and
PupilNet data sets and evaluated on the remaining data. In contrast, PuRe
reached FORMULA  on all images from these data sets – i.e., FORMULA .
2) For Vera-Olmos, {{cite:4f532262-85e7-4844-8bc0-8923af8564ff}} report an unweightedAveraged
over the use cases. detection rate of FORMULA  at the five pixel error
range averaged over a leave-one-out cross validation in the ExCuSe and
ElSe data sets. In contrast, PuRe reached FORMULA  on all images from
these data sets – i.e., FORMULA .
Nevertheless, these results indicate that PuRe is able to compete with
state-of-the-art CNN-based approaches while requiring only a small fraction of
CNN computational requirements. In fact, PuRe outperformed Vera-Olmos for FORMULA 
of use cases.
Furthermore, it is worth noticing that the training data is relatively similar
to the evaluation data (same eye tracker, similar conditions and positioning) in
both cases, which might bias the results in favor of the CNN approaches.

Beyond Pupil Detection Rate: Improving Precision, and Specificity
Through the Confidence Measure
One aspect that is often overlooked when developing pupil detection algorithms
is the rate of incorrect pupil estimates returned by the algorithm.
For instance, the aforementioned CNN-based approaches always return a
pupil estimate, regardless of one actually existing in the image.
Intuitively, one can relax pupil appearance contraints in order to increase the
detection rate, leading to an increase in the amount of incorrect pupils
returned.
However, these incorrect pupil estimates later appear as noise and can
significantly degrade gaze-estimation calibration {{cite:6d9ca9e8-22cc-4422-b75e-90bdff7d0210}},
automatic eye movement detection {{cite:8c6e6d9d-f98b-4406-ab26-64648e3d61d9}}, glanced-area ratio
estimations {{cite:bc1b5d42-ac2b-4fcd-ae57-b439992da3ae}}, eye model
construction {{cite:7a12f3f9-1d14-456a-86b2-91a48c1fda87}}, or even lead to wrong medical
diagnosis {{cite:ce47f7a3-3290-40c3-a41d-ae0c28d6e6e6}}.
Therefore, it is imperative to also analyse algorithms in terms of incorrect
detected pupils.
The pupil detection task can be formulated as a classification problem –
similar to the approach by {{cite:69c7602d-1195-4e5e-ab58-f53d4a642bae}} for frame-based tracking
metrics – such that:
True Positive (FORMULA )
represents cases in which the algorithm and
ground truth agree on the presence of a pupil. We further specialize
this class into Correct True
Positive (FORMULA ) and Incorrect True
Positive (FORMULA ) following the detection definition from
Section REF .
False Positive (FORMULA )
represents cases in which the algorithm finds
a pupil although no pupil is annotated in the ground truth.
True Negative (FORMULA )
represents cases in which the algorithm and
ground truth agree on the absence of a pupil.
False Negative (FORMULA )
represents cases in which the algorithm fails
to find the pupil annotated in the ground truth.

Note that this is not a proper binary classification problem, and the
relevant class is given only by FORMULA .
Therefore, we redefine sensitivity and precision in terms of this
class as
FORMULA 
and
FORMULA 
respectively, such that sensitivity reflects the (correct) pupil
detection rate and precision the rate of pupils that the algorithm found
that are correct. Thus, these metrics allows us to evaluate 1) the
trade-off between detection of correct and incorrect pupils, and 2) the
meaningfulness of PuRe's confidence measure.
Unfortunately, the eye image corpus employed to evaluate pupil detection rates
(in Section REF ) do not include negative samples – i.e., eye
images in which a pupil is not visible, such as during a blink.
Therefore, the capability of the algorithm to identify frames without a pupil as
such cannot be evaluated since specificity FORMULA  remains undefined without negative samples.
To evaluate this aspect of the algorithms, we have recorded a new data set
(henceforth referred to as Closed-Eyes) containing in its majority (FORMULA )
negative samples.
This data set consists of 83 use cases and contains 49,790 images with a
resolution of [product-units=single]384 x 288.
These images were collected from eleven subjects using a Dikablis
Professional eye tracker {{cite:c6499e38-4aed-4d76-a7af-d06f0dcfdfe1}} with varying illumination
conditions and camera positions.
A larger appearance variation was achieved by asking the subjects to perform
certain eye movement patternsAlthough the eye is hidden underneath the
palpebrae, eye globe movement results in changes in the folds and light
reflections in the skin. while their palpebrae remained shut in two
conditions: 1) with the palpebrae softly shut, and 2) with the palpebrae
strongly shut as to create additional skin folds.
In FORMULA  of use cases, participants wore glasses. Challenges in
the images include reflections, black eyewear frames, prominent eye lashes,
makeup, and skin folds, all of which can generate edge responses that the
algorithms might identify as parts of the pupil outline.
Fig. REF  shows representative images from the data set.
FIGURE 
We evaluated the four aforementioned algorithms using all images from the
data sets from Section REF  and the Closed-Eyes data set, totaling
316,576 images.
We assessed PuRe's confidence measure using a threshold within [0:0.99] with
strides of 0.01 units.
A pupil estimate was considered correct only if its confidence measure was above
the threshold.
Similarly, ElSe offers a validity threshold (default=10) to diminish
incorrect pupil rates, which we evaluated within the range [0:110] with strides
of 10 units.
ExCuSe and Świrski do not offer any incorrect pupil prevention mechanisms and,
therefore, result only in a single evaluation point.
The results from this evaluation are presented in Fig. REF .
As can be seen in this figure, PuRe dominates over the other algorithms, and
PuRe's confidence metric is remarkably meaningful, allowing to significantly
reduce incorrect pupil detections while preserving the correct pupil detection
rate and increasing identification of frames without pupils.
In fact, when compared to threshold 0, the threshold that maximizes the FORMULA 
score (0.66) increased precision and specificity by FORMULA  and
FORMULA , respectively, whereas sensitivity was decreased by a
negligible FORMULA .
In contrast, ElSe exhibited negligible (FORMULA ) changes for sensitivity
and precision when varying the threshold from 0 to 10, with a small gain
of FORMULA  in specificity; subsequent increases in the threshold
increase specificity at the cost of significantly deteriorating ElSe's
performance for the other two metrics.
Compared to the rival for each metric, PuReFORMULA  improved
sensitivity, precision, and specificity by FORMULA , FORMULA ,
and FORMULA  percentage points, respectively.
FIGURE 

Pupil Signal Quality
From the point of view of the image processing layer in the eye-tracking stack,
the (correct/incorrect) detection rates stand as a meaningful metric to measure
the quality of pupil detection algorithms.
However, the remaining layers (e.g., gaze estimation, eye movement
identification) often see the output of this layer as a discrete pupil
signal (as a single-object tracking-by-detection), which these detection rates
do not fully describe.
For example, consider two pupil detection algorithms: FORMULA , which detects
the pupil correctly every two frames, and FORMULA , which detects the pupil correctly
only through the first half of the data.
Based solely on the pupil detection rate (50% in both cases), these algorithms
are identical.
Nonetheless, the former algorithm enables noisyNote that the values
are not necessarily missing but might be incorrect pupil detections; thus
interpolation/smoothing might actually degrade the pupil signal even further.
eye tracking throughout the whole data, whereas the latter enables noiseless eye
tracking during only the first half of the data.
Which algorithm is preferable is then application dependent, but a method to
assess these properties is required nonetheless.
Recent analyses of widely-used object tracking performance metrics have shown
that most existing metrics are strongly correlated and propose the use of only
two weakly-correlated metrics to measure tracker performance: accuracy
and robustness {{cite:ff5ba24c-32af-47ae-8b2c-99769241269f}}, {{cite:2cddc052-2273-4180-85b9-9847fec1ff9d}}.
Whereas in those works accuracy was measured by average region overlaps, for
pupil detection data sets, only the pupil center is usually available.
Thus, we employ the center-error-based detection rate as accuracy
measure.
As an indicator of robustness, {{cite:ff5ba24c-32af-47ae-8b2c-99769241269f}} proposes the
failure rate considering the tracking from a relibility engineering point
of view as a supervised system in which an operator reinitializes the tracker
whenever it fails.
For the pupil signal, we formulate this problem slightly different
since there is no operator reinitialization.
Instead, we evaluate the robustness as the reliability
FORMULA 
where FORMULA  is the failure rate estimated through
the Mean Time Between Failures
(FORMULA ) not accounting for repair time – i.e., periods of no/incorrect
pupil detection are considered as latent faults.
In this manner, the reliability is a measure of the likelihood of the
algorithm to correctly detect the pupil for FORMULA  successive frames.
Furthermore, by measuring the Mean Time To
Repair (FORMULA ) – i.e., the mean duration of periods in which the
correct pupil signal is not available – we can achieve a similar metric in
terms of the likelihood for the algorithm to not detect the pupil
correctly for FORMULA  successive frames.
Henceforth, we well define this metric as the insufficiency (FORMULA ), which
can be evaluated as
FORMULA 
where
FORMULA . The smaller an algorithm's
insufficiency, the more sufficient it is.
It is worth noticing, that FORMULA  and FORMULA  are not true probabilities since the
events they measure are not likely to be independent nor uniformly distributed.
Consequently, these metrics only offer a qualitative and relative measure
between the algorithms.
Thus, we simplify their evaluation by fixing FORMULA .
As an illustration, let us return to our initial example considering a sequence of
FORMULA  frames:
FORMULA  yields FORMULA , whereas FORMULA  yields
FORMULA . Since FORMULA , we can conclude that
FORMULA  is more reliable but less sufficient w.r.t. FORMULA  for sequences longer
than two frames.
A quantitative conclusion is, however, not possible.
We evaluated the four aforementioned algorithms in this regard using only the
data sets from Section REF . The Closed-Eyes data set was excluded
since it is not realistic from the temporal aspect – i.e., users are not
likely to have their eyes closed for extended periods of time.
Furhermore, it is worth noticing that each use case from the ExCuSe, ElSe,
and PupilNet data sets consists of images sampled throughout a video based on the
pupil detection failure of a commercial eye tracker; these use cases can
be seen as videos with a low and inconstant sampling rate.
Results aggregated for all images are shown in Fig. REF  and
indicate PuRe as the most reliable and sufficient algorithm.
Curiously, the second most reliable algorithm was Świrski, indicating that during
use cases in which it was able to detect the pupil, it produced a more stable
signal than ElSe and ExCuSe – although its detection rate is much lower
relative to the other algorithms for challenging scenarios.
This lower detection rate reflects on the insufficiency, in which Świrski is
the worst performer; ElSe places second, followed by ExCuSe.
Furthermore, Fig. REF  details these results per use case.
In this scenario, PuRe was the most reliable algorithm in FORMULA  of the
use cases, followed by Świrski (FORMULA ), ElSe (FORMULA ), and ExCuSe
(FORMULA ).
These results demonstrate that PuRe is more reliable not only when taking
into account all images but also for the majority of use cases.
This higher reliability also reflects on PuRe's longest period of consecutive
correct pupil detections, which contained 859 frames (in LPW/21/12). In
contrast, the longest sequence for the rival was only 578 frames (ExCuSe,
also in LPW/21/12).
ElSe's longest period was of 386 frames in LPW/10/8, for which
PuRe managed 411 frames.
In terms of sufficiency – i.e., smaller insufficiency – ElSe
had a small lead with FORMULA  of use cases, closely followed by PuRe
(FORMULA ); ExCuSe and Świrski were far behind, winning FORMULA  and FORMULA 
of use cases, respectively.
The advantage of ElSe here is likely due to its second pupil detection step,
which might return the correct pupil during periods of mostly incorrect
detections, fragmenting these periods into smaller ones.
FIGURE 
FIGURE 

Run Time
The run time of pupil detection algorithms is of particular importance for
real-time usage – e.g., for human-computer interaction. In this section, we
evaluate the temporal performance of the algorithms across all images from the
Świrski, ExCuSe, ElSe, LPW, PupilNet, and Closed-Eyes data sets.
Evaluation was performed on a Intel® Core™
i5-4590 CPU @ 3.30GHz with 16GB RAM under Windows 8.1, which is similar to
systems employed by eye tracker vendors.
Results are shown in Fig. REF . All algorithms exhibited competitive
performance in terms of run time, conforming with the slack required for
operation with state-of-the-art head-mounted eye trackers. For instance, the
{{cite:8ec9b22a-2290-416f-bfe7-297fa52e2fea}} eye tracker, which provides images at 120 –
i.e., a slack of 8.33.
Henceforth, we will use the notation FORMULA  for the mean value and FORMULA  for
the standard deviation.
Run time wise, ExCuSe was the best performer (FORMULA , FORMULA ),
followed by Świrski (FORMULA , FORMULA ), PuRe (FORMULA ,
FORMULA ), and ElSe (FORMULA , FORMULA ).
It is worth noticing that ElSe operates on slightly larger images
([product-units=single]346 x 260) w.r.t. PuRe and ExCuSe
([product-units=single]320 x 240).
Furthermore, Świrski operates on the original image sizes, but its implementation
is parallelized using Intel Thread Building
Blocks {{cite:5b72bec4-8cbc-4c0a-b4e7-9e6feea5c46f}}, whereas the other algorithms were not
parallelized.
In contrast to the algorithmic approaches, {{cite:4f532262-85e7-4844-8bc0-8923af8564ff}} report
run times for their CNN-based approach of FORMULA  and
FORMULA  running on a NVidia Tesla K40 GPU and a
NVidia GTX 1060 GPU, respectively.
It is worth noticing that these run times are still more than four times larger
than the slack required by modern eye trackers and almost one order of magnitude
larger than the algorithmic approaches running on a CPU.
FIGURE 

Final Remarks
In this paper, we have proposed and evaluated PuRe, a novel edge-based
algorithm for pupil detection, which significantly improves on the
state-of-the-art in terms of sensitivity, precision, and
specificity by FORMULA , FORMULA , and FORMULA  percentage points,
respectively.
For the most challenging data sets, detection rate was improved by more than ten
percentage points.
PuRe operates in real-time for modern eye trackers (at 120 fps).
An additional contribution was made in the form of new metrics to evaluate
pupil detection algorithms.
Furthermore, results show that our single-method edge-based approach outperformed even
two-method approaches (e.g., ElSe and ExCuSe).
However, there are clear (but uncommon) cases when an edge-based approach will
not suffice due to lack of edge-information in the image.
For instance, extremely blurred images, or if a significant part of the pupil outline
is occluded.
In such cases, PuRe offers a meaningful confidence measure for the
detected pupil, which can be used to identify whenever PuRe failed.
Following from our analysis in Section REF , we recommend a threshold
of 0.66 for this confidence measure.
Thus, whenever PuRe can not find a pupil, an alternative pupil detection
method can be employed – e.g., ElSe's fast second step.
Nonetheless, care has to be taken not to compromise specificity through
this second step.
Moreover, there are extreme cases in which pupil detection might not be
feasible at all, such as when the bulk of the pupil is occluded due to
inadequate eye tracker placement relative to the eye.
For instance, use cases LPW/5/6 and LPW/4/1, for which the best
detection rates were measly FORMULA  (by ExCuSe) and FORMULA  (by Świrski),
respectively.
Sample images throughout these use cases are shown in Fig. REF .
As can be seen in this figure, in the former not only the eye is
out of focus, but there are lenses obstructing mostly of the pupil, whereas in
the latter, the pupil is mostly occluded by the eyelid and eye lashes.
In such cases, PuRe's confidence measure provides a quantitative measure of the
extend to which it can detect the pupil in current conditions: By observing the
ratio of confidence measures above the required threshold during a
periodThe period should be significantly larger than expected blink
durations since the confidence measure is also excepted to drop during blinks;
in this section we report the ratio for the whole use case..
If this ratio is too small, it can be inferred that either the pupil is not
visible or PuRe can not cope with current conditions.
In the former case, the user can be prompted to readjust the position of the eye
tracker in real time – this is the case for LPW/5/6
(FORMULA ) and LPW/4/1 (FORMULA ).
In both cases, the confidence measure ratio is useful for researchers to be
aware that the data is not reliable and requires further processing, such as
manual annotation.
An example of the cases in which adjusting the eye tracker is not likely to
improve detection rates is use case LPW/3/16
(FORMULA ), for which reflections cover most of the image as seen
in Fig. REF . The best detection rate for this use case was
FORMULA  (by PuRe).
To further support this claim, we measured the correlation between this
confidence-measure (FORMULA ) and the pupil detection rate, which
resulted in a correlation coefficient of 0.88.
FIGURE 
Envisioned future work includes investigating suitable non-edge-based second
steps, and developing a feedback system to prompt users to readjust the eye tracker
– to be integrated into EyeRecToo{{cite:f227f8ed-714d-4e5e-a8a4-1e94e3903cf6}}.
Moreover, the focus of this work is on pupil detection – even though we
formulated the resulting signal as tracking-by-detection in Section REF .
Tracking methods face many challenges in eye tracking due to the fast paced
changes in illumination, frequent pupil occlusion due to blinks, and substantial
amplitude and velocity of saccadic eye movements – as fast as
700/ {{cite:c08e9e55-ff3e-4a89-825a-d201493d534e}}.
Therefore, a comprehensive evaluation of tracking methods is out of the scope of
this paper and better left for future work.
Nonetheless, using temporal information (i.e., inter-frame) can greatly benefit
detection rates – albeit care has to be taken to track the right element in the
image. In this regard, PuRe's confidence metric provides a foundation that can
be used to build robust pupil trackers.
