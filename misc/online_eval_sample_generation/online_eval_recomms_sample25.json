[{"context": "As overall higher true positive rates indicate a more discriminating recommender, the area under the ROC curve (AUC score) is a simple indicator of the relative performance of the underlying model MAINCIT , with FORMULA being ideal, whereas FORMULA is no better than guessing (diagonal line in diagram).", "bow": [["2158698691", "An introduction to ROC analysis"], ["2157825442", "The meaning and use of the area under a receiver operating characteristic (ROC) curve."], ["2120100126", "AUC Optimization vs. Error Rate Minimization"], ["2102150307", "A method of comparing the areas under receiver operating characteristic curves derived from the same cases."], ["2083905053", "Optimising area under the ROC curve using gradient descent"]], "pp": [["2158698691", "An introduction to ROC analysis"], ["2157825442", "The meaning and use of the area under a receiver operating characteristic (ROC) curve."], ["2120100126", "AUC Optimization vs. Error Rate Minimization"], ["2102150307", "A method of comparing the areas under receiver operating characteristic curves derived from the same cases."], ["2083905053", "Optimising area under the ROC curve using gradient descent"]]}, {"context": "TABLE Evaluation Measures and Methodology To evaluate our proposal, we used a variant of the leave-one-out hold-out estimation called LeavePostOut MAINCIT .", "bow": [["2104446196", "How Good are Detection Proposals, really?"], ["7746136", "Edge Boxes: Locating Object Proposals from Edges"], ["1790312427", "Object-Proposal Evaluation Protocol is \u2018Gameable\u2019"], ["1958328135", "What Makes for Effective Detection Proposals"], ["1555385401", "Category independent object proposals"]], "pp": [["2104446196", "How Good are Detection Proposals, really?"], ["7746136", "Edge Boxes: Locating Object Proposals from Edges"], ["1790312427", "Object-Proposal Evaluation Protocol is \u2018Gameable\u2019"], ["1958328135", "What Makes for Effective Detection Proposals"], ["1555385401", "Category independent object proposals"]]}, {"context": "Huang et al. MAINCIT used an online active SVM(LASVM) to boost the speed of SVM classifier.", "bow": [["2135106139", "Fast Kernel Classifiers with Online and Active Learning"], ["2115364117", "Bundle Methods for Regularized Risk Minimization"], ["1950935069", "Crosstalk Cascades for Frame-Rate Pedestrian Detection"], ["2139224857", "P-packSVM: Parallel Primal grAdient desCent Kernel SVM"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]], "pp": [["1498437305", "Indexing similar DNA sequences"], ["2107330602", "Entropy inequalities for discrete channels"], ["2125895010", "Querying k-truss community in large and dynamic graphs"], ["2144012927", "Functional map networks for analyzing and exploring large shape collections"], ["2477205648", "Connectionist Temporal Modeling for Weakly Supervised Action Labeling"]], "np": [["1498437305", "Indexing similar DNA sequences"], ["2107330602", "Entropy inequalities for discrete channels"], ["2358307482", "Chinese Word Segmentation: A Decade Review"], ["2155493403", "Using eye tracking to investigate graph layout effects"], ["2045480089", "Randomized algorithms for tracking distributed count, frequencies, and ranks"]]}, {"context": "To overcome those problems, Huang et al. MAINCIT proposed a density-based clustering(DBSCAN) algorithm.", "bow": [["1673310716", "A density-based algorithm for discovering clusters in large spatial databases with noise"], ["2126751256", "Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications"], ["2160642098", "OPTICS: ordering points to identify the clustering structure"], ["1501500081", "A Survey of Clustering Data Mining Techniques"], ["151377110", "Density-Based Clustering Based on Hierarchical Density Estimates"]], "pp": [["1673310716", "A density-based algorithm for discovering clusters in large spatial databases with noise"], ["1498437305", "Indexing similar DNA sequences"], ["2125895010", "Querying k-truss community in large and dynamic graphs"], ["2144012927", "Functional map networks for analyzing and exploring large shape collections"], ["2477205648", "Connectionist Temporal Modeling for Weakly Supervised Action Labeling"]], "np": [["1498437305", "Indexing similar DNA sequences"], ["2107330602", "Entropy inequalities for discrete channels"], ["2358307482", "Chinese Word Segmentation: A Decade Review"], ["2155493403", "Using eye tracking to investigate graph layout effects"], ["2045480089", "Randomized algorithms for tracking distributed count, frequencies, and ranks"]]}, {"context": "DBSCAN is widely used for disambiguation, because it does not require a prior the number of clusters, and it resolves the transitivity problem MAINCIT .", "bow": [["1673310716", "A density-based algorithm for discovering clusters in large spatial databases with noise"], ["2126751256", "Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications"], ["2091656589", "Name disambiguation from link data in a collaboration graph"], ["2162337786", "Two supervised learning approaches for name disambiguation in author citations"], ["2091653681", "EMERGENCE OF COOPERATION AND ORGANIZATION IN AN EVOLUTIONARY GAME"]], "pp": [["1673310716", "A density-based algorithm for discovering clusters in large spatial databases with noise"], ["2126751256", "Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications"], ["2091656589", "Name disambiguation from link data in a collaboration graph"], ["2162337786", "Two supervised learning approaches for name disambiguation in author citations"], ["2091653681", "EMERGENCE OF COOPERATION AND ORGANIZATION IN AN EVOLUTIONARY GAME"]]}, {"context": "nearest neighbor search, quantization, source coding, high dimensional indexing, large databases Introduction Approximate nearest neighbors (ANN) search methods CIT are required to handle large databases, especially for computer vision MAINCIT and music retrieval CIT applications.", "bow": [["2427881153", "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"], ["2131929066", "On the Difficulty of Nearest Neighbor Search"], ["2167931879", "Anti-sparse coding for approximate nearest neighbor search"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"]], "pp": [["2427881153", "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"], ["2131929066", "On the Difficulty of Nearest Neighbor Search"], ["2167931879", "Anti-sparse coding for approximate nearest neighbor search"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"]]}, {"context": "Object vertices are connected to the corresponding attribute vertices, with additional edges created between the vertices containing low-level features, based on K-Nearest Neighbor search MAINCIT : for each feature vector, edges are created to its FORMULA closest neighbor vertex.", "bow": [["2051438793", "Parameterized computational complexity of finding small-diameter subgraphs"], ["2152971731", "Connectivity and generalized cliques in sociometric group structure"], ["2008292658", "Network Flow and Testing Graph Connectivity"], ["976211528", "Zur allgemeinen Kurventheorie"], ["2039715981", "A Polynomial Bound for Untangling Geometric Planar Graphs"]], "pp": [["2051438793", "Parameterized computational complexity of finding small-diameter subgraphs"], ["2152971731", "Connectivity and generalized cliques in sociometric group structure"], ["2008292658", "Network Flow and Testing Graph Connectivity"], ["976211528", "Zur allgemeinen Kurventheorie"], ["2039715981", "A Polynomial Bound for Untangling Geometric Planar Graphs"]]}, {"context": " Other adaptive algorithms, e.g., ADAM MAINCIT , ADADELTA CIT gave similar results.", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["6908809", "ADADELTA: An Adaptive Learning Rate Method"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["6908809", "ADADELTA: An Adaptive Learning Rate Method"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]]}, {"context": "The objective function in Equation REF is now completed with the belief update formulas (constraints), all of which are now summarized together as the following FORMULA subject to FORMULA It is worth noticing that the update in Equation REF is closely related to the \u201cword of mouth\u201d heuristic adopted in collaborative filtering MAINCIT .", "bow": [["2037881439", "Epidemic threshold in structured scale-free networks"], ["2123820136", "Percolation critical exponents in scale-free networks"], ["2043403353", "Fab: content-based, collaborative recommendation"], ["2142114717", "Large Scale Transductive SVMs"], ["1982853556", "Propositional belief base update and minimal change"]], "pp": [["2037881439", "Epidemic threshold in structured scale-free networks"], ["2123820136", "Percolation critical exponents in scale-free networks"], ["2043403353", "Fab: content-based, collaborative recommendation"], ["2142114717", "Large Scale Transductive SVMs"], ["1982853556", "Propositional belief base update and minimal change"]]}, {"context": "Using the heuristic, the final rating prediction is a weighted average across all similar users CIT and the similarity is usually measured by cosine similarity or Pearson's correlation coefficient and user means are used to remove the bias of mean ratings among users MAINCIT .", "bow": [["2113952909", "Thirteen Ways to Look at the Correlation Coefficient"], ["2120667260", "Exploring social influence via posterior effect of word-of-mouth recommendations"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["2085937320", "An algorithmic framework for performing collaborative filtering"], ["2124591829", "Social information filtering: algorithms for automating \u201cword of mouth\u201d"]], "pp": [["2113952909", "Thirteen Ways to Look at the Correlation Coefficient"], ["2120667260", "Exploring social influence via posterior effect of word-of-mouth recommendations"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["2085937320", "An algorithmic framework for performing collaborative filtering"], ["2124591829", "Social information filtering: algorithms for automating \u201cword of mouth\u201d"]]}, {"context": "Testing on a large scale is important, as most ANN methods are usually evaluated on sets of unrealistic size, thereby ignoring memory issues that arise in real applications, where billions of vectors have to be handled MAINCIT .", "bow": [["2002016471", "Artificial neural networks: a tutorial"], ["2225766585", "Nearest neighbor search : the old, the new, and the impossible"], ["2064671795", "Approximate nearest neighbor algorithms for Frechet distance via product metrics"], ["2411707397", "A Survey on Learning to Hash"], ["2092269560", "PERFORMANCE MEASURES FOR INFORMATION EXTRACTION"]], "pp": [["2002016471", "Artificial neural networks: a tutorial"], ["2225766585", "Nearest neighbor search : the old, the new, and the impossible"], ["2064671795", "Approximate nearest neighbor algorithms for Frechet distance via product metrics"], ["2411707397", "A Survey on Learning to Hash"], ["2092269560", "PERFORMANCE MEASURES FOR INFORMATION EXTRACTION"]]}, {"context": "CSSP (Column Subset Selection Problem) MAINCIT Datasets: We perform experiments on five textual real world datasets.", "bow": [["2093010960", "Clustered subset selection and its applications on it service metrics"], ["2582198730", "Bounds on Singular Values Revealed by QR Factorizations"], ["1787402657", "An improved approximation algorithm for the column subset selection problem"], ["2157290820", "Maximal independent subsets in Steiner systems and in planar sets"], ["1699753700", "Point Line Cover: The Easy Kernel is Essentially Tight"]], "pp": [["2093010960", "Clustered subset selection and its applications on it service metrics"], ["1787402657", "An improved approximation algorithm for the column subset selection problem"], ["2582198730", "Bounds on Singular Values Revealed by QR Factorizations"], ["2157290820", "Maximal independent subsets in Steiner systems and in planar sets"], ["1699753700", "Point Line Cover: The Easy Kernel is Essentially Tight"]], "np": [["1787402657", "An improved approximation algorithm for the column subset selection problem"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "Price labels were created using FORMULA \u2013means clustering MAINCIT of the logarithm of manufacturer suggested prices (MSRPs).", "bow": [["2176433868", "Multi-view K-means clustering on big data"], ["2096109742", "Uniform Price Auctions: Equilibria and Efficiency"], ["2132998716", "Pricing-based spectrum access control in cognitive radio networks with random access"], ["2088209891", "Networks of Scientific Papers"], ["2121907157", "Near-optimal network design with selfish agents"]], "pp": [["2176433868", "Multi-view K-means clustering on big data"], ["2096109742", "Uniform Price Auctions: Equilibria and Efficiency"], ["2132998716", "Pricing-based spectrum access control in cognitive radio networks with random access"], ["2088209891", "Networks of Scientific Papers"], ["2121907157", "Near-optimal network design with selfish agents"]]}, {"context": "A common technique to analyze the performance of such a binary classifier with variable threshold is ROC analysis MAINCIT .", "bow": [["2158698691", "An introduction to ROC analysis"], ["2096942889", "Robust Classification for Imprecise Environments"], ["2143017915", "The geometry of ROC space: understanding machine learning metrics through ROC isometrics"], ["1521843029", "ROC Graphs: Notes and Practical Considerations for Researchers"], ["1976526581", "The relationship between Precision-Recall and ROC curves"]], "pp": [["1536719366", "Signal detection theory and ROC analysis"], ["2158698691", "An introduction to ROC analysis"], ["2096942889", "Robust Classification for Imprecise Environments"], ["2143017915", "The geometry of ROC space: understanding machine learning metrics through ROC isometrics"], ["1521843029", "ROC Graphs: Notes and Practical Considerations for Researchers"]], "np": [["1536719366", "Signal detection theory and ROC analysis"], ["2166290812", "PAV and the ROC convex hull"], ["2158698691", "An introduction to ROC analysis"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"]]}, {"context": "Our current compression system can be improved to use more efficient compression algorithms which rely on building inverted indices in different ways with tree (B-Trees) CIT CIT MAINCIT CIT .", "bow": [["2150434953", "Sample Compression, Learnability, and the Vapnik-Chervonenkis Dimension"], ["2889395214", "Managing gigabytes"], ["84082900", "Compressing and indexing documents and images"], ["2039742379", "The inverted multi-index"], ["2889385527", "No occurrence obstructions in geometric complexity theory"]], "pp": [["2150434953", "Sample Compression, Learnability, and the Vapnik-Chervonenkis Dimension"], ["2889395214", "Managing gigabytes"], ["84082900", "Compressing and indexing documents and images"], ["2039742379", "The inverted multi-index"], ["2889385527", "No occurrence obstructions in geometric complexity theory"]]}, {"context": "In CF, a generally adopted similarity measure is called Pearson Correlation which measures the extent to which two variables linearly relate with each other MAINCIT .", "bow": [["2296033553", "On the Criterion that a Given System of Deviations from the Probable in the Case of a Correlated System of Variables is Such that it Can be Reasonably Supposed to have Arisen from Random Sampling"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["2313339984", "A concordance correlation coefficient to evaluate reproducibility."], ["1539057251", "Ordinal Boltzmann Machines for collaborative filtering"], ["2072528130", "Maximum Margin Correlation Filter: A New Approach for Localization and Classification"]], "pp": [["2296033553", "On the Criterion that a Given System of Deviations from the Probable in the Case of a Correlated System of Variables is Such that it Can be Reasonably Supposed to have Arisen from Random Sampling"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["2313339984", "A concordance correlation coefficient to evaluate reproducibility."], ["1539057251", "Ordinal Boltzmann Machines for collaborative filtering"], ["2072528130", "Maximum Margin Correlation Filter: A New Approach for Localization and Classification"]]}, {"context": "A weighted sum is then taken to predict the rating for target user FORMULA on a certain item FORMULA MAINCIT FORMULA Recommenders based on collaborative filtering then refer to this prediction to provide the top-FORMULA recommendations to the user.", "bow": [["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"], ["2042281163", "Item-based collaborative filtering recommendation algorithms"], ["2127480961", "Content-based book recommending using learning for text categorization"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["2043403353", "Fab: content-based, collaborative recommendation"]], "pp": [["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"], ["2042281163", "Item-based collaborative filtering recommendation algorithms"], ["2127480961", "Content-based book recommending using learning for text categorization"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["2043403353", "Fab: content-based, collaborative recommendation"]]}, {"context": "For other items, we can use rank score as an influence measure to make predictions, which is similar to memory-based collaborative filtering, using Pearson Correlation MAINCIT as a similarity measure between users and items.", "bow": [["1560991565", "An open architecture for collaborative filtering of netnews"], ["2142144955", "Item-based top- N recommendation algorithms"], ["2042281163", "Item-based collaborative filtering recommendation algorithms"], ["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"], ["2085937320", "An algorithmic framework for performing collaborative filtering"]], "pp": [["1560991565", "An open architecture for collaborative filtering of netnews"], ["2142144955", "Item-based top- N recommendation algorithms"], ["2042281163", "Item-based collaborative filtering recommendation algorithms"], ["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"], ["2085937320", "An algorithmic framework for performing collaborative filtering"]]}, {"context": "Soundex: Convert each string with Soundex algorithm MAINCIT and then do an exact string match giving credit for phonetically similar strings.", "bow": [["2001496424", "A guided tour to approximate string matching"], ["41404523", "String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage."], ["1555798330", "Word equations with length constraints: what's decidable?"], ["1994402784", "Lp String Stability of Cascaded Systems: Application to Vehicle Platooning"], ["2123234403", "Linear-time computation of local periods"]], "pp": [["2001496424", "A guided tour to approximate string matching"], ["41404523", "String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage."], ["1555798330", "Word equations with length constraints: what's decidable?"], ["1994402784", "Lp String Stability of Cascaded Systems: Application to Vehicle Platooning"], ["2123234403", "Linear-time computation of local periods"]]}, {"context": "To get an idea of the state space, it is not hard to see that there are FORMULA ways to partition and order FORMULA where FORMULA is the number of possible ways to divide a set of FORMULA objects into FORMULA partitions, otherwise known as Stirling numbers of second kind MAINCIT .", "bow": [["2029948740", "Concrete Mathematics: A Foundation for Computer Science"], ["2109548990", "Deciding DPDA Equivalence Is Primitive Recursive"], ["2895114256", "Introductory Combinatorics"], ["2097565928", "Asymptotic estimates of Stirling numbers"], ["2115529864", "A Bayesian View of the Poisson-Dirichlet Process"]], "pp": [["2895114256", "Introductory Combinatorics"], ["2097565928", "Asymptotic estimates of Stirling numbers"], ["2029948740", "Concrete Mathematics: A Foundation for Computer Science"], ["2120062331", "Handbook of Mathematical Functions"], ["2109548990", "Deciding DPDA Equivalence Is Primitive Recursive"]], "np": [["2895114256", "Introductory Combinatorics"], ["1599335667", "A Course in Combinatorics"], ["1978026911", "On the Product of Independent Complex Gaussians"], ["2097565928", "Asymptotic estimates of Stirling numbers"], ["32262330", "Combinatorics: Topics, Techniques, Algorithms"]]}, {"context": "Convolutional neural networks have also been shown to be highly effective for natural language processing and have achieved excellent results in information retrieval CIT , semantic parsing MAINCIT , sentence modeling CIT and other traditional natural language processing tasks CIT .", "bow": [["2120615054", "A Convolutional Neural Network for Modelling Sentences"], ["1423339008", "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"], ["179875071", "Recurrent neural network based language model"], ["1869752048", "Grammar as a foreign language"], ["2158899491", "Natural Language Processing (Almost) from Scratch"]], "pp": [["2251957808", "Building a Semantic Parser Overnight"], ["2251673953", "Scaling Semantic Parsers with On-the-Fly Ontology Matching"], ["1496189301", "Learning to map sentences to logical form: structured classification with probabilistic categorial grammars"], ["2250225488", "Semantic Parsing via Paraphrasing"], ["2250623140", "Broad-coverage CCG Semantic Parsing with AMR"]], "np": [["2250748818", "A* CCG Parsing with a Supertag-factored Model"], ["2250623140", "Broad-coverage CCG Semantic Parsing with AMR"], ["2251957808", "Building a Semantic Parser Overnight"], ["1496189301", "Learning to map sentences to logical form: structured classification with probabilistic categorial grammars"], ["2251673953", "Scaling Semantic Parsers with On-the-Fly Ontology Matching"]]}, {"context": "Distributional Semantics for IR In this section we first introduce the Continuous Bag-of-Words (CBOW) model made popular by the software Word2Vec MAINCIT .", "bow": [["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["2611099133", "Neural Models for Information Retrieval."], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"]], "pp": [["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["2611099133", "Neural Models for Information Retrieval."], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"]]}, {"context": "Firstly, much of the existing literature MAINCIT on CBOW and SG uses cosine similarity and normalized unit vectors (for performing vector algebra for word analogies).", "bow": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1970997001", "A New Class of Incremental Gradient Methods for Least Squares Problems"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2164637474", "Correlated-Q learning"], ["2387546565", "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks"]], "pp": [["1781640942", "Some Effectivity Problems in Polynomial Ideal Theory"], ["2115044435", "Efficient Exploration and Value Function Generalization in Deterministic Systems"], ["1561838634", "Probabilistic Lossy Channel Systems"], ["2121219412", "Quickest detection in censoring sensor networks"], ["2158617780", "Evaluation of Gender Classification Methods with Automatically Detected and Aligned Faces"]], "np": [["1561838634", "Probabilistic Lossy Channel Systems"], ["1781640942", "Some Effectivity Problems in Polynomial Ideal Theory"], ["2115044435", "Efficient Exploration and Value Function Generalization in Deterministic Systems"], ["2026318959", "Want to be Retweeted? Large Scale Analytics on Factors Impacting Retweet in Twitter Network"], ["2096610849", "Verification of probabilistic systems with faulty communication"]]}, {"context": "Neural embeddings for IR The word embeddings produced by the CBOW and SG models have been shown to be surprisingly effective at capturing detailed semantics useful for various Natural Language Processing (NLP) and reasoning tasks, including word analogies MAINCIT .", "bow": [["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."], ["2251103205", "Semantic Clustering and Convolutional Neural Network for Short Text Categorization"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2587019100", "Comparative Study of CNN and RNN for Natural Language Processing."]], "pp": [["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."], ["2251103205", "Semantic Clustering and Convolutional Neural Network for Short Text Categorization"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2587019100", "Comparative Study of CNN and RNN for Natural Language Processing."]]}, {"context": "Although IN-IN, for example, works well for word analogy tasks MAINCIT , it might perform less effectively for other tasks, such as those in information retrieval.", "bow": [["2572730214", "Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis"], ["2015419638", "A basis for information retrieval in context"], ["2141599568", "Linguistic Regularities in Continuous Space Word Representations"], ["2252211741", "Evaluation methods for unsupervised word embeddings"], ["1615991656", "Improving Distributional Similarity with Lessons Learned from Word Embeddings"]], "pp": [["2572730214", "Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis"], ["2015419638", "A basis for information retrieval in context"], ["2141599568", "Linguistic Regularities in Continuous Space Word Representations"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["2252211741", "Evaluation methods for unsupervised word embeddings"]], "np": [["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["2141599568", "Linguistic Regularities in Continuous Space Word Representations"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "Recent papers have explored in detail the SG and CBOW training methodology CIT and its connection to other approaches for learning word embeddings such as explicit vector space representations MAINCIT , matrix factorization CIT and density-based representations CIT .", "bow": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "pp": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "np": [["2611511037", "Computable Analysis"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "nearest neighbor search, quantization, source coding, high dimensional indexing, large databases Introduction Approximate nearest neighbors (ANN) search methods MAINCIT are required to handle large databases, especially for computer vision CIT and music retrieval CIT applications.", "bow": [["2427881153", "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"], ["2131929066", "On the Difficulty of Nearest Neighbor Search"], ["2167931879", "Anti-sparse coding for approximate nearest neighbor search"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"]], "pp": [["2427881153", "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"], ["2131929066", "On the Difficulty of Nearest Neighbor Search"], ["2167931879", "Anti-sparse coding for approximate nearest neighbor search"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"]], "np": [["2008995227", "Approximate Nearest Subspace Search"], ["2171790913", "Kernelized locality-sensitive hashing for scalable image search"], ["1502916507", "Similarity Search in High Dimensions via Hashing"], ["2293597654", "Spectral Hashing"], ["2124509324", "Product Quantization for Nearest Neighbor Search"]]}, {"context": "This is always done in partitioning based method such as LSH CIT or FLANN MAINCIT , as the neighbor hypotheses are not ranked on output of the index.", "bow": [["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"], ["1627400044", "Fast approximate nearest neighbors with automatic algorithm configuration"], ["2146020873", "Fast Matching of Binary Features"], ["2055839530", "Modeling LSH for performance tuning"], ["1898304433", "Spherical lsh for approximate nearest neighbor search on unit hypersphere"]], "pp": [["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"], ["1627400044", "Fast approximate nearest neighbors with automatic algorithm configuration"], ["2146020873", "Fast Matching of Binary Features"], ["2055839530", "Modeling LSH for performance tuning"], ["1898304433", "Spherical lsh for approximate nearest neighbor search on unit hypersphere"]]}, {"context": "We focus on the method presented in CIT , which offers state-of-the-art performance, outperforming the FLANN which was previously shown to outperform LSH MAINCIT .", "bow": [["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"], ["1627400044", "Fast approximate nearest neighbors with automatic algorithm configuration"], ["2146020873", "Fast Matching of Binary Features"], ["2055839530", "Modeling LSH for performance tuning"], ["1898304433", "Spherical lsh for approximate nearest neighbor search on unit hypersphere"]], "pp": [["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"], ["1627400044", "Fast approximate nearest neighbors with automatic algorithm configuration"], ["2146020873", "Fast Matching of Binary Features"], ["2055839530", "Modeling LSH for performance tuning"], ["1898304433", "Spherical lsh for approximate nearest neighbor search on unit hypersphere"]]}, {"context": "Re-ranking neighbors using source coding Refinement: principle The objective of the method proposed in this paper is to avoid the costly post-verification scheme adopted in most state-of-the-art approximate search techniques MAINCIT .", "bow": [["2114085948", "Information transmission with additional noise"], ["2101085206", "Refinement types for ML"], ["2114990184", "An axiomatization of Borda's rule"], ["2120342618", "A unified approach to ranking in probabilistic databases"], ["2076188996", "Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors"]], "pp": [["2054358802", "Learning Markov networks: maximum bounded tree-width graphs"], ["2127366419", "Dynamic Euclidean minimum spanning trees and extrema of binary functions"], ["2295690548", "Imitation Learning of Agenda-based Semantic Parsers"], ["1667614912", "A new method for solving hard satisfiability problems"], ["1589232796", "Programming with Constraints: An Introduction"]], "np": [["2054358802", "Learning Markov networks: maximum bounded tree-width graphs"], ["2127366419", "Dynamic Euclidean minimum spanning trees and extrema of binary functions"], ["1667614912", "A new method for solving hard satisfiability problems"], ["2295690548", "Imitation Learning of Agenda-based Semantic Parsers"], ["1589232796", "Programming with Constraints: An Introduction"]]}, {"context": "to the \u201cprecision\u201d measure used in MAINCIT to evaluate ANN methods.", "bow": [["2002016471", "Artificial neural networks: a tutorial"], ["2225766585", "Nearest neighbor search : the old, the new, and the impossible"], ["2092269560", "PERFORMANCE MEASURES FOR INFORMATION EXTRACTION"], ["2064671795", "Approximate nearest neighbor algorithms for Frechet distance via product metrics"], ["2017851434", "Optimal Data-Dependent Hashing for Approximate Near Neighbors"]], "pp": [["2002016471", "Artificial neural networks: a tutorial"], ["2225766585", "Nearest neighbor search : the old, the new, and the impossible"], ["2092269560", "PERFORMANCE MEASURES FOR INFORMATION EXTRACTION"], ["2064671795", "Approximate nearest neighbor algorithms for Frechet distance via product metrics"], ["2017851434", "Optimal Data-Dependent Hashing for Approximate Near Neighbors"]]}, {"context": "FIGURE TABLE Clustering Using DBSCAN We use a density-based clustering algorithm, DBSCAN MAINCIT to cluster inventor records.", "bow": [["1673310716", "A density-based algorithm for discovering clusters in large spatial databases with noise"], ["2126751256", "Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications"], ["151377110", "Density-Based Clustering Based on Hierarchical Density Estimates"], ["2160642098", "OPTICS: ordering points to identify the clustering structure"], ["1969642980", "Density-based clustering of uncertain data"]], "pp": [["1673310716", "A density-based algorithm for discovering clusters in large spatial databases with noise"], ["2126751256", "Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications"], ["151377110", "Density-Based Clustering Based on Hierarchical Density Estimates"], ["2160642098", "OPTICS: ordering points to identify the clustering structure"], ["1969642980", "Density-based clustering of uncertain data"]]}, {"context": "This definition can also be used to demonstrate the inadequacy of folk designs for private queries: in the first design, a client queries an untrusted database by looking up the sought record along with other `dummy' records MAINCIT to confuse the adversary; in the second design, a client fetches a record from an untrusted database through an anonymity system CIT to hide the correspondence between the client and the server.", "bow": [["2137552535", "Two-Hop Secure Communication Using an Untrusted Relay: A Case for Cooperative Jamming"], ["1557386445", "Non-interactive verifiable computing: outsourcing computation to untrusted workers"], ["2052267638", "Random oracles are practical: a paradigm for designing efficient protocols"], ["5855678", "Differentially private continual monitoring of heavy hitters from distributed streams"], ["2097480027", "Cooperation With an Untrusted Relay: A Secrecy Perspective"]], "pp": [["2137552535", "Two-Hop Secure Communication Using an Untrusted Relay: A Case for Cooperative Jamming"], ["1557386445", "Non-interactive verifiable computing: outsourcing computation to untrusted workers"], ["2052267638", "Random oracles are practical: a paradigm for designing efficient protocols"], ["5855678", "Differentially private continual monitoring of heavy hitters from distributed streams"], ["2097480027", "Cooperation With an Untrusted Relay: A Secrecy Perspective"]]}, {"context": "Naive Dummy Requests A number of works attempt to hide a true user query to a single untrusted database, by hiding it among a number FORMULA of artificially generated user queries (`dummies') to achieve some privacy; for example OB-PWS CIT in the context of web search, and Hong and Landay CIT and Kido et al. MAINCIT in the context of private location queries.", "bow": [["2154592552", "Protection of Location Privacy using Dummies for Location-based Services"], ["2056773559", "Anonymous Usage of Location-Based Services Through Spatial and Temporal Cloaking"], ["1586209290", "Iterative constructions and private data release"], ["1689932539", "An anonymous communication technique using dummies for location-based services"], ["2169570643", "A learning theory approach to non-interactive database privacy"]], "pp": [["2154592552", "Protection of Location Privacy using Dummies for Location-based Services"], ["2056773559", "Anonymous Usage of Location-Based Services Through Spatial and Temporal Cloaking"], ["1586209290", "Iterative constructions and private data release"], ["1689932539", "An anonymous communication technique using dummies for location-based services"], ["2169570643", "A learning theory approach to non-interactive database privacy"]]}, {"context": "Introduction In recent years Deep Belief Networks have achieved remarkable results in natural language processing MAINCIT , computer vision CIT CIT and speech recognition CIT tasks.", "bow": [["2160815625", "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups"], ["2147768505", "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"], ["2091432990", "New types of deep neural network learning for speech recognition and related applications: an overview"], ["2184045248", "Deep Neural Networks for Acoustic Modeling in Speech Recognition"], ["1984541135", "Recent advances in deep learning for speech research at Microsoft"]], "pp": [["2117130368", "A unified architecture for natural language processing: deep neural networks with multitask learning"], ["2250644439", "A Recursive Recurrent Neural Network for Statistical Machine Translation"], ["1579838312", "Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition"], ["2163361328", "Crowdsourcing Translation: Professional Quality from Non-Professionals"], ["2158899491", "Natural Language Processing (Almost) from Scratch"]], "np": [["1989115577", "Dependency Parsing by Belief Propagation"], ["2050273484", "Understanding the Value of Features for Coreference Resolution"], ["2099938389", "Wikipedia-based semantic interpretation for natural language processing"], ["2126706529", "large scale canonical correlation analysis with iterative least squares"], ["2162942021", "Regression approaches for microarray data analysis."]]}, {"context": "TABLE Model Architecture The model architecture shown in Figure REF , follows CIT and MAINCIT .", "bow": [["2162366870", "A classification and comparison framework for software architecture description languages"], ["2157418168", "Service-oriented computing: concepts, characteristics and directions"], ["2498153988", "The World Wide Web"], ["2556833785", "Designing Neural Network Architectures using Reinforcement Learning"], ["2293909219", "Algebraic statistics"]], "pp": [["2162366870", "A classification and comparison framework for software architecture description languages"], ["2157418168", "Service-oriented computing: concepts, characteristics and directions"], ["2498153988", "The World Wide Web"], ["2556833785", "Designing Neural Network Architectures using Reinforcement Learning"], ["2293909219", "Algebraic statistics"]]}, {"context": "Modern applications of multilabel learning are motivated by recommendation and ranking problems; for instance, in MAINCIT each search engine query is treated as a label and the task is to get the most relevant queries to a given webpage.", "bow": [["605727707", "Ensemble Methods: Foundations and Algorithms"], ["2107753812", "A family of additive online algorithms for category ranking"], ["2145827727", "An extensive experimental comparison of methods for multi-label learning"], ["2097645701", "Cluster ensembles --- a knowledge reuse framework for combining multiple partitions"], ["2115119296", "Multi-Label Prediction via Compressed Sensing"]], "pp": [["605727707", "Ensemble Methods: Foundations and Algorithms"], ["2107753812", "A family of additive online algorithms for category ranking"], ["2145827727", "An extensive experimental comparison of methods for multi-label learning"], ["2097645701", "Cluster ensembles --- a knowledge reuse framework for combining multiple partitions"], ["2115119296", "Multi-Label Prediction via Compressed Sensing"]]}, {"context": "Another suitable criteria that is applicable to our problem is proposed by Johnson and Zhang MAINCIT in 2014, where they propose a similar model, but swapped in high dimensional `one-hot' vector representations of words as CNN inputs.", "bow": [["2142293082", "A new upper bound for error-correcting codes"], ["1554096626", "New Improvements on the Echelon-Ferrers Construction"], ["39763826", "Worst case behavior of graph coloring algorithms"], ["1568510176", "Johnson type bounds on constant dimension codes"], ["33983427", "New upper bounds for error correcting codes"]], "pp": [["1810499140", "Semi-supervised convolutional neural networks for text categorization via region embedding"], ["2107438106", "Accelerating Stochastic Gradient Descent using Predictive Variance Reduction"], ["2254361154", "Supervised and semi-supervised text categorization using LSTM for region embeddings"], ["2142293082", "A new upper bound for error-correcting codes"], ["1841724727", "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks"]], "np": [["1810499140", "Semi-supervised convolutional neural networks for text categorization via region embedding"], ["2107438106", "Accelerating Stochastic Gradient Descent using Predictive Variance Reduction"], ["2254361154", "Supervised and semi-supervised text categorization using LSTM for region embeddings"], ["1841724727", "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks"], ["100047375", "Parallel algorithm configuration"]]}, {"context": "Researchers have attempted to infer attributes of Twitter users such as age CIT , gender CIT , political orientation MAINCIT or a range of social identities CIT .", "bow": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]], "pp": [["2017729405", "Classifying latent user attributes in twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["9292421", "Discriminating Gender on Twitter"], ["2013416264", "Democrats, republicans and starbucks afficionados: user classification in twitter"], ["1014449310", "Homophily and Latent Attribute Inference: Inferring Latent Attributes of Twitter Users from Neighbors"]], "np": [["2017729405", "Classifying latent user attributes in twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2013416264", "Democrats, republicans and starbucks afficionados: user classification in twitter"], ["1014449310", "Homophily and Latent Attribute Inference: Inferring Latent Attributes of Twitter Users from Neighbors"], ["68298479", "Classifying Political Orientation on Twitter: It\u2019s Not Easy!"]]}, {"context": "In contrast to the classical vector space model, LSA CIT , PLSA CIT and LDA MAINCIT learn dense vector representations of much lower dimensionality.", "bow": [["2134731454", "Unsupervised Learning by Probabilistic Latent Semantic Analysis"], ["2091273188", "Using latent semantic analysis to improve access to textual information"], ["2130324521", "Exploring Topic Coherence over Many Models and Many Topics"], ["2158997610", "An introduction to latent semantic analysis"], ["1985809919", "A direct LDA algorithm for high-dimensional data \u2014 with application to face recognition"]], "pp": [["2134731454", "Unsupervised Learning by Probabilistic Latent Semantic Analysis"], ["2091273188", "Using latent semantic analysis to improve access to textual information"], ["2130324521", "Exploring Topic Coherence over Many Models and Many Topics"], ["2158997610", "An introduction to latent semantic analysis"], ["1985809919", "A direct LDA algorithm for high-dimensional data \u2014 with application to face recognition"]]}, {"context": "This can be invaluable when curation of the tweet stream is needed for applications such as country-specific trending topic detection MAINCIT , or for more specific applications where only tweets coming from a specific country are sought, e.g. sentiment analysis or reputation management CIT .", "bow": [["2084591134", "Information credibility on twitter"], ["2252215182", "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["2004192095", "Improving LDA topic models for microblogs via tweet pooling and automatic labeling"], ["2168400688", "Twevent: segment-based event detection from tweets"]], "pp": [["1983012012", "Emerging topic detection on Twitter based on temporal and social terms evaluation"], ["2137553870", "Finding Bursty Topics from Microblogs"], ["2084591134", "Information credibility on twitter"], ["2252215182", "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"]], "np": [["1983012012", "Emerging topic detection on Twitter based on temporal and social terms evaluation"], ["2137553870", "Finding Bursty Topics from Microblogs"], ["2154359981", "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification"], ["2124499489", "Earthquake shakes Twitter users: real-time event detection by social sensors"], ["100047375", "Parallel algorithm configuration"]]}, {"context": "Introduction In recent years Deep Belief Networks have achieved remarkable results in natural language processing CIT , computer vision MAINCIT CIT and speech recognition CIT tasks.", "bow": [["2160815625", "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups"], ["2147768505", "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"], ["2091432990", "New types of deep neural network learning for speech recognition and related applications: an overview"], ["2184045248", "Deep Neural Networks for Acoustic Modeling in Speech Recognition"], ["1984541135", "Recent advances in deep learning for speech research at Microsoft"]], "pp": [["2160815625", "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups"], ["2147768505", "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"], ["2091432990", "New types of deep neural network learning for speech recognition and related applications: an overview"], ["2184045248", "Deep Neural Networks for Acoustic Modeling in Speech Recognition"], ["1984541135", "Recent advances in deep learning for speech research at Microsoft"]]}, {"context": "Introduction In recent years Deep Belief Networks have achieved remarkable results in natural language processing CIT , computer vision CIT CIT and speech recognition MAINCIT tasks.", "bow": [["2160815625", "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups"], ["2147768505", "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"], ["2091432990", "New types of deep neural network learning for speech recognition and related applications: an overview"], ["2184045248", "Deep Neural Networks for Acoustic Modeling in Speech Recognition"], ["1984541135", "Recent advances in deep learning for speech research at Microsoft"]], "pp": [["2160815625", "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups"], ["2147768505", "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"], ["2091432990", "New types of deep neural network learning for speech recognition and related applications: an overview"], ["2184045248", "Deep Neural Networks for Acoustic Modeling in Speech Recognition"], ["2143612262", "Speech recognition with deep recurrent neural networks"]], "np": [["1512626953", "Computation of the probability of initial substring generation by stochastic context-free grammars"], ["1601654835", "Models of Peano Arithmetic"], ["1895481600", "Speech Recognition with Weighted Finite-State Transducers"], ["2063224314", "Recurrent deep neural networks for robust speech recognition"], ["2121486117", "Audio-visual speech modeling for continuous speech recognition"]]}, {"context": "We have employed dropout for regularization on the penultimate layer MAINCIT .", "bow": [["2095705004", "Dropout: a simple way to prevent neural networks from overfitting"], ["1904365287", "Improving neural networks by preventing co-adaptation of feature detectors"], ["2136836265", "Adaptive dropout for training deep neural networks"], ["2164370980", "The dropout learning algorithm"], ["2292443655", "RNNDROP: A novel dropout for RNNS in ASR"]], "pp": [["1904365287", "Improving neural networks by preventing co-adaptation of feature detectors"], ["2095705004", "Dropout: a simple way to prevent neural networks from overfitting"], ["2136836265", "Adaptive dropout for training deep neural networks"], ["2164370980", "The dropout learning algorithm"], ["2292443655", "RNNDROP: A novel dropout for RNNS in ASR"]], "np": [["2203224402", "Factors of Transferability for a Generic ConvNet Representation"], ["204268067", "Neural Codes for Image Retrieval"], ["2120615054", "A Convolutional Neural Network for Modelling Sentences"], ["1904365287", "Improving neural networks by preventing co-adaptation of feature detectors"], ["1832693441", "Convolutional Neural Networks for Sentence Classification"]]}, {"context": "AS Internet The Internet topology at the autonomous systems (AS) level has been extensively studied in recent years MAINCIT .", "bow": [["2050353626", "Towards capturing representative AS-level Internet topologies"], ["2139905147", "The internet AS-level topology: three data sources and one definitive metric"], ["2151972741", "On inferring autonomous system relationships in the internet"], ["2120514843", "AS relationships: inference and validation"], ["2165503871", "Scalable and accurate identification of AS-level forwarding paths"]], "pp": [["2050353626", "Towards capturing representative AS-level Internet topologies"], ["2139905147", "The internet AS-level topology: three data sources and one definitive metric"], ["2151972741", "On inferring autonomous system relationships in the internet"], ["2155589809", "Topology discovery by active probing"], ["2120514843", "AS relationships: inference and validation"]], "np": [["1495663756", "Signal/collect: graph algorithms for the (semantic) web"], ["1513398909", "Identifying expressions of emotion in text"], ["1564629734", "Computability, inference and modeling in probabilistic programming"], ["1694262664", "The Komlos Conjecture Holds for Vector Colorings"], ["1793674997", "Edge routing with ordered bundles"]]}, {"context": "Nowadays the growth in popularity of social media sites has made the area of recommender systems for social tagging systems an active and growing topic of research MAINCIT .", "bow": [["2093219534", "Learning to recommend with social trust ensemble"], ["2126539515", "Circle-based recommendation in online social networks"], ["2084527756", "TrustWalker : a random walk model for combining trust-based and item-based recommendation"], ["2144487656", "Recommender systems with social regularization"], ["2062261051", "A survey on predicting the popularity of web content"]], "pp": [["2093219534", "Learning to recommend with social trust ensemble"], ["2126539515", "Circle-based recommendation in online social networks"], ["2084527756", "TrustWalker : a random walk model for combining trust-based and item-based recommendation"], ["2144487656", "Recommender systems with social regularization"], ["2062261051", "A survey on predicting the popularity of web content"]]}, {"context": "icio.us, movielens, and last.fm from HetRec 2011 CIT and the two other ones from Bibsonomy : a post-core at level 5 and a one at level 2 MAINCIT .", "bow": [["2162943632", "An analysis of tag-recommender evaluation procedures"], ["2157989362", "Gaussian Interference Channel Capacity to Within One Bit"], ["1971717360", "Distributed k-Core Decomposition"], ["1979393293", "Distance Regularized Level Set Evolution and Its Application to Image Segmentation"], ["645058498", "Survival and Event History Analysis: A Process Point of View"]], "pp": [["2140476040", "Safety is not a restriction at level 2 for string languages"], ["2013085432", "Classifying regular events in symbolic logic"], ["2162943632", "An analysis of tag-recommender evaluation procedures"], ["2157989362", "Gaussian Interference Channel Capacity to Within One Bit"], ["1971717360", "Distributed k-Core Decomposition"]], "np": [["2140476040", "Safety is not a restriction at level 2 for string languages"], ["2013085432", "Classifying regular events in symbolic logic"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "TABLE Evaluation Measures and Methodology To evaluate our proposal, we used a variant of the leave-one-out hold-out estimation called LeavePostOut MAINCIT .", "bow": [["2104446196", "How Good are Detection Proposals, really?"], ["7746136", "Edge Boxes: Locating Object Proposals from Edges"], ["1790312427", "Object-Proposal Evaluation Protocol is \u2018Gameable\u2019"], ["1958328135", "What Makes for Effective Detection Proposals"], ["1555385401", "Category independent object proposals"]], "pp": [["2104446196", "How Good are Detection Proposals, really?"], ["7746136", "Edge Boxes: Locating Object Proposals from Edges"], ["1790312427", "Object-Proposal Evaluation Protocol is \u2018Gameable\u2019"], ["1958328135", "What Makes for Effective Detection Proposals"], ["1555385401", "Category independent object proposals"]]}, {"context": "Digging more deeply into the demographics of Twitter users, other researchers have attempted to infer socioeconomic demographics such as occupational class CIT , income MAINCIT and socioeconomic status CIT .", "bow": [["2144364794", "Discovering Sociolinguistic Associations with Structured Sparsity"], ["1948823840", "Studying User Income through Language, Behaviour and Affect in Social Media."], ["2104925568", "Estimating county health statistics with twitter"], ["2166434810", "An analysis of the user occupational class through Twitter content"], ["2119595472", "Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach"]], "pp": [["2144364794", "Discovering Sociolinguistic Associations with Structured Sparsity"], ["1948823840", "Studying User Income through Language, Behaviour and Affect in Social Media."], ["2104925568", "Estimating county health statistics with twitter"], ["2166434810", "An analysis of the user occupational class through Twitter content"], ["2119595472", "Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach"]]}, {"context": "A common application of deep learning to IR is for learning to rank, covering a wide range of subtopics within this area, for instance from the earlier RankNet CIT , and methods for hyperlinked webpages CIT , to studies of listwise comparisons CIT , short text pairwise reranking MAINCIT , and elimination strategies CIT .", "bow": [["2149427297", "Learning to Rank for Information Retrieval"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"]], "pp": [["2149427297", "Learning to Rank for Information Retrieval"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"]]}, {"context": "Collaborative-filtering based recommender systems MAINCIT suffer from the \u201ccold-start problem\u201d CIT : Individual predictions for new customers and articles cannot be calculated in the absence of prior purchase data.", "bow": [["2100235918", "A survey of collaborative filtering techniques"], ["2171960770", "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions"], ["1690919088", "Recommender Systems Handbook"], ["1966553486", "Using collaborative filtering to weave an information tapestry"], ["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"]], "pp": [["1690919088", "Recommender Systems Handbook"], ["2159721641", "Research Objects: Towards Exchange and Reuse of Digital Knowledge."], ["2072992969", "Time-aware recommender systems: a comprehensive survey and analysis of existing evaluation protocols"], ["2171960770", "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions"], ["2115186087", "Recommender Systems Research: A Connection-Centric Survey"]], "np": [["2047870719", "Topology representing networks"], ["2072992969", "Time-aware recommender systems: a comprehensive survey and analysis of existing evaluation protocols"], ["2115186087", "Recommender Systems Research: A Connection-Centric Survey"], ["2159721641", "Research Objects: Towards Exchange and Reuse of Digital Knowledge."], ["2161308157", "A lower-bound on the number of rankings required in recommender systems using collaborativ filtering"]]}, {"context": "Average-Precision FORMULA (FORMULA ) Average-Precision FORMULA ( MAINCIT ) (or FORMULA ) extends on Kendall's FORMULA by incorporating the position of errors.", "bow": [["1510632636", "Socializing the Semantic Gap: A Comparative Survey on Image Tag Assignment, Refinement, and Retrieval"], ["2543932557", "Packing bag-of-features"], ["2060427373", "A New Initiative on Precision Medicine"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2169783907", "User performance versus precision measures for simple search tasks"]], "pp": [["1510632636", "Socializing the Semantic Gap: A Comparative Survey on Image Tag Assignment, Refinement, and Retrieval"], ["2543932557", "Packing bag-of-features"], ["2060427373", "A New Initiative on Precision Medicine"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2169783907", "User performance versus precision measures for simple search tasks"]]}, {"context": "Closeness centrality describes the efficiency of the information propagation from one node to the other nodes CIT MAINCIT CIT .", "bow": [["2025844353", "Communication Patterns in Task\u2010Oriented Groups"], ["1967570846", "The centrality index of a graph"], ["1620903941", "The h-Index of a Graph and Its Application to Dynamic Subgraph Statistics"], ["1992250165", "Identifying influential nodes in complex networks"], ["2134784378", "Axioms for Centrality"]], "pp": [["2025844353", "Communication Patterns in Task\u2010Oriented Groups"], ["1967570846", "The centrality index of a graph"], ["1620903941", "The h-Index of a Graph and Its Application to Dynamic Subgraph Statistics"], ["1992250165", "Identifying influential nodes in complex networks"], ["2134784378", "Axioms for Centrality"]]}, {"context": "Eigenvector centrality describes the importance of nodes according to the adjacent matrix of a connected graph MAINCIT .", "bow": [["2087194317", "Power and Centrality: A Family of Measures"], ["1967570846", "The centrality index of a graph"], ["2134784378", "Axioms for Centrality"], ["2127800791", "Some unique properties of eigenvector centrality"], ["2127405101", "A Graph-theoretic perspective on centrality"]], "pp": [["2087194317", "Power and Centrality: A Family of Measures"], ["1967570846", "The centrality index of a graph"], ["2134784378", "Axioms for Centrality"], ["2127800791", "Some unique properties of eigenvector centrality"], ["2127405101", "A Graph-theoretic perspective on centrality"]], "np": [["2048146300", "Large System Analysis of Linear Precoding in MISO Broadcast Channels with Confidential Messages"], ["1976190799", "The interlace polynomial of a graph"], ["2050159143", "Efficient inference for fully-connected CRFs with stationarity"], ["2053973158", "Scene Labeling Using Beam Search under Mutex Constraints"], ["2099901267", "Distributed optimization for cooperative agents: application to formation flight"]]}, {"context": "Betweenness centrality has been used to study community structure of social and biological networks MAINCIT .", "bow": [["1513185775", "The rush in a directed graph"], ["1971937094", "A set of measures of centrality based on betweenness"], ["1964678328", "Total Ordering Problem"], ["2053374102", "A set of measures of centrality based upon betweenness"], ["2046868922", "Egocentric and sociocentric measures of network centrality"]], "pp": [["2088062845", "Graph theory and networks in Biology"], ["2062533676", "Network medicine: a network-based approach to human disease."], ["1513185775", "The rush in a directed graph"], ["1971937094", "A set of measures of centrality based on betweenness"], ["2130790725", "Lethality and centrality in protein networks."]], "np": [["2062533676", "Network medicine: a network-based approach to human disease."], ["2065341612", "Vector sets for exhaustive testing of logic circuits"], ["2088062845", "Graph theory and networks in Biology"], ["2122122715", "Biomimicry of bacterial foraging for distributed optimization and control"], ["2164610565", "Spike-timing-dependent plasticity: a comprehensive overview"]]}, {"context": "Betweenness centrality describes the frequencies of nodes in the shortest paths between two indirectly connected nodes CIT CIT MAINCIT .", "bow": [["1513185775", "The rush in a directed graph"], ["1971937094", "A set of measures of centrality based on betweenness"], ["2053374102", "A set of measures of centrality based upon betweenness"], ["1964678328", "Total Ordering Problem"], ["1967570846", "The centrality index of a graph"]], "pp": [["1513185775", "The rush in a directed graph"], ["1971937094", "A set of measures of centrality based on betweenness"], ["2053374102", "A set of measures of centrality based upon betweenness"], ["1964678328", "Total Ordering Problem"], ["1967570846", "The centrality index of a graph"]]}, {"context": "Word embeddings have also been studied in other IR contexts such as term reweighting MAINCIT , cross-lingual retrieval CIT and short-text similarity CIT .", "bow": [["2260194779", "A Dual Embedding Space Model for Document Ranking."], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"], ["2539671052", "Learning to Match using Local and Distributed Representations of Text for Web Search"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"]], "pp": [["2260194779", "A Dual Embedding Space Model for Document Ranking."], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"], ["2539671052", "Learning to Match using Local and Distributed Representations of Text for Web Search"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"]]}, {"context": "Finding a Kemeny ranking is equivalent to a minimum feedback arc set problem MAINCIT .", "bow": [["2057573268", "The complexity of Kemeny elections"], ["2016963000", "Ordering by weighted number of wins gives a good ranking for weighted tournaments"], ["2077651660", "Mathematical models in the social sciences"], ["2072755230", "Synthesizing constraint expressions"], ["2162918886", "Fixed-parameter algorithms for Kemeny rankings"]], "pp": [["2057573268", "The complexity of Kemeny elections"], ["2016963000", "Ordering by weighted number of wins gives a good ranking for weighted tournaments"], ["2077651660", "Mathematical models in the social sciences"], ["2072755230", "Synthesizing constraint expressions"], ["2162918886", "Fixed-parameter algorithms for Kemeny rankings"]]}, {"context": " Consequently, rank data has been widely studied in statistical sciences (e.g. see MAINCIT for a comprehensive survey).", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["2070907364", "Manifesto of computational social science"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["2070907364", "Manifesto of computational social science"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]]}, {"context": " Probabilistic models of permutation in general and of rank in particular have been widely analysed in statistical sciences (e.g. MAINCIT for a comprehensive survey).", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["1569736468", "Permutation, parametric and bootstrap tests of hypotheses"], ["2028889910", "The complexity of finding minimum-length generator sequences"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["1569736468", "Permutation, parametric and bootstrap tests of hypotheses"], ["2028889910", "The complexity of finding minimum-length generator sequences"]]}, {"context": "One way to define top-FORMULA rank is that a partial rank contains FORMULA items which minimizes the disagreement with all individual user's preferences, as explicitly formulated below: FORMULA FORMULA is the Kendall tau distance MAINCIT , defined by the number of disagreement of pairwise comparisons between two (partial) ranks.", "bow": [["1985514943", "A NEW MEASURE OF RANK CORRELATION"], ["2114232233", "A bound on the label complexity of agnostic active learning"], ["1928593089", "Semi-supervised clustering: probabilistic models, algorithms and experiments"], ["2155817812", "Active Ranking using Pairwise Comparisons"], ["2063244649", "Comparing Partial Rankings"]], "pp": [["1985514943", "A NEW MEASURE OF RANK CORRELATION"], ["2114232233", "A bound on the label complexity of agnostic active learning"], ["1928593089", "Semi-supervised clustering: probabilistic models, algorithms and experiments"], ["2124379907", "Rank Correlation Methods"], ["2155817812", "Active Ranking using Pairwise Comparisons"]], "np": [["2124379907", "Rank Correlation Methods"], ["1985514943", "A NEW MEASURE OF RANK CORRELATION"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "Otherwise, techniques of fake accounts and malicious users detection in social networks can be used MAINCIT CIT .", "bow": [["1922851884", "BotGraph: large scale spamming botnet detection"], ["2233384038", "EVILCOHORT: detecting communities of malicious accounts on online services"], ["1850562331", "Spam detection on twitter using traditional classifiers"], ["1986678144", "Detecting spammers on social networks"], ["9223698", "Detecting Spammers on Twitter"]], "pp": [["1922851884", "BotGraph: large scale spamming botnet detection"], ["2233384038", "EVILCOHORT: detecting communities of malicious accounts on online services"], ["1850562331", "Spam detection on twitter using traditional classifiers"], ["1986678144", "Detecting spammers on social networks"], ["9223698", "Detecting Spammers on Twitter"]]}, {"context": "Much work has been done on heuristic methods for computing optimal Kendall tau distance (Kemeny-Young method) CIT CIT MAINCIT CIT .", "bow": [["1985514943", "A NEW MEASURE OF RANK CORRELATION"], ["2057573268", "The complexity of Kemeny elections"], ["2100560478", "Extending Condorcet's rule"], ["2077651660", "Mathematical models in the social sciences"], ["1646465584", "Exact Complexity of the Winner Problem for Young Elections"]], "pp": [["1985514943", "A NEW MEASURE OF RANK CORRELATION"], ["2057573268", "The complexity of Kemeny elections"], ["2100560478", "Extending Condorcet's rule"], ["2077651660", "Mathematical models in the social sciences"], ["1646465584", "Exact Complexity of the Winner Problem for Young Elections"]]}, {"context": "Borda count is a 5-approximation of the Kemeny-Young method, and is often computational effective in practice MAINCIT .", "bow": [["2100560478", "Extending Condorcet's rule"], ["2057573268", "The complexity of Kemeny elections"], ["2077651660", "Mathematical models in the social sciences"], ["1646465584", "Exact Complexity of the Winner Problem for Young Elections"], ["1988686126", "Models for metasearch"]], "pp": [["2100560478", "Extending Condorcet's rule"], ["2057573268", "The complexity of Kemeny elections"], ["2077651660", "Mathematical models in the social sciences"], ["1646465584", "Exact Complexity of the Winner Problem for Young Elections"], ["1988686126", "Models for metasearch"]]}, {"context": "Deviation-based contextual sparse linear method (CSLIM) CIT is another example which utilizes the prediction function in sparse linear method MAINCIT as the component FORMULA .", "bow": [["2242689508", "The deviation constraint"], ["2052120747", "A new implementation of Yen\u2019s ranking loopless paths algorithm"], ["2138382427", "Learning Brain Connectivity of Alzheimer's Disease from Neuroimaging Data"], ["2143163931", "An affine scaling methodology for best basis selection"], ["1978494516", "Sparse principal component analysis and iterative thresholding"]], "pp": [["2242689508", "The deviation constraint"], ["2052120747", "A new implementation of Yen\u2019s ranking loopless paths algorithm"], ["2138382427", "Learning Brain Connectivity of Alzheimer's Disease from Neuroimaging Data"], ["2143163931", "An affine scaling methodology for best basis selection"], ["1978494516", "Sparse principal component analysis and iterative thresholding"]]}, {"context": "While more sophisticated length normalization strategies, such as pivoted document length normalization MAINCIT , are reasonable, we leave this also for future work.", "bow": [["1988711048", "Document length normalization"], ["2061272711", "Score normalization in multimodal biometric systems"], ["1836465849", "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"], ["2078953162", "Score Normalization for Text-Independent Speaker Verification Systems"], ["2460144244", "Group sparse regularization for deep neural networks"]], "pp": [["1988711048", "Document length normalization"], ["2061272711", "Score normalization in multimodal biometric systems"], ["1836465849", "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"], ["2078953162", "Score Normalization for Text-Independent Speaker Verification Systems"], ["2460144244", "Group sparse regularization for deep neural networks"]], "np": [["1988711048", "Document length normalization"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "Collaborative-filtering based recommender systems MAINCIT suffer from the \u201ccold-start problem\u201d CIT : Individual predictions for new customers and articles cannot be calculated in the absence of prior purchase data.", "bow": [["2100235918", "A survey of collaborative filtering techniques"], ["2171960770", "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions"], ["1690919088", "Recommender Systems Handbook"], ["1966553486", "Using collaborative filtering to weave an information tapestry"], ["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"]], "pp": [["1690919088", "Recommender Systems Handbook"], ["2159721641", "Research Objects: Towards Exchange and Reuse of Digital Knowledge."], ["2072992969", "Time-aware recommender systems: a comprehensive survey and analysis of existing evaluation protocols"], ["2171960770", "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions"], ["2115186087", "Recommender Systems Research: A Connection-Centric Survey"]], "np": [["2047870719", "Topology representing networks"], ["2072992969", "Time-aware recommender systems: a comprehensive survey and analysis of existing evaluation protocols"], ["2115186087", "Recommender Systems Research: A Connection-Centric Survey"], ["2159721641", "Research Objects: Towards Exchange and Reuse of Digital Knowledge."], ["2161308157", "A lower-bound on the number of rankings required in recommender systems using collaborativ filtering"]]}, {"context": " H.3.3Information Search and RetrievalRetrieval Models Introduction Deep neural networks aim to mimick the multiple layers of neurons that operate in the human brain in order to learn how to solve a wide range of interesting problems, like identifying photos CIT or responding to web search queries MAINCIT .", "bow": [["1999653836", "Complex brain networks: graph theoretical analysis of structural and functional systems"], ["2212676342", "Steps Toward Deep Kernel Methods from Infinite Neural Networks"], ["2146693559", "The Human Connectome: A Structural Description of the Human Brain"], ["2305488978", "Probabilistic brains: knowns and"], ["2044770804", "Scale-free brain functional networks"]], "pp": [["2117239687", "Detecting influenza epidemics using search engine query data"], ["1999653836", "Complex brain networks: graph theoretical analysis of structural and functional systems"], ["2212676342", "Steps Toward Deep Kernel Methods from Infinite Neural Networks"], ["2146693559", "The Human Connectome: A Structural Description of the Human Brain"], ["2305488978", "Probabilistic brains: knowns and"]], "np": [["2117239687", "Detecting influenza epidemics using search engine query data"], ["2558203065", "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "It relies on the users' collaborative behaviors on items for recommendation, among which the typical KNN CIT and latent factor models MAINCIT are the traditional state-of-the-art methods.", "bow": [["2042281163", "Item-based collaborative filtering recommendation algorithms"], ["2135505871", "fLDA: matrix factorization through latent dirichlet allocation"], ["1971040550", "Evaluating collaborative filtering recommender systems"], ["2142144955", "Item-based top- N recommendation algorithms"], ["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"]], "pp": [["2102716594", "Variational Learning for Switching State-Space Models"], ["2042281163", "Item-based collaborative filtering recommendation algorithms"], ["2049455633", "Latent semantic models for collaborative filtering"], ["2135505871", "fLDA: matrix factorization through latent dirichlet allocation"], ["1971040550", "Evaluating collaborative filtering recommender systems"]], "np": [["2102716594", "Variational Learning for Switching State-Space Models"], ["2115870554", "A Tutorial on Bayesian Nonparametric Models"], ["2253995343", "Collaborative Denoising Auto-Encoders for Top-N Recommender Systems"], ["1497675750", "Semiparametric Latent Factor Models"], ["1996919757", "Multiplicative latent factor models for description and prediction of social networks"]]}, {"context": "The seven examined methods are listed as below: FORMULA Popularity: the simple personalized re-ranking baseline provided by the official site; FORMULA KNN: the typical item-based collaborative filtering recommendation algorithm CIT ; FORMULA LFM: state-of-the-art Latent Factor Model MAINCIT , which is mainly designed to address the sparsity problem; FORMULA BPR: the typical bayesian personalized ranking method with the implicit feedback CIT ; FORMULA S-RNN: a typical session-based recommendation method with recurrent neural networks CIT ; FORMULA SIE: the proposed solution that directly use the class probabilities from session information embedding part for ranking; FORMULA ListRank: the proposed solution considers both the session information embedding and deep ListNet ranking.", "bow": [["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["2140310134", "BPR: Bayesian personalized ranking from implicit feedback"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2128280268", "Session types revisited"], ["2040105559", "Multiparty asynchronous session types"]], "pp": [["2113300847", "Interpretation and generalization of score matching"], ["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["2140310134", "BPR: Bayesian personalized ranking from implicit feedback"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2128280268", "Session types revisited"]], "np": [["2113300847", "Interpretation and generalization of score matching"], ["1497675750", "Semiparametric Latent Factor Models"], ["2054141820", "Matrix Factorization Techniques for Recommender Systems"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"]]}, {"context": "Tensor factorization CIT , context-aware matrix factorization MAINCIT and contextual sparse linear modeling CIT are the examples of the most effective contextual modeling algorithms in the recommender systems.", "bow": [["2102937240", "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering"], ["2130868038", "Context-aware recommender systems"], ["2054141820", "Matrix Factorization Techniques for Recommender Systems"], ["1998889130", "Matrix factorization techniques for context aware recommendation"], ["2112430581", "Incorporating contextual information in recommender systems using a multidimensional approach"]], "pp": [["2054141820", "Matrix Factorization Techniques for Recommender Systems"], ["1976618413", "Fast maximum margin matrix factorization for collaborative prediction"], ["2020098476", "Scalable Coordinate Descent Approaches to Parallel Matrix Factorization for Recommender Systems"], ["1985915435", "Alternating direction method of multipliers for non-negative matrix factorization with the beta-divergence"], ["880548201", "Self-paced learning for matrix factorization"]], "np": [["1481507124", "Diagonal and Low-Rank Matrix Decompositions, Correlation Matrices, and Ellipsoid Fitting"], ["2086325844", "MDL4BMF: Minimum Description Length for Boolean Matrix Factorization"], ["880548201", "Self-paced learning for matrix factorization"], ["1985915435", "Alternating direction method of multipliers for non-negative matrix factorization with the beta-divergence"], ["2040969041", "Iterative estimation of constrained rank-one matrices in noise"]]}, {"context": "Context-aware matrix factorization (CAMF) MAINCIT is the first attempt as the deviation-based contextual modeling approach, where it replaces the FORMULA by the predictive function in matrix factorization.", "bow": [["1998889130", "Matrix factorization techniques for context aware recommendation"], ["2137245235", "Probabilistic Matrix Factorization"], ["1479822238", "Learning from Incomplete Ratings Using Non-negative Matrix Factorization."], ["1976618413", "Fast maximum margin matrix factorization for collaborative prediction"], ["2054141820", "Matrix Factorization Techniques for Recommender Systems"]], "pp": [["1998889130", "Matrix factorization techniques for context aware recommendation"], ["2137245235", "Probabilistic Matrix Factorization"], ["1479822238", "Learning from Incomplete Ratings Using Non-negative Matrix Factorization."], ["1976618413", "Fast maximum margin matrix factorization for collaborative prediction"], ["2054141820", "Matrix Factorization Techniques for Recommender Systems"]]}, {"context": "Partial rank aggregation is known to be NP-hard MAINCIT .", "bow": [["87579370", "Rank Aggregation for Similar Items."], ["2018556306", "Quadratic programming is in NP"], ["2889845368", "Evaluation Techniques for Large Databases"], ["2089546742", "On truth-table reducibility to SAT"], ["2139587243", "In-network aggregation techniques for wireless sensor networks: a survey"]], "pp": [["87579370", "Rank Aggregation for Similar Items."], ["2018556306", "Quadratic programming is in NP"], ["2889845368", "Evaluation Techniques for Large Databases"], ["2089546742", "On truth-table reducibility to SAT"], ["2139587243", "In-network aggregation techniques for wireless sensor networks: a survey"]]}, {"context": "Much work has been done on heuristic methods for computing optimal Kendall tau distance (Kemeny-Young method) MAINCIT CIT CIT CIT .", "bow": [["1985514943", "A NEW MEASURE OF RANK CORRELATION"], ["2057573268", "The complexity of Kemeny elections"], ["2100560478", "Extending Condorcet's rule"], ["2077651660", "Mathematical models in the social sciences"], ["1646465584", "Exact Complexity of the Winner Problem for Young Elections"]], "pp": [["1985514943", "A NEW MEASURE OF RANK CORRELATION"], ["2057573268", "The complexity of Kemeny elections"], ["2100560478", "Extending Condorcet's rule"], ["2000851114", "A Consistent Extension of Condorcet\u2019s Election Principle"], ["2077651660", "Mathematical models in the social sciences"]], "np": [["2000851114", "A Consistent Extension of Condorcet\u2019s Election Principle"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "We propose a heuristic method based on strong connected component detection to compute Kemeny-Young ranking MAINCIT .", "bow": [["2100560478", "Extending Condorcet's rule"], ["2057573268", "The complexity of Kemeny elections"], ["2077651660", "Mathematical models in the social sciences"], ["1646465584", "Exact Complexity of the Winner Problem for Young Elections"], ["2116910210", "Practical Robust Communication in DHTs Tolerating a Byzantine Adversary"]], "pp": [["2100560478", "Extending Condorcet's rule"], ["2057573268", "The complexity of Kemeny elections"], ["2077651660", "Mathematical models in the social sciences"], ["1646465584", "Exact Complexity of the Winner Problem for Young Elections"], ["2116910210", "Practical Robust Communication in DHTs Tolerating a Byzantine Adversary"]]}, {"context": "In this recommendation system, we propose to use Kemeny ranking MAINCIT as the aggregated group preference, which is a ranking that minimizes the disagreement among group members.", "bow": [["2172118809", "Group recommendation: semantics and efficiency"], ["2003605635", "Group recommendations with rank aggregation and collaborative filtering"], ["2114990184", "An axiomatization of Borda's rule"], ["2149166361", "Log-Linear Models for Label Ranking"], ["2142646165", "Learning to Order Things"]], "pp": [["2172118809", "Group recommendation: semantics and efficiency"], ["2003605635", "Group recommendations with rank aggregation and collaborative filtering"], ["2114990184", "An axiomatization of Borda's rule"], ["2149166361", "Log-Linear Models for Label Ranking"], ["2142646165", "Learning to Order Things"]], "np": [["2233988666", "Bypassing combinatorial protections: polynomial-time algorithms for single-peaked electorates"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": " More specifically, FORMULA If FORMULA is the size of the items, i.e. FORMULA and FORMULA satisfies (REF ), FORMULA is called a Kemeny ranking MAINCIT .", "bow": [["2057573268", "The complexity of Kemeny elections"], ["2077651660", "Mathematical models in the social sciences"], ["2166347079", "New Methods in Automatic Extracting"], ["2162918886", "Fixed-parameter algorithms for Kemeny rankings"], ["2150107436", "Comparing and aggregating rankings with ties"]], "pp": [["2057573268", "The complexity of Kemeny elections"], ["2077651660", "Mathematical models in the social sciences"], ["2166347079", "New Methods in Automatic Extracting"], ["2162918886", "Fixed-parameter algorithms for Kemeny rankings"], ["2150107436", "Comparing and aggregating rankings with ties"]], "np": [["2233988666", "Bypassing combinatorial protections: polynomial-time algorithms for single-peaked electorates"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": " =1 2017 2017 acmlicensed CIKM'17 November 6\u201310, 2017Singapore, Singapore15.0010.1145/3132847.3133015 978-1-4503-4918-5/17/11 printacmref=false, printfolios=false algorithmProcedure square,comma,numbers,sortcompress,sectionbib Active Sampling for Large-scale Information Retrieval Evaluation Dan Li University of Amsterdam Amsterdam, The Netherlands d.li@uva.nl Evangelos Kanoulas University of Amsterdam Amsterdam, The Netherlands e.kanoulas@uva.nl Evaluation is crucial in Information Retrieval. The development of models, tools and methods has significantly benefited from the availability of reusable test collections formed through a standardized and thoroughly tested methodology, known as the Cranfield paradigm. Constructing these collections requires obtaining relevance judgments for a pool of documents, retrieved by systems participating in an evaluation task; thus involves immense human labor. To alleviate this effort different methods for constructing collections have been proposed in the literature, falling under two broad categories: (a) sampling, and (b) active selection of documents. The former devises a smart sampling strategy by choosing only a subset of documents to be assessed and inferring evaluation measure on the basis of the obtained sample; the sampling distribution is being fixed at the beginning of the process. The latter recognizes that systems contributing documents to be judged vary in quality, and actively selects documents from good systems. The quality of systems is measured every time a new document is being judged. In this paper we seek to solve the problem of large-scale retrieval evaluation combining the two approaches. We devise an active sampling method that avoids the bias of the active selection methods towards good systems, and at the same time reduces the variance of the current sampling approaches by placing a distribution over systems, which varies as judgments become available. We validate the proposed method using TREC data and demonstrate the advantages of this new method compared to past approaches. <ccs2012> <concept> <conceptid>10002951.10003317.10003359.10003361</conceptid> <conceptdesc>Information systems Relevance assessment</conceptdesc> <conceptsignificance>300</conceptsignificance> </concept> </ccs2012> [300]Information systems Relevance assessment Evaluation, Cranfield, Sampling with varying probabilities, Horvitz-Thompson estimator Introduction Evaluation is crucial in Information Retrieval (IR). The development of models, tools and methods has significantly benefited from the availability of reusable test collections formed through a standardized and thoroughly tested methodology, known as the Cranfield paradigm CIT . Under the Cranfield paradigm the evaluation of retrieval systems typically involves assembling a document collection, creating a set of information needs (topics), and identifying a set of documents relevant to the topics. One of the simplifying assumptions made by the Cranfield paradigm is that the relevance judgments are complete, i.e. for each topic all relevant documents in the collection have been identified. When the document collection is large identifying all relevant documents is difficult due to the immense human labor required. In order to avoid judging the entire document collection depth-k pooling CIT is being used: a set of retrieval systems (also called runs) ranks the document collection against each topic, and only the union of the top-FORMULA retrieved documents is being assessed by human assessors. Documents outside the depth-k pool are considered irrelevant. Pooling aims at being fair to all runs and hopes for a diverse set of submitted runs that can provide a good coverage of all relevant documents. Nevertheless, the underestimation of recall CIT and the pooling bias generated when re-using these pooled collections to evaluate novel systems that retrieve relevant but unjudged documents CIT are well-known problems. The literature suggests a number of approaches to cope with missing judgments (an overview can be found in CIT and CIT ): (1) Defining IR measures that are robust to missing judgments, like bpref CIT . The developed measures however may not precisely capture the notion of retrieval effectiveness one requires, while some have been shown to remain biased CIT . (2) Running a meta-experiment where runs are \u201cleft out\u201d from contributing to the pool and measuring the bias experienced by these left-out runs compared to the original pool, which is then used to correct measurements over new retrieval systems CIT . (3) Leaving the design of the evaluation measure unrestricted, but instead introducing a document selection methodology that carefully chooses which documents to be judged. Methods proposed under this approach belong to two categories: (a) sample-based methods CIT , and (b) active selection methods CIT . Sample-based methods devise a sampling strategy that randomly selects a subset of documents to be assessed; evaluation measures are then inferred on the basis of the obtained sample. Different methods employ different sampling distributions. CIT and CIT use a uniform distribution over the ranked document collection, while CIT and CIT recognize that relevant documents typically reside at the top of the ranked lists returned by participating runs and use stratified sampling to draw larger sample from the top ranks. CIT also use a weighted-importance sampling method on documents with the sampling distribution optimized for a comparative evaluation between runs. In all aforementioned work, an experiment that dictates the probability distribution under which documents are being sampled is being designed in such a way that evaluation measures can be defined as the expected outcome of this experiment. Evaluation measures can then be estimated by the judged documents sampled. In all cases the sampling distribution is being defined at the beginning of the sampling process and remains fixed throughout the experiment. Sample-based methods have the following desirable properties: (1) on average, estimates have no systematic error, (2) past data can be re-used by new, novel runs without introducing bias, and (3) sampling distributions can be designed to optimize the number of judgments needed to confidently and accurately estimate a measure. On the other hand, active-selection methods recognize that systems contributing documents to the pool vary in quality. Based on this observation they bias the selection of documents towards those retrieved by good retrieval systems. The selection process is deterministic and depends on how accurately the methods can estimate the quality of each retrieval system. Judging is performed in multiple rounds: at each round the best system is identified, and the next unjudged document in the ranked list of this system is selected to be judged. The quality of systems is calculated at the end of each round, as soon as a new judgment becomes available. Active-selection methods include Move-to-Front CIT , Fixed-Budget Pooling CIT , and Multi-Armed Bandits CIT . CIT considers the problem as an exploration-exploitation dilemma, balancing between selecting documents from the best-quality run, and exploring the possibility that the quality of some runs might be underestimated at different rounds of the experiment. The advantage of active-selection methods compared to sample-based methods is that they are designed to identify as many relevant document as possible, by selecting documents with the highest relevance probability. The disadvantage is that the judging process is not fair to all runs, with the selection of documents being biased towards good-performing runs. In this paper, we follow a sample-based approach for an efficient large-scale evaluation. Different from past sample-based approaches we account for the fact that some systems are of higher quality than others, and we design our sampling distribution to over-sample documents from these systems. At the same time, given that our approach is a sample-based approach the estimated evaluation measures are, by construction, unbiased on average, and judgments can be used to evaluate new, novel systems without introducing any systematic error. The method we propose therefore is an active sampling method with the probability distribution over documents changing at every round of judgments through the re-estimation of the quality of the participating runs. Accordingly, our solution consists of a sampling step and an estimation step. In the sampling step, we construct a distribution over runs and a distribution over documents in a ranked list and calculate a joint distribution over documents to sample from. In the estimation step, we use the Horvitz-Thompson estimator to correct for the bias in the sampling distribution and estimate evaluation measure values for all the runs. The estimated measures then dictate the new sampling distribution over systems, and hence a new joint distribution over the ranked collection of documents. Therefore, the contribution of this paper is a new sampling methodology for large-scale retrieval evaluation that combines the advantages of the sample-based and the active-selection approaches. We demonstrate that the proposed method outperforms state-of-the-art methods in terms of effectiveness, efficiency, and reusability. Active sampling In this section we introduce our new sampling method. TABLE Active sampling algorithm The key idea underlying our sampling strategy is to place a probability distribution over runs and a probability distribution over documents in the ranked lists of the runs, and iteratively sample documents from the joint distribution. At each round, we sample a set of documents from the joint probability distribution (batch sampling) and request relevance judgments by human assessors. The judged documents are then used to update the probability distribution over runs. The process is repeated until we reach a fixed budget of human assessments (Figure REF ). FIGURE Active sampling [1] Prior distribution over runs FORMULA , prior distributions over document ranks FORMULA , document collection FORMULA , batch size FORMULA Sampled documents FORMULA , associated with relevance judgment and selection probability: FORMULA FORMULA Calculate the joint document sampling distribution FORMULA Sample FORMULA documents with replacement (so that it contains FORMULA unique documents) from FORMULA Let the sampled document be FORMULA ; judge relevance of the sampled documents FORMULA Augment data FORMULA Update distribution over runs FORMULA The process is illustrated in Algorithm REF , while Table 1 shows the notation used throughout the paper. Initially, we provide a prior distribution over runs FORMULA , a prior distribution over the ranks of the documents FORMULA for each run FORMULA , and the document collection FORMULA . Given that we have no prior knowledge of the system quality it is reasonable to use a uniform probability distribution over runs, i.e. FORMULA . At each round FORMULA , we calculate the selection probabilities of the documents (that is the probability that a document is selected at each sampling time) FORMULA for each document FORMULA , and then sample a document on the basis of this distribution. We use sampling with replacement with varying probabilities to sample documents, which is closely related to how we calculate the unbiased estimators and it is describe in Section 3. The sampled documents FORMULA are then judged by human assessors, with the relevance of these documents denoted as FORMULA , and the new data are added to FORMULA which is used to update the FORMULA posterior distribution over runs. Distribution over runs The distribution over runs determines the probability of sampling documents from each run. Similar to active-selection methods, we make the assumption that good systems retrieve more relevant documents at the top of their rankings compared to bad systems. Based on this assumption we wish to over-sample from rankings of good systems. Any distribution that places a higher probability to better performing systems could be used here. In this work we consider the estimated performance of the retrieval systems on the basis of the relevance judgments accumulated at each round of assessments as system weights and normalize these weights to obtain a probability distribution over runs. Different evaluation measures can be used to estimate the performance of each run after every sampling round. Here we define a probability distribution proportional to estimated average precision FORMULA introduced in Section REF . FORMULA Figure REF demonstrates the accuracy of the estimated (normalized) average precision at the end of four sampling rounds compared to the (normalized) average precision when the entire document collection (or to be more accurate the depth-100 pool for topic 251 in TREC 5) is used. At every round the estimates (denoted with circular markers of different sizes for different rounds) better approximates the target values (denoted with a line). The details of the measure approximations are provided at Section . FIGURE Distribution over document ranks The distribution over document ranks for a system FORMULA determines the probability of sampling a document at a certain rank of the ordered list returned by run FORMULA . The underlying assumption that defines this probability distribution is that runs satisfy the Probability Ranking Principle (PRP) CIT which dictates that the probability of relevance monotonically decreases with the rank of the document. Hence, if we let FORMULA denote the probability of sampling a document at rank FORMULA , then it is natural to assume FORMULA is a function of FORMULA and FORMULA monotonically decreases with FORMULA . Once again, any distribution that agrees with PRP can be used; researcher have used a number of such distributions (e.g. see CIT ). In this work we consider an AP-prior distribution proposed by CIT and CIT which aims to define the probability at each rank on the basis of the contribution of this rank in the calculation of average precision. The intuition is that when rewriting FORMULA , the implicit weight associated with rank FORMULA can be obtained by summing weights associated with all pairs involving FORMULA , i.e. FORMULA . Then the AP-prior distribution is defined as follows: FORMULA where FORMULA is the rank of a document and FORMULA the total number of documents in the collection. Similar to CIT and all other sample-based methods, this distribution is defined at the beginning of the sampling process and remains fixed throughout the experiment. Retrieval performance estimator In this section, we discuss the estimation of evaluation measures on the basis of the sampling procedure described in Algorithm REF . We first calculate the inclusion probabilities of each document in the collection, and then demonstrate how these probabilities can be used by a Horvitz-Thompson estimator to produce unbiased estimators of the population mean, and subsequently of some popular evaluation measures. The Horvitz-Thompson estimator, together with the calculated inclusion probabilities can be used to calculate the majority of the evaluation measures used in IR; in this paper we focus on three of them, Precision, Recall, and Average Precision. Other measures can be derived in similar ways (e.g. see Table 1 in CIT ). Sampling with replacement with varying probabilities Sampling procedure. At each round of our iterative sampling process described in Algorithm REF , FORMULA documents are sampled from a collection of size FORMULA . At each round, the unconditional probability of sampling a document FORMULA (selection probability) is FORMULA , as defined in Step 2 of Algorithm REF , with FORMULA Let FORMULA denote the index of the FORMULA documents composing the sample set. The probability of a document FORMULA being sampled (first-order inclusion probability) at the end of the sampling process is given by FORMULA which accounts for varying probabilities across different rounds, while the probability of any two different document FORMULA and FORMULA being sampled (second-order inclusion probability) is given by FORMULA For the details of the derivation of the inclusion probabilities the reader can refer to CIT . Using these inclusion probabilities together with the Horvitz-Thompson estimator allows us to construct unbiased estimators for different evaluation measures in IR. Horvitz-Thompson estimator of population total. MAINCIT propose a general sampling theory for constructing unbiased estimators of population totals.", "bow": [["2610698371", "Semantic hashing"], ["2116219421", "Stability-based validation of clustering solutions"], ["2118020653", "Machine learning in automated text categorization"], ["2149427297", "Learning to Rank for Information Retrieval"], ["2144100511", "Evaluation methods for topic models"]], "pp": [["2610698371", "Semantic hashing"], ["2116219421", "Stability-based validation of clustering solutions"], ["2118020653", "Machine learning in automated text categorization"], ["2149427297", "Learning to Rank for Information Retrieval"], ["2144100511", "Evaluation methods for topic models"]]}, {"context": "Degree Correlation The nearest-neighbours average degree, FORMULA , of FORMULA -degree nodes MAINCIT , is a projection of the joint degree distribution, given by FORMULA A network is called an assortative network if FORMULA increases with FORMULA , which means nodes tend to attach to similar nodes, i.e. high\u2013degree nodes to high\u2013degree nodes and low\u2013degree nodes to low\u2013degree nodes (`assortative mixing').", "bow": [["2002396510", "Dynamical and correlation properties of the Internet"], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2033193852", "Mixing patterns in networks"], ["1584977346", "Detection of topological patterns in complex networks: correlation profile of the internet"], ["2040956707", "Assortative Mixing in Networks"]], "pp": [["2002396510", "Dynamical and correlation properties of the Internet"], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2033193852", "Mixing patterns in networks"], ["1584977346", "Detection of topological patterns in complex networks: correlation profile of the internet"], ["2040956707", "Assortative Mixing in Networks"]]}, {"context": "Most of the other studies documented in the literature have also relied on tweet content, using different techniques such as topic modelling to find locally relevant keywords that reveal a user's likely location MAINCIT .", "bow": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"]], "pp": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"]], "np": [["2098854771", "Visual tracking decomposition"], ["2154889144", "High-Speed Tracking with Kernelized Correlation Filters"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "Song et al. MAINCIT used probabilistic latent semantic analysis(pLSA) and Latent Dirichlet allocation(LDA) to disambiguate names based on publication content.", "bow": [["1880262756", "Latent dirichlet allocation"], ["1589362500", "Scene classification via pLSA"], ["2058013187", "Latent Semantic Analysis"], ["2134731454", "Unsupervised Learning by Probabilistic Latent Semantic Analysis"], ["2149035855", "Spatial Latent Dirichlet Allocation"]], "pp": [["1880262756", "Latent dirichlet allocation"], ["1589362500", "Scene classification via pLSA"], ["2058013187", "Latent Semantic Analysis"], ["2134731454", "Unsupervised Learning by Probabilistic Latent Semantic Analysis"], ["2149035855", "Spatial Latent Dirichlet Allocation"]]}, {"context": "In addition to comparing data across web sites, we also compare with (i) subsets of the Web, (ii) a citation network, (iii) the AS-level Internet network, and (iv) the generative model of Barab\u00e1si and Albert MAINCIT .", "bow": [["3976408", "Inet-3.0: Internet Topology Generator"], ["2050353626", "Towards capturing representative AS-level Internet topologies"], ["2151972741", "On inferring autonomous system relationships in the internet"], ["2120514843", "AS relationships: inference and validation"], ["2139905147", "The internet AS-level topology: three data sources and one definitive metric"]], "pp": [["2008620264", "Emergence of scaling in random networks"], ["1999647190", "Evaluating North American Electric Grid Reliability Using the Barabasi-Albert Network Model"], ["3976408", "Inet-3.0: Internet Topology Generator"], ["2050353626", "Towards capturing representative AS-level Internet topologies"], ["2151972741", "On inferring autonomous system relationships in the internet"]], "np": [["2008620264", "Emergence of scaling in random networks"], ["1999647190", "Evaluating North American Electric Grid Reliability Using the Barabasi-Albert Network Model"], ["2124637492", "Statistical mechanics of complex networks"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"]]}, {"context": "It has been reported that a number of networks MAINCIT follow a power-law degree distribution, FORMULA This means that most nodes have very few links, while a few nodes have a very large number of links.", "bow": [["1750839748", "Wireless scheduling with power control"], ["1976969221", "On power-law relationships of the Internet topology"], ["2000042664", "Power-Law Distributions in Empirical Data"], ["2077783414", "Efficient bounds for the stable set, vertex cover and set packing problems"], ["1511088604", "An Economic Model of Friendship: Homophily, Minorities and Segregation"]], "pp": [["1750839748", "Wireless scheduling with power control"], ["1976969221", "On power-law relationships of the Internet topology"], ["2000042664", "Power-Law Distributions in Empirical Data"], ["2077783414", "Efficient bounds for the stable set, vertex cover and set packing problems"], ["1511088604", "An Economic Model of Friendship: Homophily, Minorities and Segregation"]]}, {"context": "Barab\u00e1si-Albert Model The Barab\u00e1si and Albert (BA) model MAINCIT has been widely used in the study of complex networks.", "bow": [["3976408", "Inet-3.0: Internet Topology Generator"], ["2008620264", "Emergence of scaling in random networks"], ["2080201763", "Topology of evolving networks: local events and universality"], ["2121821841", "Mean-field theory for scale-free random networks"], ["1572923654", "Mathematical results on scale\u2010free random graphs"]], "pp": [["2008620264", "Emergence of scaling in random networks"], ["2121821841", "Mean-field theory for scale-free random networks"], ["2124637492", "Statistical mechanics of complex networks"], ["3976408", "Inet-3.0: Internet Topology Generator"], ["2104977387", "Competition and multiscaling in evolving networks"]], "np": [["2121821841", "Mean-field theory for scale-free random networks"], ["2008620264", "Emergence of scaling in random networks"], ["2104977387", "Competition and multiscaling in evolving networks"], ["2124637492", "Statistical mechanics of complex networks"], ["100047375", "Parallel algorithm configuration"]]}, {"context": "Traditionally the research question is limited to how to choose advertisers to make (private) contracts with, as elaborated in MAINCIT .", "bow": [["2032399648", "A theory of contracts for Web services"], ["2015937909", "Making components contract aware"], ["2104477831", "On the realizability of contracts in dishonest systems"], ["2114178685", "A comparative study of programmer-written and automatically inferred contracts"], ["2073358075", "Pricing guaranteed contracts in online display advertising"]], "pp": [["2032399648", "A theory of contracts for Web services"], ["2015937909", "Making components contract aware"], ["2104477831", "On the realizability of contracts in dishonest systems"], ["2114178685", "A comparative study of programmer-written and automatically inferred contracts"], ["2073358075", "Pricing guaranteed contracts in online display advertising"]]}, {"context": "A dynamic programming solution was provided in MAINCIT by combining the available ad inventories and dynamically delivery of promised advertising contract to the viewers.", "bow": [["2152314154", "Accurately interpreting clickthrough data as implicit feedback"], ["2073448073", "A semantic approach to contextual advertising"], ["2009193773", "Dynamic Revenue Management for Online Display Advertising"], ["2056705371", "Optimizing relevance and revenue in ad search: a query substitution approach"], ["2123937625", "Evaluating implicit measures to improve web search"]], "pp": [["2152314154", "Accurately interpreting clickthrough data as implicit feedback"], ["2073448073", "A semantic approach to contextual advertising"], ["2009193773", "Dynamic Revenue Management for Online Display Advertising"], ["2056705371", "Optimizing relevance and revenue in ad search: a query substitution approach"], ["2123937625", "Evaluating implicit measures to improve web search"]]}, {"context": "Another aspect of publisher revenue optimization is ad scheduling with the constraint of geometrical features of website, uncertainty of advertisers and available ad inventory (impressions) MAINCIT .", "bow": [["2009193773", "Dynamic Revenue Management for Online Display Advertising"], ["2095916875", "Selling futures online advertising slots via option contracts"], ["2123937625", "Evaluating implicit measures to improve web search"], ["2056705371", "Optimizing relevance and revenue in ad search: a query substitution approach"], ["2188273680", "Bidding for Representative Allocations for Display Advertising"]], "pp": [["2009193773", "Dynamic Revenue Management for Online Display Advertising"], ["2095916875", "Selling futures online advertising slots via option contracts"], ["2123937625", "Evaluating implicit measures to improve web search"], ["2056705371", "Optimizing relevance and revenue in ad search: a query substitution approach"], ["2188273680", "Bidding for Representative Allocations for Display Advertising"]]}, {"context": "They are, however, significantly outperformed in terms of the trade-off between memory usage and accuracy by recent methods that cast high dimensional indexing to a source coding problem MAINCIT , in particular the product quantization-based method of CIT exhibits impressive results for large scale image search CIT .", "bow": [["2161451718", "Distance Encoded Product Quantization"], ["2111006384", "Optimized Product Quantization for Approximate Nearest Neighbor Search"], ["2119913432", "Approximate Nearest Neighbor Search by Residual Vector Quantization"], ["1634005169", "Vector Quantization and Signal Compression"], ["138284169", "Composite Quantization for Approximate Nearest Neighbor Search"]], "pp": [["2136670370", "Reduction and Fixed Points of Boolean Networks and Linear Network Coding Solvability"], ["2105406958", "Coding on demand by an informed source (ISCOD) for efficient broadcast of different supplemental data to caching clients"], ["2114085948", "Information transmission with additional noise"], ["2106630500", "Efficient algorithms for Index Coding"], ["2077246452", "Shape description using weighted symmetric axis features"]], "np": [["2077246452", "Shape description using weighted symmetric axis features"], ["2136670370", "Reduction and Fixed Points of Boolean Networks and Linear Network Coding Solvability"], ["2105406958", "Coding on demand by an informed source (ISCOD) for efficient broadcast of different supplemental data to caching clients"], ["2106630500", "Efficient algorithms for Index Coding"], ["2001935257", "A coding theorem and R\u00e9nyi's entropy*"]]}, {"context": "They are, however, significantly outperformed in terms of the trade-off between memory usage and accuracy by recent methods that cast high dimensional indexing to a source coding problem CIT , in particular the product quantization-based method of CIT exhibits impressive results for large scale image search MAINCIT .", "bow": [["2161451718", "Distance Encoded Product Quantization"], ["2111006384", "Optimized Product Quantization for Approximate Nearest Neighbor Search"], ["2119913432", "Approximate Nearest Neighbor Search by Residual Vector Quantization"], ["1634005169", "Vector Quantization and Signal Compression"], ["138284169", "Composite Quantization for Approximate Nearest Neighbor Search"]], "pp": [["2161451718", "Distance Encoded Product Quantization"], ["2111006384", "Optimized Product Quantization for Approximate Nearest Neighbor Search"], ["2119913432", "Approximate Nearest Neighbor Search by Residual Vector Quantization"], ["1634005169", "Vector Quantization and Signal Compression"], ["138284169", "Composite Quantization for Approximate Nearest Neighbor Search"]]}, {"context": "But as shown in CIT , this post verification is also important for methods based on binary CIT or quantized codes MAINCIT , as the ranking provided on output of the large scale search is significantly improved when verifying the few first hypotheses.", "bow": [["2341106557", "Quantized consensus"], ["2130025446", "In transition from global to modular temporal reasoning about programs"], ["2148575324", "Polynomial Codes Over Certain Finite Fields"], ["2166966427", "Perfect binary codes: constructions, properties, and enumeration"], ["1771040533", "Three-weight cyclic codes and their weight distributions"]], "pp": [["2341106557", "Quantized consensus"], ["2130025446", "In transition from global to modular temporal reasoning about programs"], ["2148575324", "Polynomial Codes Over Certain Finite Fields"], ["2166966427", "Perfect binary codes: constructions, properties, and enumeration"], ["1771040533", "Three-weight cyclic codes and their weight distributions"]]}, {"context": "Researchers have attempted to infer attributes of Twitter users such as age CIT , gender CIT , political orientation MAINCIT or a range of social identities CIT .", "bow": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]], "pp": [["2017729405", "Classifying latent user attributes in twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["9292421", "Discriminating Gender on Twitter"], ["2013416264", "Democrats, republicans and starbucks afficionados: user classification in twitter"], ["1014449310", "Homophily and Latent Attribute Inference: Inferring Latent Attributes of Twitter Users from Neighbors"]], "np": [["2017729405", "Classifying latent user attributes in twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2013416264", "Democrats, republicans and starbucks afficionados: user classification in twitter"], ["1014449310", "Homophily and Latent Attribute Inference: Inferring Latent Attributes of Twitter Users from Neighbors"], ["68298479", "Classifying Political Orientation on Twitter: It\u2019s Not Easy!"]]}, {"context": "In the probabilistic approach, the 2-Poisson model forms the basis for counting term frequency MAINCIT .", "bow": [["2110264793", "A Variational Approach to Reconstructing Images Corrupted by Poisson Noise"], ["1987057208", "Sparsity-based Poisson denoising with dictionary learning."], ["143080133", "Bayesian Nonparametric Poisson Factorization for Recommendation Systems"], ["2090209167", "Capacity and cutoff rate for Poisson-type channels"], ["2060954994", "Upper bounds on Poisson tail probabilities"]], "pp": [["2110264793", "A Variational Approach to Reconstructing Images Corrupted by Poisson Noise"], ["1987057208", "Sparsity-based Poisson denoising with dictionary learning."], ["143080133", "Bayesian Nonparametric Poisson Factorization for Recommendation Systems"], ["2090209167", "Capacity and cutoff rate for Poisson-type channels"], ["2060954994", "Upper bounds on Poisson tail probabilities"]], "np": [["2105157020", "Probabilistic models of information retrieval based on measuring the divergence from randomness"], ["2166706824", "Thumbs up? Sentiment Classification using Machine Learning Techniques"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "This explanation for the relationship between term frequency and aboutness is the basis for the TF function in BM25 MAINCIT .", "bow": [["1482214997", "Okapi at TREC"], ["2155482025", "The Probabilistic Relevance Framework: BM25 and Beyond"], ["2153252192", "A probabilistic model of information retrieval: development and comparative experiments"], ["2102353396", "Multitaper Time-Frequency Reassignment for Nonstationary Spectrum Estimation and Chirp Enhancement"], ["1971864278", "TF-Label: a topological-folding labeling scheme for reachability querying in a large graph"]], "pp": [["1482214997", "Okapi at TREC"], ["2155482025", "The Probabilistic Relevance Framework: BM25 and Beyond"], ["2153252192", "A probabilistic model of information retrieval: development and comparative experiments"], ["2102353396", "Multitaper Time-Frequency Reassignment for Nonstationary Spectrum Estimation and Chirp Enhancement"], ["1971864278", "TF-Label: a topological-folding labeling scheme for reachability querying in a large graph"]]}, {"context": "The increase in BM25 as term frequency increases is justified according to the 2-Poisson model MAINCIT , which makes a distinction between documents about a term and documents that merely mention that term.", "bow": [["1482214997", "Okapi at TREC"], ["2153252192", "A probabilistic model of information retrieval: development and comparative experiments"], ["2155482025", "The Probabilistic Relevance Framework: BM25 and Beyond"], ["2110264793", "A Variational Approach to Reconstructing Images Corrupted by Poisson Noise"], ["1985697096", "A statistical approach to mechanized encoding and searching of literary information"]], "pp": [["1482214997", "Okapi at TREC"], ["2153252192", "A probabilistic model of information retrieval: development and comparative experiments"], ["2155482025", "The Probabilistic Relevance Framework: BM25 and Beyond"], ["2110264793", "A Variational Approach to Reconstructing Images Corrupted by Poisson Noise"], ["1985697096", "A statistical approach to mechanized encoding and searching of literary information"]]}, {"context": "Researchers have attempted to infer attributes of Twitter users such as age MAINCIT , gender CIT , political orientation CIT or a range of social identities CIT .", "bow": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]], "pp": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]]}, {"context": "Researchers have attempted to infer attributes of Twitter users such as age CIT , gender MAINCIT , political orientation CIT or a range of social identities CIT .", "bow": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]], "pp": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]]}, {"context": "One of the earliest studies is that by Cheng et al. MAINCIT , who introduced a probabilistic, content-based approach that identifies the most representative words of each of the major cities in the USA; these words are then used to classify new tweets.", "bow": [["2093588201", "Mapping the global Twitter heartbeat: The geography of Twitter"], ["2018277822", "You are where you tweet: a content-based approach to geo-locating twitter users"], ["1945748771", "Effective learning-based illuminant estimation using simple features"], ["2294703018", "Emotional Tweets"], ["2022082910", "A polynomial-time approximation scheme for the minimum-connected dominating set in ad hoc wireless networks"]], "pp": [["2093588201", "Mapping the global Twitter heartbeat: The geography of Twitter"], ["2018277822", "You are where you tweet: a content-based approach to geo-locating twitter users"], ["1945748771", "Effective learning-based illuminant estimation using simple features"], ["2294703018", "Emotional Tweets"], ["2022082910", "A polynomial-time approximation scheme for the minimum-connected dominating set in ad hoc wireless networks"]]}, {"context": "Most of the other studies documented in the literature have also relied on tweet content, using different techniques such as topic modelling to find locally relevant keywords that reveal a user's likely location MAINCIT .", "bow": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"]], "pp": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"]], "np": [["2098854771", "Visual tracking decomposition"], ["2154889144", "High-Speed Tracking with Kernelized Correlation Filters"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "Recall: Given a recommendation test, we consider any item in the top-FORMULA recommendations that matches any item in the test set as a \u201chit\", as in MAINCIT .", "bow": [["2046974451", "Item popularity and recommendation accuracy"], ["2127480961", "Content-based book recommending using learning for text categorization"], ["2177562712", "Neural Network Matrix Factorization"], ["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"], ["2508504774", "Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence"]], "pp": [["2046974451", "Item popularity and recommendation accuracy"], ["2127480961", "Content-based book recommending using learning for text categorization"], ["2177562712", "Neural Network Matrix Factorization"], ["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"], ["2508504774", "Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence"]]}, {"context": "Most of the other studies documented in the literature have also relied on tweet content, using different techniques such as topic modelling to find locally relevant keywords that reveal a user's likely location MAINCIT .", "bow": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"]], "pp": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"]], "np": [["2098854771", "Visual tracking decomposition"], ["2154889144", "High-Speed Tracking with Kernelized Correlation Filters"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "FIGURE Related Work Current approaches to protect privacy in recommendation systems mostly address two different privacy concerns: protecting users' privacy from curious peers or malicious users MAINCIT , and against unreliable service providers CIT .", "bow": [["2610955953", "Differential privacy"], ["1526797722", "Towards privacy for social networks: a zero-knowledge based definition of privacy"], ["2077217970", "A firm foundation for private data analysis"], ["2088517895", "Privacy against statistical inference"], ["2054922243", "No free lunch in data privacy"]], "pp": [["2610955953", "Differential privacy"], ["1526797722", "Towards privacy for social networks: a zero-knowledge based definition of privacy"], ["2077217970", "A firm foundation for private data analysis"], ["2088517895", "Privacy against statistical inference"], ["2054922243", "No free lunch in data privacy"]], "np": [["2200869402", "Proactively accountable anonymous messaging in verdict"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "McSherry et al. MAINCIT adapted the leading approaches in the Netflix Prize competition to provide differential privacy and recommendations on movies.", "bow": [["2341535507", "The Netflix Prize"], ["2135930857", "Robust De-anonymization of Large Sparse Datasets"], ["40442397", "The BellKor Solution to the Netflix Grand Prize"], ["2289525833", "The BellKor solution to the Netflix Prize"], ["2122997783", "Personalized social recommendations: accurate or private"]], "pp": [["2341535507", "The Netflix Prize"], ["2135930857", "Robust De-anonymization of Large Sparse Datasets"], ["40442397", "The BellKor Solution to the Netflix Grand Prize"], ["2289525833", "The BellKor solution to the Netflix Prize"], ["2122997783", "Personalized social recommendations: accurate or private"]]}, {"context": "Our framework can be combined with other differentially private recommendation solutions such as MAINCIT .", "bow": [["2399682466", "Private Convex Optimization for Empirical Risk Minimization with Applications to High-dimensional Regression."], ["2144190046", "A near-optimal algorithm for differentially-private principal components"], ["2104743167", "A Stability-based Validation Procedure for Differentially Private Machine Learning"], ["2127555295", "Privacy-Preserving Data Sharing for Genome-Wide Association Studies."], ["2606014625", "Revisiting Differentially Private Hypothesis Tests for Categorical Data"]], "pp": [["2399682466", "Private Convex Optimization for Empirical Risk Minimization with Applications to High-dimensional Regression."], ["2144190046", "A near-optimal algorithm for differentially-private principal components"], ["2104743167", "A Stability-based Validation Procedure for Differentially Private Machine Learning"], ["2127555295", "Privacy-Preserving Data Sharing for Genome-Wide Association Studies."], ["2606014625", "Revisiting Differentially Private Hypothesis Tests for Categorical Data"]]}, {"context": " Previous work often involves paired comparisons (e.g. see MAINCIT CIT CIT ), ignoring simultaneous interactions among objects.", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"], ["1965499304", "Modelling disease outbreaks in realistic urban social networks"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"], ["1965499304", "Modelling disease outbreaks in realistic urban social networks"]]}, {"context": "The basic idea is to assign some probability mass for the event of ties MAINCIT CIT CIT .", "bow": [["2153000253", "A Survey of the Stable Marriage Problem and Its Variants"], ["2109469951", "The Strength of Weak Ties"], ["259727857", "Predicting Actions from Static Scenes"], ["2336732495", "Event structures"], ["2096335387", "Joint Entity and Event Coreference Resolution across Documents"]], "pp": [["2153000253", "A Survey of the Stable Marriage Problem and Its Variants"], ["2109469951", "The Strength of Weak Ties"], ["259727857", "Predicting Actions from Static Scenes"], ["2336732495", "Event structures"], ["2096335387", "Joint Entity and Event Coreference Resolution across Documents"]]}, {"context": "Another method is introduced in MAINCIT , where the probability masses are defined as FORMULA where FORMULA .", "bow": [["2625580685", "Information Fusion Based on New Proportional Conflict"], ["2294923432", "Deep Learning and Structured Prediction for the Segmentation of Mass in Mammograms"], ["1734353855", "Proportional Conflict Redistribution Rules for Information"], ["2102263452", "Controlled Lagrangians and the stabilization of mechanical systems. I. The first matching theorem"], ["2586431331", "Preparing for the Unknown: Learning a Universal Policy with Online System Identification"]], "pp": [["2625580685", "Information Fusion Based on New Proportional Conflict"], ["2294923432", "Deep Learning and Structured Prediction for the Segmentation of Mass in Mammograms"], ["1734353855", "Proportional Conflict Redistribution Rules for Information"], ["2102263452", "Controlled Lagrangians and the stabilization of mechanical systems. I. The first matching theorem"], ["2586431331", "Preparing for the Unknown: Learning a Universal Policy with Online System Identification"]]}, {"context": "We also implement two variants of the Bradley-Terry model with ties handling, one by Rao-Kupper CIT (denoted by PairTies-RK; this also appears to be implemented in CIT under the functional gradient setting) and another by Davidson MAINCIT (denoted by PairTies-D; and this is the first time the Davidson method is applied to learning to rank).", "bow": [["1868882999", "Formal verification techniques using quantum process calculus"], ["2107173440", "Work-Efficient Parallel GPU Methods for Single-Source Shortest Paths"], ["1899609882", "Method and system for generating and auditing a signature for a computer program"], ["2004322147", "Small distortion and volume preserving embeddings for planar and Euclidean metrics"], ["2021053884", "Drawing graphs nicely using simulated annealing"]], "pp": [["1868882999", "Formal verification techniques using quantum process calculus"], ["2107173440", "Work-Efficient Parallel GPU Methods for Single-Source Shortest Paths"], ["1899609882", "Method and system for generating and auditing a signature for a computer program"], ["2004322147", "Small distortion and volume preserving embeddings for planar and Euclidean metrics"], ["2021053884", "Drawing graphs nicely using simulated annealing"]]}, {"context": "The literature suggests a number of approaches to cope with missing judgments (an overview can be found in MAINCIT and CIT ): (1) Defining IR measures that are robust to missing judgments, like bpref CIT .", "bow": [["2109244020", "Retrieval evaluation with incomplete information"], ["2065269597", "Understanding the intrinsic memorability of images"], ["2044758663", "Statistical Analysis with Missing Data"], ["2159048649", "Here or there: preference judgments for relevance"], ["2156267802", "Missing Data: Our View of the State of the Art"]], "pp": [["2109244020", "Retrieval evaluation with incomplete information"], ["2065269597", "Understanding the intrinsic memorability of images"], ["2044758663", "Statistical Analysis with Missing Data"], ["2159048649", "Here or there: preference judgments for relevance"], ["2156267802", "Missing Data: Our View of the State of the Art"]]}, {"context": "Finally, for completeness, we mention in passing the third approach, which treats a permutation as a symmetric group and applying spectral decomposition techniques MAINCIT CIT .", "bow": [["2028889910", "The complexity of finding minimum-length generator sequences"], ["2075350371", "Soundness and Completeness of an Axiom System for Program Verification"], ["2336611437", "Permutation groups"], ["1569736468", "Permutation, parametric and bootstrap tests of hypotheses"], ["2160254449", "Complete axiomatization and decidability of alternating-time temporal logic"]], "pp": [["2098395403", "Empirical mode decomposition as a filter bank"], ["1604213221", "Differential Equations and Dynamical Systems"], ["2028889910", "The complexity of finding minimum-length generator sequences"], ["2075350371", "Soundness and Completeness of an Axiom System for Program Verification"], ["2336611437", "Permutation groups"]], "np": [["2098395403", "Empirical mode decomposition as a filter bank"], ["1604213221", "Differential Equations and Dynamical Systems"], ["2195423816", "Efficient Multi-User Computation Offloading for Mobile-Edge Cloud Computing"], ["2058532290", "On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators"], ["2000955051", "Excessive Gap Technique in Nonsmooth Convex Minimization"]]}, {"context": "Word embeddings have also been studied in other IR contexts such as term reweighting CIT , cross-lingual retrieval CIT and short-text similarity MAINCIT .", "bow": [["2260194779", "A Dual Embedding Space Model for Document Ranking."], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"], ["2539671052", "Learning to Match using Local and Distributed Representations of Text for Web Search"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"]], "pp": [["2260194779", "A Dual Embedding Space Model for Document Ranking."], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"], ["2539671052", "Learning to Match using Local and Distributed Representations of Text for Web Search"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"]]}, {"context": "A network's degree correlation, or mixing pattern, can be summarised by a single scalar called the assortative coefficient MAINCIT , FORMULA where FORMULA is the number of links and FORMULA , FORMULA are degrees of the end nodes of the FORMULA th link with FORMULA .", "bow": [["2040956707", "Assortative Mixing in Networks"], ["2033193852", "Mixing patterns in networks"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2002396510", "Dynamical and correlation properties of the Internet"]], "pp": [["2033193852", "Mixing patterns in networks"], ["2040956707", "Assortative Mixing in Networks"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2002396510", "Dynamical and correlation properties of the Internet"]], "np": [["2033193852", "Mixing patterns in networks"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked ( MAINCIT ).", "bow": [["2035720976", "Training linear SVMs in linear time"], ["1971747485", "An Optimal Set of Discriminant Vectors"], ["2133958955", "The Entire Regularization Path for the Support Vector Machine"], ["2051381803", "Robust linear programming discrimination of two linearly inseparable sets"], ["2117202609", "Accuracy at the Top"]], "pp": [["2035720976", "Training linear SVMs in linear time"], ["1971747485", "An Optimal Set of Discriminant Vectors"], ["2133958955", "The Entire Regularization Path for the Support Vector Machine"], ["2051381803", "Robust linear programming discrimination of two linearly inseparable sets"], ["2117202609", "Accuracy at the Top"]]}, {"context": "nearest neighbor search, quantization, source coding, high dimensional indexing, large databases Introduction Approximate nearest neighbors (ANN) search methods CIT are required to handle large databases, especially for computer vision CIT and music retrieval MAINCIT applications.", "bow": [["2427881153", "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"], ["2131929066", "On the Difficulty of Nearest Neighbor Search"], ["2167931879", "Anti-sparse coding for approximate nearest neighbor search"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"]], "pp": [["2140013053", "MULTIPLE-INSTANCE LEARNING FOR MUSIC INFORMATION RETRIEVAL"], ["2427881153", "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"], ["2131929066", "On the Difficulty of Nearest Neighbor Search"], ["2167931879", "Anti-sparse coding for approximate nearest neighbor search"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"]], "np": [["2140013053", "MULTIPLE-INSTANCE LEARNING FOR MUSIC INFORMATION RETRIEVAL"], ["2138079527", "Support vector machine active learning for image retrieval"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "A network's degree correlation, or mixing pattern, can be summarised by a single scalar called the assortative coefficient MAINCIT , FORMULA where FORMULA is the number of links and FORMULA , FORMULA are degrees of the end nodes of the FORMULA th link with FORMULA .", "bow": [["2040956707", "Assortative Mixing in Networks"], ["2033193852", "Mixing patterns in networks"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2002396510", "Dynamical and correlation properties of the Internet"]], "pp": [["2033193852", "Mixing patterns in networks"], ["2040956707", "Assortative Mixing in Networks"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2002396510", "Dynamical and correlation properties of the Internet"]], "np": [["2033193852", "Mixing patterns in networks"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "It relies on the users' collaborative behaviors on items for recommendation, among which the typical KNN MAINCIT and latent factor models CIT are the traditional state-of-the-art methods.", "bow": [["2042281163", "Item-based collaborative filtering recommendation algorithms"], ["2135505871", "fLDA: matrix factorization through latent dirichlet allocation"], ["1971040550", "Evaluating collaborative filtering recommender systems"], ["2142144955", "Item-based top- N recommendation algorithms"], ["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"]], "pp": [["2042281163", "Item-based collaborative filtering recommendation algorithms"], ["2135505871", "fLDA: matrix factorization through latent dirichlet allocation"], ["1971040550", "Evaluating collaborative filtering recommender systems"], ["2142144955", "Item-based top- N recommendation algorithms"], ["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"]]}, {"context": "In contrast to the classical vector space model, LSA CIT , PLSA CIT and LDA MAINCIT learn dense vector representations of much lower dimensionality.", "bow": [["2134731454", "Unsupervised Learning by Probabilistic Latent Semantic Analysis"], ["2091273188", "Using latent semantic analysis to improve access to textual information"], ["2130324521", "Exploring Topic Coherence over Many Models and Many Topics"], ["2158997610", "An introduction to latent semantic analysis"], ["1985809919", "A direct LDA algorithm for high-dimensional data \u2014 with application to face recognition"]], "pp": [["2134731454", "Unsupervised Learning by Probabilistic Latent Semantic Analysis"], ["2091273188", "Using latent semantic analysis to improve access to textual information"], ["2130324521", "Exploring Topic Coherence over Many Models and Many Topics"], ["2158997610", "An introduction to latent semantic analysis"], ["1985809919", "A direct LDA algorithm for high-dimensional data \u2014 with application to face recognition"]]}, {"context": "Beyond word co-occurrence, recent studies have also explored learning text embeddings from clickthrough data CIT , session data MAINCIT , query prefix-suffix pairs CIT , via auto-encoders CIT , and for sentiment classification CIT and for long text CIT .", "bow": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]], "pp": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]]}, {"context": "We show that our results both in terms of predictive performance and training time are competitive with other well-known methods such as RankNet CIT , Ranking SVM MAINCIT and ListMLE CIT .", "bow": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]], "pp": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]]}, {"context": "Pairwise approach which spans preference to binary classification CIT CIT MAINCIT methods, where the goal is to learn a classifier that can separate two documents (per query).", "bow": [["2002699418", "Preference formulas in relational queries"], ["175248941", "Interval orders and interval graphs"], ["1558832481", "Foundations of databases"], ["2170188482", "The Skyline operator"], ["2610698371", "Semantic hashing"]], "pp": [["2002699418", "Preference formulas in relational queries"], ["175248941", "Interval orders and interval graphs"], ["1558832481", "Foundations of databases"], ["2170188482", "The Skyline operator"], ["2610698371", "Semantic hashing"]]}, {"context": "This casts the ranking problem into a standard classification framework, wherein many algorithms are readily available, for example, SVM MAINCIT , neural network and logistic regression CIT , and boosting CIT .", "bow": [["2125607229", "On the Dual Formulation of Boosting Algorithms"], ["1973948212", "Applied Logistic Regression"], ["2067802667", "Discriminative models for information retrieval"], ["2093717447", "The Strength of Weak Learnability"], ["2108263314", "Boosting Algorithms as Gradient Descent"]], "pp": [["2125607229", "On the Dual Formulation of Boosting Algorithms"], ["1973948212", "Applied Logistic Regression"], ["2067802667", "Discriminative models for information retrieval"], ["2093717447", "The Strength of Weak Learnability"], ["2108263314", "Boosting Algorithms as Gradient Descent"]]}, {"context": "For comparison, we implement several well-known methods, including RankNet CIT , Ranking SVM MAINCIT and ListMLE CIT .", "bow": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]], "pp": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]]}, {"context": "Coherence assessment is then formulated as a ranking task in an SVM preference ranking framework MAINCIT .", "bow": [["2114990184", "An axiomatization of Borda's rule"], ["1581941705", "Ranking the Best Instances"], ["2149166361", "Log-Linear Models for Label Ranking"], ["2142646165", "Learning to Order Things"], ["2102705755", "Label ranking by learning pairwise preferences"]], "pp": [["2114990184", "An axiomatization of Borda's rule"], ["1581941705", "Ranking the Best Instances"], ["2149166361", "Log-Linear Models for Label Ranking"], ["2142646165", "Learning to Order Things"], ["2102705755", "Label ranking by learning pairwise preferences"]]}, {"context": "However, this definition has since been used in machine learning MAINCIT , cloud computing CIT , and location indistinguishability together with PIR CIT to evaluate and minimize the privacy risk.", "bow": [["1986070285", "Private information retrieval"], ["2007208840", "General constructions for information-theoretic private information retrieval"], ["1619839175", "Codes for distributed PIR with low storage overhead"], ["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"]], "pp": [["2156512439", "An equivalence between sparse approximation and support vector machines"], ["1986070285", "Private information retrieval"], ["2003048487", "A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features"], ["1982725637", "Applications of tensor (multiway array) factorizations and decompositions in data mining"], ["2295869408", "Minimizing the maximal loss: how and why"]], "np": [["1539739406", "Generalizing from case studies: a case study"], ["1546501475", "Multimodal diffusion geometry by joint diagonalization of Laplacians"], ["1978925672", "The Persistent Homology of a Self-Map"], ["1982725637", "Applications of tensor (multiway array) factorizations and decompositions in data mining"], ["2003048487", "A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features"]]}, {"context": "FIGURE Mathematical model A possible strategy to solve a recommender problem is by logistic factorization MAINCIT of the purchase matrix FORMULA .", "bow": [["2054141820", "Matrix Factorization Techniques for Recommender Systems"], ["1976618413", "Fast maximum margin matrix factorization for collaborative prediction"], ["2127113848", "Blockbuster Culture's Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity"], ["2137245235", "Probabilistic Matrix Factorization"], ["2165395308", "Weighted low-rank approximations"]], "pp": [["2054141820", "Matrix Factorization Techniques for Recommender Systems"], ["1976618413", "Fast maximum margin matrix factorization for collaborative prediction"], ["2127113848", "Blockbuster Culture's Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity"], ["2137245235", "Probabilistic Matrix Factorization"], ["2165395308", "Weighted low-rank approximations"]]}, {"context": "An intuitive approach is collaborative filtering (CF) MAINCIT CIT CIT .", "bow": [["2100235918", "A survey of collaborative filtering techniques"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"], ["1539057251", "Ordinal Boltzmann Machines for collaborative filtering"], ["2043403353", "Fab: content-based, collaborative recommendation"]], "pp": [["2100235918", "A survey of collaborative filtering techniques"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"], ["1539057251", "Ordinal Boltzmann Machines for collaborative filtering"], ["2043403353", "Fab: content-based, collaborative recommendation"]]}, {"context": " 8.5in 11in pdfauthor = Anonymous, pdftitle = A Dual Embedding Space Model for Document Ranking, pdfsubject = A Dual Embedding Space Model for Document Ranking, pdfkeywords = H.3 [Information Storage and Retrieval], pdfcreator = LaTeX with hyperref package, pdfproducer = pdflatex to appearThis paper is an extended evaluation and analysis of the model proposed by CIT to appear in WWW'16, April 11 - 15, 2016, Montreal, Canada. Copyright 2016 by the author(s). A Dual Embedding Space Model for Document Ranking 4 Bhaskar Mitra Microsoft Cambridge, UK bmitra@microsoft.com Eric Nalisnick University of California Irvine, USA enalisni@uci.edu Nick Craswell, Rich Caruana Microsoft Redmond, USA nickcr, rcaruana@microsoft.com A fundamental goal of search engines is to identify, given a query, documents that have relevant text. This is intrinsically difficult because the query and the document may use different vocabulary, or the document may contain query words without being relevant. We investigate neural word embeddings as a source of evidence in document ranking. We train a word2vec embedding model on a large unlabelled query corpus, but in contrast to how the model is commonly used, we retain both the input and the output projections, allowing us to leverage both the embedding spaces to derive richer distributional relationships. During ranking we map the query words into the input space and the document words into the output space, and compute a query-document relevance score by aggregating the cosine similarities across all the query-document word pairs. We postulate that the proposed Dual Embedding Space Model (DESM) captures evidence on whether a document is about a query term in addition to what is modelled by traditional term-frequency based approaches. Our experiments show that the DESM can re-rank top documents returned by a commercial Web search engine, like Bing, better than a term-matching based signal like TF-IDF. However, when ranking a larger set of candidate documents, we find the embeddings-based approach is prone to false positives, retrieving documents that are only loosely related to the query. We demonstrate that this problem can be solved effectively by ranking based on a linear mixture of the DESM and the word counting features. H.3Information Storage and RetrievalH.3.3 Information Search and Retrieval Keywords: Document ranking; Word embeddings; Word2vec Introduction FIGURE TABLE Identifying relevant documents for a given query is a core challenge for Web search. For large-scale search engines, it is possible to identify a very small set of pages that can answer a good proportion of queries CIT . For such popular pages, clicks and hyperlinks may provide sufficient ranking evidence and it may not be important to match the query against the body text. However, in many Web search scenarios such query-content matching is crucial. If new content is available, the new and updated documents may not have click evidence or may have evidence that is out of date. For new or tail queries, there may be no memorized connections between the queries and the documents. Furthermore, many search engines and apps have a relatively smaller number of users, which limits their ability to answer queries based on memorized clicks. There may even be insufficient behaviour data to learn a click-based embedding CIT or a translation model CIT . In these cases it is crucial to model the relationship between the query and the document content, without click data. When considering the relevance of document body text to a query, the traditional approach is to count repetitions of query terms in the document. Different transformation and weighting schemes for those counts lead to a variety of possible TF-IDF ranking features. One theoretical basis for such features is the probabilistic model of information retrieval, which has yielded the very successful TF-IDF formulation BM25 CIT . As noted by CIT , the probabilistic approach can be restricted to consider only the original query terms or it can automatically identify additional terms that are correlated with relevance. However, the basic commonly-used form of BM25 considers query terms only, under the assumption that non-query terms are less useful for document ranking. In the probabilistic approach, the 2-Poisson model forms the basis for counting term frequency CIT . The stated goal is to distinguish between a document that is about a term and a document that merely mentions that term. These two types of documents have term frequencies from two different Poisson distributions, such that documents about the term tend to have higher term frequency than those that merely mention it. This explanation for the relationship between term frequency and aboutness is the basis for the TF function in BM25 CIT . The new approach in this paper uses word occurrences as evidence of aboutness, as in the probabilistic approach. However, instead of considering term repetition as evidence of aboutness it considers the relationship between the query terms and all the terms in the document. For example, given a query term \u201cyale\u201d, in addition to considering the number of times Yale is mentioned in the document, we look at whether related terms occur in the document, such as \u201cfaculty\u201d and \u201calumni\u201d. Similarly, in a document about the Seahawks sports team one may expect to see the terms \u201chighlights\u201d and \u201cjerseys\u201d. The occurrence of these related terms in sufficient numbers is a way to distinguish between documents that merely mention Yale or Seahawks and the documents that are about the university or about the sports team. With this motivation, in Section we describe how the input and the output embedding spaces learned by a word2vec model may be jointly particularly attractive for modelling the aboutness aspect of document ranking. Table REF gives some anecdotal evidence of why this is true. If we look in the neighbourhood of the IN vector of the word \u201cyale\u201d then the other IN vectors that are close correspond to words that are functionally similar or of the same type, e.g., \u201charvard\u201d and \u201cnyu\u201d. A similar pattern emerges if we look at the OUT vectors in the neighbourhood of the OUT vector of \u201cyale\u201d. On the other hand, if we look at the OUT vectors that are closest to the IN vector of \u201cyale\u201d we find words like \u201cfaculty\u201d and \u201calumni\u201d. We use this property of the IN-OUT embeddings to propose a novel Dual Embedding Space Model (DESM) for document ranking. Figure REF further illustrates how in this Dual Embedding Space model, using the IN embeddings for the query words and the OUT embeddings for the document words we get a much more useful similarity definition between the query and the relevant document centroids. The main contributions of this paper are, A novel Dual Embedding Space Model, with one embedding for query words and a separate embedding for document words, learned jointly based on an unlabelled text corpus. We propose a document ranking feature based on comparing all the query words with all the document words, which is equivalent to comparing each query word to a centroid of the document word embeddings. We analyse the positive aspects of the new feature, preferring documents that contain many words related to the query words, but also note the potential of the feature to have false positive matches. We empirically compare the new approach to a single embedding and the traditional word counting features. The new approach works well on its own in a telescoping setting, re-ranking the top documents returned by a commercial Web search engine, and in combination with word counting for a more general document retrieval task. Distributional Semantics for IR In this section we first introduce the Continuous Bag-of-Words (CBOW) model made popular by the software Word2Vec CIT . Then, inspired by our findings that distinctly different topic-based relationships can be found by using both the input and the output embeddings jointly \u2013 the latter of which is usually discarded after training \u2013 we propose the Dual Embedding Space Model (DESM) for document ranking. Continuous Bag-of-Words While many word embedding models have been proposed recently, the Continuous Bag-of-Words (CBOW) and the Skip-Gram (SG) architectures proposed by CIT are arguably the most popular (perhaps due to the popularity of the software Word2Vechttps://code.google.com/p/word2vec/, which implements both). Although here we will concentrate exclusively on the CBOW model, our proposed IR ranking methodology is just as applicable to vectors produced by SG, as both models produce qualitatively and quantitatively similar embeddings. The CBOW model learns a word's embedding via maximizing the log conditional probability of the word given the context words occurring within a fixed-sized window around that word. That is, the words in the context window serve as input, and from them, the model attempts to predict the center (missing) word. For a formal definition, let FORMULA be a FORMULA -dimensional, real-valued vector representing the FORMULA th context word FORMULA appearing in a FORMULA -sized window around an instance of word FORMULA , which is represented by a vector FORMULA . The model `predicts' word FORMULA by adapting its representation vector such that it has a large inner-product with the mean of the context word vectors. Training CBOW requires minimization of the following objective FORMULA where FORMULA and FORMULA represents the training corpus. Notice that the probability is normalized by summing over all the vocabulary, which is quite costly when training on web-scale data. To make CBOW scalable, CIT proposed the following slightly altered negative sampling objective: FORMULA where FORMULA is the Sigmoid function and FORMULA is the number of negative sample words drawn either from the uniform or empirical distribution over the vocabulary. All our experiments were performed with the negative sampling objective. FIGURE A crucial detail often overlooked when using Word2Vec is that there are two different sets of vectors (represented above by FORMULA and FORMULA respectively and henceforth referred to as the IN and OUT embedding spaces), which correspond to the FORMULA and FORMULA weight matrices in Figure REF . By default, Word2Vec discards FORMULA at the end of training and outputs only FORMULA . Subsequent tasks determine word-to-word semantic relatedness by computing the cosine similarity: FORMULA Dual Embedding Space Model FIGURE TABLE A key challenge for term-matching based retrieval is to distinguish whether a document merely references a term or is about that entity. See Figure REF for a concrete example of two passages that contain the term \"Albuquerque\" an equal number of times although only one of the passages is about that entity. The presence of the words like \"population\" and \"metropolitan\" indicate that the left passage is about Albuquerque, whereas the passage on the right just mentions it. However, these passages would be indistinguishable under term counting. The semantic similarity of non-matched terms (i.e. the words a TF feature would overlook) are crucial for inferring a document's topic of focus\u2013its aboutness. Due to its ability to capture word co-occurrence (i.e. perform missing word prediction), CBOW is a natural fit for modelling the aboutness of a document. The learnt embedding spaces contain useful knowledge about the distributional properties of words, allowing, in the case of Figure REF , an IR system to recognize the city-related terms in the left document. With this motivation, we define a simple yet, as we will demonstrate, effective ranking function we call the Dual Embedding Space Model: FORMULA where FORMULA Here FORMULA is the centroid of all the normalized vectors for the words in the document serving as a single embedding for the whole document. In this formulation of the DESM, the document embeddings can be pre-computed, and at the time of ranking, we only need to sum the score contributions across the query terms. We expect that the ability to pre-compute a single document embedding is a very useful property when considering runtime efficiency. IN-IN vs. IN-OUT CIT noted, \"Not all neural embeddings are born equal\". As previously mentioned, the CBOW (and SG) model contains two separate embedding spaces (IN and OUT) whose interactions capture additional distributional semantics of words that are not observable by considering any of the two embeddings spaces in isolation. Table REF illustrates clearly how the CBOW model \"pushes\" the IN vectors of words closer to the OUT vectors of other words that they commonly co-occur with. In doing so, words that appear in similar contexts get pushed closer to each other within the IN embedding space (and also within the OUT embedding space). Therefore the IN-IN (or the OUT-OUT) cosine similarities are higher for words that are typically (by type or by function) similar, whereas the IN-OUT cosine similarities are higher for words that co-occur often in the training corpus (topically similar). This gives us at least two variants of the DESM, corresponding to retrieval in the IN-OUT space or the IN-IN spaceIt is also possible to define FORMULA and FORMULA , but based on limited experimentation we expect them to behave similar to FORMULA and FORMULA , respectively.. FORMULA FORMULA In Section , we show that the FORMULA is a better indication of aboutness than BM25, because of its knowledge of the word distributional properties, and FORMULA , since topical similarity is a better indicator of aboutness than typical similarity. Modelling document aboutness We perform a simple word perturbation analysis to illustrate how the DESM can collect evidence on document aboutness from both matched and non-matched terms in the document. In Table REF , we consider five small passages of text. The first three passages are about Cambridge, Oxford and giraffes respectively. The next two passages are generated by replacing the word \"giraffe\" by the word \"Cambridge\" in the passage about giraffes, and vice versa. We compute the FORMULA and the FORMULA scores along with the term frequencies for each of these passages for the query term \"cambridge\". As expected, all three models score the passage about Cambridge highly. However, unlike the term frequency feature, the DESM seem robust towards keyword stuffinghttps://en.wikipedia.org/wiki/Keyword_stuffing, at least in this specific example where we replace the word \"giraffe\" with \"cambridge\" in the passage about giraffes, but the DESMs still score the passage relatively low. This is exactly the kind of evidence that we expect the DESM to capture that may not be possible by simple term counting. On the other hand, both the DESMs score the passage about Oxford very highly. This is expected because both these passages contain many words that are likely to co-occur with the word \"cambridge\" in the training corpus. This implies that the DESM features are very susceptible to false positive matches and can only be used either in conjunction with other document ranking features, such as TF-IDF, or for re-ranking a smaller set of candidate documents already deemed at least somewhat relevant. This is similar to the telescoping evaluation setup described by CIT , where multiple nested rankers are used to achieve better retrieval performance over a single ranker. At each stage of telescoping, a ranker is used to reduce the set of candidate documents that is passed on to the next. Improved performance is possible because the ranker that sees only top-scoring documents can specialize in handling such documents, for example by using different feature weights. In our experiments, we will see the DESM to be a poor standalone ranking signal on a larger set of documents, but performs significantly better against the BM25 and the LSA baselines once we reach a small high-quality candidate document set. This evaluation strategy of focusing at ranking for top positions is in fact quite common and has been used by many recent studies (e.g., CIT ). Dot product vs. cosine similarity In the DESM formulation (Equation REF ) we compute the cosine similaritiy between every query word and the normalized document centroid. The use of cosine similarity (as opposed to, say, dot-product) is motivated by several factors. Firstly, much of the existing literature CIT on CBOW and SG uses cosine similarity and normalized unit vectors (for performing vector algebra for word analogies). As the cosine similarity has been shown to perform well in practice in these embedding spaces we adopt the same strategy here. A secondary justification can be drawn based on the observations made by CIT that the length of the non-normalized word vectors has a direct relation to the frequency of the word. In information retrieval (IR), it is well known that frequently occurring words are ineffective features for distinguishing relevant documents from irrelevant ones. The inverse-document frequency weighting is often used in IR to capture this effect. By normalizing the word vectors in the document before computing the document centroids, we are counteracting the extra influence frequent words would have on the sum. Training corpus Our CBOW model is trained on a query corpusWe provide the IN and OUT word embeddings trained using word2vec on the Bing query corpus at http://research.microsoft.com/projects/DESM. consisting of 618,644,170 queries and a vocabulary size of 2,748,230 words. The queries are sampled from Bing's large scale search logs from the period of August 19, 2014 to August 25, 2014. We repeat all our experiments using another CBOW model trained on a corpus of document body text with 341,787,174 distinct sentences sampled from the Bing search index and a corresponding vocabulary size of 5,108,278 words. Empirical results on the performance of both the models are presented in Section . Out-of-vocabulary (OOV) words One of the challenges of the embedding models is that they can only be applied to a fixed size vocabulary. It is possible to explore different strategies to deal with out-of-vocab (OOV) words in the Equation REF In machine translation there are examples of interesting strategies to handle out-of-vocabulary words (e.g., CIT ). But we leave this for future investigation and instead, in this paper, all the OOV words are ignored for computing the DESM score, but not for computing the TF-IDF feature, a potential advantage for the latter. Document length normalization In Equation REF we normalize the scores linearly by both the query and the document lengths. While more sophisticated length normalization strategies, such as pivoted document length normalization CIT , are reasonable, we leave this also for future work. The Mixture Model The DESM is a weak ranker and while it models some important aspects of document ranking, our experiments will show that it's effective only at ranking at high positions (i.e. documents we already know are at least somewhat relevant). We are inspired by previous work in neural language models, for example by CIT , which demonstrates that combining a neural model for predicting the next word with a more traditional counting-based language model is effective because the two models make different kinds of mistakes. Adopting a similar strategy we propose a simple and intuitive mixture model combining DESM with a term based feature, such as BM25, for the non-telescoping evaluation setup described in Section REF . We define the mixture model MM(Q, D) as, FORMULA To choose the appropriate value for FORMULA , we perform a parameter sweep between zero and one at intervals of 0.01 on the implicit feedback based training set described in Section REF . Experiments TABLE We compare the retrieval performance of DESM against BM25, a traditional count-based method, and Latent Semantic Analysis (LSA), a traditional vector-based method. We conduct our evaluations on two different test sets (explicit and implicit relevance judgements) and under two different experimental conditions (a large collection of documents and a telescoped subset). Datasets All the datasets that are used for this study are sampled from Bing's large scale query logs. The body text for all the candidate documents are extracted from Bing's document index. Explicitly judged test set This evaluation set consists of 7,741 queries randomly sampled from Bing's query logs from the period of October, 2014 to December, 2014. For each sampled query, a set of candidate documents is constructed by retrieving the top results from Bing over multiple scrapes during a period of a few months. In total the final evaluation set contains 171,302 unique documents across all queries which are then judged by human evaluators on a five point relevance scale (Perfect, Excellent, Good, Fair and Bad). Implicit feedback based test set This dataset is sampled from the Bing logs from the period of the September 22, 2014 to September 28, 2014. The dataset consists of the search queries submitted by the user and the corresponding documents that were returned by the search engine in response. The documents are associated with a binary relevance judgment based on whether the document was clicked by the user. This test set contains 7,477 queries and the 42,573 distinct documents. Implicit feedback based training set This dataset is sampled exactly the same way as the previous test but from the period of September 15, 2014 to September 21, 2014 and has 7,429 queries and 42,253 distinct documents. This set is used for tuning the parameters for the BM25 baseline and the mixture model. Experiment Setup We perform two distinct sets of evaluations for all the experimental and baseline models. In the first experiment, we consider all documents retrieved by Bing (from the online scrapes in the case of the explicitly judged set or as recorded in the search logs in the case of the implicit feedback based sets) as the candidate set of documents to be re-ranked for each query. The fact that each of the documents were retrieved by the search engine implies that they are all at least marginally relevant to the query. Therefore, this experimental design isolates performance at the top ranks. As mentioned in Section REF , there is a parallel between this experiment setup and the telescoping CIT evaluation strategy, and has been used often in recent literature (e.g., CIT ). Note that by having a strong retrieval model, in the form of the Bing search engine, for first stage retrieval enables us to have a high confidence candidate set and in turn ensures reliable comparison with the baseline BM25 feature. In our non-telescoped experiment, we consider every distinct document in the test set as a candidate for every query in the same dataset. This setup is more in line with the traditional IR evaluation methodologies, where the model needs to retrieve the most relevant documents from a single large document collection. Our empirical results in Section will show that the DESM model is a strong re-ranking signal, but as a standalone ranker, it is prone to false positives. Yet, when we mix our neural model (DESM) with a counting based model (BM25), good performance is achieved. For all the experiments we report the normalized discounted cumulative gain (NDCG) at different rank positions as a measure of performance for the different models under study. Baseline models We compare the DESM models to a term-matching based baseline, in BM25, and a vector space model baseline, in Latent Semantic Analysis (LSA) CIT . For the BM25 baseline we use the values of 1.7 for the FORMULA parameter and 0.95 for the FORMULA parameter based on a parameter sweep on the implicit feedback based training set. The LSA model is trained on the body text of 366,470 randomly sampled documents from Bing's index with a vocabulary size of 480,608 words. Note that unlike the word2vec models that train on word co-occurrence data, the LSA model by default trains on a word-document matrix. Results TABLE FIGURE FIGURE Table REF shows the NCDG based performance evaluations under the telescoping setup. On both the explicitly judged and the implicit feedback based test sets the FORMULA performs significantly better than the BM25 and the LSA baselines, as well as the FORMULA model. Under the all documents as candidates setup in Table REF , however, the DESMs (both IN-IN and IN-OUT) are clearly seen to not perform well as standalone document rankers. The mixture of FORMULA (trained on queries) and BM25 rectifies this problem and gives the best NDCG result under the non-telescoping settings and demonstrates a statistically significant improvement over the BM25 baseline. Figure REF illustrates that the FORMULA is the most discriminating feature for the relevant and the irrelevant documents retrieved by a first stage retrieval system. However, BM25 is clearly superior in separating out the random irrelevant documents in the candidate set. The mixture model, unsurprisingly, has the good properties from both the FORMULA and the BM25 models. Figure REF shows the joint distribution of the scores from the different models which further reinforces these points and shows that the DESM and the BM25 models make different errors. We do not report the results of evaluating the mixture models under the telescoping setup because tuning the FORMULA parameter under those settings on the training set results in the best performance from the standalone DESM models. Overall, we conclude that the DESM is primarily suited for ranking at top positions or in conjunction with other document ranking features. Interestingly, under the telescoping settings, the LSA baseline also shows some (albeit small) improvement over the BM25 baseline on the implicit feedback based test set but a loss on the explicitly judged test set. With respect to the CBOW's training data, the DESM models with the embeddings trained on the query corpus performs significantly better than the models trained on document body text across different configurations. We have a plausible hypothesis on why this happens. Users tend to choose the most significant terms that they expect to match in the target document to formulate their search queries. Therefore in the query corpus, one may say that, the less important terms from the document corpus has been filtered out. Therefore when training on the query corpus the CBOW model is more likely to see important terms within the context window compared to when trained on a corpus of document body text, which may make it a better training dataset for the Word2vec model. Related Work Term based IR For an overview of lexical matching approaches for information retrieval, such as the vector space, probabilistic and language modelling approach, see CIT . In Salton's classic vector space model CIT queries and documents are represented as sparse vectors in a vector space of dimensionality |V|, where V is the word vocabulary. Elements in the vector are non-zero if that term occurs. Documents can be ranked in descending order of cosine similarity with the query, although a wide variety of weighting and similarity functions are possible CIT . In contrast to the classical vector space model, LSA CIT , PLSA CIT and LDA CIT learn dense vector representations of much lower dimensionality. It has been suggested that these models perform poorly as standalone retrieval models CIT unless combined with other TF-IDF like features. In our approach the query and documents are also low dimensional dense vectors. We learn 200-dimensional neural word embeddings, and generate document vectors as the centroids of all the word vectors. CIT suggested that term correlation data is less sparse than term-document matrix and hence may be more effective for training embeddings. The probabilistic model of information retrieval leads to the development of the BM25 ranking feature CIT . The increase in BM25 as term frequency increases is justified according to the 2-Poisson model CIT , which makes a distinction between documents about a term and documents that merely mention that term. Those two types of document have term frequencies from two different Poisson distributions, which justifies the use of term frequency as evidence of aboutness. By contrast, the model introduced in this paper uses the occurrence of other related terms as evidence of aboutness. For example, under the 2-Poisson model a document about Eminem will tend to mention the term `eminem' repeatedly. Under our all-pairs vector model, a document about Eminem will tend to contain more related terms such as `rap', `tracklist' and `performs'. Our experiments show both notions of aboutness to be useful. Neural embeddings for IR The word embeddings produced by the CBOW and SG models have been shown to be surprisingly effective at capturing detailed semantics useful for various Natural Language Processing (NLP) and reasoning tasks, including word analogies CIT . Recent papers have explored in detail the SG and CBOW training methodology CIT and its connection to other approaches for learning word embeddings such as explicit vector space representations CIT , matrix factorization CIT and density-based representations CIT . CIT evaluated neural word embeddings against traditional word counting approaches and demonstrated the success of the former on a variety of NLP tasks. However, more recent works CIT have shown that there does not seem to be one embedding approach that is best for all tasks. This observation is similar to ours, where we note that IN-IN and IN-OUT model different kinds of word relationships. Although IN-IN, for example, works well for word analogy tasks CIT , it might perform less effectively for other tasks, such as those in information retrieval. If so, instead of claiming that any one embedding captures \u201csemantics\u201d, it is probably better to characterize embeddings according to which tasks they perform well on. Our paper is not the first to apply neural word embeddings in IR. MAINCIT recently proposed a generalized language model for IR that incorporates IN-IN similarities.", "bow": [["2405884322", "Query Expansion with Locally-Trained Word Embeddings"], ["2250460709", "Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content"], ["2143196462", "Exploring Session Context using Distributed Representations of Queries and Reformulations"], ["2515351093", "Embedding-based Query Language Models"], ["2466445867", "Using Word Embeddings for Automatic Query Expansion."]], "pp": [["2405884322", "Query Expansion with Locally-Trained Word Embeddings"], ["2250460709", "Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content"], ["2143196462", "Exploring Session Context using Distributed Representations of Queries and Reformulations"], ["2515351093", "Embedding-based Query Language Models"], ["2466445867", "Using Word Embeddings for Automatic Query Expansion."]]}, {"context": " The transitivity problem occurs when there are three records a, b, c and while a matches to b, b matches to c, a does not match with c. Han et al. MAINCIT used K-spectral clustering which had scaling issues and the K (number of clusters) was heuristically determined.", "bow": [["2042924775", "Completion of a set of rules modulo a set of equations"], ["2104618058", "Characterization of failures in an IP backbone"], ["2797226258", "Functions of one complex variable"], ["2487908531", "Formal verification of probabilistic algorithms"], ["1530061632", "Succinct Data Structures for Searchable Partial Sums"]], "pp": [["2042924775", "Completion of a set of rules modulo a set of equations"], ["2104618058", "Characterization of failures in an IP backbone"], ["2797226258", "Functions of one complex variable"], ["2487908531", "Formal verification of probabilistic algorithms"], ["1530061632", "Succinct Data Structures for Searchable Partial Sums"]]}, {"context": "Degree centrality describes the degree information of each node MAINCIT CIT .", "bow": [["2134784378", "Axioms for Centrality"], ["1967570846", "The centrality index of a graph"], ["1992250165", "Identifying influential nodes in complex networks"], ["2127405101", "A Graph-theoretic perspective on centrality"], ["1986310535", "Rethinking centrality: Methods and examples\u2606"]], "pp": [["2134784378", "Axioms for Centrality"], ["1967570846", "The centrality index of a graph"], ["1992250165", "Identifying influential nodes in complex networks"], ["2127405101", "A Graph-theoretic perspective on centrality"], ["1986310535", "Rethinking centrality: Methods and examples\u2606"]]}, {"context": "Betweenness centrality describes the frequencies of nodes in the shortest paths between two indirectly connected nodes MAINCIT CIT CIT .", "bow": [["1513185775", "The rush in a directed graph"], ["1971937094", "A set of measures of centrality based on betweenness"], ["2053374102", "A set of measures of centrality based upon betweenness"], ["1964678328", "Total Ordering Problem"], ["1967570846", "The centrality index of a graph"]], "pp": [["1513185775", "The rush in a directed graph"], ["1971937094", "A set of measures of centrality based on betweenness"], ["2053374102", "A set of measures of centrality based upon betweenness"], ["1964678328", "Total Ordering Problem"], ["1967570846", "The centrality index of a graph"]]}, {"context": "Closeness centrality describes the efficiency of the information propagation from one node to the other nodes MAINCIT CIT CIT .", "bow": [["2025844353", "Communication Patterns in Task\u2010Oriented Groups"], ["1967570846", "The centrality index of a graph"], ["1620903941", "The h-Index of a Graph and Its Application to Dynamic Subgraph Statistics"], ["1992250165", "Identifying influential nodes in complex networks"], ["2134784378", "Axioms for Centrality"]], "pp": [["2025844353", "Communication Patterns in Task\u2010Oriented Groups"], ["1967570846", "The centrality index of a graph"], ["1620903941", "The h-Index of a Graph and Its Application to Dynamic Subgraph Statistics"], ["1992250165", "Identifying influential nodes in complex networks"], ["2134784378", "Axioms for Centrality"]]}, {"context": "Another widely used technique relies on the social network that a user is connected to, in order to infer a user's location from that of their followers and followees MAINCIT .", "bow": [["2076219102", "TwitterRank: finding topic-sensitive influential twitterers"], ["2019177758", "Analyzing user modeling on twitter for personalized news recommendations"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"], ["2140535046", "Location prediction in social media based on tie strength"]], "pp": [["2076219102", "TwitterRank: finding topic-sensitive influential twitterers"], ["2019177758", "Analyzing user modeling on twitter for personalized news recommendations"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"], ["2140535046", "Location prediction in social media based on tie strength"]]}, {"context": "Closeness centrality describes the efficiency of the information propagation from one node to the other nodes CIT CIT MAINCIT .", "bow": [["2025844353", "Communication Patterns in Task\u2010Oriented Groups"], ["1967570846", "The centrality index of a graph"], ["1620903941", "The h-Index of a Graph and Its Application to Dynamic Subgraph Statistics"], ["1992250165", "Identifying influential nodes in complex networks"], ["2134784378", "Axioms for Centrality"]], "pp": [["2025844353", "Communication Patterns in Task\u2010Oriented Groups"], ["1967570846", "The centrality index of a graph"], ["1620903941", "The h-Index of a Graph and Its Application to Dynamic Subgraph Statistics"], ["1992250165", "Identifying influential nodes in complex networks"], ["2134784378", "Axioms for Centrality"]]}, {"context": "Citation relations between scientific papers, and the citation distribution of papers was studied CIT CIT MAINCIT , and shows that some papers are not cited at all, most papers are cited once, while a little part of papers covers the references of most papers in a research area.", "bow": [["2019672642", "The first-mover advantage in scientific publication"], ["2060106983", "Stretched exponential distributions in nature and economy: \u201cfat tails\u201d with characteristic scales"], ["2064568176", "The simultaneous evolution of author and paper networks"], ["2168160236", "The skewness of science"], ["2151866568", "Universality of citation distributions: Toward an objective measure of scientific impact"]], "pp": [["2019672642", "The first-mover advantage in scientific publication"], ["2060106983", "Stretched exponential distributions in nature and economy: \u201cfat tails\u201d with characteristic scales"], ["2064568176", "The simultaneous evolution of author and paper networks"], ["2168160236", "The skewness of science"], ["2151866568", "Universality of citation distributions: Toward an objective measure of scientific impact"]]}, {"context": "More specifically, the type of RNNs we use are Long Short Term Memory (LSTM) networks MAINCIT , which are capable of learning long-term dependencies, i.e. estimating the probability of the next character in the sequence based on the characters not just immediately preceding it, but also occurring further back.", "bow": [["2064675550", "Long short-term memory"], ["2026243162", "LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework"], ["2053831280", "Long short-term memory in recurrent neural networks"], ["2513222501", "Framewise phoneme classification with bidirectional LSTM and other neural network architectures"], ["1566256432", "Bidirectional LSTM networks for improved phoneme classification and recognition"]], "pp": [["2064675550", "Long short-term memory"], ["2026243162", "LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework"], ["1566256432", "Bidirectional LSTM networks for improved phoneme classification and recognition"], ["1560739766", "Applying LSTM to Time Series Predictable through Time-Window Approaches"], ["2136848157", "Learning to Forget: Continual Prediction with LSTM"]], "np": [["1566256432", "Bidirectional LSTM networks for improved phoneme classification and recognition"], ["2026243162", "LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework"], ["1560739766", "Applying LSTM to Time Series Predictable through Time-Window Approaches"], ["2064675550", "Long short-term memory"], ["2507974895", "context2vec: Learning Generic Context Embedding with Bidirectional LSTM."]]}, {"context": "As a result, we are not interested in finding the single mode for the rank distribution over all queriesThis would lead to something like the static rank over all possible objects in the database - like those in Google's PageRank MAINCIT ., but in finding the rank mode for each query.", "bow": [["2240863905", "On defining generalized rank weights"], ["2066636486", "The anatomy of a large-scale hypertextual Web search engine"], ["2165899219", "Tensor Rank: Some Lower and Upper Bounds"], ["2460641408", "On the similarities between generalized rank and Hamming weights and their applications to network coding"], ["1854214752", "The PageRank Citation Ranking: Bringing Order to the Web."]], "pp": [["2240863905", "On defining generalized rank weights"], ["2066636486", "The anatomy of a large-scale hypertextual Web search engine"], ["2165899219", "Tensor Rank: Some Lower and Upper Bounds"], ["2460641408", "On the similarities between generalized rank and Hamming weights and their applications to network coding"], ["1854214752", "The PageRank Citation Ranking: Bringing Order to the Web."]]}, {"context": "A better search engine that support searching in various categories and relatively more complex queries (union, intersection) on patterns is therefore required MAINCIT .", "bow": [["299839057", "Similarity Search: The Metric Space Approach"], ["2043708011", "A real-time search engine for the Web of Things"], ["2063039343", "Adaptive intersection and t -threshold problems"], ["1521225740", "Geometric range searching and its relatives"], ["1564094940", "Query recommendation using query logs in search engines"]], "pp": [["299839057", "Similarity Search: The Metric Space Approach"], ["2043708011", "A real-time search engine for the Web of Things"], ["2063039343", "Adaptive intersection and t -threshold problems"], ["1521225740", "Geometric range searching and its relatives"], ["1564094940", "Query recommendation using query logs in search engines"]]}, {"context": " Citation networks have also been explored for ranking articles by importance, i.e., authority ( MAINCIT ).", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2132631086", "The History and Meaning of the Journal Impact Factor"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["2146593092", "Generating extractive summaries of scientific paradigms"], ["2095972207", "Clickstream data yields high-resolution maps of science"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2132631086", "The History and Meaning of the Journal Impact Factor"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["2146593092", "Generating extractive summaries of scientific paradigms"], ["2095972207", "Clickstream data yields high-resolution maps of science"]]}, {"context": "Inspired by MAINCIT and CIT , we create a recommendation graph, as shown in Fig. REF , consisting of items, groups, and item genres as nodes.", "bow": [["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"], ["2177562712", "Neural Network Matrix Factorization"], ["2262817822", "Session-based Recommendations with Recurrent Neural Networks"], ["2003605635", "Group recommendations with rank aggregation and collaborative filtering"], ["2508504774", "Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence"]], "pp": [["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"], ["2177562712", "Neural Network Matrix Factorization"], ["2262817822", "Session-based Recommendations with Recurrent Neural Networks"], ["2003605635", "Group recommendations with rank aggregation and collaborative filtering"], ["2508504774", "Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence"]]}, {"context": "Word embeddings have also been studied in other IR contexts such as term reweighting CIT , cross-lingual retrieval MAINCIT and short-text similarity CIT .", "bow": [["2260194779", "A Dual Embedding Space Model for Document Ranking."], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"], ["2539671052", "Learning to Match using Local and Distributed Representations of Text for Web Search"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"]], "pp": [["2260194779", "A Dual Embedding Space Model for Document Ranking."], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"], ["2539671052", "Learning to Match using Local and Distributed Representations of Text for Web Search"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"]]}, {"context": "The result of their study shows that Normalized Discounted Cumulative Gain (NDCG) MAINCIT is the measure that correlates best with the user perceived quality.", "bow": [["1985554184", "IR evaluation methods for retrieving highly relevant documents"], ["2069870183", "Cumulated gain-based evaluation of IR techniques"], ["2155211665", "Learning to Rank by Optimizing NDCG Measure"], ["2296244148", "Collaborative ranking"], ["2140472169", "On NDCG consistency of listwise ranking methods"]], "pp": [["1985554184", "IR evaluation methods for retrieving highly relevant documents"], ["2069870183", "Cumulated gain-based evaluation of IR techniques"], ["2155211665", "Learning to Rank by Optimizing NDCG Measure"], ["2296244148", "Collaborative ranking"], ["2140472169", "On NDCG consistency of listwise ranking methods"]]}, {"context": "Private Information Retrieval, Anonymous communications, Private Queries, Differential Privacy Lower-Cost FORMULA -Private Information Retrieval[2018/12/11 13:53:03 Introduction Despite many years of research and significant advances, Private Information Retrieval (PIR) still suffers from scalability issues that were identified some time ago CIT : both information theoretic MAINCIT (IT-PIR) and computational CIT PIR schemes require database servers to operate on all records for each private query to conceal the sought record.", "bow": [["2073346043", "Private information retrieval"], ["2154654620", "Replication is not needed: single database, computationally-private information retrieval"], ["1593123147", "Information-Theoretic Private Information Retrieval: A Unified Construction"], ["1619839175", "Codes for distributed PIR with low storage overhead"], ["1986070285", "Private information retrieval"]], "pp": [["2073346043", "Private information retrieval"], ["2154654620", "Replication is not needed: single database, computationally-private information retrieval"], ["1593123147", "Information-Theoretic Private Information Retrieval: A Unified Construction"], ["1619839175", "Codes for distributed PIR with low storage overhead"], ["1986070285", "Private information retrieval"]]}, {"context": "Related Work Private Information Retrieval (PIR) was introduced in 1995 by Chor et al. MAINCIT to enable private access to public database records.", "bow": [["2073346043", "Private information retrieval"], ["1593123147", "Information-Theoretic Private Information Retrieval: A Unified Construction"], ["2154654620", "Replication is not needed: single database, computationally-private information retrieval"], ["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["1986070285", "Private information retrieval"]], "pp": [["2073346043", "Private information retrieval"], ["1593123147", "Information-Theoretic Private Information Retrieval: A Unified Construction"], ["2154654620", "Replication is not needed: single database, computationally-private information retrieval"], ["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["1986070285", "Private information retrieval"]]}, {"context": "We note that Chor et al. MAINCIT also make passing allusion to statistical and leaky definitions of PIR in their seminal paper, only to focus on perfectly information-theoretic schemes.", "bow": [["2007208840", "General constructions for information-theoretic private information retrieval"], ["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["1593123147", "Information-Theoretic Private Information Retrieval: A Unified Construction"], ["1986070285", "Private information retrieval"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"]], "pp": [["2007208840", "General constructions for information-theoretic private information retrieval"], ["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["1593123147", "Information-Theoretic Private Information Retrieval: A Unified Construction"], ["1986070285", "Private information retrieval"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"]]}, {"context": "Sparse-PIR We next adapt Chor's simplest IT-PIR scheme MAINCIT to reduce the number of database records accessed to answer each query.", "bow": [["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["2007208840", "General constructions for information-theoretic private information retrieval"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"], ["1986070285", "Private information retrieval"], ["1619839175", "Codes for distributed PIR with low storage overhead"]], "pp": [["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["2007208840", "General constructions for information-theoretic private information retrieval"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"], ["1986070285", "Private information retrieval"], ["1619839175", "Codes for distributed PIR with low storage overhead"]]}, {"context": "The Sparse-PIR scheme diminishes the computation cost by a factor of FORMULA compared to Chor PIR MAINCIT , while the Direct Request schemes induce no record processing.", "bow": [["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["2007208840", "General constructions for information-theoretic private information retrieval"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"], ["1619839175", "Codes for distributed PIR with low storage overhead"], ["1986070285", "Private information retrieval"]], "pp": [["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["2007208840", "General constructions for information-theoretic private information retrieval"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"], ["1619839175", "Codes for distributed PIR with low storage overhead"], ["1986070285", "Private information retrieval"]]}, {"context": "In CIT a queuing system was proposed to find the optimal policy of selecting advertisers to make private contracts, whereas in MAINCIT , the focus is on pricing ads properly in either of the two settings.", "bow": [["2073358075", "Pricing guaranteed contracts in online display advertising"], ["2123937625", "Evaluating implicit measures to improve web search"], ["2056705371", "Optimizing relevance and revenue in ad search: a query substitution approach"], ["2073448073", "A semantic approach to contextual advertising"], ["1769120835", "AdWords and generalized on-line matching"]], "pp": [["2073358075", "Pricing guaranteed contracts in online display advertising"], ["2123937625", "Evaluating implicit measures to improve web search"], ["2056705371", "Optimizing relevance and revenue in ad search: a query substitution approach"], ["2073448073", "A semantic approach to contextual advertising"], ["1769120835", "AdWords and generalized on-line matching"]]}, {"context": "Lastly and most importantly, a decision has to be made on which page MAINCIT and which users CIT these ads should be matched with, probably in a real-time fashion using text summaries CIT .", "bow": [["1529686416", "On the Equilibria and Efficiency of the GSP Mechanism in Keyword Auctions with Externalities"], ["2133198311", "Position Auctions with Consumer Search"], ["2123937625", "Evaluating implicit measures to improve web search"], ["2090883204", "Predicting clicks: estimating the click-through rate for new ads"], ["1769120835", "AdWords and generalized on-line matching"]], "pp": [["1529686416", "On the Equilibria and Efficiency of the GSP Mechanism in Keyword Auctions with Externalities"], ["2133198311", "Position Auctions with Consumer Search"], ["2123937625", "Evaluating implicit measures to improve web search"], ["2090883204", "Predicting clicks: estimating the click-through rate for new ads"], ["1769120835", "AdWords and generalized on-line matching"]]}, {"context": "Traditionally optimizing matching between webpages and ads belongs to the field of contextual advertising research MAINCIT .", "bow": [["1769120835", "AdWords and generalized on-line matching"], ["2123937625", "Evaluating implicit measures to improve web search"], ["2073448073", "A semantic approach to contextual advertising"], ["2152314154", "Accurately interpreting clickthrough data as implicit feedback"], ["2056705371", "Optimizing relevance and revenue in ad search: a query substitution approach"]], "pp": [["1769120835", "AdWords and generalized on-line matching"], ["2123937625", "Evaluating implicit measures to improve web search"], ["2073448073", "A semantic approach to contextual advertising"], ["2152314154", "Accurately interpreting clickthrough data as implicit feedback"], ["2056705371", "Optimizing relevance and revenue in ad search: a query substitution approach"]]}, {"context": "In MAINCIT a system utilizing both semantic and syntactic features was proposed to address the problem.", "bow": [["2111024941", "Unsupervised Semantic Role Induction with Graph Partitioning"], ["2132529109", "Composition of Word Representations Improves Semantic Role Labelling"], ["2168963845", "Semantic Frame Identification with Distributed Word Representations"], ["2102450109", "A Bayesian Model for Unsupervised Semantic Parsing"], ["2105544934", "Patterns in syntactic dependency networks"]], "pp": [["2111024941", "Unsupervised Semantic Role Induction with Graph Partitioning"], ["2132529109", "Composition of Word Representations Improves Semantic Role Labelling"], ["2168963845", "Semantic Frame Identification with Distributed Word Representations"], ["2102450109", "A Bayesian Model for Unsupervised Semantic Parsing"], ["2105544934", "Patterns in syntactic dependency networks"]]}, {"context": "However, this definition has since been used in machine learning CIT , cloud computing CIT , and location indistinguishability together with PIR MAINCIT to evaluate and minimize the privacy risk.", "bow": [["1986070285", "Private information retrieval"], ["2007208840", "General constructions for information-theoretic private information retrieval"], ["1619839175", "Codes for distributed PIR with low storage overhead"], ["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"]], "pp": [["1986070285", "Private information retrieval"], ["2007208840", "General constructions for information-theoretic private information retrieval"], ["1619839175", "Codes for distributed PIR with low storage overhead"], ["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"]]}, {"context": "The objective function in Equation REF is now completed with the belief update formulas (constraints), all of which are now summarized together as the following FORMULA subject to FORMULA It is worth noticing that the update in Equation REF is closely related to the \u201cword of mouth\u201d heuristic adopted in collaborative filtering MAINCIT .", "bow": [["2037881439", "Epidemic threshold in structured scale-free networks"], ["2123820136", "Percolation critical exponents in scale-free networks"], ["2043403353", "Fab: content-based, collaborative recommendation"], ["2142114717", "Large Scale Transductive SVMs"], ["1982853556", "Propositional belief base update and minimal change"]], "pp": [["2037881439", "Epidemic threshold in structured scale-free networks"], ["2123820136", "Percolation critical exponents in scale-free networks"], ["2043403353", "Fab: content-based, collaborative recommendation"], ["2142114717", "Large Scale Transductive SVMs"], ["1982853556", "Propositional belief base update and minimal change"]]}, {"context": "Using the heuristic, the final rating prediction is a weighted average across all similar users MAINCIT and the similarity is usually measured by cosine similarity or Pearson's correlation coefficient and user means are used to remove the bias of mean ratings among users CIT .", "bow": [["2113952909", "Thirteen Ways to Look at the Correlation Coefficient"], ["2120667260", "Exploring social influence via posterior effect of word-of-mouth recommendations"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["2085937320", "An algorithmic framework for performing collaborative filtering"], ["2124591829", "Social information filtering: algorithms for automating \u201cword of mouth\u201d"]], "pp": [["2113952909", "Thirteen Ways to Look at the Correlation Coefficient"], ["2120667260", "Exploring social influence via posterior effect of word-of-mouth recommendations"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["2085937320", "An algorithmic framework for performing collaborative filtering"], ["2124591829", "Social information filtering: algorithms for automating \u201cword of mouth\u201d"]]}, {"context": "One popular measurement is the notion of an anonymity set, which was introduced for the dining cryptographers problem MAINCIT .", "bow": [["2087811006", "The dining cryptographers problem: unconditional sender and recipient untraceability"], ["1812425403", "CSP and Anonymity"], ["2056277260", "Anonymity protocols as noisy channels"], ["2482903976", "Towards an information theoretic metric for anonymity"], ["1541510840", "Stabilizing Link-Coloration of Arbitrary Networks with Unbounded Byzantine Faults"]], "pp": [["2087811006", "The dining cryptographers problem: unconditional sender and recipient untraceability"], ["1812425403", "CSP and Anonymity"], ["2056277260", "Anonymity protocols as noisy channels"], ["2482903976", "Towards an information theoretic metric for anonymity"], ["1541510840", "Stabilizing Link-Coloration of Arbitrary Networks with Unbounded Byzantine Faults"]]}, {"context": "Citation relations between scientific papers, and the citation distribution of papers was studied MAINCIT CIT CIT , and shows that some papers are not cited at all, most papers are cited once, while a little part of papers covers the references of most papers in a research area.", "bow": [["2019672642", "The first-mover advantage in scientific publication"], ["2060106983", "Stretched exponential distributions in nature and economy: \u201cfat tails\u201d with characteristic scales"], ["2064568176", "The simultaneous evolution of author and paper networks"], ["2168160236", "The skewness of science"], ["2151866568", "Universality of citation distributions: Toward an objective measure of scientific impact"]], "pp": [["2019672642", "The first-mover advantage in scientific publication"], ["2060106983", "Stretched exponential distributions in nature and economy: \u201cfat tails\u201d with characteristic scales"], ["2064568176", "The simultaneous evolution of author and paper networks"], ["2168160236", "The skewness of science"], ["2151866568", "Universality of citation distributions: Toward an objective measure of scientific impact"]]}, {"context": "The methods of MAINCIT , which embeds the vector into a binary space, better satisfies the memory constraint.", "bow": [["2014279255", "Approximation of zonoids by zonotopes"], ["1530572757", "Embeddings and extensions in analysis"], ["2044902313", "Memory coherence in shared virtual memory systems"], ["1905029509", "Metric embeddings with relaxed guarantees"], ["2342395274", "Recurrent Memory Networks for Language Modeling"]], "pp": [["2014279255", "Approximation of zonoids by zonotopes"], ["1530572757", "Embeddings and extensions in analysis"], ["2044902313", "Memory coherence in shared virtual memory systems"], ["1905029509", "Metric embeddings with relaxed guarantees"], ["2342395274", "Recurrent Memory Networks for Language Modeling"]]}, {"context": "But as shown in CIT , this post verification is also important for methods based on binary MAINCIT or quantized codes CIT , as the ranking provided on output of the large scale search is significantly improved when verifying the few first hypotheses.", "bow": [["2341106557", "Quantized consensus"], ["2130025446", "In transition from global to modular temporal reasoning about programs"], ["2148575324", "Polynomial Codes Over Certain Finite Fields"], ["2166966427", "Perfect binary codes: constructions, properties, and enumeration"], ["1771040533", "Three-weight cyclic codes and their weight distributions"]], "pp": [["2341106557", "Quantized consensus"], ["2130025446", "In transition from global to modular temporal reasoning about programs"], ["2148575324", "Polynomial Codes Over Certain Finite Fields"], ["2166966427", "Perfect binary codes: constructions, properties, and enumeration"], ["1771040533", "Three-weight cyclic codes and their weight distributions"]]}, {"context": "Testing on a large scale is important, as most ANN methods are usually evaluated on sets of unrealistic size, thereby ignoring memory issues that arise in real applications, where billions of vectors have to be handled MAINCIT .", "bow": [["2002016471", "Artificial neural networks: a tutorial"], ["2225766585", "Nearest neighbor search : the old, the new, and the impossible"], ["2064671795", "Approximate nearest neighbor algorithms for Frechet distance via product metrics"], ["2411707397", "A Survey on Learning to Hash"], ["2092269560", "PERFORMANCE MEASURES FOR INFORMATION EXTRACTION"]], "pp": [["2002016471", "Artificial neural networks: a tutorial"], ["2225766585", "Nearest neighbor search : the old, the new, and the impossible"], ["2064671795", "Approximate nearest neighbor algorithms for Frechet distance via product metrics"], ["2411707397", "A Survey on Learning to Hash"], ["2092269560", "PERFORMANCE MEASURES FOR INFORMATION EXTRACTION"]]}, {"context": "To stay focused, we consider the impression has only one ad slot, while bearing in mind that the scenario of multiple ad slots per impression can be addressed by incorporating user click-through models (for pay-per-click ads) to remove rank bias MAINCIT or consider them from different impressions (for pay-per-view ads).", "bow": [["2133198311", "Position Auctions with Consumer Search"], ["1529686416", "On the Equilibria and Efficiency of the GSP Mechanism in Keyword Auctions with Externalities"], ["2090883204", "Predicting clicks: estimating the click-through rate for new ads"], ["2123937625", "Evaluating implicit measures to improve web search"], ["1992549066", "An experimental comparison of click position-bias models"]], "pp": [["2133198311", "Position Auctions with Consumer Search"], ["1529686416", "On the Equilibria and Efficiency of the GSP Mechanism in Keyword Auctions with Externalities"], ["2090883204", "Predicting clicks: estimating the click-through rate for new ads"], ["2123937625", "Evaluating implicit measures to improve web search"], ["1992549066", "An experimental comparison of click position-bias models"]]}, {"context": "A further study is also needed in order to incorporate the user click-through model and remove rank bias on the same webpage MAINCIT .", "bow": [["2402441596", "Click Models for Web Search"], ["2106630408", "Click chain model in web search"], ["1992549066", "An experimental comparison of click position-bias models"], ["2154739689", "Efficient multiple-click models in web search"], ["2259177609", "DCM bandits: learning to rank with multiple clicks"]], "pp": [["2402441596", "Click Models for Web Search"], ["2106630408", "Click chain model in web search"], ["1992549066", "An experimental comparison of click position-bias models"], ["2154739689", "Efficient multiple-click models in web search"], ["2259177609", "DCM bandits: learning to rank with multiple clicks"]]}, {"context": "We show that our results both in terms of predictive performance and training time are competitive with other well-known methods such as RankNet CIT , Ranking SVM CIT and ListMLE MAINCIT .", "bow": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]], "pp": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]]}, {"context": "Listwise approach which models the distribution of permutations CIT CIT MAINCIT .", "bow": [["2096916804", "The enumeration of simple permutations."], ["1499300513", "Metrics on Permutations, a Survey"], ["2070286422", "Critically indecomposable partially ordered sets, graphs, tournaments and other binary relational structures"], ["2083354900", "Pattern matching for permutations"], ["1981803420", "Simple permutations and pattern restricted permutations"]], "pp": [["2096916804", "The enumeration of simple permutations."], ["1499300513", "Metrics on Permutations, a Survey"], ["2070286422", "Critically indecomposable partially ordered sets, graphs, tournaments and other binary relational structures"], ["2083354900", "Pattern matching for permutations"], ["1981803420", "Simple permutations and pattern restricted permutations"]]}, {"context": "In fact, the methods suggested in CIT MAINCIT are applications of the Plackett-Luce model.", "bow": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"], ["2167598090", "Autonomous Demand Side Management Based on Energy Consumption Scheduling and Instantaneous Load Billing: An Aggregative Game Approach"]], "pp": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"], ["2167598090", "Autonomous Demand Side Management Based on Energy Consumption Scheduling and Instantaneous Load Billing: An Aggregative Game Approach"]]}, {"context": "For comparison, we implement several well-known methods, including RankNet CIT , Ranking SVM CIT and ListMLE MAINCIT .", "bow": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]], "pp": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]]}, {"context": "We use dropout MAINCIT of hidden units to avoid overfitting, and do early stopping by observing accuracy on the dev set \u2013 if the performance (accuracy score) does not increase for 10 consecutive epochs, we stop training and pick the best model recorded so far.", "bow": [["2156876426", "Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping"], ["2113457868", "Generalization and Parameter Estimation in Feedforward Nets: Some Experiments"], ["1904365287", "Improving neural networks by preventing co-adaptation of feature detectors"], ["2095705004", "Dropout: a simple way to prevent neural networks from overfitting"], ["2016043834", "Automatic early stopping using cross validation: quantifying the criteria"]], "pp": [["2156876426", "Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping"], ["2113457868", "Generalization and Parameter Estimation in Feedforward Nets: Some Experiments"], ["1904365287", "Improving neural networks by preventing co-adaptation of feature detectors"], ["2095705004", "Dropout: a simple way to prevent neural networks from overfitting"], ["2016043834", "Automatic early stopping using cross validation: quantifying the criteria"]]}, {"context": "This is similar to the telescoping evaluation setup described by MAINCIT , where multiple nested rankers are used to achieve better retrieval performance over a single ranker.", "bow": [["2073248814", "Fidelity, Soundness, and Efficiency of Interleaved Comparison Methods"], ["2096937925", "High accuracy retrieval with multiple nested ranker"], ["2094965692", "The method of creative telescoping"], ["1672862537", "Parameterized telescoping proves algebraic independence of sums"], ["2003473029", "Multileaved Comparisons for Fast Online Evaluation"]], "pp": [["2073248814", "Fidelity, Soundness, and Efficiency of Interleaved Comparison Methods"], ["2096937925", "High accuracy retrieval with multiple nested ranker"], ["2094965692", "The method of creative telescoping"], ["1672862537", "Parameterized telescoping proves algebraic independence of sums"], ["2003473029", "Multileaved Comparisons for Fast Online Evaluation"]]}, {"context": "As mentioned in Section REF , there is a parallel between this experiment setup and the telescoping MAINCIT evaluation strategy, and has been used often in recent literature (e.g., CIT ).", "bow": [["2094965692", "The method of creative telescoping"], ["1672862537", "Parameterized telescoping proves algebraic independence of sums"], ["2087893470", "A refined difference field theory for symbolic summation"], ["2145236745", "Telescopers for rational and algebraic functions via residues"], ["2166347079", "New Methods in Automatic Extracting"]], "pp": [["2094965692", "The method of creative telescoping"], ["1672862537", "Parameterized telescoping proves algebraic independence of sums"], ["2087893470", "A refined difference field theory for symbolic summation"], ["2145236745", "Telescopers for rational and algebraic functions via residues"], ["2166347079", "New Methods in Automatic Extracting"]]}, {"context": "Exact Solution Value Iteration Our revenue optimization problem in Equation REF could be solved exactly following a value iteration approach using Dynamic Programming MAINCIT .", "bow": [["2134289401", "Reinforcement learning as classification: leveraging modern classifiers"], ["2165060096", "Analysis of a Classification-based Policy Iteration Algorithm"], ["1968517895", "Pricing randomized allocations"], ["2168024904", "On the complexity of solving Markov decision problems"], ["2123917165", "Approximate Modified Policy Iteration"]], "pp": [["2001868457", "Dynamic programming meets the principle of inclusion and exclusion"], ["2096846396", "Robust People Tracking with Global Trajectory Optimization"], ["2165421048", "Policy Search by Dynamic Programming"], ["2294680424", "THE LINEAR PROGRAMMING APPROACH TO APPROXIMATE DYNAMIC PROGRAMMING"], ["2134289401", "Reinforcement learning as classification: leveraging modern classifiers"]], "np": [["2001868457", "Dynamic programming meets the principle of inclusion and exclusion"], ["2096846396", "Robust People Tracking with Global Trajectory Optimization"], ["2294680424", "THE LINEAR PROGRAMMING APPROACH TO APPROXIMATE DYNAMIC PROGRAMMING"], ["2165421048", "Policy Search by Dynamic Programming"], ["2798713234", "Dynamic programming"]]}, {"context": "We have the following statement and the Bellman equation MAINCIT .", "bow": [["2159810454", "A unifying framework for computational reinforcement learning theory"], ["2407775036", "Reinforcement Learning of POMDPs using Spectral Methods"], ["2072931156", "Linear least-squares algorithms for temporal difference learning"], ["2130005627", "Least-squares policy iteration"], ["2341171179", "Dynamic Programming"]], "pp": [["2095095223", "DYNAMIC PROGRAMMING AND LAGRANGE MULTIPLIERS"], ["2056653303", "On the Theory of Dynamic Programming"], ["2402108766", "Taming the noise in reinforcement learning via soft updates"], ["2159810454", "A unifying framework for computational reinforcement learning theory"], ["2407775036", "Reinforcement Learning of POMDPs using Spectral Methods"]], "np": [["2095095223", "DYNAMIC PROGRAMMING AND LAGRANGE MULTIPLIERS"], ["2056653303", "On the Theory of Dynamic Programming"], ["2124477018", "Approximate policy iteration: a survey and some new methods"], ["1545148916", "Dynamic programming: deterministic and stochastic models"], ["2402108766", "Taming the noise in reinforcement learning via soft updates"]]}, {"context": " Associaton Rule and Similarity (ARS) Another implementation of co-occurrence is a formulation of basic data mining, e.g. the association rule MAINCIT .", "bow": [["2045487373", "Algorithms for association rule mining \u2014 a general survey and comparison"], ["2040263743", "Confluence by decreasing diagrams"], ["2078937669", "Privacy preserving mining of association rules"], ["2000473687", "Mining quantitative association rules in large relational tables"], ["2166347079", "New Methods in Automatic Extracting"]], "pp": [["2045487373", "Algorithms for association rule mining \u2014 a general survey and comparison"], ["2040263743", "Confluence by decreasing diagrams"], ["2078937669", "Privacy preserving mining of association rules"], ["2000473687", "Mining quantitative association rules in large relational tables"], ["2166347079", "New Methods in Automatic Extracting"]]}, {"context": "Neural Entity Grid Model Although the entity grid and its extensions have been successfully applied to many downstream applications including coherence rating CIT , readability assessment CIT , essay scoring CIT , and story generation MAINCIT , they have some limitations.", "bow": [["41621595", "A Coherence Model Based on Syntactic Patterns"], ["2019416425", "Revisiting Readability: A Unified Framework for Predicting Text Quality"], ["2140676672", "Modeling local coherence: An entity-based approach"], ["2133330855", "Automatically Evaluating Text Coherence Using Discourse Relations"], ["2020278455", "A survey of named entity recognition and classification"]], "pp": [["25648700", "Story generation with crowdsourced plot graphs"], ["41621595", "A Coherence Model Based on Syntactic Patterns"], ["2019416425", "Revisiting Readability: A Unified Framework for Predicting Text Quality"], ["2140676672", "Modeling local coherence: An entity-based approach"], ["2133330855", "Automatically Evaluating Text Coherence Using Discourse Relations"]], "np": [["25648700", "Story generation with crowdsourced plot graphs"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "An intuitive approach is collaborative filtering (CF) CIT MAINCIT CIT .", "bow": [["2100235918", "A survey of collaborative filtering techniques"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"], ["1539057251", "Ordinal Boltzmann Machines for collaborative filtering"], ["2043403353", "Fab: content-based, collaborative recommendation"]], "pp": [["2100235918", "A survey of collaborative filtering techniques"], ["1560991565", "An open architecture for collaborative filtering of netnews"], ["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"], ["1539057251", "Ordinal Boltzmann Machines for collaborative filtering"], ["2043403353", "Fab: content-based, collaborative recommendation"]]}, {"context": "It uses the known preferences of users to make recommendations or predictions to a target user MAINCIT .", "bow": [["2014607560", "On the Rationale of Group Decision-making"], ["1988793717", "Preferences in AI: An overview"], ["2123605402", "Dynamic matrix factorization: A state space approach"], ["2049670925", "Being accurate is not enough: how accuracy metrics have hurt recommender systems"], ["1971040550", "Evaluating collaborative filtering recommender systems"]], "pp": [["2100235918", "A survey of collaborative filtering techniques"], ["2014607560", "On the Rationale of Group Decision-making"], ["1988793717", "Preferences in AI: An overview"], ["2123605402", "Dynamic matrix factorization: A state space approach"], ["2049670925", "Being accurate is not enough: how accuracy metrics have hurt recommender systems"]], "np": [["2121710227", "A Comprehensive Survey of Neighborhood-Based Recommendation Methods"], ["2100235918", "A survey of collaborative filtering techniques"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "Traditional recommender systems mainly rely on the long-term user behavior history for recommendation, which follow the idea of collaborative filtering MAINCIT .", "bow": [["2043403353", "Fab: content-based, collaborative recommendation"], ["2171960770", "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions"], ["1966553486", "Using collaborative filtering to weave an information tapestry"], ["281665770", "Hybrid Recommender Systems: Survey and Experiments"], ["2100235918", "A survey of collaborative filtering techniques"]], "pp": [["2043403353", "Fab: content-based, collaborative recommendation"], ["2171960770", "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions"], ["1966553486", "Using collaborative filtering to weave an information tapestry"], ["281665770", "Hybrid Recommender Systems: Survey and Experiments"], ["2100235918", "A survey of collaborative filtering techniques"]]}, {"context": "Definition of Topological Properties We briefly review and define the following topological properties, which are grouped into three orders according to the scope of information required to compute them MAINCIT .", "bow": [["191595573", "Elementary Computable Topology"], ["1991566301", "Topology and data"], ["1570973162", "Topology; a first course"], ["2016000144", "A structured program to generate all topological sorting arrangements"], ["1530375570", "Modal Logics of Space"]], "pp": [["191595573", "Elementary Computable Topology"], ["1991566301", "Topology and data"], ["1570973162", "Topology; a first course"], ["2016000144", "A structured program to generate all topological sorting arrangements"], ["1530375570", "Modal Logics of Space"]]}, {"context": "AS Internet The Internet topology at the autonomous systems (AS) level has been extensively studied in recent years MAINCIT .", "bow": [["2050353626", "Towards capturing representative AS-level Internet topologies"], ["2139905147", "The internet AS-level topology: three data sources and one definitive metric"], ["2151972741", "On inferring autonomous system relationships in the internet"], ["2120514843", "AS relationships: inference and validation"], ["2165503871", "Scalable and accurate identification of AS-level forwarding paths"]], "pp": [["2050353626", "Towards capturing representative AS-level Internet topologies"], ["2139905147", "The internet AS-level topology: three data sources and one definitive metric"], ["2151972741", "On inferring autonomous system relationships in the internet"], ["2155589809", "Topology discovery by active probing"], ["2120514843", "AS relationships: inference and validation"]], "np": [["1495663756", "Signal/collect: graph algorithms for the (semantic) web"], ["1513398909", "Identifying expressions of emotion in text"], ["1564629734", "Computability, inference and modeling in probabilistic programming"], ["1694262664", "The Komlos Conjecture Holds for Vector Colorings"], ["1793674997", "Edge routing with ordered bundles"]]}, {"context": "Otherwise, techniques of fake accounts and malicious users detection in social networks can be used CIT MAINCIT .", "bow": [["1922851884", "BotGraph: large scale spamming botnet detection"], ["2233384038", "EVILCOHORT: detecting communities of malicious accounts on online services"], ["1850562331", "Spam detection on twitter using traditional classifiers"], ["1986678144", "Detecting spammers on social networks"], ["9223698", "Detecting Spammers on Twitter"]], "pp": [["1922851884", "BotGraph: large scale spamming botnet detection"], ["2233384038", "EVILCOHORT: detecting communities of malicious accounts on online services"], ["1850562331", "Spam detection on twitter using traditional classifiers"], ["1986678144", "Detecting spammers on social networks"], ["9223698", "Detecting Spammers on Twitter"]]}, {"context": "Tensor factorization MAINCIT , context-aware matrix factorization CIT and contextual sparse linear modeling CIT are the examples of the most effective contextual modeling algorithms in the recommender systems.", "bow": [["2102937240", "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering"], ["2130868038", "Context-aware recommender systems"], ["2054141820", "Matrix Factorization Techniques for Recommender Systems"], ["1998889130", "Matrix factorization techniques for context aware recommendation"], ["2112430581", "Incorporating contextual information in recommender systems using a multidimensional approach"]], "pp": [["2102937240", "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering"], ["2130868038", "Context-aware recommender systems"], ["2054141820", "Matrix Factorization Techniques for Recommender Systems"], ["1998889130", "Matrix factorization techniques for context aware recommendation"], ["2112430581", "Incorporating contextual information in recommender systems using a multidimensional approach"]], "np": [["1246381107", "Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation"], ["2024165284", "Tensor Decompositions and Applications"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "One example is the tensor factorization (TF) MAINCIT that was applied in the context-aware recommendations.", "bow": [["2073502026", "Positive tensor factorization"], ["1976801265", "Rubik: Knowledge Guided Tensor Factorization and Completion for Health Data Analytics"], ["2144920235", "Non-negative tensor factorization with applications to statistics and computer vision"], ["2102353396", "Multitaper Time-Frequency Reassignment for Nonstationary Spectrum Estimation and Chirp Enhancement"], ["2114510817", "Analyzing temporal dynamics in Twitter profiles for personalized recommendations in the social web"]], "pp": [["2073502026", "Positive tensor factorization"], ["1976801265", "Rubik: Knowledge Guided Tensor Factorization and Completion for Health Data Analytics"], ["2144920235", "Non-negative tensor factorization with applications to statistics and computer vision"], ["2102353396", "Multitaper Time-Frequency Reassignment for Nonstationary Spectrum Estimation and Chirp Enhancement"], ["2114510817", "Analyzing temporal dynamics in Twitter profiles for personalized recommendations in the social web"]]}, {"context": "Evaluation Protocol Our add the following models in the comparison: tensor factorization (TF) MAINCIT , similarity-based contextual sparse linear method using multidimensional context similarity (CSLIM_MCS) CIT and the semantic pre-filtering algorithm (SPF) CIT .", "bow": [["2073502026", "Positive tensor factorization"], ["2102937240", "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering"], ["2036543679", "Motivating Smartphone Collaboration in Data Acquisition and Distributed Computing"], ["2156890722", "Solving #SAT and Bayesian inference with backtracking search"], ["1976801265", "Rubik: Knowledge Guided Tensor Factorization and Completion for Health Data Analytics"]], "pp": [["2073502026", "Positive tensor factorization"], ["2102937240", "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering"], ["2036543679", "Motivating Smartphone Collaboration in Data Acquisition and Distributed Computing"], ["2156890722", "Solving #SAT and Bayesian inference with backtracking search"], ["1976801265", "Rubik: Knowledge Guided Tensor Factorization and Completion for Health Data Analytics"]]}, {"context": "Research on Anonymity Systems (AS) began in 1981 by David Chaum introducing the mixnet for anonymous e-mail MAINCIT .", "bow": [["1494929562", "Making Mix Nets Robust for Electronic Voting by Randomized Partial Checking"], ["1601001795", "Blind Signatures for Untraceable Payments"], ["2103647628", "Untraceable electronic mail, return addresses, and digital pseudonyms"], ["2050353626", "Towards capturing representative AS-level Internet topologies"], ["2151972741", "On inferring autonomous system relationships in the internet"]], "pp": [["1494929562", "Making Mix Nets Robust for Electronic Voting by Randomized Partial Checking"], ["1601001795", "Blind Signatures for Untraceable Payments"], ["2103647628", "Untraceable electronic mail, return addresses, and digital pseudonyms"], ["2050353626", "Towards capturing representative AS-level Internet topologies"], ["2151972741", "On inferring autonomous system relationships in the internet"]]}, {"context": "The shortest paths between each pair of nodes in a graph can be found by Floyd-Warshall algorithm with time complexity FORMULA MAINCIT , so the time complexity of betweenness centrality is also FORMULA .", "bow": [["2007572995", "Algorithm 97: Shortest path"], ["2154897810", "A measure of betweenness centrality based on random walks"], ["2099970772", "On variants of shortest-path betweenness centrality and their generic computation"], ["1971937094", "A set of measures of centrality based on betweenness"], ["2053374102", "A set of measures of centrality based upon betweenness"]], "pp": [["2007572995", "Algorithm 97: Shortest path"], ["2154897810", "A measure of betweenness centrality based on random walks"], ["2099970772", "On variants of shortest-path betweenness centrality and their generic computation"], ["1971937094", "A set of measures of centrality based on betweenness"], ["2053374102", "A set of measures of centrality based upon betweenness"]]}, {"context": "Pairwise approach which spans preference to binary classification CIT MAINCIT CIT methods, where the goal is to learn a classifier that can separate two documents (per query).", "bow": [["2002699418", "Preference formulas in relational queries"], ["175248941", "Interval orders and interval graphs"], ["1558832481", "Foundations of databases"], ["2170188482", "The Skyline operator"], ["2610698371", "Semantic hashing"]], "pp": [["2002699418", "Preference formulas in relational queries"], ["175248941", "Interval orders and interval graphs"], ["1558832481", "Foundations of databases"], ["2170188482", "The Skyline operator"], ["2610698371", "Semantic hashing"]]}, {"context": "This casts the ranking problem into a standard classification framework, wherein many algorithms are readily available, for example, SVM CIT , neural network and logistic regression CIT , and boosting MAINCIT .", "bow": [["2125607229", "On the Dual Formulation of Boosting Algorithms"], ["1973948212", "Applied Logistic Regression"], ["2067802667", "Discriminative models for information retrieval"], ["2093717447", "The Strength of Weak Learnability"], ["2108263314", "Boosting Algorithms as Gradient Descent"]], "pp": [["2125607229", "On the Dual Formulation of Boosting Algorithms"], ["1973948212", "Applied Logistic Regression"], ["2067802667", "Discriminative models for information retrieval"], ["2093717447", "The Strength of Weak Learnability"], ["2108263314", "Boosting Algorithms as Gradient Descent"]]}, {"context": "Listwise approach which models the distribution of permutations MAINCIT CIT CIT .", "bow": [["2096916804", "The enumeration of simple permutations."], ["1499300513", "Metrics on Permutations, a Survey"], ["2070286422", "Critically indecomposable partially ordered sets, graphs, tournaments and other binary relational structures"], ["2083354900", "Pattern matching for permutations"], ["1981803420", "Simple permutations and pattern restricted permutations"]], "pp": [["2096916804", "The enumeration of simple permutations."], ["1499300513", "Metrics on Permutations, a Survey"], ["2070286422", "Critically indecomposable partially ordered sets, graphs, tournaments and other binary relational structures"], ["2083354900", "Pattern matching for permutations"], ["1981803420", "Simple permutations and pattern restricted permutations"]]}, {"context": "In fact, the methods suggested in MAINCIT CIT are applications of the Plackett-Luce model.", "bow": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"], ["2167598090", "Autonomous Demand Side Management Based on Energy Consumption Scheduling and Instantaneous Load Billing: An Aggregative Game Approach"]], "pp": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"], ["2167598090", "Autonomous Demand Side Management Based on Energy Consumption Scheduling and Instantaneous Load Billing: An Aggregative Game Approach"]]}, {"context": "A common application of deep learning to IR is for learning to rank, covering a wide range of subtopics within this area, for instance from the earlier RankNet CIT , and methods for hyperlinked webpages CIT , to studies of listwise comparisons MAINCIT , short text pairwise reranking CIT , and elimination strategies CIT .", "bow": [["2149427297", "Learning to Rank for Information Retrieval"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"]], "pp": [["2149427297", "Learning to Rank for Information Retrieval"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"]]}, {"context": "The proposed list-wise learning method is based on the typical ListNet method MAINCIT , and a full-connected DNN network is introduced to fine-tune the item representations with a list-wise ranking loss.", "bow": [["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2121824931", "The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List"], ["2142537246", "AdaRank: a boosting algorithm for information retrieval"], ["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2395021187", "The Infinite Push: A New Support Vector Ranking Algorithm that Directly Optimizes Accuracy at the Absolute Top of the List."]], "pp": [["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2121824931", "The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List"], ["2142537246", "AdaRank: a boosting algorithm for information retrieval"], ["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2395021187", "The Infinite Push: A New Support Vector Ranking Algorithm that Directly Optimizes Accuracy at the Absolute Top of the List."]]}, {"context": "The literature suggests a number of approaches to cope with missing judgments (an overview can be found in CIT and CIT ): (1) Defining IR measures that are robust to missing judgments, like bpref MAINCIT .", "bow": [["2109244020", "Retrieval evaluation with incomplete information"], ["2065269597", "Understanding the intrinsic memorability of images"], ["2044758663", "Statistical Analysis with Missing Data"], ["2159048649", "Here or there: preference judgments for relevance"], ["2156267802", "Missing Data: Our View of the State of the Art"]], "pp": [["2109244020", "Retrieval evaluation with incomplete information"], ["2065269597", "Understanding the intrinsic memorability of images"], ["2044758663", "Statistical Analysis with Missing Data"], ["2159048649", "Here or there: preference judgments for relevance"], ["2156267802", "Missing Data: Our View of the State of the Art"]]}, {"context": "The function should be carefully selected so that records from the same person are in the same block with high probability MAINCIT .", "bow": [["2032005951", "A Survey of Statistical Network Models"], ["2591888901", "Joint Detection and Identification Feature Learning for Person Search"], ["2145307410", "Human Re-identification by Matching Compositional Template with Cluster Sampling"], ["2346369283", "Top-Push Video-Based Person Re-identification"], ["2519803806", "Person Re-identification via Recurrent Feature Aggregation"]], "pp": [["2032005951", "A Survey of Statistical Network Models"], ["2591888901", "Joint Detection and Identification Feature Learning for Person Search"], ["2145307410", "Human Re-identification by Matching Compositional Template with Cluster Sampling"], ["2346369283", "Top-Push Video-Based Person Re-identification"], ["2519803806", "Person Re-identification via Recurrent Feature Aggregation"]]}, {"context": "Better way of blocking such as MAINCIT is needed for efficient memory usage, for fast performance and scalability.", "bow": [["1540269031", "Learning blocking schemes for record linkage"], ["2111116800", "Adaptive Blocking: Learning to Scale Up Record Linkage"], ["2031250218", "A Survey of Indexing Techniques for Scalable Record Linkage and Deduplication"], ["2117974736", "Entity resolution with iterative blocking"], ["2024770506", "The merge/purge problem for large databases"]], "pp": [["1540269031", "Learning blocking schemes for record linkage"], ["2111116800", "Adaptive Blocking: Learning to Scale Up Record Linkage"], ["2031250218", "A Survey of Indexing Techniques for Scalable Record Linkage and Deduplication"], ["2117974736", "Entity resolution with iterative blocking"], ["2024770506", "The merge/purge problem for large databases"]]}, {"context": "For example, noun-phrase extraction has been used for extracting short, descriptive phrases from the original lengthy text ( MAINCIT ).", "bow": [["2150815390", "Mining Quality Phrases from Massive Text Corpora"], ["2167299284", "Memory-Based Lexical Acquisition and Processing"], ["2064418625", "Improved automatic keyword extraction given more linguistic knowledge"], ["1623072288", "TEXT CHUNKING USING TRANSFORMATION-BASED LEARNING"], ["2151835172", "An Exploration of Embeddings for Generalized Phrases"]], "pp": [["2150815390", "Mining Quality Phrases from Massive Text Corpora"], ["2167299284", "Memory-Based Lexical Acquisition and Processing"], ["2064418625", "Improved automatic keyword extraction given more linguistic knowledge"], ["1623072288", "TEXT CHUNKING USING TRANSFORMATION-BASED LEARNING"], ["2151835172", "An Exploration of Embeddings for Generalized Phrases"]]}, {"context": "Clustering Coefficient A more widely studied 3rd-order property is the clustering coefficient FORMULA , which is defined as the ratio of actual links among a node's neighbours to the maximal possible number of links they can share MAINCIT .", "bow": [["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2112090702", "Collective dynamics of \u2018small-world\u2019 networks"], ["1491317234", "Research Design and Statistical Analysis"], ["2011430131", "Data clustering: 50 years beyond K-means"], ["1750839748", "Wireless scheduling with power control"]], "pp": [["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2112090702", "Collective dynamics of \u2018small-world\u2019 networks"], ["1491317234", "Research Design and Statistical Analysis"], ["2011430131", "Data clustering: 50 years beyond K-means"], ["1750839748", "Wireless scheduling with power control"]]}, {"context": "Having Twitter as a new kind of data source, researchers have looked into the development of tools for real-time trend analytics CIT or early detection of newsworthy events CIT , as well as into analytical approaches for understanding the sentiment expressed by users towards a target MAINCIT , or public opinion on a specific topic CIT .", "bow": [["2122369144", "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series"], ["1956559956", "Introduction to Modern Information Retrieval"], ["1517771839", "Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields"], ["2108646579", "Sentiment Analysis and Opinion Mining"], ["2128721751", "Twitinfo: aggregating and visualizing microblogs for event exploration"]], "pp": [["2122369144", "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series"], ["1956559956", "Introduction to Modern Information Retrieval"], ["1517771839", "Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields"], ["2108646579", "Sentiment Analysis and Opinion Mining"], ["2128721751", "Twitinfo: aggregating and visualizing microblogs for event exploration"]]}, {"context": "Sanderson et al. MAINCIT employ pairwise comparisons with Amazon's Mechanical Turk CIT to obtain the correlation between user preference for text retrieval results and the effectiveness measures computed from a test collection.", "bow": [["2169800142", "The promise of Mechanical Turk: How online labor markets can help theorists run behavioral experiments"], ["2164976104", "The Online Laboratory: Conducting Experiments in a Real Labor Market"], ["2149489787", "Utility data annotation with Amazon Mechanical Turk"], ["2141708418", "Amazon's Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?"], ["1984022436", "Conducting behavioral research on Amazon's Mechanical Turk"]], "pp": [["2169800142", "The promise of Mechanical Turk: How online labor markets can help theorists run behavioral experiments"], ["2164976104", "The Online Laboratory: Conducting Experiments in a Real Labor Market"], ["2149489787", "Utility data annotation with Amazon Mechanical Turk"], ["2141708418", "Amazon's Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?"], ["1984022436", "Conducting behavioral research on Amazon's Mechanical Turk"]]}, {"context": "Due to its low price and high scalability, crowd sourcing is a popular technique to obtain relevance assessments for information retrieval tasks MAINCIT .", "bow": [["2042015300", "Scene-Independent Group Profiling in Crowd"], ["2079023123", "Crowded Scene Analysis: A Survey"], ["1978232622", "Deep People Counting in Extremely Dense Crowds"], ["1962468782", "Deeply learned attributes for crowded scene understanding"], ["2039430607", "Crowd Analysis Using Computer Vision Techniques"]], "pp": [["2042015300", "Scene-Independent Group Profiling in Crowd"], ["2079023123", "Crowded Scene Analysis: A Survey"], ["1978232622", "Deep People Counting in Extremely Dense Crowds"], ["1962468782", "Deeply learned attributes for crowded scene understanding"], ["2039430607", "Crowd Analysis Using Computer Vision Techniques"]]}, {"context": "There are studies MAINCIT on the usage of the Mechanical Turk service to collect relevance assessments.", "bow": [["2141708418", "Amazon's Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?"], ["2149489787", "Utility data annotation with Amazon Mechanical Turk"], ["2169800142", "The promise of Mechanical Turk: How online labor markets can help theorists run behavioral experiments"], ["2164976104", "The Online Laboratory: Conducting Experiments in a Real Labor Market"], ["1984022436", "Conducting behavioral research on Amazon's Mechanical Turk"]], "pp": [["2141708418", "Amazon's Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?"], ["2149489787", "Utility data annotation with Amazon Mechanical Turk"], ["2169800142", "The promise of Mechanical Turk: How online labor markets can help theorists run behavioral experiments"], ["2164976104", "The Online Laboratory: Conducting Experiments in a Real Labor Market"], ["1984022436", "Conducting behavioral research on Amazon's Mechanical Turk"]]}, {"context": "Finally, we note that in practical implementation of learning, we follow the proposal in MAINCIT wherein for each local distribution at FORMULA th round we run the MCMC for only a few steps starting from the observed subset FORMULA .", "bow": [["2305001871", "Bayes and big data: the consensus Monte Carlo algorithm"], ["2106706098", "Reversible jump Markov chain Monte Carlo computation and Bayesian model determination"], ["2145536610", "Asymptotically exact, embarrassingly parallel MCMC"], ["2108207895", "Understanding the Metropolis-Hastings Algorithm"], ["2135973421", "A tutorial on adaptive MCMC"]], "pp": [["2305001871", "Bayes and big data: the consensus Monte Carlo algorithm"], ["2106706098", "Reversible jump Markov chain Monte Carlo computation and Bayesian model determination"], ["2145536610", "Asymptotically exact, embarrassingly parallel MCMC"], ["2108207895", "Understanding the Metropolis-Hastings Algorithm"], ["2135973421", "A tutorial on adaptive MCMC"]]}, {"context": "We follow the suggestion in MAINCIT to start the Markov chain from the observed subset FORMULA and run for a few iterations.", "bow": [["1660492679", "How Many Iterations in the Gibbs Sampler"], ["2120997209", "Mathematical aspects of mixing times in Markov chains"], ["2075379212", "Finite Markov chains"], ["2803679814", "Deep Learning Software Repositories"], ["2043805242", "Faster random generation of linear extensions"]], "pp": [["1660492679", "How Many Iterations in the Gibbs Sampler"], ["2120997209", "Mathematical aspects of mixing times in Markov chains"], ["2075379212", "Finite Markov chains"], ["2803679814", "Deep Learning Software Repositories"], ["2043805242", "Faster random generation of linear extensions"]]}, {"context": "Similarly the correlation of user-ad (behaviour targeting) has also been studied previously in MAINCIT , focusing on improving advertising effectiveness by modelling attitude and feedback from users.", "bow": [["2106728444", "How much can behavioral targeting help online advertising"], ["2152314154", "Accurately interpreting clickthrough data as implicit feedback"], ["2073448073", "A semantic approach to contextual advertising"], ["2056705371", "Optimizing relevance and revenue in ad search: a query substitution approach"], ["1769120835", "AdWords and generalized on-line matching"]], "pp": [["2106728444", "How much can behavioral targeting help online advertising"], ["2152314154", "Accurately interpreting clickthrough data as implicit feedback"], ["2073448073", "A semantic approach to contextual advertising"], ["2056705371", "Optimizing relevance and revenue in ad search: a query substitution approach"], ["1769120835", "AdWords and generalized on-line matching"]]}, {"context": "Word embeddings have also been studied in other IR contexts such as term reweighting CIT , cross-lingual retrieval MAINCIT and short-text similarity CIT .", "bow": [["2260194779", "A Dual Embedding Space Model for Document Ranking."], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"], ["2539671052", "Learning to Match using Local and Distributed Representations of Text for Web Search"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"]], "pp": [["2260194779", "A Dual Embedding Space Model for Document Ranking."], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"], ["2539671052", "Learning to Match using Local and Distributed Representations of Text for Web Search"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"]]}, {"context": "We modify Tarjan's strongly connected components (SCC) algorithm MAINCIT to find the top-FORMULA list in linear time if the size of the top SCC is small compared to the size of item list FORMULA .", "bow": [["2141998202", "Foundations of a Multi-way Spectral Clustering Framework for Hybrid Linear Modeling"], ["2013712253", "Spectral Curvature Clustering (SCC)"], ["2118382442", "Depth-First Search and Linear Graph Algorithms"], ["2023658462", "Applications of Path Compression on Balanced Trees"], ["2048790997", "Finding minimum-cost circulations by canceling negative cycles"]], "pp": [["2141998202", "Foundations of a Multi-way Spectral Clustering Framework for Hybrid Linear Modeling"], ["2013712253", "Spectral Curvature Clustering (SCC)"], ["2118382442", "Depth-First Search and Linear Graph Algorithms"], ["2023658462", "Applications of Path Compression on Balanced Trees"], ["2048790997", "Finding minimum-cost circulations by canceling negative cycles"]]}, {"context": "We use a modified version of TarjanSCC from MAINCIT in order to update FORMULA and FORMULA .", "bow": [["1543451444", "Updating logical databases"], ["853930207", "Fully Dynamic Matching in Bipartite Graphs"], ["1982853556", "Propositional belief base update and minimal change"], ["2058616629", "Disjunctive networks and update schedules"], ["2142466236", "Parallel matrix factorization for recommender systems"]], "pp": [["1543451444", "Updating logical databases"], ["853930207", "Fully Dynamic Matching in Bipartite Graphs"], ["1982853556", "Propositional belief base update and minimal change"], ["2058616629", "Disjunctive networks and update schedules"], ["2142466236", "Parallel matrix factorization for recommender systems"]]}, {"context": " It is possible to explore different strategies to deal with out-of-vocab (OOV) words in the Equation REF In machine translation there are examples of interesting strategies to handle out-of-vocabulary words (e.g., MAINCIT ).", "bow": [["2117717100", "Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages"], ["2118434577", "Addressing the Rare Word Problem in Neural Machine Translation"], ["2339995566", "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"], ["1816313093", "Neural Machine Translation of Rare Words with Subword Units"], ["2100664567", "On Using Very Large Target Vocabulary for Neural Machine Translation"]], "pp": [["2117717100", "Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages"], ["2118434577", "Addressing the Rare Word Problem in Neural Machine Translation"], ["2339995566", "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"], ["1816313093", "Neural Machine Translation of Rare Words with Subword Units"], ["2100664567", "On Using Very Large Target Vocabulary for Neural Machine Translation"]]}, {"context": "Given a question Q and a set of relevant articles A, (FORMULA ), if Q is answered by FORMULA (FORMULA ), then FORMULA entails Q MAINCIT .", "bow": [["2038037493", "A polymorphic type system for PROLOG."], ["2170141744", "A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning"], ["166862392", "Neural fitted q iteration \u2013 first experiences with a data efficient neural reinforcement learning method"], ["1941248864", "Variational methods for Reinforcement Learning"], ["2087304615", "On the consecutive ones property"]], "pp": [["2038037493", "A polymorphic type system for PROLOG."], ["2170141744", "A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning"], ["166862392", "Neural fitted q iteration \u2013 first experiences with a data efficient neural reinforcement learning method"], ["1941248864", "Variational methods for Reinforcement Learning"], ["2087304615", "On the consecutive ones property"]]}, {"context": "ntu.edu.tw/FORMULA cjlin/libsvm/ MAINCIT with Weka.", "bow": [["2133990480", "The WEKA data mining software: an update"], ["2123504579", "Data mining: practical machine learning tools and techniques with Java implementations"], ["2611832198", "Data Mining"], ["1512098439", "Fast training of support vector machines using sequential minimal optimization"], ["2153635508", "LIBSVM: A library for support vector machines"]], "pp": [["2133990480", "The WEKA data mining software: an update"], ["2123504579", "Data mining: practical machine learning tools and techniques with Java implementations"], ["2611832198", "Data Mining"], ["1512098439", "Fast training of support vector machines using sequential minimal optimization"], ["2153635508", "LIBSVM: A library for support vector machines"]]}, {"context": "FIGURE Support Vector Machine (SVM) ( MAINCIT ) is a commonly used supervised classification algorithm that has shown good performance over a range of tasks.", "bow": [["2119821739", "Support-Vector Networks"], ["2087347434", "A training algorithm for optimal margin classifiers"], ["1596717185", "Least Squares Support Vector Machine Classifiers"], ["740415", "The Nature of Statistical Learning"], ["2139212933", "A Tutorial on Support Vector Machines for Pattern Recognition"]], "pp": [["2119821739", "Support-Vector Networks"], ["2087347434", "A training algorithm for optimal margin classifiers"], ["1596717185", "Least Squares Support Vector Machine Classifiers"], ["740415", "The Nature of Statistical Learning"], ["2139212933", "A Tutorial on Support Vector Machines for Pattern Recognition"]]}, {"context": "Finally, for completeness, we mention in passing the third approach, which treats a permutation as a symmetric group and applying spectral decomposition techniques CIT MAINCIT .", "bow": [["2028889910", "The complexity of finding minimum-length generator sequences"], ["2075350371", "Soundness and Completeness of an Axiom System for Program Verification"], ["2336611437", "Permutation groups"], ["1569736468", "Permutation, parametric and bootstrap tests of hypotheses"], ["2160254449", "Complete axiomatization and decidability of alternating-time temporal logic"]], "pp": [["2028889910", "The complexity of finding minimum-length generator sequences"], ["2075350371", "Soundness and Completeness of an Axiom System for Program Verification"], ["2336611437", "Permutation groups"], ["1569736468", "Permutation, parametric and bootstrap tests of hypotheses"], ["2160254449", "Complete axiomatization and decidability of alternating-time temporal logic"]]}, {"context": "Convolutional neural networks have also been shown to be highly effective for natural language processing and have achieved excellent results in information retrieval CIT , semantic parsing CIT , sentence modeling MAINCIT and other traditional natural language processing tasks CIT .", "bow": [["2120615054", "A Convolutional Neural Network for Modelling Sentences"], ["1423339008", "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"], ["179875071", "Recurrent neural network based language model"], ["1869752048", "Grammar as a foreign language"], ["2158899491", "Natural Language Processing (Almost) from Scratch"]], "pp": [["2120615054", "A Convolutional Neural Network for Modelling Sentences"], ["2250877157", "Sentence Modeling with Gated Recursive Neural Network"], ["1784932861", "Discriminative Neural Sentence Modeling by Tree-Based Convolution"], ["1423339008", "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"], ["179875071", "Recurrent neural network based language model"]], "np": [["2250877157", "Sentence Modeling with Gated Recursive Neural Network"], ["1784932861", "Discriminative Neural Sentence Modeling by Tree-Based Convolution"], ["2216973458", "A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification"], ["2120615054", "A Convolutional Neural Network for Modelling Sentences"], ["2735642330", "Toward Controlled Generation of Text"]]}, {"context": "FIGURE Related Work Current approaches to protect privacy in recommendation systems mostly address two different privacy concerns: protecting users' privacy from curious peers or malicious users MAINCIT , and against unreliable service providers CIT .", "bow": [["2610955953", "Differential privacy"], ["1526797722", "Towards privacy for social networks: a zero-knowledge based definition of privacy"], ["2077217970", "A firm foundation for private data analysis"], ["2088517895", "Privacy against statistical inference"], ["2054922243", "No free lunch in data privacy"]], "pp": [["2610955953", "Differential privacy"], ["1526797722", "Towards privacy for social networks: a zero-knowledge based definition of privacy"], ["2077217970", "A firm foundation for private data analysis"], ["2088517895", "Privacy against statistical inference"], ["2054922243", "No free lunch in data privacy"]], "np": [["2200869402", "Proactively accountable anonymous messaging in verdict"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "Machanavajjhala et al. MAINCIT studied recommendations based on a user's social network with differential privacy constraints.", "bow": [["2054922243", "No free lunch in data privacy"], ["2080044359", "Privacy: Theory meets Practice on the Map"], ["2122997783", "Personalized social recommendations: accurate or private"], ["1992697866", "A rigorous and customizable framework for privacy"], ["2610955953", "Differential privacy"]], "pp": [["2054922243", "No free lunch in data privacy"], ["2080044359", "Privacy: Theory meets Practice on the Map"], ["2122997783", "Personalized social recommendations: accurate or private"], ["1992697866", "A rigorous and customizable framework for privacy"], ["2610955953", "Differential privacy"]]}, {"context": "Having Twitter as a new kind of data source, researchers have looked into the development of tools for real-time trend analytics CIT or early detection of newsworthy events MAINCIT , as well as into analytical approaches for understanding the sentiment expressed by users towards a target CIT , or public opinion on a specific topic CIT .", "bow": [["2122369144", "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series"], ["1956559956", "Introduction to Modern Information Retrieval"], ["1517771839", "Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields"], ["2108646579", "Sentiment Analysis and Opinion Mining"], ["2128721751", "Twitinfo: aggregating and visualizing microblogs for event exploration"]], "pp": [["2122369144", "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series"], ["1956559956", "Introduction to Modern Information Retrieval"], ["1517771839", "Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields"], ["2108646579", "Sentiment Analysis and Opinion Mining"], ["2128721751", "Twitinfo: aggregating and visualizing microblogs for event exploration"]]}, {"context": "They are, however, significantly outperformed in terms of the trade-off between memory usage and accuracy by recent methods that cast high dimensional indexing to a source coding problem MAINCIT , in particular the product quantization-based method of CIT exhibits impressive results for large scale image search CIT .", "bow": [["2161451718", "Distance Encoded Product Quantization"], ["2111006384", "Optimized Product Quantization for Approximate Nearest Neighbor Search"], ["2119913432", "Approximate Nearest Neighbor Search by Residual Vector Quantization"], ["1634005169", "Vector Quantization and Signal Compression"], ["138284169", "Composite Quantization for Approximate Nearest Neighbor Search"]], "pp": [["2136670370", "Reduction and Fixed Points of Boolean Networks and Linear Network Coding Solvability"], ["2105406958", "Coding on demand by an informed source (ISCOD) for efficient broadcast of different supplemental data to caching clients"], ["2114085948", "Information transmission with additional noise"], ["2106630500", "Efficient algorithms for Index Coding"], ["2077246452", "Shape description using weighted symmetric axis features"]], "np": [["2077246452", "Shape description using weighted symmetric axis features"], ["2136670370", "Reduction and Fixed Points of Boolean Networks and Linear Network Coding Solvability"], ["2105406958", "Coding on demand by an informed source (ISCOD) for efficient broadcast of different supplemental data to caching clients"], ["2106630500", "Efficient algorithms for Index Coding"], ["2001935257", "A coding theorem and R\u00e9nyi's entropy*"]]}, {"context": "They are, however, significantly outperformed in terms of the trade-off between memory usage and accuracy by recent methods that cast high dimensional indexing to a source coding problem CIT , in particular the product quantization-based method of MAINCIT exhibits impressive results for large scale image search CIT .", "bow": [["2161451718", "Distance Encoded Product Quantization"], ["2111006384", "Optimized Product Quantization for Approximate Nearest Neighbor Search"], ["2119913432", "Approximate Nearest Neighbor Search by Residual Vector Quantization"], ["1634005169", "Vector Quantization and Signal Compression"], ["138284169", "Composite Quantization for Approximate Nearest Neighbor Search"]], "pp": [["2161451718", "Distance Encoded Product Quantization"], ["2111006384", "Optimized Product Quantization for Approximate Nearest Neighbor Search"], ["2119913432", "Approximate Nearest Neighbor Search by Residual Vector Quantization"], ["1634005169", "Vector Quantization and Signal Compression"], ["138284169", "Composite Quantization for Approximate Nearest Neighbor Search"]]}, {"context": "But as shown in MAINCIT , this post verification is also important for methods based on binary CIT or quantized codes CIT , as the ranking provided on output of the large scale search is significantly improved when verifying the few first hypotheses.", "bow": [["2341106557", "Quantized consensus"], ["2130025446", "In transition from global to modular temporal reasoning about programs"], ["2148575324", "Polynomial Codes Over Certain Finite Fields"], ["2166966427", "Perfect binary codes: constructions, properties, and enumeration"], ["1771040533", "Three-weight cyclic codes and their weight distributions"]], "pp": [["2341106557", "Quantized consensus"], ["2130025446", "In transition from global to modular temporal reasoning about programs"], ["2148575324", "Polynomial Codes Over Certain Finite Fields"], ["2166966427", "Perfect binary codes: constructions, properties, and enumeration"], ["1771040533", "Three-weight cyclic codes and their weight distributions"]]}, {"context": "But as shown in CIT , this post verification is also important for methods based on binary CIT or quantized codes MAINCIT , as the ranking provided on output of the large scale search is significantly improved when verifying the few first hypotheses.", "bow": [["2341106557", "Quantized consensus"], ["2130025446", "In transition from global to modular temporal reasoning about programs"], ["2148575324", "Polynomial Codes Over Certain Finite Fields"], ["2166966427", "Perfect binary codes: constructions, properties, and enumeration"], ["1771040533", "Three-weight cyclic codes and their weight distributions"]], "pp": [["2341106557", "Quantized consensus"], ["2130025446", "In transition from global to modular temporal reasoning about programs"], ["2148575324", "Polynomial Codes Over Certain Finite Fields"], ["2166966427", "Perfect binary codes: constructions, properties, and enumeration"], ["1771040533", "Three-weight cyclic codes and their weight distributions"]]}, {"context": "We focus on the method presented in MAINCIT , which offers state-of-the-art performance, outperforming the FLANN which was previously shown to outperform LSH CIT .", "bow": [["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"], ["1627400044", "Fast approximate nearest neighbors with automatic algorithm configuration"], ["2146020873", "Fast Matching of Binary Features"], ["2055839530", "Modeling LSH for performance tuning"], ["1898304433", "Spherical lsh for approximate nearest neighbor search on unit hypersphere"]], "pp": [["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"], ["1627400044", "Fast approximate nearest neighbors with automatic algorithm configuration"], ["2146020873", "Fast Matching of Binary Features"], ["2055839530", "Modeling LSH for performance tuning"], ["1898304433", "Spherical lsh for approximate nearest neighbor search on unit hypersphere"]]}, {"context": "Context: compression based indexing In this section, we briefly review the indexing method of MAINCIT , which finds the approximate FORMULA nearest neighbors using a source coding approach.", "bow": [["2038044292", "Searching in metric spaces"], ["2049644877", "Satisfying general proximity / similarity queries with metric trees"], ["1484435918", "Indexing Methods for Approximate String Matching."], ["1541459201", "A Quantitative Analysis and Performance Study for Similarity-Search Methods in High-Dimensional Spaces"], ["188912188", "An Introduction to Random Indexing"]], "pp": [["2038044292", "Searching in metric spaces"], ["2049644877", "Satisfying general proximity / similarity queries with metric trees"], ["1484435918", "Indexing Methods for Approximate String Matching."], ["1541459201", "A Quantitative Analysis and Performance Study for Similarity-Search Methods in High-Dimensional Spaces"], ["188912188", "An Introduction to Random Indexing"]]}, {"context": "For the sake of presentation, we describe only the Asymmetric Distance Computation (ADC) method proposed in MAINCIT .", "bow": [["2262641688", "Uplink Achievable Rate for Massive MIMO Systems With Low-Resolution ADC"], ["2402821993", "On the Optimization of ADC Resolution in Multi-antenna Systems"], ["2189415357", "On the Spectral Efficiency of Massive MIMO Systems With Low-Resolution ADCs"], ["2001991593", "High SNR capacity of millimeter wave MIMO systems with one-bit quantization"], ["2134179788", "Analog-to-digital converter survey and analysis"]], "pp": [["2262641688", "Uplink Achievable Rate for Massive MIMO Systems With Low-Resolution ADC"], ["2402821993", "On the Optimization of ADC Resolution in Multi-antenna Systems"], ["2189415357", "On the Spectral Efficiency of Massive MIMO Systems With Low-Resolution ADCs"], ["2001991593", "High SNR capacity of millimeter wave MIMO systems with one-bit quantization"], ["2134179788", "Analog-to-digital converter survey and analysis"]]}, {"context": "To address this issue, MAINCIT uses a product quantizer, for which there is no need to explicitly enumerate the centroids.", "bow": [["2063678710", "A pyramid vector quantizer"], ["2096102601", "Design of multiple description scalar quantizers"], ["2102187818", "High-Rate Uncorrelated Bit Extraction for Shared Secret Key Generation from Channel Measurements"], ["2020644915", "Asymptotic analysis of optimal fixed-rate uniform scalar quantization"], ["2026324833", "Hybrid feedback stabilization of systems with quantized signals"]], "pp": [["2063678710", "A pyramid vector quantizer"], ["2096102601", "Design of multiple description scalar quantizers"], ["2102187818", "High-Rate Uncorrelated Bit Extraction for Shared Secret Key Generation from Channel Measurements"], ["2020644915", "Asymptotic analysis of optimal fixed-rate uniform scalar quantization"], ["2026324833", "Hybrid feedback stabilization of systems with quantized signals"]]}, {"context": "It is proved MAINCIT that the square error between the distance and its estimation is bounded, on average, by the quantization error.", "bow": [["2060256514", "Universal Rate-Efficient Scalar Quantization"], ["1582983303", "Sobolev Duals for Random Frames and Sigma-Delta Quantization of Compressed Sensing Measurements"], ["2152373327", "Quantifying the power loss when transmit beamforming relies on finite-rate feedback"], ["2171149307", "Sparse composite quantization"], ["2183650117", "Quantization and Finite Frames"]], "pp": [["2060256514", "Universal Rate-Efficient Scalar Quantization"], ["1582983303", "Sobolev Duals for Random Frames and Sigma-Delta Quantization of Compressed Sensing Measurements"], ["2152373327", "Quantifying the power loss when transmit beamforming relies on finite-rate feedback"], ["2171149307", "Sparse composite quantization"], ["2183650117", "Quantization and Finite Frames"]]}, {"context": " In the following, we set FORMULA (i.e., FORMULA ), as suggested in MAINCIT , which means that we use exactly FORMULA bytes per indexed vector.", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["2442545619", "Indexed Grammars\u2014An Extension of Context-Free Grammars"], ["1646789098", "An Approach to Computing Downward Closures"], ["2129424879", "Understanding data center traffic characteristics"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["2442545619", "Indexed Grammars\u2014An Extension of Context-Free Grammars"], ["1646789098", "An Approach to Computing Downward Closures"], ["2129424879", "Understanding data center traffic characteristics"]]}, {"context": "This is possible when using the ADC method MAINCIT : this search algorithm provides an explicit approximation FORMULA of database vector FORMULA .", "bow": [["2262641688", "Uplink Achievable Rate for Massive MIMO Systems With Low-Resolution ADC"], ["2402821993", "On the Optimization of ADC Resolution in Multi-antenna Systems"], ["2189415357", "On the Spectral Efficiency of Massive MIMO Systems With Low-Resolution ADCs"], ["2001991593", "High SNR capacity of millimeter wave MIMO systems with one-bit quantization"], ["2134179788", "Analog-to-digital converter survey and analysis"]], "pp": [["2262641688", "Uplink Achievable Rate for Massive MIMO Systems With Low-Resolution ADC"], ["2402821993", "On the Optimization of ADC Resolution in Multi-antenna Systems"], ["2189415357", "On the Spectral Efficiency of Massive MIMO Systems With Low-Resolution ADCs"], ["2001991593", "High SNR capacity of millimeter wave MIMO systems with one-bit quantization"], ["2134179788", "Analog-to-digital converter survey and analysis"]]}, {"context": "The selected vectors minimize the estimator of Equation REF , which is computed directly in the compressed domain MAINCIT .", "bow": [["2037881439", "Epidemic threshold in structured scale-free networks"], ["2123820136", "Percolation critical exponents in scale-free networks"], ["2171999519", "Median Selection Subset Aggregation for Parallel Inference"], ["2158307424", "Distributed EM algorithms for density estimation and clustering in sensor networks"], ["2160553465", "Random projection-based multiplicative data perturbation for privacy preserving distributed data mining"]], "pp": [["2037881439", "Epidemic threshold in structured scale-free networks"], ["2123820136", "Percolation critical exponents in scale-free networks"], ["2171999519", "Median Selection Subset Aggregation for Parallel Inference"], ["2158307424", "Distributed EM algorithms for density estimation and clustering in sensor networks"], ["2160553465", "Random projection-based multiplicative data perturbation for privacy preserving distributed data mining"]]}, {"context": "Non exhaustive variant Up to now, we have only considered the ADC method of MAINCIT , which requires an exhaustive scan of the dataset codes.", "bow": [["2262641688", "Uplink Achievable Rate for Massive MIMO Systems With Low-Resolution ADC"], ["2402821993", "On the Optimization of ADC Resolution in Multi-antenna Systems"], ["2123659014", "Fast subset scan for spatial pattern detection"], ["2189415357", "On the Spectral Efficiency of Massive MIMO Systems With Low-Resolution ADCs"], ["2002151188", "A spatial scan statistic"]], "pp": [["2262641688", "Uplink Achievable Rate for Massive MIMO Systems With Low-Resolution ADC"], ["2402821993", "On the Optimization of ADC Resolution in Multi-antenna Systems"], ["2123659014", "Fast subset scan for spatial pattern detection"], ["2189415357", "On the Spectral Efficiency of Massive MIMO Systems With Low-Resolution ADCs"], ["2002151188", "A spatial scan statistic"]]}, {"context": "In particular, in the experimental section we evaluate our approach with the IVFADC variant of MAINCIT , that avoids the aforementioned exhaustive scan by using an inverted file structure.", "bow": [["2039742379", "The inverted multi-index"], ["2138662031", "Inverted files for text search engines"], ["2889395214", "Managing gigabytes"], ["2123659014", "Fast subset scan for spatial pattern detection"], ["2133995768", "Locally Optimized Product Quantization for Approximate Nearest Neighbor Search"]], "pp": [["2039742379", "The inverted multi-index"], ["2138662031", "Inverted files for text search engines"], ["2889395214", "Managing gigabytes"], ["2123659014", "Fast subset scan for spatial pattern detection"], ["2133995768", "Locally Optimized Product Quantization for Approximate Nearest Neighbor Search"]]}, {"context": "As already reported in MAINCIT , the IVFADC version is better than the ADC method, at the cost of 4 additional bytes per indexed vector.", "bow": [["2262641688", "Uplink Achievable Rate for Massive MIMO Systems With Low-Resolution ADC"], ["2402821993", "On the Optimization of ADC Resolution in Multi-antenna Systems"], ["2109521352", "Large-scale image classification with trace-norm regularization"], ["2189415357", "On the Spectral Efficiency of Massive MIMO Systems With Low-Resolution ADCs"], ["2001991593", "High SNR capacity of millimeter wave MIMO systems with one-bit quantization"]], "pp": [["2262641688", "Uplink Achievable Rate for Massive MIMO Systems With Low-Resolution ADC"], ["2402821993", "On the Optimization of ADC Resolution in Multi-antenna Systems"], ["2109521352", "Large-scale image classification with trace-norm regularization"], ["2189415357", "On the Spectral Efficiency of Massive MIMO Systems With Low-Resolution ADCs"], ["2001991593", "High SNR capacity of millimeter wave MIMO systems with one-bit quantization"]]}, {"context": "Recent papers have explored in detail the SG and CBOW training methodology CIT and its connection to other approaches for learning word embeddings such as explicit vector space representations CIT , matrix factorization MAINCIT and density-based representations CIT .", "bow": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "pp": [["880548201", "Self-paced learning for matrix factorization"], ["2086325844", "MDL4BMF: Minimum Description Length for Boolean Matrix Factorization"], ["1985915435", "Alternating direction method of multipliers for non-negative matrix factorization with the beta-divergence"], ["2020098476", "Scalable Coordinate Descent Approaches to Parallel Matrix Factorization for Recommender Systems"], ["2047779269", "NMF-KNN: Image Annotation Using Weighted Multi-view Non-negative Matrix Factorization"]], "np": [["1481507124", "Diagonal and Low-Rank Matrix Decompositions, Correlation Matrices, and Ellipsoid Fitting"], ["2086325844", "MDL4BMF: Minimum Description Length for Boolean Matrix Factorization"], ["880548201", "Self-paced learning for matrix factorization"], ["1985915435", "Alternating direction method of multipliers for non-negative matrix factorization with the beta-divergence"], ["2040969041", "Iterative estimation of constrained rank-one matrices in noise"]]}, {"context": "SVMRank has previously been used in the task of document retrieval in ( MAINCIT ) for a more traditional short query task and has been shown to be a top-performing system for ranking.", "bow": [["2039118116", "Manifold-ranking based image retrieval"], ["2536015822", "A Deep Relevance Matching Model for Ad-hoc Retrieval"], ["2149427297", "Learning to Rank for Information Retrieval"], ["2014415866", "Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval"], ["2270593706", "Integrating and Evaluating Neural Word Embeddings in Information Retrieval"]], "pp": [["2039118116", "Manifold-ranking based image retrieval"], ["2536015822", "A Deep Relevance Matching Model for Ad-hoc Retrieval"], ["2149427297", "Learning to Rank for Information Retrieval"], ["2014415866", "Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval"], ["2270593706", "Integrating and Evaluating Neural Word Embeddings in Information Retrieval"]]}, {"context": "Recent papers have explored in detail the SG and CBOW training methodology CIT and its connection to other approaches for learning word embeddings such as explicit vector space representations CIT , matrix factorization CIT and density-based representations MAINCIT .", "bow": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "pp": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]]}, {"context": "As a result, context-aware recommendation turns the prediction task into a multidimensional rating function \u2013 R: Users FORMULA Items FORMULA Contexts FORMULA Ratings MAINCIT .", "bow": [["2582743722", "R: A language and environment for statistical computing."], ["2575006718", "Joint Deep Modeling of Users and Items Using Reviews for Recommendation"], ["2605350416", "Neural Collaborative Filtering"], ["2020631728", "Collaborative prediction and ranking with non-random missing data"], ["2100235918", "A survey of collaborative filtering techniques"]], "pp": [["2582743722", "R: A language and environment for statistical computing."], ["2575006718", "Joint Deep Modeling of Users and Items Using Reviews for Recommendation"], ["2605350416", "Neural Collaborative Filtering"], ["2020631728", "Collaborative prediction and ranking with non-random missing data"], ["2100235918", "A survey of collaborative filtering techniques"]]}, {"context": "Context-aware Recommendation Models Context can be applied in recommendation using three basic strategies: pre-filtering, post-filtering and contextual modeling MAINCIT .", "bow": [["2100755716", "A survey of collaborative filtering based social recommender systems"], ["1964248020", "Maximum-likelihood recursive nonlinear filtering"], ["2100235918", "A survey of collaborative filtering techniques"], ["1998761108", "Extracting Hidden Information from Knowledge Networks"], ["1560991565", "An open architecture for collaborative filtering of netnews"]], "pp": [["2100755716", "A survey of collaborative filtering based social recommender systems"], ["1964248020", "Maximum-likelihood recursive nonlinear filtering"], ["2100235918", "A survey of collaborative filtering techniques"], ["1998761108", "Extracting Hidden Information from Knowledge Networks"], ["1560991565", "An open architecture for collaborative filtering of netnews"]]}, {"context": "These scenarios are depicted in Figure REF MAINCIT .", "bow": [["2077975330", "Approximating the min-max (regret) selecting items problem"], ["2080208950", "On the complexity of a class of combinatorial optimization problems with uncertainty"], ["2106101173", "Near Optimal Bayesian Active Learning for Decision Making"], ["2018552671", "Cloud technologies for flexible 5G radio access networks"], ["152430686", "Learning with Graphical Models"]], "pp": [["2077975330", "Approximating the min-max (regret) selecting items problem"], ["2080208950", "On the complexity of a class of combinatorial optimization problems with uncertainty"], ["2106101173", "Near Optimal Bayesian Active Learning for Decision Making"], ["2018552671", "Cloud technologies for flexible 5G radio access networks"], ["152430686", "Learning with Graphical Models"]]}, {"context": "Recent papers have explored in detail the SG and CBOW training methodology MAINCIT and its connection to other approaches for learning word embeddings such as explicit vector space representations CIT , matrix factorization CIT and density-based representations CIT .", "bow": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "pp": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "np": [["2131479143", "Multi-Task Learning for Classification with Dirichlet Process Priors"], ["2156163116", "Best practices for convolutional neural networks applied to visual document analysis"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "Using the community detection techniques in MAINCIT , we detected 18 groups based on the trust network.", "bow": [["1557680663", "Simplification and analysis of transitive trust networks"], ["2487872411", "Trust and Distrust"], ["1490838413", "Modelling a Public-Key Infrastructure"], ["2109187517", "More Anonymous Onion Routing Through Trust"], ["1497683195", "Empirical Processes: Theory and Applications"]], "pp": [["1557680663", "Simplification and analysis of transitive trust networks"], ["2487872411", "Trust and Distrust"], ["1490838413", "Modelling a Public-Key Infrastructure"], ["2109187517", "More Anonymous Onion Routing Through Trust"], ["1497683195", "Empirical Processes: Theory and Applications"]]}, {"context": "For the document embeddings of the above mentioned two datasets, we use doc2vec MAINCIT to learn dense low-dimensional vectors.", "bow": [["2131744502", "Distributed Representations of Sentences and Documents"], ["1777150271", "ENSEMBLE OF GENERATIVE AND DISCRIMINATIVE TECHNIQUES FOR SENTIMENT ANALYSIS OF MOVIE REVIEWS"], ["2250460709", "Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content"], ["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["658020064", "From Word Embeddings To Document Distances"]], "pp": [["2131744502", "Distributed Representations of Sentences and Documents"], ["1777150271", "ENSEMBLE OF GENERATIVE AND DISCRIMINATIVE TECHNIQUES FOR SENTIMENT ANALYSIS OF MOVIE REVIEWS"], ["2250460709", "Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content"], ["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["658020064", "From Word Embeddings To Document Distances"]]}, {"context": "For the embedding of documents we exploit the distributed memory model since it usually performs well for most tasks MAINCIT .", "bow": [["2044902313", "Memory coherence in shared virtual memory systems"], ["2604942799", "Community Preserving Network Embedding"], ["2250376704", "Semantically Smooth Knowledge Graph Embedding"], ["2743104969", "metapath2vec: Scalable Representation Learning for Heterogeneous Networks"], ["2540057053", "From Node Embedding To Community Embedding."]], "pp": [["2044902313", "Memory coherence in shared virtual memory systems"], ["2604942799", "Community Preserving Network Embedding"], ["2250376704", "Semantically Smooth Knowledge Graph Embedding"], ["2743104969", "metapath2vec: Scalable Representation Learning for Heterogeneous Networks"], ["2540057053", "From Node Embedding To Community Embedding."]]}, {"context": "Beyond word co-occurrence, recent studies have also explored learning text embeddings from clickthrough data CIT , session data CIT , query prefix-suffix pairs CIT , via auto-encoders CIT , and for sentiment classification CIT and for long text MAINCIT .", "bow": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]], "pp": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]]}, {"context": "Most of the work with deep learning has involved learning word vector representations through neural language models MAINCIT CIT CIT and performing composition over the learned word vectors for classification CIT .", "bow": [["2132339004", "A neural probabilistic language model"], ["2164019165", "Improving Word Representations via Global Context and Multiple Word Prototypes"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2141599568", "Linguistic Regularities in Continuous Space Word Representations"], ["1979185006", "Neural net language models"]], "pp": [["2132339004", "A neural probabilistic language model"], ["2164019165", "Improving Word Representations via Global Context and Multiple Word Prototypes"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2141599568", "Linguistic Regularities in Continuous Space Word Representations"], ["1979185006", "Neural net language models"]]}, {"context": " Huang et al. CIT and Shen et al. MAINCIT deep learned latent semantic models to map queries to their relevant documents using clickthrough data.", "bow": [["2130318956", "Positive semidefinite metric learning using boosting-like algorithms"], ["2108168165", "Implicit user modeling for personalized search"], ["1500269355", "Complexity of Coloring Graphs without Paths and Cycles"], ["2065296697", "Object retrieval and localization with spatially-constrained similarity measure and k-NN re-ranking"], ["2099391294", "Learning user interaction models for predicting web search result preferences"]], "pp": [["2130318956", "Positive semidefinite metric learning using boosting-like algorithms"], ["2108168165", "Implicit user modeling for personalized search"], ["1500269355", "Complexity of Coloring Graphs without Paths and Cycles"], ["2065296697", "Object retrieval and localization with spatially-constrained similarity measure and k-NN re-ranking"], ["2099391294", "Learning user interaction models for predicting web search result preferences"]]}, {"context": "We are inspired by previous work in neural language models, for example by MAINCIT , which demonstrates that combining a neural model for predicting the next word with a more traditional counting-based language model is effective because the two models make different kinds of mistakes.", "bow": [["2132339004", "A neural probabilistic language model"], ["179875071", "Recurrent neural network based language model"], ["1979185006", "Neural net language models"], ["932413789", "Decoding with Large-Scale Neural Language Models Improves Translation"], ["2251098065", "Continuous Space Translation Models with Neural Networks"]], "pp": [["2132339004", "A neural probabilistic language model"], ["179875071", "Recurrent neural network based language model"], ["1979185006", "Neural net language models"], ["932413789", "Decoding with Large-Scale Neural Language Models Improves Translation"], ["2251098065", "Continuous Space Translation Models with Neural Networks"]]}, {"context": "First, they use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem MAINCIT .", "bow": [["2171029115", "Adaptive control processes : a guided tour"], ["2096132608", "An improved grid-based approximation algorithm for POMDPs"], ["1672197616", "When Is ''Nearest Neighbor'' Meaningful?"], ["2057685268", "RolX: structural role extraction & mining in large graphs"], ["2303501228", "Supervised classification in high-dimensional space: geometrical, statistical, and asymptotical properties of multivariate data"]], "pp": [["2171029115", "Adaptive control processes : a guided tour"], ["2096132608", "An improved grid-based approximation algorithm for POMDPs"], ["1672197616", "When Is ''Nearest Neighbor'' Meaningful?"], ["2057685268", "RolX: structural role extraction & mining in large graphs"], ["2303501228", "Supervised classification in high-dimensional space: geometrical, statistical, and asymptotical properties of multivariate data"]], "np": [["100623710", "Neural Probabilistic Language Models"], ["2132339004", "A neural probabilistic language model"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "There may even be insufficient behaviour data to learn a click-based embedding MAINCIT or a translation model CIT .", "bow": [["2106630408", "Click chain model in web search"], ["2402441596", "Click Models for Web Search"], ["2154739689", "Efficient multiple-click models in web search"], ["1992549066", "An experimental comparison of click position-bias models"], ["2259177609", "DCM bandits: learning to rank with multiple clicks"]], "pp": [["2106630408", "Click chain model in web search"], ["2402441596", "Click Models for Web Search"], ["2154739689", "Efficient multiple-click models in web search"], ["1992549066", "An experimental comparison of click position-bias models"], ["2259177609", "DCM bandits: learning to rank with multiple clicks"]]}, {"context": " This evaluation strategy of focusing at ranking for top positions is in fact quite common and has been used by many recent studies (e.g., MAINCIT ).", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["2114990184", "An axiomatization of Borda's rule"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["2114990184", "An axiomatization of Borda's rule"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]]}, {"context": " As mentioned in Section REF , there is a parallel between this experiment setup and the telescoping CIT evaluation strategy, and has been used often in recent literature (e.g., MAINCIT ).", "bow": [["2094965692", "The method of creative telescoping"], ["1672862537", "Parameterized telescoping proves algebraic independence of sums"], ["2087893470", "A refined difference field theory for symbolic summation"], ["2145236745", "Telescopers for rational and algebraic functions via residues"], ["2166347079", "New Methods in Automatic Extracting"]], "pp": [["2094965692", "The method of creative telescoping"], ["1672862537", "Parameterized telescoping proves algebraic independence of sums"], ["2087893470", "A refined difference field theory for symbolic summation"], ["2145236745", "Telescopers for rational and algebraic functions via residues"], ["2166347079", "New Methods in Automatic Extracting"]]}, {"context": "Beyond word co-occurrence, recent studies have also explored learning text embeddings from clickthrough data MAINCIT , session data CIT , query prefix-suffix pairs CIT , via auto-encoders CIT , and for sentiment classification CIT and for long text CIT .", "bow": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]], "pp": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]], "np": [["2047221353", "Optimizing search engines using clickthrough data"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "We remark that \u201cdeep structured semantic models\u201d MAINCIT solve problems similar to the one studied here.", "bow": [["1515020792", "Predicting Structured Data"], ["2333621733", "Fast, Exact and Multi-scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs"], ["2105644991", "Max-Margin Markov Networks"], ["2087542520", "Sparse Principal Components Analysis"], ["2105842272", "Large Margin Methods for Structured and Interdependent Output Variables"]], "pp": [["2250676463", "Improving Multi-Modal Representations Using Image Dispersion: Why Less is Sometimes More"], ["1526036637", "Semantics of Interaction: an introduction to Game Semantics"], ["1515020792", "Predicting Structured Data"], ["2049455633", "Latent semantic models for collaborative filtering"], ["2333621733", "Fast, Exact and Multi-scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs"]], "np": [["2250676463", "Improving Multi-Modal Representations Using Image Dispersion: Why Less is Sometimes More"], ["1526036637", "Semantics of Interaction: an introduction to Game Semantics"], ["2049455633", "Latent semantic models for collaborative filtering"], ["2142900973", "YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition"], ["1662133657", "From frequency to meaning: vector space models of semantics"]]}, {"context": "Huang et al. MAINCIT and Shen et al. CIT deep learned latent semantic models to map queries to their relevant documents using clickthrough data.", "bow": [["2130318956", "Positive semidefinite metric learning using boosting-like algorithms"], ["2108168165", "Implicit user modeling for personalized search"], ["1500269355", "Complexity of Coloring Graphs without Paths and Cycles"], ["2065296697", "Object retrieval and localization with spatially-constrained similarity measure and k-NN re-ranking"], ["2099391294", "Learning user interaction models for predicting web search result preferences"]], "pp": [["1498437305", "Indexing similar DNA sequences"], ["2136189984", "Learning deep structured semantic models for web search using clickthrough data"], ["2144012927", "Functional map networks for analyzing and exploring large shape collections"], ["2107330602", "Entropy inequalities for discrete channels"], ["2125895010", "Querying k-truss community in large and dynamic graphs"]], "np": [["1498437305", "Indexing similar DNA sequences"], ["2107330602", "Entropy inequalities for discrete channels"], ["2358307482", "Chinese Word Segmentation: A Decade Review"], ["2155493403", "Using eye tracking to investigate graph layout effects"], ["2045480089", "Randomized algorithms for tracking distributed count, frequencies, and ranks"]]}, {"context": "The 2nd-Order Properties Topological properties are classified as 2nd-order properties if they are based on the degree information of the two end nodes of a link, such as the joint degree distribution FORMULA MAINCIT , which is the probability that a randomly selected link connects a node of degree FORMULA with a node of degree FORMULA .", "bow": [["2002396510", "Dynamical and correlation properties of the Internet"], ["1973556323", "Evolution of social-attribute networks: measurements, modeling, and implications using google+"], ["2096559782", "The evolution of the minimum degree ordering algorithm"], ["1511088604", "An Economic Model of Friendship: Homophily, Minorities and Segregation"], ["2014730252", "An Approximate Minimum Degree Ordering Algorithm"]], "pp": [["2002396510", "Dynamical and correlation properties of the Internet"], ["1973556323", "Evolution of social-attribute networks: measurements, modeling, and implications using google+"], ["2096559782", "The evolution of the minimum degree ordering algorithm"], ["1511088604", "An Economic Model of Friendship: Homophily, Minorities and Segregation"], ["2014730252", "An Approximate Minimum Degree Ordering Algorithm"]]}, {"context": "Link structure of the Web has been used in algorithms like Pagerank CIT and HITS MAINCIT to estimate the importance of web pages, and in CIT for community discovery and clustering.", "bow": [["2138621811", "Authoritative sources in a hyperlinked environment"], ["2141075029", "Stable algorithms for link analysis"], ["2089199911", "The stochastic approach for link-structure analysis (SALSA) and the TKC effect"], ["1974709826", "Authority Rankings from HITS, PageRank, and SALSA: Existence, Uniqueness, and Effect of Initialization"], ["2030035054", "Hubs, authorities, and communities"]], "pp": [["2138621811", "Authoritative sources in a hyperlinked environment"], ["2141075029", "Stable algorithms for link analysis"], ["2089199911", "The stochastic approach for link-structure analysis (SALSA) and the TKC effect"], ["1974709826", "Authority Rankings from HITS, PageRank, and SALSA: Existence, Uniqueness, and Effect of Initialization"], ["2030035054", "Hubs, authorities, and communities"]]}, {"context": "The authority and hub reflect in-degree and out-degree characteristics of a node in the Web respectively MAINCIT .", "bow": [["2030035054", "Hubs, authorities, and communities"], ["2146583842", "Hierarchical hub labelings for shortest paths"], ["2129620481", "The web as a graph: measurements, models, and methods"], ["2002396510", "Dynamical and correlation properties of the Internet"], ["2122750868", "Abusing social networks for automated user profiling"]], "pp": [["2030035054", "Hubs, authorities, and communities"], ["2146583842", "Hierarchical hub labelings for shortest paths"], ["2129620481", "The web as a graph: measurements, models, and methods"], ["2002396510", "Dynamical and correlation properties of the Internet"], ["2122750868", "Abusing social networks for automated user profiling"]]}, {"context": "Elasticsearch is built on top of Lucene which is a key value data store that uses inverted index MAINCIT as its main data structure.", "bow": [["2039742379", "The inverted multi-index"], ["2889395214", "Managing gigabytes"], ["2138662031", "Inverted files for text search engines"], ["84082900", "Compressing and indexing documents and images"], ["2171139251", "Semantic-Aware Co-indexing for Image Retrieval"]], "pp": [["2039742379", "The inverted multi-index"], ["2889395214", "Managing gigabytes"], ["2138662031", "Inverted files for text search engines"], ["84082900", "Compressing and indexing documents and images"], ["2171139251", "Semantic-Aware Co-indexing for Image Retrieval"]]}, {"context": "AS Internet The Internet topology at the autonomous systems (AS) level has been extensively studied in recent years MAINCIT .", "bow": [["2050353626", "Towards capturing representative AS-level Internet topologies"], ["2139905147", "The internet AS-level topology: three data sources and one definitive metric"], ["2151972741", "On inferring autonomous system relationships in the internet"], ["2120514843", "AS relationships: inference and validation"], ["2165503871", "Scalable and accurate identification of AS-level forwarding paths"]], "pp": [["2050353626", "Towards capturing representative AS-level Internet topologies"], ["2139905147", "The internet AS-level topology: three data sources and one definitive metric"], ["2151972741", "On inferring autonomous system relationships in the internet"], ["2155589809", "Topology discovery by active probing"], ["2120514843", "AS relationships: inference and validation"]], "np": [["1495663756", "Signal/collect: graph algorithms for the (semantic) web"], ["1513398909", "Identifying expressions of emotion in text"], ["1564629734", "Computability, inference and modeling in probabilistic programming"], ["1694262664", "The Komlos Conjecture Holds for Vector Colorings"], ["1793674997", "Edge routing with ordered bundles"]]}, {"context": "To further base the recommendation on the implicit feedback, a generic learning algorithm BPR MAINCIT is proposed and applied to the typical KNN and MF methods.", "bow": [["153313452", "GBPR: group preference based Bayesian personalized ranking for one-class collaborative filtering"], ["2140310134", "BPR: Bayesian personalized ranking from implicit feedback"], ["2268318962", "VBPR: visual Bayesian Personalized Ranking from implicit feedback"], ["2101409192", "Collaborative Filtering for Implicit Feedback Datasets"], ["187383899", "Unsupervised sentiment analysis with emotional signals"]], "pp": [["153313452", "GBPR: group preference based Bayesian personalized ranking for one-class collaborative filtering"], ["2140310134", "BPR: Bayesian personalized ranking from implicit feedback"], ["2268318962", "VBPR: visual Bayesian Personalized Ranking from implicit feedback"], ["2101409192", "Collaborative Filtering for Implicit Feedback Datasets"], ["187383899", "Unsupervised sentiment analysis with emotional signals"]]}, {"context": "The seven examined methods are listed as below: FORMULA Popularity: the simple personalized re-ranking baseline provided by the official site; FORMULA KNN: the typical item-based collaborative filtering recommendation algorithm CIT ; FORMULA LFM: state-of-the-art Latent Factor Model CIT , which is mainly designed to address the sparsity problem; FORMULA BPR: the typical bayesian personalized ranking method with the implicit feedback MAINCIT ; FORMULA S-RNN: a typical session-based recommendation method with recurrent neural networks CIT ; FORMULA SIE: the proposed solution that directly use the class probabilities from session information embedding part for ranking; FORMULA ListRank: the proposed solution considers both the session information embedding and deep ListNet ranking.", "bow": [["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["2140310134", "BPR: Bayesian personalized ranking from implicit feedback"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2128280268", "Session types revisited"], ["2040105559", "Multiparty asynchronous session types"]], "pp": [["2140310134", "BPR: Bayesian personalized ranking from implicit feedback"], ["2101409192", "Collaborative Filtering for Implicit Feedback Datasets"], ["153313452", "GBPR: group preference based Bayesian personalized ranking for one-class collaborative filtering"], ["2268318962", "VBPR: visual Bayesian Personalized Ranking from implicit feedback"], ["2062792908", "Two notions of sub-behaviour for session-based client/server systems"]], "np": [["2101409192", "Collaborative Filtering for Implicit Feedback Datasets"], ["2140310134", "BPR: Bayesian personalized ranking from implicit feedback"], ["1564163720", "Improving Web Search Ranking by Incorporating User Behavior"], ["1984127251", "A Probabilistic Model for Using Social Networks in Personalized Item Recommendation"], ["2124187902", "One-Class Collaborative Filtering"]]}, {"context": "Degree Correlation The nearest-neighbours average degree, FORMULA , of FORMULA -degree nodes MAINCIT , is a projection of the joint degree distribution, given by FORMULA A network is called an assortative network if FORMULA increases with FORMULA , which means nodes tend to attach to similar nodes, i.e. high\u2013degree nodes to high\u2013degree nodes and low\u2013degree nodes to low\u2013degree nodes (`assortative mixing').", "bow": [["2002396510", "Dynamical and correlation properties of the Internet"], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2033193852", "Mixing patterns in networks"], ["1584977346", "Detection of topological patterns in complex networks: correlation profile of the internet"], ["2040956707", "Assortative Mixing in Networks"]], "pp": [["2002396510", "Dynamical and correlation properties of the Internet"], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2033193852", "Mixing patterns in networks"], ["1584977346", "Detection of topological patterns in complex networks: correlation profile of the internet"], ["2040956707", "Assortative Mixing in Networks"]]}, {"context": "Coherence models MAINCIT were originally proposed for coherence assessment of monologues (e.g., news articles, books).", "bow": [["2250968833", "Topical Coherence for Graph-based Extractive Summarization"], ["2130339025", "Optimizing Semantic Coherence in Topic Models"], ["2134033146", "Optimized Projections for Compressed Sensing"], ["1911814792", "Breaking the news: First impressions matter on online news"], ["2166347079", "New Methods in Automatic Extracting"]], "pp": [["2250968833", "Topical Coherence for Graph-based Extractive Summarization"], ["2130339025", "Optimizing Semantic Coherence in Topic Models"], ["2134033146", "Optimized Projections for Compressed Sensing"], ["1911814792", "Breaking the news: First impressions matter on online news"], ["2166347079", "New Methods in Automatic Extracting"]]}, {"context": "First, we hypothesize that coherence models should consider the thread structure of a conversation and we extend the original grid representation proposed by Barzilay and Lapata MAINCIT to encode the thread structure of a forum conversation.", "bow": [["2106042170", "Code aware resource management"], ["2102531443", "A Dataset for Research on Short-Text Conversations"], ["2109947791", "Thread algebra for strategic interleaving"], ["2539809671", "Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems"], ["2118370253", "Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization"]], "pp": [["2106042170", "Code aware resource management"], ["2102531443", "A Dataset for Research on Short-Text Conversations"], ["2109947791", "Thread algebra for strategic interleaving"], ["2539809671", "Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems"], ["2118370253", "Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization"]]}, {"context": "Entity Grid and Its Extensions Barzilay and Lapata MAINCIT proposed an entity-based model for representing and assessing text coherence.", "bow": [["41621595", "A Coherence Model Based on Syntactic Patterns"], ["2107130271", "Extracting Paraphrases from a Parallel Corpus"], ["2020278455", "A survey of named entity recognition and classification"], ["2140676672", "Modeling local coherence: An entity-based approach"], ["2046240631", "TwiNER: named entity recognition in targeted twitter stream"]], "pp": [["41621595", "A Coherence Model Based on Syntactic Patterns"], ["2107130271", "Extracting Paraphrases from a Parallel Corpus"], ["2020278455", "A survey of named entity recognition and classification"], ["2140676672", "Modeling local coherence: An entity-based approach"], ["2046240631", "TwiNER: named entity recognition in targeted twitter stream"]]}, {"context": "To represent the grid using a feature vector, Barzilay and Lapata MAINCIT compute probability for each local entity transition of length FORMULA (i.e., FORMULA ), and represent each grid by a vector of FORMULA transitions probabilities.", "bow": [["2107130271", "Extracting Paraphrases from a Parallel Corpus"], ["2149476049", "Computationally feasible bounds for partially observed Markov decision processes"], ["2133330855", "Automatically Evaluating Text Coherence Using Discourse Relations"], ["2162053220", "Hamiltonian cycles in solid grid graphs"], ["2055336889", "Hamilton Paths in Grid Graphs"]], "pp": [["2107130271", "Extracting Paraphrases from a Parallel Corpus"], ["2149476049", "Computationally feasible bounds for partially observed Markov decision processes"], ["2133330855", "Automatically Evaluating Text Coherence Using Discourse Relations"], ["2162053220", "Hamiltonian cycles in solid grid graphs"], ["2055336889", "Hamilton Paths in Grid Graphs"]]}, {"context": "Neural Entity Grid Model Although the entity grid and its extensions have been successfully applied to many downstream applications including coherence rating MAINCIT , readability assessment CIT , essay scoring CIT , and story generation CIT , they have some limitations.", "bow": [["41621595", "A Coherence Model Based on Syntactic Patterns"], ["2019416425", "Revisiting Readability: A Unified Framework for Predicting Text Quality"], ["2140676672", "Modeling local coherence: An entity-based approach"], ["2133330855", "Automatically Evaluating Text Coherence Using Discourse Relations"], ["2020278455", "A survey of named entity recognition and classification"]], "pp": [["41621595", "A Coherence Model Based on Syntactic Patterns"], ["2019416425", "Revisiting Readability: A Unified Framework for Predicting Text Quality"], ["2140676672", "Modeling local coherence: An entity-based approach"], ["2133330855", "Automatically Evaluating Text Coherence Using Discourse Relations"], ["2020278455", "A survey of named entity recognition and classification"]]}, {"context": "Neural Entity Grid Model Although the entity grid and its extensions have been successfully applied to many downstream applications including coherence rating CIT , readability assessment MAINCIT , essay scoring CIT , and story generation CIT , they have some limitations.", "bow": [["41621595", "A Coherence Model Based on Syntactic Patterns"], ["2019416425", "Revisiting Readability: A Unified Framework for Predicting Text Quality"], ["2140676672", "Modeling local coherence: An entity-based approach"], ["2133330855", "Automatically Evaluating Text Coherence Using Discourse Relations"], ["2020278455", "A survey of named entity recognition and classification"]], "pp": [["2019416425", "Revisiting Readability: A Unified Framework for Predicting Text Quality"], ["41621595", "A Coherence Model Based on Syntactic Patterns"], ["2140676672", "Modeling local coherence: An entity-based approach"], ["2133330855", "Automatically Evaluating Text Coherence Using Discourse Relations"], ["2020278455", "A survey of named entity recognition and classification"]], "np": [["2019416425", "Revisiting Readability: A Unified Framework for Predicting Text Quality"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "The seven examined methods are listed as below: FORMULA Popularity: the simple personalized re-ranking baseline provided by the official site; FORMULA KNN: the typical item-based collaborative filtering recommendation algorithm MAINCIT ; FORMULA LFM: state-of-the-art Latent Factor Model CIT , which is mainly designed to address the sparsity problem; FORMULA BPR: the typical bayesian personalized ranking method with the implicit feedback CIT ; FORMULA S-RNN: a typical session-based recommendation method with recurrent neural networks CIT ; FORMULA SIE: the proposed solution that directly use the class probabilities from session information embedding part for ranking; FORMULA ListRank: the proposed solution considers both the session information embedding and deep ListNet ranking.", "bow": [["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["2140310134", "BPR: Bayesian personalized ranking from implicit feedback"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2128280268", "Session types revisited"], ["2040105559", "Multiparty asynchronous session types"]], "pp": [["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["2140310134", "BPR: Bayesian personalized ranking from implicit feedback"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2128280268", "Session types revisited"], ["2040105559", "Multiparty asynchronous session types"]], "np": [["2100235918", "A survey of collaborative filtering techniques"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "Most of the other studies documented in the literature have also relied on tweet content, using different techniques such as topic modelling to find locally relevant keywords that reveal a user's likely location MAINCIT .", "bow": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"]], "pp": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"]], "np": [["2098854771", "Visual tracking decomposition"], ["2154889144", "High-Speed Tracking with Kernelized Correlation Filters"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "Only Han et al. MAINCIT and Dredze et al. CIT perform country-level classification, although they also restricted themselves to English language tweets posted from a limited number of cities.", "bow": [["2093588201", "Mapping the global Twitter heartbeat: The geography of Twitter"], ["2294703018", "Emotional Tweets"], ["2095817220", "An information-spectrum approach to capacity theorems for the general multiple-access channel"], ["2161291053", "Growth, innovation, scaling, and the pace of life in cities"], ["2277420157", "Where Is This Tweet From? Inferring Home Locations of Twitter Users"]], "pp": [["2093588201", "Mapping the global Twitter heartbeat: The geography of Twitter"], ["2294703018", "Emotional Tweets"], ["2095817220", "An information-spectrum approach to capacity theorems for the general multiple-access channel"], ["2161291053", "Growth, innovation, scaling, and the pace of life in cities"], ["2277420157", "Where Is This Tweet From? Inferring Home Locations of Twitter Users"]]}, {"context": "To date, the work by Han et al. MAINCIT is the most relevant to our new study.", "bow": [["2095817220", "An information-spectrum approach to capacity theorems for the general multiple-access channel"], ["1595696424", "Fast Integer Sorting in Linear Space"], ["176406899", "On strategy-proof allocation without payments or priors"], ["2162346812", "Interval algorithm for random number generation"], ["2170792487", "Distributive Opportunistic Spectrum Access for Cognitive Radio using Correlated Equilibrium and No-Regret Learning"]], "pp": [["2095817220", "An information-spectrum approach to capacity theorems for the general multiple-access channel"], ["1595696424", "Fast Integer Sorting in Linear Space"], ["176406899", "On strategy-proof allocation without payments or priors"], ["2162346812", "Interval algorithm for random number generation"], ["2170792487", "Distributive Opportunistic Spectrum Access for Cognitive Radio using Correlated Equilibrium and No-Regret Learning"]]}, {"context": "The selection of these classifiers is in line with those used in the literature, especially with those tested by Han et al. MAINCIT .", "bow": [["2095817220", "An information-spectrum approach to capacity theorems for the general multiple-access channel"], ["1595696424", "Fast Integer Sorting in Linear Space"], ["2162346812", "Interval algorithm for random number generation"], ["176406899", "On strategy-proof allocation without payments or priors"], ["2170792487", "Distributive Opportunistic Spectrum Access for Cognitive Radio using Correlated Equilibrium and No-Regret Learning"]], "pp": [["2095817220", "An information-spectrum approach to capacity theorems for the general multiple-access channel"], ["1595696424", "Fast Integer Sorting in Linear Space"], ["2162346812", "Interval algorithm for random number generation"], ["176406899", "On strategy-proof allocation without payments or priors"], ["2170792487", "Distributive Opportunistic Spectrum Access for Cognitive Radio using Correlated Equilibrium and No-Regret Learning"]]}, {"context": "This decision favours the baseline by assigning the most likely country and is also in line with the baseline approaches used in previous work MAINCIT .", "bow": [["2156718681", "The Optimal Reward Baseline for Gradient-Based Reinforcement Learning"], ["1659247065", "Regions and borders of mobile telephony in Belgium and in the Brussels metropolitan zone"], ["2161792612", "A Phrase-Based,Joint Probability Model for Statistical Machine Translation"], ["1966402429", "The Building Blocks of Economic Complexity"], ["2163569945", "Learning in Extensive-Form Games: Experimental Data and Simple Dynamic Models in the Intermediate Term*"]], "pp": [["2156718681", "The Optimal Reward Baseline for Gradient-Based Reinforcement Learning"], ["1659247065", "Regions and borders of mobile telephony in Belgium and in the Brussels metropolitan zone"], ["2161792612", "A Phrase-Based,Joint Probability Model for Statistical Machine Translation"], ["1966402429", "The Building Blocks of Economic Complexity"], ["2163569945", "Learning in Extensive-Form Games: Experimental Data and Simple Dynamic Models in the Intermediate Term*"]]}, {"context": "Anonymity Analysis Definition 1 Anonymity is the state of being not identifiable within a set of subjects, which is called the anonymity set MAINCIT .", "bow": [["1977843657", "A brief survey on anonymization techniques for privacy preserving publishing of social network data"], ["1798924124", "Information hiding, anonymity and privacy: a modular approach"], ["2482903976", "Towards an information theoretic metric for anonymity"], ["2142229514", "Anonymity, Unobservability, and Pseudonymity - A Proposal for Terminology."], ["1524103604", "Anonymity Loves Company: Usability and the Network Effect."]], "pp": [["2142229514", "Anonymity, Unobservability, and Pseudonymity - A Proposal for Terminology."], ["1977843657", "A brief survey on anonymization techniques for privacy preserving publishing of social network data"], ["1798924124", "Information hiding, anonymity and privacy: a modular approach"], ["2482903976", "Towards an information theoretic metric for anonymity"], ["1975016298", "Towards efficient traffic-analysis resistant anonymity networks"]], "np": [["2142229514", "Anonymity, Unobservability, and Pseudonymity - A Proposal for Terminology."], ["1975016298", "Towards efficient traffic-analysis resistant anonymity networks"], ["2099858845", "Dissent in numbers: making strong anonymity scale"], ["2087811006", "The dining cryptographers problem: unconditional sender and recipient untraceability"], ["100047375", "Parallel algorithm configuration"]]}, {"context": "As discussed in CIT , machine learning methods extended to ranking can be divided into: Pointwise approach which includes methods such as ordinal regression MAINCIT CIT .", "bow": [["2120391124", "McRank: Learning to Rank Using Multiple Classification and Gradient Boosting"], ["2124105163", "Ranking with Large Margin Principle: Two Approaches"], ["1708221419", "Subset ranking using regression"], ["1508409909", "Large margin rank boundaries for ordinal regression"], ["1491036874", "Structured output ordinal regression for dynamic facial emotion intensity prediction"]], "pp": [["2120391124", "McRank: Learning to Rank Using Multiple Classification and Gradient Boosting"], ["2124105163", "Ranking with Large Margin Principle: Two Approaches"], ["1708221419", "Subset ranking using regression"], ["1508409909", "Large margin rank boundaries for ordinal regression"], ["1491036874", "Structured output ordinal regression for dynamic facial emotion intensity prediction"]]}, {"context": "Only a handful of studies have relied solely on the content of a single tweet to infer its location MAINCIT .", "bow": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["2252215182", "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"], ["2167686542", "Mining user similarity based on location history"]], "pp": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["2252215182", "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"], ["2167686542", "Mining user similarity based on location history"]]}, {"context": "Again, most of these have actually worked on very restricted geographical areas, with tweets being limited to different regions, such as the United States MAINCIT , four different cities CIT , and New York only CIT .", "bow": [["2093588201", "Mapping the global Twitter heartbeat: The geography of Twitter"], ["2161291053", "Growth, innovation, scaling, and the pace of life in cities"], ["2294703018", "Emotional Tweets"], ["2123329834", "The Geography of Happiness: Connecting Twitter Sentiment and Expression, Demographics, and Objective Characteristics of Place"], ["2149510050", "Discovering geographical topics in the twitter stream"]], "pp": [["2061744336", "The Brain Activity Map Project and the Challenge of Functional Connectomics"], ["2065451462", "Approximation Theory and Methods"], ["116278252", "The Impact of Driver Inattention on Near-Crash/Crash Risk: An Analysis Using the 100-Car Naturalistic Driving Study Data"], ["2093588201", "Mapping the global Twitter heartbeat: The geography of Twitter"], ["1744091570", "Party Polarization in Congress: A Network Science Approach"]], "np": [["2061744336", "The Brain Activity Map Project and the Challenge of Functional Connectomics"], ["116278252", "The Impact of Driver Inattention on Near-Crash/Crash Risk: An Analysis Using the 100-Car Naturalistic Driving Study Data"], ["2065451462", "Approximation Theory and Methods"], ["1744091570", "Party Polarization in Congress: A Network Science Approach"], ["2117239687", "Detecting influenza epidemics using search engine query data"]]}, {"context": "As discussed in CIT , machine learning methods extended to ranking can be divided into: Pointwise approach which includes methods such as ordinal regression CIT MAINCIT .", "bow": [["2120391124", "McRank: Learning to Rank Using Multiple Classification and Gradient Boosting"], ["2124105163", "Ranking with Large Margin Principle: Two Approaches"], ["1708221419", "Subset ranking using regression"], ["1508409909", "Large margin rank boundaries for ordinal regression"], ["1491036874", "Structured output ordinal regression for dynamic facial emotion intensity prediction"]], "pp": [["2120391124", "McRank: Learning to Rank Using Multiple Classification and Gradient Boosting"], ["2124105163", "Ranking with Large Margin Principle: Two Approaches"], ["1708221419", "Subset ranking using regression"], ["1508409909", "Large margin rank boundaries for ordinal regression"], ["1491036874", "Structured output ordinal regression for dynamic facial emotion intensity prediction"]]}, {"context": "Beyond word co-occurrence, recent studies have also explored learning text embeddings from clickthrough data CIT , session data MAINCIT , query prefix-suffix pairs CIT , via auto-encoders CIT , and for sentiment classification CIT and for long text CIT .", "bow": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]], "pp": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]]}, {"context": "We show that our results both in terms of predictive performance and training time are competitive with other well-known methods such as RankNet MAINCIT , Ranking SVM CIT and ListMLE CIT .", "bow": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]], "pp": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]]}, {"context": "Pairwise approach which spans preference to binary classification MAINCIT CIT CIT methods, where the goal is to learn a classifier that can separate two documents (per query).", "bow": [["2002699418", "Preference formulas in relational queries"], ["175248941", "Interval orders and interval graphs"], ["1558832481", "Foundations of databases"], ["2170188482", "The Skyline operator"], ["2610698371", "Semantic hashing"]], "pp": [["148834693", "Reductions Between Classification Tasks"], ["79354004", "Regret Minimization with Concept Drift"], ["2049393399", "Smooth discrimination analysis"], ["2122537498", "Confidence-weighted linear classification"], ["2146022472", "Semantic Modeling of Natural Scenes for Content-Based Image Retrieval"]], "np": [["148834693", "Reductions Between Classification Tasks"], ["79354004", "Regret Minimization with Concept Drift"], ["2049393399", "Smooth discrimination analysis"], ["2122537498", "Confidence-weighted linear classification"], ["2146022472", "Semantic Modeling of Natural Scenes for Content-Based Image Retrieval"]]}, {"context": "This casts the ranking problem into a standard classification framework, wherein many algorithms are readily available, for example, SVM CIT , neural network and logistic regression MAINCIT , and boosting CIT .", "bow": [["2125607229", "On the Dual Formulation of Boosting Algorithms"], ["1973948212", "Applied Logistic Regression"], ["2067802667", "Discriminative models for information retrieval"], ["2093717447", "The Strength of Weak Learnability"], ["2108263314", "Boosting Algorithms as Gradient Descent"]], "pp": [["1973948212", "Applied Logistic Regression"], ["2000483484", "Self-concordant analysis for logistic regression"], ["2043182541", "Kernel Logistic Regression and the Import Vector Machine"], ["2112380340", "Privacy-preserving logistic regression"], ["2089442574", "Large-scale logistic regression and linear support vector machines using spark"]], "np": [["2089442574", "Large-scale logistic regression and linear support vector machines using spark"], ["2142334564", "Machine learning, neural and statistical classification"], ["2112380340", "Privacy-preserving logistic regression"], ["2000483484", "Self-concordant analysis for logistic regression"], ["1973948212", "Applied Logistic Regression"]]}, {"context": "For comparison, we implement several well-known methods, including RankNet MAINCIT , Ranking SVM CIT and ListMLE CIT .", "bow": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]], "pp": [["2091158010", "Listwise approach to learning to rank: theory and algorithm"], ["2128877075", "Learning to Rank with Nonsmooth Cost Functions"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2108862644", "Learning to rank: from pairwise approach to listwise approach"], ["2117990954", "Parallel Support Vector Machines: The Cascade SVM"]]}, {"context": "A common application of deep learning to IR is for learning to rank, covering a wide range of subtopics within this area, for instance from the earlier RankNet MAINCIT , and methods for hyperlinked webpages CIT , to studies of listwise comparisons CIT , short text pairwise reranking CIT , and elimination strategies CIT .", "bow": [["2149427297", "Learning to Rank for Information Retrieval"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"]], "pp": [["2149427297", "Learning to Rank for Information Retrieval"], ["2125398996", "Adapting ranking SVM to document retrieval"], ["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"]]}, {"context": "Evolutions of the social networks of scientific collaborations in mathematics and neuro-science were studied MAINCIT .", "bow": [["2097777089", "Coauthorship networks and patterns of scientific collaboration"], ["2096523843", "Multi-University Research Teams: Shifting Impact, Geography, and Stratification in Science"], ["2150443611", "Team Assembly Mechanisms Determine Collaboration Network Structure and Team Performance"], ["2159397589", "Computational Social Science"], ["2066752129", "Atypical Combinations and Scientific Impact"]], "pp": [["2097777089", "Coauthorship networks and patterns of scientific collaboration"], ["2096523843", "Multi-University Research Teams: Shifting Impact, Geography, and Stratification in Science"], ["2150443611", "Team Assembly Mechanisms Determine Collaboration Network Structure and Team Performance"], ["2159397589", "Computational Social Science"], ["2066752129", "Atypical Combinations and Scientific Impact"]]}, {"context": "Recently, dimensionality reduction based approaches have gained popularity, for example, by using Compressive Sensing (CS) MAINCIT and the state-of-the-art Low Rank Empirical Risk Minimization (LEML) algorithm CIT .", "bow": [["1940008012", "Large-scale Multi-label Learning with Missing Labels"], ["2104266187", "Compressive Sensing [Lecture Notes]"], ["2041101642", "Compressive sensing: From theory to applications, a survey"], ["1810798640", "Imaging via Compressive Sampling [Introduction to compressive sampling and recovery via convex programming]"], ["1975189025", "Recursive Robust PCA or Recursive Sparse Recovery in Large but Structured Noise"]], "pp": [["1940008012", "Large-scale Multi-label Learning with Missing Labels"], ["2104266187", "Compressive Sensing [Lecture Notes]"], ["2041101642", "Compressive sensing: From theory to applications, a survey"], ["1810798640", "Imaging via Compressive Sampling [Introduction to compressive sampling and recovery via convex programming]"], ["1975189025", "Recursive Robust PCA or Recursive Sparse Recovery in Large but Structured Noise"]]}, {"context": "The idea of Compressive Sensing based approaches MAINCIT is to project the high-dimensional label vector into a smaller random-subspace and then solve a sparse recovery problem in this low-dimensional space.", "bow": [["1975189025", "Recursive Robust PCA or Recursive Sparse Recovery in Large but Structured Noise"], ["2041101642", "Compressive sensing: From theory to applications, a survey"], ["1810798640", "Imaging via Compressive Sampling [Introduction to compressive sampling and recovery via convex programming]"], ["2104266187", "Compressive Sensing [Lecture Notes]"], ["2119667497", "An Introduction To Compressive Sampling"]], "pp": [["1756618330", "The Stochastic Motion Roadmap: A Sampling Framework for Planning with Markov Motion Uncertainty."], ["2144040805", "A Translation of Pseudo Boolean Constraints to SAT"], ["1979446688", "A Monte-Carlo algorithm for estimating the permanent"], ["2143363350", "On graph query optimization in large networks"], ["2168990813", "On the Optimal Robot Routing Problem in Wireless Sensor Networks"]], "np": [["1756618330", "The Stochastic Motion Roadmap: A Sampling Framework for Planning with Markov Motion Uncertainty."], ["1979446688", "A Monte-Carlo algorithm for estimating the permanent"], ["2143363350", "On graph query optimization in large networks"], ["2144040805", "A Translation of Pseudo Boolean Constraints to SAT"], ["2168990813", "On the Optimal Robot Routing Problem in Wireless Sensor Networks"]]}, {"context": "We use vanilla kNN for the experiments in this paper, but this step can be made scalable and fast by using techniques such as Locality Sensitive Hashing MAINCIT .", "bow": [["1502916507", "Similarity Search in High Dimensions via Hashing"], ["2162006472", "Locality-sensitive hashing scheme based on p-stable distributions"], ["2040918046", "Density Sensitive Hashing"], ["2402125293", "Scalable graph hashing with feature transformation"], ["2038276547", "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions"]], "pp": [["1502916507", "Similarity Search in High Dimensions via Hashing"], ["2162006472", "Locality-sensitive hashing scheme based on p-stable distributions"], ["2171790913", "Kernelized locality-sensitive hashing for scalable image search"], ["2109034006", "Efficient large-scale sequence comparison by locality-sensitive hashing"], ["2038276547", "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions"]], "np": [["1502916507", "Similarity Search in High Dimensions via Hashing"], ["2109034006", "Efficient large-scale sequence comparison by locality-sensitive hashing"], ["2162006472", "Locality-sensitive hashing scheme based on p-stable distributions"], ["2171790913", "Kernelized locality-sensitive hashing for scalable image search"], ["1870428314", "Hashing for Similarity Search: A Survey."]]}, {"context": " CIT employ supervised machine learning model (i.e., Conditional Random Fields) ( MAINCIT ) for query generation.", "bow": [["2147880316", "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"], ["2154716422", "Facial expression recognition from video sequences: temporal and static modeling"], ["2036502167", "Conditional models for contextual human motion recognition"], ["179757531", "Document summarization using conditional random fields"], ["2028277005", "Efficiently selecting regions for scene understanding"]], "pp": [["2147880316", "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"], ["2154716422", "Facial expression recognition from video sequences: temporal and static modeling"], ["2036502167", "Conditional models for contextual human motion recognition"], ["179757531", "Document summarization using conditional random fields"], ["2028277005", "Efficiently selecting regions for scene understanding"]]}, {"context": "Evaluation Measures NDCG Normalized Discounted Cumulative Gain (NDCG) is a common measure for comparing a list of estimated document relevance judgments with a list of known judgments ( MAINCIT ).", "bow": [["2069870183", "Cumulated gain-based evaluation of IR techniques"], ["1985554184", "IR evaluation methods for retrieving highly relevant documents"], ["2140472169", "On NDCG consistency of listwise ranking methods"], ["2155211665", "Learning to Rank by Optimizing NDCG Measure"], ["2053584122", "Evaluation by highly relevant documents"]], "pp": [["2069870183", "Cumulated gain-based evaluation of IR techniques"], ["1985554184", "IR evaluation methods for retrieving highly relevant documents"], ["2140472169", "On NDCG consistency of listwise ranking methods"], ["2155211665", "Learning to Rank by Optimizing NDCG Measure"], ["2053584122", "Evaluation by highly relevant documents"]]}, {"context": "It is shown in MAINCIT that a random walk approach is very effective in link prediction on social networks.", "bow": [["2148847267", "The link-prediction problem for social networks"], ["2003707464", "New perspectives and methods in link prediction"], ["2057037070", "Distributed Random Walks"], ["2063832254", "Efficient distributed random walks with applications"], ["1864134408", "Temporal Link Prediction Using Matrix and Tensor Factorizations"]], "pp": [["2148847267", "The link-prediction problem for social networks"], ["2003707464", "New perspectives and methods in link prediction"], ["2057037070", "Distributed Random Walks"], ["2063832254", "Efficient distributed random walks with applications"], ["1864134408", "Temporal Link Prediction Using Matrix and Tensor Factorizations"]]}, {"context": "Inspired by CIT and MAINCIT , we create a recommendation graph, as shown in Fig. REF , consisting of items, groups, and item genres as nodes.", "bow": [["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"], ["2177562712", "Neural Network Matrix Factorization"], ["2262817822", "Session-based Recommendations with Recurrent Neural Networks"], ["2003605635", "Group recommendations with rank aggregation and collaborative filtering"], ["2508504774", "Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence"]], "pp": [["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"], ["2177562712", "Neural Network Matrix Factorization"], ["2262817822", "Session-based Recommendations with Recurrent Neural Networks"], ["2003605635", "Group recommendations with rank aggregation and collaborative filtering"], ["2508504774", "Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence"]]}, {"context": "Low-latency anonymity systems, however, may still fail under attacks such as traffic analysis and fingerprinting MAINCIT .", "bow": [["2128214680", "On the capacity game of private fingerprinting systems under collusion attacks"], ["2170085959", "Touching from a distance: website fingerprinting attacks and defenses"], ["2736554729", "Bridging and fingerprinting: Epistemic attacks on route selection"], ["2031722321", "Optimal probabilistic fingerprint codes"], ["2156410527", "Low-resource routing attacks against tor"]], "pp": [["2128214680", "On the capacity game of private fingerprinting systems under collusion attacks"], ["2170085959", "Touching from a distance: website fingerprinting attacks and defenses"], ["2736554729", "Bridging and fingerprinting: Epistemic attacks on route selection"], ["2031722321", "Optimal probabilistic fingerprint codes"], ["2156410527", "Low-resource routing attacks against tor"]]}, {"context": " More recently, the intersection between machine learning and information retrieval has resulted in a fruitful sub-area called learning to rank (e.g. see MAINCIT for a recent review), where the goal is to learn rank functions that can accurately order objects from retrieval systems.", "bow": [["2240863905", "On defining generalized rank weights"], ["2165899219", "Tensor Rank: Some Lower and Upper Bounds"], ["2149427297", "Learning to Rank for Information Retrieval"], ["2460641408", "On the similarities between generalized rank and Hamming weights and their applications to network coding"], ["2114456882", "Clip-based similarity measure for query-dependent clip retrieval and video summarization"]], "pp": [["2240863905", "On defining generalized rank weights"], ["2165899219", "Tensor Rank: Some Lower and Upper Bounds"], ["2149427297", "Learning to Rank for Information Retrieval"], ["2460641408", "On the similarities between generalized rank and Hamming weights and their applications to network coding"], ["2114456882", "Clip-based similarity measure for query-dependent clip retrieval and video summarization"]]}, {"context": " Learning-to-rank is an active topic in the intersection between machine learning and information retrieval (e.g. see MAINCIT for a recent survey).", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["2750606726", "Active Learning"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["2750606726", "Active Learning"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]]}, {"context": "As discussed in MAINCIT , machine learning methods extended to ranking can be divided into: Pointwise approach which includes methods such as ordinal regression CIT CIT .", "bow": [["2120391124", "McRank: Learning to Rank Using Multiple Classification and Gradient Boosting"], ["2124105163", "Ranking with Large Margin Principle: Two Approaches"], ["1708221419", "Subset ranking using regression"], ["1508409909", "Large margin rank boundaries for ordinal regression"], ["1491036874", "Structured output ordinal regression for dynamic facial emotion intensity prediction"]], "pp": [["2120391124", "McRank: Learning to Rank Using Multiple Classification and Gradient Boosting"], ["2124105163", "Ranking with Large Margin Principle: Two Approaches"], ["1708221419", "Subset ranking using regression"], ["1508409909", "Large margin rank boundaries for ordinal regression"], ["1491036874", "Structured output ordinal regression for dynamic facial emotion intensity prediction"]]}, {"context": "In order to evaluate our approach, we introduce a dataset of one billion vectors extracted from millions of images using the standard SIFT descriptor MAINCIT .", "bow": [["2151103935", "Distinctive Image Features from Scale-Invariant Keypoints"], ["2124386111", "Object recognition from local scale-invariant features"], ["2132234208", "Searching in one billion vectors: Re-rank with source coding"], ["2118153703", "CSIFT: A SIFT Descriptor with Color Invariant Characteristics"], ["2090518410", "SIFT Flow: Dense Correspondence across Scenes and Its Applications"]], "pp": [["2151103935", "Distinctive Image Features from Scale-Invariant Keypoints"], ["2124386111", "Object recognition from local scale-invariant features"], ["2132234208", "Searching in one billion vectors: Re-rank with source coding"], ["2118153703", "CSIFT: A SIFT Descriptor with Color Invariant Characteristics"], ["2090518410", "SIFT Flow: Dense Correspondence across Scenes and Its Applications"]], "np": [["2151103935", "Distinctive Image Features from Scale-Invariant Keypoints"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "This benchmark, called BIGANN, consists of 128-dimensional SIFT descriptors (widely adopted image descriptors MAINCIT ) extracted from approximately 1 million images.", "bow": [["2132234208", "Searching in one billion vectors: Re-rank with source coding"], ["2151103935", "Distinctive Image Features from Scale-Invariant Keypoints"], ["2124386111", "Object recognition from local scale-invariant features"], ["2007178811", "Learning Local Feature Descriptors Using Convex Optimisation"], ["2150782236", "CHoG: Compressed histogram of gradients A low bit-rate feature descriptor"]], "pp": [["2132234208", "Searching in one billion vectors: Re-rank with source coding"], ["1869500417", "Discriminative Learning of Deep Convolutional Feature Point Descriptors"], ["2124386111", "Object recognition from local scale-invariant features"], ["2155490028", "Visual Word Ambiguity"], ["2151103935", "Distinctive Image Features from Scale-Invariant Keypoints"]], "np": [["2032481801", "Vision-based human motion analysis: An overview"], ["2142785776", "Probabilistic Graphlet Transfer for Photo Cropping"], ["2048835603", "Assessing the aesthetic quality of photographs using generic image descriptors"], ["2155490028", "Visual Word Ambiguity"], ["2038597090", "Large-scale image categorization with explicit data embedding"]]}, {"context": "Looking at users' self-reported locations, Hecht et al. MAINCIT found that 66% report information that can be translated, accurately or inaccurately, to a geographic location, with the other 34% being either empty or not geolocalisable.", "bow": [["2151378814", "Tweets from Justin Bieber's heart: the dynamics of the location field in user profiles"], ["4055677", "Distance matters: geo-social metrics for online social networks"], ["1586448612", "A theorem proving approach to analysis of secure information flow"], ["2168346693", "Find me if you can: improving geographical prediction with social and spatial proximity"], ["2119595472", "Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach"]], "pp": [["2151378814", "Tweets from Justin Bieber's heart: the dynamics of the location field in user profiles"], ["4055677", "Distance matters: geo-social metrics for online social networks"], ["1586448612", "A theorem proving approach to analysis of secure information flow"], ["2168346693", "Find me if you can: improving geographical prediction with social and spatial proximity"], ["2119595472", "Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach"]]}, {"context": "Link structure of the Web has been used in algorithms like Pagerank CIT and HITS CIT to estimate the importance of web pages, and in MAINCIT for community discovery and clustering.", "bow": [["2138621811", "Authoritative sources in a hyperlinked environment"], ["2141075029", "Stable algorithms for link analysis"], ["2089199911", "The stochastic approach for link-structure analysis (SALSA) and the TKC effect"], ["1974709826", "Authority Rankings from HITS, PageRank, and SALSA: Existence, Uniqueness, and Effect of Initialization"], ["2030035054", "Hubs, authorities, and communities"]], "pp": [["2138621811", "Authoritative sources in a hyperlinked environment"], ["2141075029", "Stable algorithms for link analysis"], ["2089199911", "The stochastic approach for link-structure analysis (SALSA) and the TKC effect"], ["1974709826", "Authority Rankings from HITS, PageRank, and SALSA: Existence, Uniqueness, and Effect of Initialization"], ["2030035054", "Hubs, authorities, and communities"]]}, {"context": "Baseline models We compare the DESM models to a term-matching based baseline, in BM25, and a vector space model baseline, in Latent Semantic Analysis (LSA) MAINCIT .", "bow": [["1482214997", "Okapi at TREC"], ["2158997610", "An introduction to latent semantic analysis"], ["2058013187", "Latent Semantic Analysis"], ["2091273188", "Using latent semantic analysis to improve access to textual information"], ["2155482025", "The Probabilistic Relevance Framework: BM25 and Beyond"]], "pp": [["1482214997", "Okapi at TREC"], ["2158997610", "An introduction to latent semantic analysis"], ["2058013187", "Latent Semantic Analysis"], ["2091273188", "Using latent semantic analysis to improve access to textual information"], ["2155482025", "The Probabilistic Relevance Framework: BM25 and Beyond"]]}, {"context": "In contrast to the classical vector space model, LSA MAINCIT , PLSA CIT and LDA CIT learn dense vector representations of much lower dimensionality.", "bow": [["2134731454", "Unsupervised Learning by Probabilistic Latent Semantic Analysis"], ["2091273188", "Using latent semantic analysis to improve access to textual information"], ["2130324521", "Exploring Topic Coherence over Many Models and Many Topics"], ["2158997610", "An introduction to latent semantic analysis"], ["1985809919", "A direct LDA algorithm for high-dimensional data \u2014 with application to face recognition"]], "pp": [["2134731454", "Unsupervised Learning by Probabilistic Latent Semantic Analysis"], ["2091273188", "Using latent semantic analysis to improve access to textual information"], ["2130324521", "Exploring Topic Coherence over Many Models and Many Topics"], ["2158997610", "An introduction to latent semantic analysis"], ["1985809919", "A direct LDA algorithm for high-dimensional data \u2014 with application to face recognition"]]}, {"context": "Distributional Semantics for IR In this section we first introduce the Continuous Bag-of-Words (CBOW) model made popular by the software Word2Vec MAINCIT .", "bow": [["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["2611099133", "Neural Models for Information Retrieval."], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"]], "pp": [["2516925101", "Analysis of the Paragraph Vector Model for Information Retrieval"], ["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["2611099133", "Neural Models for Information Retrieval."], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["1610356397", "Learning Discriminative Projections for Text Similarity Measures"]]}, {"context": "Continuous Bag-of-Words While many word embedding models have been proposed recently, the Continuous Bag-of-Words (CBOW) and the Skip-Gram (SG) architectures proposed by MAINCIT are arguably the most popular (perhaps due to the popularity of the software Word2Vechttps://code.google.com/p/word2vec/, which implements both).", "bow": [["2097508275", "Improved audio features for large-scale multimedia event detection"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2293292771", "Skip graphs"], ["2408491073", "Bag-of-Audio-Words Approach for Multimedia Event Classification."], ["2153579005", "Distributed Representations of Words and Phrases and their Compositionality"]], "pp": [["2097508275", "Improved audio features for large-scale multimedia event detection"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2293292771", "Skip graphs"], ["2408491073", "Bag-of-Audio-Words Approach for Multimedia Event Classification."], ["2153579005", "Distributed Representations of Words and Phrases and their Compositionality"]]}, {"context": "To make CBOW scalable, MAINCIT proposed the following slightly altered negative sampling objective: FORMULA where FORMULA is the Sigmoid function and FORMULA is the number of negative sample words drawn either from the uniform or empirical distribution over the vocabulary.", "bow": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["2131571251", "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method."], ["2152808281", "Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model"], ["2752172973", "A Simple but Tough-to-Beat Baseline for Sentence Embeddings"], ["2178333206", "Natural Language Understanding with Distributed Representation"]], "pp": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["2131571251", "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method."], ["2152808281", "Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model"], ["2752172973", "A Simple but Tough-to-Beat Baseline for Sentence Embeddings"], ["2178333206", "Natural Language Understanding with Distributed Representation"]]}, {"context": "Firstly, much of the existing literature MAINCIT on CBOW and SG uses cosine similarity and normalized unit vectors (for performing vector algebra for word analogies).", "bow": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1970997001", "A New Class of Incremental Gradient Methods for Least Squares Problems"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2164637474", "Correlated-Q learning"], ["2387546565", "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks"]], "pp": [["1781640942", "Some Effectivity Problems in Polynomial Ideal Theory"], ["2115044435", "Efficient Exploration and Value Function Generalization in Deterministic Systems"], ["1561838634", "Probabilistic Lossy Channel Systems"], ["2121219412", "Quickest detection in censoring sensor networks"], ["2158617780", "Evaluation of Gender Classification Methods with Automatically Detected and Aligned Faces"]], "np": [["1561838634", "Probabilistic Lossy Channel Systems"], ["1781640942", "Some Effectivity Problems in Polynomial Ideal Theory"], ["2115044435", "Efficient Exploration and Value Function Generalization in Deterministic Systems"], ["2026318959", "Want to be Retweeted? Large Scale Analytics on Factors Impacting Retweet in Twitter Network"], ["2096610849", "Verification of probabilistic systems with faulty communication"]]}, {"context": "Neural embeddings for IR The word embeddings produced by the CBOW and SG models have been shown to be surprisingly effective at capturing detailed semantics useful for various Natural Language Processing (NLP) and reasoning tasks, including word analogies MAINCIT .", "bow": [["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."], ["2251103205", "Semantic Clustering and Convolutional Neural Network for Short Text Categorization"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2587019100", "Comparative Study of CNN and RNN for Natural Language Processing."]], "pp": [["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."], ["2251103205", "Semantic Clustering and Convolutional Neural Network for Short Text Categorization"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2587019100", "Comparative Study of CNN and RNN for Natural Language Processing."]]}, {"context": "Although IN-IN, for example, works well for word analogy tasks MAINCIT , it might perform less effectively for other tasks, such as those in information retrieval.", "bow": [["2572730214", "Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis"], ["2015419638", "A basis for information retrieval in context"], ["2141599568", "Linguistic Regularities in Continuous Space Word Representations"], ["2252211741", "Evaluation methods for unsupervised word embeddings"], ["1615991656", "Improving Distributional Similarity with Lessons Learned from Word Embeddings"]], "pp": [["2572730214", "Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis"], ["2015419638", "A basis for information retrieval in context"], ["2141599568", "Linguistic Regularities in Continuous Space Word Representations"], ["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["2252211741", "Evaluation methods for unsupervised word embeddings"]], "np": [["2250189634", "Linguistic Regularities in Sparse and Explicit Word Representations"], ["2141599568", "Linguistic Regularities in Continuous Space Word Representations"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "Our approach differs from CIT in using Word2Vec MAINCIT similarity instead of WordNet.", "bow": [["2081580037", "WordNet: a lexical database for English"], ["2038721957", "WordNet : an electronic lexical database"], ["1643741109", "Wordnet: an on-line lexical database"], ["136846643", "An Electronic Lexical Database"], ["2613678836", "WordNet: similarity - measuring the relatedness of concepts"]], "pp": [["2081580037", "WordNet: a lexical database for English"], ["2038721957", "WordNet : an electronic lexical database"], ["1643741109", "Wordnet: an on-line lexical database"], ["136846643", "An Electronic Lexical Database"], ["2613678836", "WordNet: similarity - measuring the relatedness of concepts"]]}, {"context": "Coauthors of a paper formulate the motif MAINCIT of research network.", "bow": [["2161917555", "Efficient Detection of Network Motifs"], ["2018472652", "Upper and lower bounds for finding connected motifs in vertex-colored graphs"], ["1966406756", "Detecting time series motifs under uniform scaling"], ["2089418726", "Efficiently Estimating Motif Statistics of Large Networks"], ["2120905266", "Detecting Subdimensional Motifs: An Efficient Algorithm for Generalized Multivariate Pattern Discovery"]], "pp": [["2161917555", "Efficient Detection of Network Motifs"], ["2018472652", "Upper and lower bounds for finding connected motifs in vertex-colored graphs"], ["1966406756", "Detecting time series motifs under uniform scaling"], ["2089418726", "Efficiently Estimating Motif Statistics of Large Networks"], ["2120905266", "Detecting Subdimensional Motifs: An Efficient Algorithm for Generalized Multivariate Pattern Discovery"]]}, {"context": "For clustering algorithms for disambiguation, Mann and Yarowsky MAINCIT used a simple agglomerative clustering which still had a transitivity problem.", "bow": [["2097922870", "Graph degree linkage: agglomerative clustering on a directed graph"], ["2103958034", "A Deep Semi-NMF Model for Learning Hidden Representations"], ["178113997", "Locality preserving nonnegative matrix factorization"], ["2034768355", "Agglomerative Mean-Shift Clustering"], ["2131828344", "Spectral Embedded Clustering: A Framework for In-Sample and Out-of-Sample Spectral Clustering"]], "pp": [["2097922870", "Graph degree linkage: agglomerative clustering on a directed graph"], ["2103958034", "A Deep Semi-NMF Model for Learning Hidden Representations"], ["178113997", "Locality preserving nonnegative matrix factorization"], ["2034768355", "Agglomerative Mean-Shift Clustering"], ["2131828344", "Spectral Embedded Clustering: A Framework for In-Sample and Out-of-Sample Spectral Clustering"]]}, {"context": "Private Information Retrieval, Anonymous communications, Private Queries, Differential Privacy Lower-Cost FORMULA -Private Information Retrieval[2018/12/11 13:53:03 Introduction Despite many years of research and significant advances, Private Information Retrieval (PIR) still suffers from scalability issues that were identified some time ago CIT : both information theoretic CIT (IT-PIR) and computational MAINCIT PIR schemes require database servers to operate on all records for each private query to conceal the sought record.", "bow": [["2073346043", "Private information retrieval"], ["2154654620", "Replication is not needed: single database, computationally-private information retrieval"], ["1593123147", "Information-Theoretic Private Information Retrieval: A Unified Construction"], ["1619839175", "Codes for distributed PIR with low storage overhead"], ["1986070285", "Private information retrieval"]], "pp": [["2073346043", "Private information retrieval"], ["2154654620", "Replication is not needed: single database, computationally-private information retrieval"], ["1593123147", "Information-Theoretic Private Information Retrieval: A Unified Construction"], ["1619839175", "Codes for distributed PIR with low storage overhead"], ["1986070285", "Private information retrieval"]]}, {"context": "Later, Computational PIR (CPIR) MAINCIT was proposed using a single server, but its practicality has been questioned as being slower than simply downloading the entire database at typical network speeds CIT .", "bow": [["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["2154654620", "Replication is not needed: single database, computationally-private information retrieval"], ["1986070285", "Private information retrieval"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"], ["2007208840", "General constructions for information-theoretic private information retrieval"]], "pp": [["1963094505", "Computationally private information retrieval with polylogarithmic communication"], ["2154654620", "Replication is not needed: single database, computationally-private information retrieval"], ["1986070285", "Private information retrieval"], ["2132937716", "A geometric approach to information-theoretic private information retrieval"], ["2007208840", "General constructions for information-theoretic private information retrieval"]]}, {"context": "Rich-Club Connectivity The rich-club connectivity MAINCIT measures how tightly the high-degree nodes, rich nodes, interconnect with themselves.", "bow": [["2075984423", "Spectra of \"real-world\" graphs: beyond the semicircle law."], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2018438786", "On the minimum node degree and connectivity of a wireless multihop network"], ["2039302875", "Connectivity properties of a packet radio network model"], ["2138561727", "The critical transmitting range for connectivity in sparse wireless ad hoc networks"]], "pp": [["2075984423", "Spectra of \"real-world\" graphs: beyond the semicircle law."], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2018438786", "On the minimum node degree and connectivity of a wireless multihop network"], ["2039302875", "Connectivity properties of a packet radio network model"], ["2138561727", "The critical transmitting range for connectivity in sparse wireless ad hoc networks"]]}, {"context": "One theoretical basis for such features is the probabilistic model of information retrieval, which has yielded the very successful TF-IDF formulation BM25 MAINCIT .", "bow": [["2122557169", "Lp-Norm IDF for Large Scale Image Search"], ["1482214997", "Okapi at TREC"], ["2114510817", "Analyzing temporal dynamics in Twitter profiles for personalized recommendations in the social web"], ["2153252192", "A probabilistic model of information retrieval: development and comparative experiments"], ["2155482025", "The Probabilistic Relevance Framework: BM25 and Beyond"]], "pp": [["2122557169", "Lp-Norm IDF for Large Scale Image Search"], ["1482214997", "Okapi at TREC"], ["2114510817", "Analyzing temporal dynamics in Twitter profiles for personalized recommendations in the social web"], ["2153252192", "A probabilistic model of information retrieval: development and comparative experiments"], ["2155482025", "The Probabilistic Relevance Framework: BM25 and Beyond"]]}, {"context": "The probabilistic model of information retrieval leads to the development of the BM25 ranking feature MAINCIT .", "bow": [["1482214997", "Okapi at TREC"], ["2153252192", "A probabilistic model of information retrieval: development and comparative experiments"], ["2155482025", "The Probabilistic Relevance Framework: BM25 and Beyond"], ["2015441003", "Parameterized concept weighting in verbose queries"], ["1521988707", "Probability Ranking Principle."]], "pp": [["1482214997", "Okapi at TREC"], ["2153252192", "A probabilistic model of information retrieval: development and comparative experiments"], ["2155482025", "The Probabilistic Relevance Framework: BM25 and Beyond"], ["2015441003", "Parameterized concept weighting in verbose queries"], ["1521988707", "Probability Ranking Principle."]]}, {"context": "FIGURE All three models were implemented in the Caffe framework MAINCIT and used to generate fDNA vectors for the standard SKU set, with corresponding customer weights and biases.", "bow": [["2155893237", "Caffe: Convolutional Architecture for Fast Feature Embedding"], ["2160054705", "Two-tree algorithms for full bandwidth broadcast, reduction and scan"], ["2298765043", "ITEM2VEC: Neural item embedding for collaborative filtering"], ["2254715784", "Theano: Deep Learning on GPUs with Python"], ["1548328233", "Torch: a modular machine learning software library"]], "pp": [["2155893237", "Caffe: Convolutional Architecture for Fast Feature Embedding"], ["2160054705", "Two-tree algorithms for full bandwidth broadcast, reduction and scan"], ["2298765043", "ITEM2VEC: Neural item embedding for collaborative filtering"], ["2254715784", "Theano: Deep Learning on GPUs with Python"], ["1548328233", "Torch: a modular machine learning software library"]], "np": [["2155893237", "Caffe: Convolutional Architecture for Fast Feature Embedding"], ["2618530766", "ImageNet classification with deep convolutional neural networks"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": " Once again, any distribution that agrees with PRP can be used; researcher have used a number of such distributions (e.g. see MAINCIT ).", "bow": [["1980724276", "Estimation error guarantees for Poisson denoising with sparse and structured dictionary models"], ["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["1514415753", "Parameterized verification of ad hoc networks"]], "pp": [["1980724276", "Estimation error guarantees for Poisson denoising with sparse and structured dictionary models"], ["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["1514415753", "Parameterized verification of ad hoc networks"]]}, {"context": "Collaborative-filtering based recommender systems CIT suffer from the \u201ccold-start problem\u201d MAINCIT : Individual predictions for new customers and articles cannot be calculated in the absence of prior purchase data.", "bow": [["2100235918", "A survey of collaborative filtering techniques"], ["2171960770", "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions"], ["1690919088", "Recommender Systems Handbook"], ["1966553486", "Using collaborative filtering to weave an information tapestry"], ["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"]], "pp": [["2100235918", "A survey of collaborative filtering techniques"], ["2171960770", "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions"], ["1690919088", "Recommender Systems Handbook"], ["1966553486", "Using collaborative filtering to weave an information tapestry"], ["1530559104", "Empirical Analysis of Predictive Algorithms for Collaborative"]]}, {"context": "Most of the work with deep learning has involved learning word vector representations through neural language models CIT CIT CIT and performing composition over the learned word vectors for classification MAINCIT .", "bow": [["2132339004", "A neural probabilistic language model"], ["2164019165", "Improving Word Representations via Global Context and Multiple Word Prototypes"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2141599568", "Linguistic Regularities in Continuous Space Word Representations"], ["1979185006", "Neural net language models"]], "pp": [["2132339004", "A neural probabilistic language model"], ["2164019165", "Improving Word Representations via Global Context and Multiple Word Prototypes"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["2141599568", "Linguistic Regularities in Continuous Space Word Representations"], ["1979185006", "Neural net language models"]]}, {"context": "Convolutional neural networks have also been shown to be highly effective for natural language processing and have achieved excellent results in information retrieval CIT , semantic parsing CIT , sentence modeling CIT and other traditional natural language processing tasks MAINCIT .", "bow": [["2120615054", "A Convolutional Neural Network for Modelling Sentences"], ["1423339008", "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"], ["179875071", "Recurrent neural network based language model"], ["1869752048", "Grammar as a foreign language"], ["2158899491", "Natural Language Processing (Almost) from Scratch"]], "pp": [["2120615054", "A Convolutional Neural Network for Modelling Sentences"], ["1423339008", "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"], ["179875071", "Recurrent neural network based language model"], ["1869752048", "Grammar as a foreign language"], ["2158899491", "Natural Language Processing (Almost) from Scratch"]]}, {"context": "We use a pairwise ranking approach MAINCIT to train the Grid-CNN model.", "bow": [["2105089228", "Efficient Ranking from Pairwise Comparisons"], ["2155202145", "TOPOLOGICAL VULNERABILITY OF THE EUROPEAN POWER GRID UNDER ERRORS AND ATTACKS"], ["2102011767", "Robust reductions from ranking to classification"], ["2114990184", "An axiomatization of Borda's rule"], ["2111996486", "Globus: a Metacomputing Infrastructure Toolkit"]], "pp": [["2142646165", "Learning to Order Things"], ["2105089228", "Efficient Ranking from Pairwise Comparisons"], ["2155202145", "TOPOLOGICAL VULNERABILITY OF THE EUROPEAN POWER GRID UNDER ERRORS AND ATTACKS"], ["2102011767", "Robust reductions from ranking to classification"], ["2114990184", "An axiomatization of Borda's rule"]], "np": [["2142646165", "Learning to Order Things"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "Using preference-based test collections is introduced by Rorvig CIT and later developed for text retrieval by Carterette et al. MAINCIT .", "bow": [["2340378363", "A Context-aware Time Model for Web Search"], ["2002699418", "Preference formulas in relational queries"], ["175248941", "Interval orders and interval graphs"], ["1558832481", "Foundations of databases"], ["2170188482", "The Skyline operator"]], "pp": [["2340378363", "A Context-aware Time Model for Web Search"], ["2002699418", "Preference formulas in relational queries"], ["175248941", "Interval orders and interval graphs"], ["1558832481", "Foundations of databases"], ["2170188482", "The Skyline operator"]]}, {"context": "It is a convenient way to obtain relevance assessments, compared to obtaining absolute relevance judgments MAINCIT .", "bow": [["2053584122", "Evaluation by highly relevant documents"], ["2169213601", "Relevance-Based Language Models"], ["2113640060", "Expected reciprocal rank for graded relevance"], ["2065269597", "Understanding the intrinsic memorability of images"], ["2115606304", "Analysis of Sparse Bayesian Learning"]], "pp": [["2053584122", "Evaluation by highly relevant documents"], ["2160892561", "Variations in relevance judgments and the measurement of retrieval effectiveness"], ["2169213601", "Relevance-Based Language Models"], ["2113640060", "Expected reciprocal rank for graded relevance"], ["2065269597", "Understanding the intrinsic memorability of images"]], "np": [["2160892561", "Variations in relevance judgments and the measurement of retrieval effectiveness"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "This is lower than the agreement level for the traditional text retrieval task (75.85%) MAINCIT , which indicates that the task of soundtrack recommendation is more subjective.", "bow": [["2133138357", "Evaluation of different biological data and computational classification methods for use in protein interaction prediction"], ["2141766660", "Inter-coder agreement for computational linguistics"], ["2167665700", "Towards task recommendation in micro-task markets"], ["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"], ["2164361866", "Trust-based recommendation systems: an axiomatic approach"]], "pp": [["2133138357", "Evaluation of different biological data and computational classification methods for use in protein interaction prediction"], ["2141766660", "Inter-coder agreement for computational linguistics"], ["2167665700", "Towards task recommendation in micro-task markets"], ["2114079787", "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems"], ["2164361866", "Trust-based recommendation systems: an axiomatic approach"]]}, {"context": " Our model can be placed into the framework of probabilistic graphical models (e.g. see CIT MAINCIT ).", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["114565888", "Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["114565888", "Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]]}, {"context": "Listwise approach which models the distribution of permutations CIT MAINCIT CIT .", "bow": [["2096916804", "The enumeration of simple permutations."], ["1499300513", "Metrics on Permutations, a Survey"], ["2070286422", "Critically indecomposable partially ordered sets, graphs, tournaments and other binary relational structures"], ["2083354900", "Pattern matching for permutations"], ["1981803420", "Simple permutations and pattern restricted permutations"]], "pp": [["2096916804", "The enumeration of simple permutations."], ["1499300513", "Metrics on Permutations, a Survey"], ["2070286422", "Critically indecomposable partially ordered sets, graphs, tournaments and other binary relational structures"], ["2083354900", "Pattern matching for permutations"], ["1981803420", "Simple permutations and pattern restricted permutations"]]}, {"context": "Note that such fine-grained scales emphasize the point of possible disagreement between human assessors, when determining how relevant a document is MAINCIT .", "bow": [["2113640060", "Expected reciprocal rank for graded relevance"], ["2114232233", "A bound on the label complexity of agnostic active learning"], ["1928593089", "Semi-supervised clustering: probabilistic models, algorithms and experiments"], ["2260677151", "Can machine translation systems be evaluated by the crowd alone"], ["2085443648", "Semi-supervised learning by disagreement"]], "pp": [["2113640060", "Expected reciprocal rank for graded relevance"], ["2114232233", "A bound on the label complexity of agnostic active learning"], ["1928593089", "Semi-supervised clustering: probabilistic models, algorithms and experiments"], ["2260677151", "Can machine translation systems be evaluated by the crowd alone"], ["2085443648", "Semi-supervised learning by disagreement"]]}, {"context": "We compare the proposed method with two state-of-art personalized recommendation systems: L+ MAINCIT and ItemRank CIT .", "bow": [["2164361866", "Trust-based recommendation systems: an axiomatic approach"], ["2167032986", "A model of a trust-based recommendation system on a social network"], ["2108168165", "Implicit user modeling for personalized search"], ["2100755716", "A survey of collaborative filtering based social recommender systems"], ["2122514299", "Stochastic language generation in dialogue using factored language models"]], "pp": [["2164361866", "Trust-based recommendation systems: an axiomatic approach"], ["2167032986", "A model of a trust-based recommendation system on a social network"], ["2108168165", "Implicit user modeling for personalized search"], ["2100755716", "A survey of collaborative filtering based social recommender systems"], ["2122514299", "Stochastic language generation in dialogue using factored language models"]]}, {"context": "L+ suggested a dissimilarity measure between nodes of a graph, the expected commute time between two nodes, which the authors applied to recommendation MAINCIT .", "bow": [["150179085", "The Johnson-Lindenstrauss transform: an empirical study"], ["2161984370", "Random-Walk Computation of Similarities between Nodes of a Graph with Application to Collaborative Recommendation"], ["1480708938", "The Dissimilarity Representation for Pattern Recognition: Foundations and Applications"], ["1781665229", "The principal components analysis of a graph, and its relationships to spectral clustering"], ["2126337883", "Parallel Spectral Clustering in Distributed Systems"]], "pp": [["150179085", "The Johnson-Lindenstrauss transform: an empirical study"], ["2161984370", "Random-Walk Computation of Similarities between Nodes of a Graph with Application to Collaborative Recommendation"], ["1480708938", "The Dissimilarity Representation for Pattern Recognition: Foundations and Applications"], ["1781665229", "The principal components analysis of a graph, and its relationships to spectral clustering"], ["2126337883", "Parallel Spectral Clustering in Distributed Systems"]]}, {"context": "nearest neighbor search, quantization, source coding, high dimensional indexing, large databases Introduction Approximate nearest neighbors (ANN) search methods MAINCIT are required to handle large databases, especially for computer vision CIT and music retrieval CIT applications.", "bow": [["2427881153", "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"], ["2131929066", "On the Difficulty of Nearest Neighbor Search"], ["2167931879", "Anti-sparse coding for approximate nearest neighbor search"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"]], "pp": [["2427881153", "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"], ["2131929066", "On the Difficulty of Nearest Neighbor Search"], ["2167931879", "Anti-sparse coding for approximate nearest neighbor search"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"]], "np": [["2008995227", "Approximate Nearest Subspace Search"], ["2171790913", "Kernelized locality-sensitive hashing for scalable image search"], ["1502916507", "Similarity Search in High Dimensions via Hashing"], ["2293597654", "Spectral Hashing"], ["2124509324", "Product Quantization for Nearest Neighbor Search"]]}, {"context": "One of the most popular techniques is Euclidean Locality-Sensitive Hashing MAINCIT .", "bow": [["1502916507", "Similarity Search in High Dimensions via Hashing"], ["2162006472", "Locality-sensitive hashing scheme based on p-stable distributions"], ["2040918046", "Density Sensitive Hashing"], ["2397770138", "Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality"], ["2038276547", "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions"]], "pp": [["1502916507", "Similarity Search in High Dimensions via Hashing"], ["2162006472", "Locality-sensitive hashing scheme based on p-stable distributions"], ["2397770138", "Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality"], ["2171790913", "Kernelized locality-sensitive hashing for scalable image search"], ["2040918046", "Density Sensitive Hashing"]], "np": [["1647207848", "Beyond locality-sensitive hashing"], ["2171790913", "Kernelized locality-sensitive hashing for scalable image search"], ["2162006472", "Locality-sensitive hashing scheme based on p-stable distributions"], ["2169054943", "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)"], ["2397770138", "Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality"]]}, {"context": "This is always done in partitioning based method such as LSH MAINCIT or FLANN CIT , as the neighbor hypotheses are not ranked on output of the index.", "bow": [["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"], ["1627400044", "Fast approximate nearest neighbors with automatic algorithm configuration"], ["2146020873", "Fast Matching of Binary Features"], ["2055839530", "Modeling LSH for performance tuning"], ["1898304433", "Spherical lsh for approximate nearest neighbor search on unit hypersphere"]], "pp": [["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"], ["1627400044", "Fast approximate nearest neighbors with automatic algorithm configuration"], ["2146020873", "Fast Matching of Binary Features"], ["2055839530", "Modeling LSH for performance tuning"], ["1898304433", "Spherical lsh for approximate nearest neighbor search on unit hypersphere"]]}, {"context": "Re-ranking neighbors using source coding Refinement: principle The objective of the method proposed in this paper is to avoid the costly post-verification scheme adopted in most state-of-the-art approximate search techniques MAINCIT .", "bow": [["2114085948", "Information transmission with additional noise"], ["2101085206", "Refinement types for ML"], ["2114990184", "An axiomatization of Borda's rule"], ["2120342618", "A unified approach to ranking in probabilistic databases"], ["2076188996", "Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors"]], "pp": [["2054358802", "Learning Markov networks: maximum bounded tree-width graphs"], ["2127366419", "Dynamic Euclidean minimum spanning trees and extrema of binary functions"], ["2295690548", "Imitation Learning of Agenda-based Semantic Parsers"], ["1667614912", "A new method for solving hard satisfiability problems"], ["1589232796", "Programming with Constraints: An Introduction"]], "np": [["2054358802", "Learning Markov networks: maximum bounded tree-width graphs"], ["2127366419", "Dynamic Euclidean minimum spanning trees and extrema of binary functions"], ["1667614912", "A new method for solving hard satisfiability problems"], ["2295690548", "Imitation Learning of Agenda-based Semantic Parsers"], ["1589232796", "Programming with Constraints: An Introduction"]]}, {"context": "For certain random ensembles like Gaussian, the locality-sensitive functions are already well-known MAINCIT .", "bow": [["2100006909", "Locality of order-invariant first-order formulas"], ["2137315663", "Spatially-coupled MacKay-Neal codes and Hsu-Anastasopoulos codes"], ["1864365761", "Spatially Coupled LDPC Codes Constructed From Protographs"], ["1976430875", "Approaching capacity with asymptotically regular LDPC codes"], ["2163706521", "Capacity-Achieving Sequences"]], "pp": [["2100006909", "Locality of order-invariant first-order formulas"], ["2137315663", "Spatially-coupled MacKay-Neal codes and Hsu-Anastasopoulos codes"], ["1864365761", "Spatially Coupled LDPC Codes Constructed From Protographs"], ["1976430875", "Approaching capacity with asymptotically regular LDPC codes"], ["2163706521", "Capacity-Achieving Sequences"]]}, {"context": "Ranzato et al. MAINCIT used deep learning in a semi-supervised way to build document representations that retain term co-occurrence information (as opposed to only bag of words).", "bow": [["2114036236", "A New Efficient Simulation Equivalence Algorithm"], ["2162262658", "Semi-supervised learning of compact document representations with deep networks"], ["2135463121", "Similarity Learning for Provably Accurate Sparse Linear Classification"], ["2144211451", "A statistical interpretation of term specificity and its application in retrieval"], ["1985697096", "A statistical approach to mechanized encoding and searching of literary information"]], "pp": [["2114036236", "A New Efficient Simulation Equivalence Algorithm"], ["2162262658", "Semi-supervised learning of compact document representations with deep networks"], ["2135463121", "Similarity Learning for Provably Accurate Sparse Linear Classification"], ["2144211451", "A statistical interpretation of term specificity and its application in retrieval"], ["1985697096", "A statistical approach to mechanized encoding and searching of literary information"]]}, {"context": "Han et al. MAINCIT proposed two approaches using a Hybrid Naive Bayes and support vector machine(SVM) classifier.", "bow": [["179179905", "An empirical study of the naive Bayes classifier"], ["2032026767", "Idiot's Bayes-Not So Stupid After All?"], ["349770100", "The Optimality of Naive Bayes."], ["1924689489", "Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval"], ["19144255", "Pattern Classification and Scene"]], "pp": [["179179905", "An empirical study of the naive Bayes classifier"], ["2032026767", "Idiot's Bayes-Not So Stupid After All?"], ["349770100", "The Optimality of Naive Bayes."], ["1924689489", "Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval"], ["19144255", "Pattern Classification and Scene"]]}, {"context": "Only a handful of studies have relied solely on the content of a single tweet to infer its location MAINCIT .", "bow": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["2252215182", "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"], ["2167686542", "Mining user similarity based on location history"]], "pp": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["2252215182", "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"], ["2167686542", "Mining user similarity based on location history"]]}, {"context": "Again, most of these have actually worked on very restricted geographical areas, with tweets being limited to different regions, such as the United States MAINCIT , four different cities CIT , and New York only CIT .", "bow": [["2093588201", "Mapping the global Twitter heartbeat: The geography of Twitter"], ["2161291053", "Growth, innovation, scaling, and the pace of life in cities"], ["2294703018", "Emotional Tweets"], ["2123329834", "The Geography of Happiness: Connecting Twitter Sentiment and Expression, Demographics, and Objective Characteristics of Place"], ["2149510050", "Discovering geographical topics in the twitter stream"]], "pp": [["2061744336", "The Brain Activity Map Project and the Challenge of Functional Connectomics"], ["2065451462", "Approximation Theory and Methods"], ["116278252", "The Impact of Driver Inattention on Near-Crash/Crash Risk: An Analysis Using the 100-Car Naturalistic Driving Study Data"], ["2093588201", "Mapping the global Twitter heartbeat: The geography of Twitter"], ["1744091570", "Party Polarization in Congress: A Network Science Approach"]], "np": [["2061744336", "The Brain Activity Map Project and the Challenge of Functional Connectomics"], ["116278252", "The Impact of Driver Inattention on Near-Crash/Crash Risk: An Analysis Using the 100-Car Naturalistic Driving Study Data"], ["2065451462", "Approximation Theory and Methods"], ["1744091570", "Party Polarization in Congress: A Network Science Approach"], ["2117239687", "Detecting influenza epidemics using search engine query data"]]}, {"context": "In Salton's classic vector space model MAINCIT queries and documents are represented as sparse vectors in a vector space of dimensionality |V|, where V is the word vocabulary.", "bow": [["1971906578", "Modelling environments in call-by-value programming languages"], ["2164424155", "Blossom V: a new implementation of a minimum cost perfect matching algorithm"], ["2165612380", "A vector space model for automatic indexing"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2055452728", "Automatic text structuring and summarization"]], "pp": [["2165612380", "A vector space model for automatic indexing"], ["1971906578", "Modelling environments in call-by-value programming languages"], ["2148212498", "Search Engines: Information Retrieval in Practice"], ["1662133657", "From frequency to meaning: vector space models of semantics"], ["2164424155", "Blossom V: a new implementation of a minimum cost perfect matching algorithm"]], "np": [["2165612380", "A vector space model for automatic indexing"], ["1956559956", "Introduction to Modern Information Retrieval"], ["2148212498", "Search Engines: Information Retrieval in Practice"], ["1988711048", "Document length normalization"], ["1833785989", "Automatic text processing: the transformation, analysis, and retrieval of information by computer"]]}, {"context": "Digging more deeply into the demographics of Twitter users, other researchers have attempted to infer socioeconomic demographics such as occupational class MAINCIT , income CIT and socioeconomic status CIT .", "bow": [["2144364794", "Discovering Sociolinguistic Associations with Structured Sparsity"], ["1948823840", "Studying User Income through Language, Behaviour and Affect in Social Media."], ["2104925568", "Estimating county health statistics with twitter"], ["2166434810", "An analysis of the user occupational class through Twitter content"], ["2119595472", "Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach"]], "pp": [["2144364794", "Discovering Sociolinguistic Associations with Structured Sparsity"], ["1948823840", "Studying User Income through Language, Behaviour and Affect in Social Media."], ["2104925568", "Estimating county health statistics with twitter"], ["2166434810", "An analysis of the user occupational class through Twitter content"], ["2119595472", "Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach"]]}, {"context": "However, Twitter data lacks reliable demographic details that would enable a representative sample of users to be collected and/or a focus on a specific user subgroup MAINCIT , or other specific applications such as helping establish the trustworthiness of information posted CIT .", "bow": [["797227816", "Predicting the demographics of twitter users from website traffic data"], ["2101196063", "What is Twitter, a social network or a news media?"], ["2397005828", "Jane, John ... Leslie? A Historical Method for Algorithmic Gender Prediction."], ["2021278340", "Geotagging one hundred million Twitter accounts with total variation minimization"], ["1814023381", "Measuring user influence in Twitter : the million follower fallacy"]], "pp": [["797227816", "Predicting the demographics of twitter users from website traffic data"], ["2101196063", "What is Twitter, a social network or a news media?"], ["2397005828", "Jane, John ... Leslie? A Historical Method for Algorithmic Gender Prediction."], ["2021278340", "Geotagging one hundred million Twitter accounts with total variation minimization"], ["1814023381", "Measuring user influence in Twitter : the million follower fallacy"]]}, {"context": "Related Work A growing body of research deals with the automated inference of demographic details of Twitter users MAINCIT .", "bow": [["797227816", "Predicting the demographics of twitter users from website traffic data"], ["2397005828", "Jane, John ... Leslie? A Historical Method for Algorithmic Gender Prediction."], ["2153802318", "On the entropy of sums"], ["2101196063", "What is Twitter, a social network or a news media?"], ["2106275692", "Geographic Dissection of the Twitter Network"]], "pp": [["2167102709", "Understanding the Demographics of Twitter Users"], ["797227816", "Predicting the demographics of twitter users from website traffic data"], ["1993784310", "Dissecting a Social Botnet: Growth, Content and Influence in Twitter"], ["2013416264", "Democrats, republicans and starbucks afficionados: user classification in twitter"], ["2100974526", "Truthy: mapping the spread of astroturf in microblog streams"]], "np": [["1993784310", "Dissecting a Social Botnet: Growth, Content and Influence in Twitter"], ["2013416264", "Democrats, republicans and starbucks afficionados: user classification in twitter"], ["2100974526", "Truthy: mapping the spread of astroturf in microblog streams"], ["2167102709", "Understanding the Demographics of Twitter Users"], ["2000200507", "A few chirps about twitter"]]}, {"context": "For tag recommendation they outperform the other approaches like Folkrank and adapted Pagerank MAINCIT .", "bow": [["2118597426", "Tag Completion for Image Retrieval"], ["2014854862", "Visual-Textual Joint Relevance Learning for Tag-Based Social Image Search"], ["2161258050", "Learning Social Tag Relevance by Neighbor Voting"], ["1964810009", "Image tag refinement towards low-rank, content-tag prior and error sparsity"], ["2236279451", "Image Tag Completion by Noisy Matrix Recovery"]], "pp": [["2118597426", "Tag Completion for Image Retrieval"], ["2014854862", "Visual-Textual Joint Relevance Learning for Tag-Based Social Image Search"], ["2161258050", "Learning Social Tag Relevance by Neighbor Voting"], ["1964810009", "Image tag refinement towards low-rank, content-tag prior and error sparsity"], ["2236279451", "Image Tag Completion by Noisy Matrix Recovery"]]}, {"context": "Citation relations between scientific papers, and the citation distribution of papers was studied CIT MAINCIT CIT , and shows that some papers are not cited at all, most papers are cited once, while a little part of papers covers the references of most papers in a research area.", "bow": [["2019672642", "The first-mover advantage in scientific publication"], ["2060106983", "Stretched exponential distributions in nature and economy: \u201cfat tails\u201d with characteristic scales"], ["2064568176", "The simultaneous evolution of author and paper networks"], ["2168160236", "The skewness of science"], ["2151866568", "Universality of citation distributions: Toward an objective measure of scientific impact"]], "pp": [["2019672642", "The first-mover advantage in scientific publication"], ["2060106983", "Stretched exponential distributions in nature and economy: \u201cfat tails\u201d with characteristic scales"], ["2064568176", "The simultaneous evolution of author and paper networks"], ["2168160236", "The skewness of science"], ["2151866568", "Universality of citation distributions: Toward an objective measure of scientific impact"]]}, {"context": "As a complement of the approximation approach, we also extend an Upper Confidence Bound algorithm MAINCIT by integrating it with our belief update method.", "bow": [["2148642357", "The maximum numbers of faces of a convex polytope"], ["2091568354", "An Experimental Comparison of Bayesian Optimization for Bipedal Locomotion"], ["2104554891", "Exact Convex Confidence-Weighted Learning"], ["2085022454", "Knowledge in flux : modeling the dynamics of epistemic states"], ["2110518760", "Multi-domain learning by confidence-weighted parameter combination"]], "pp": [["2148642357", "The maximum numbers of faces of a convex polytope"], ["2091568354", "An Experimental Comparison of Bayesian Optimization for Bipedal Locomotion"], ["2104554891", "Exact Convex Confidence-Weighted Learning"], ["2085022454", "Knowledge in flux : modeling the dynamics of epistemic states"], ["2110518760", "Multi-domain learning by confidence-weighted parameter combination"]]}, {"context": "The UCB1-Normal-COR Algorithm The multi-armed bandit is a popular approach dealing with exploration-exploitation dilemma in sequential optimization problems MAINCIT .", "bow": [["1968049117", "Exploitation and exploration in a performance based contextual advertising system"], ["2168405694", "Finite-time Analysis of the Multiarmed Bandit Problem"], ["1973712811", "Bandit Problems With Infinitely Many Arms"], ["2058879737", "Bandit Problems-Sequential Allocation of Experiments."], ["2182000050", "On Bayesian Upper Confidence Bounds for Bandit Problems"]], "pp": [["1968049117", "Exploitation and exploration in a performance based contextual advertising system"], ["2168405694", "Finite-time Analysis of the Multiarmed Bandit Problem"], ["1973712811", "Bandit Problems With Infinitely Many Arms"], ["2058879737", "Bandit Problems-Sequential Allocation of Experiments."], ["2182000050", "On Bayesian Upper Confidence Bounds for Bandit Problems"]]}, {"context": "In this paper we base on the deterministic policy ucb1-normal MAINCIT and improve its performance by adding the correlated update.", "bow": [["2112583795", "Safe Policy Iteration"], ["1575592356", "Approximately Optimal Approximate Reinforcement Learning"], ["2130801532", "A Natural Policy Gradient"], ["2012587148", "A Survey on Policy Search for Robotics"], ["1499669280", "Relative entropy policy search"]], "pp": [["2112583795", "Safe Policy Iteration"], ["1575592356", "Approximately Optimal Approximate Reinforcement Learning"], ["2130801532", "A Natural Policy Gradient"], ["2012587148", "A Survey on Policy Search for Robotics"], ["1499669280", "Relative entropy policy search"]]}, {"context": "Baselines and Experiment Setup TABLE The baselines used in our experiment are: random policy, which selects candidates randomly (uniform); myopic policy, which selects candidates based on immediate reward estimated so far; ucb1 policy, which assumes independent between candidates and is model-free of reward distribution MAINCIT ; and ucb1-normal policy, which assumes independent between candidates and the reward following Gaussian distribution; And our algorithms are: vi-cor policy, which uses value iteration with Monte Carlo sampling (Algorithm REF ); and ucb1-normal-cor policy, which consider the dependencies between candidates (Algorithm REF ).", "bow": [["1575592356", "Approximately Optimal Approximate Reinforcement Learning"], ["2112583795", "Safe Policy Iteration"], ["2012587148", "A Survey on Policy Search for Robotics"], ["1983804795", "Hitting-time and occupation-time bounds implied by drift analysis with applications"], ["1499669280", "Relative entropy policy search"]], "pp": [["1575592356", "Approximately Optimal Approximate Reinforcement Learning"], ["2112583795", "Safe Policy Iteration"], ["2012587148", "A Survey on Policy Search for Robotics"], ["1983804795", "Hitting-time and occupation-time bounds implied by drift analysis with applications"], ["1499669280", "Relative entropy policy search"]], "np": [["2168405694", "Finite-time Analysis of the Multiarmed Bandit Problem"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "FIGURE Related Work Current approaches to protect privacy in recommendation systems mostly address two different privacy concerns: protecting users' privacy from curious peers or malicious users CIT , and against unreliable service providers MAINCIT .", "bow": [["2610955953", "Differential privacy"], ["1526797722", "Towards privacy for social networks: a zero-knowledge based definition of privacy"], ["2077217970", "A firm foundation for private data analysis"], ["2088517895", "Privacy against statistical inference"], ["2054922243", "No free lunch in data privacy"]], "pp": [["2610955953", "Differential privacy"], ["1526797722", "Towards privacy for social networks: a zero-knowledge based definition of privacy"], ["2077217970", "A firm foundation for private data analysis"], ["2088517895", "Privacy against statistical inference"], ["2054922243", "No free lunch in data privacy"]], "np": [["2040534455", "Anatomy of a large european IXP"], ["2021689699", "Energy and Performance Management of Green Data Centers: A Profit Maximization Approach"], ["2133019626", "The global footprint of mobile communications: The ecological and economic perspective"], ["2154126105", "Finding your Way in the Fog: Towards a Comprehensive Definition of Fog Computing"], ["2537667581", "SYSTRAN's Pure Neural Machine Translation Systems."]]}, {"context": " On the other side, in order to prevent a single party, e.g. the service provider, from gaining access to every user's data, cryptographic solutions are proposed in MAINCIT , however, cryptography could be computationally expensive, especially for end-users.", "bow": [["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2065642592", "Dynamic Service Placement in Geographically Distributed Clouds"], ["657518228", "Group-based cryptography"], ["2611028116", "Visual Cryptography"], ["2166347079", "New Methods in Automatic Extracting"]], "pp": [["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["2065642592", "Dynamic Service Placement in Geographically Distributed Clouds"], ["657518228", "Group-based cryptography"], ["2611028116", "Visual Cryptography"], ["2166347079", "New Methods in Automatic Extracting"]]}, {"context": "However, pseudo-identities still expose users to privacy risks unless the user data is further protected MAINCIT .", "bow": [["2610955953", "Differential privacy"], ["1526797722", "Towards privacy for social networks: a zero-knowledge based definition of privacy"], ["2054922243", "No free lunch in data privacy"], ["2118994807", "Analyzing facebook privacy settings: user expectations vs. reality"], ["2077217970", "A firm foundation for private data analysis"]], "pp": [["2610955953", "Differential privacy"], ["1526797722", "Towards privacy for social networks: a zero-knowledge based definition of privacy"], ["2054922243", "No free lunch in data privacy"], ["2118994807", "Analyzing facebook privacy settings: user expectations vs. reality"], ["2077217970", "A firm foundation for private data analysis"]]}, {"context": "Zhao et al. propose a dummy-based privacy mechanism for DNS lookups MAINCIT , but Hermann et al. find its security lacking CIT .", "bow": [["2169197659", "Analysis of Privacy Disclosure in DNS Query"], ["2401054255", "Detecting malware domains at the upper DNS hierarchy"], ["2091345559", "Complexity of generalized satisfiability counting problems"], ["2107276343", "Low latency via redundancy"], ["155384935", "Building a dynamic reputation system for DNS"]], "pp": [["2169197659", "Analysis of Privacy Disclosure in DNS Query"], ["2401054255", "Detecting malware domains at the upper DNS hierarchy"], ["2091345559", "Complexity of generalized satisfiability counting problems"], ["2107276343", "Low latency via redundancy"], ["155384935", "Building a dynamic reputation system for DNS"]]}, {"context": "Wang et al. MAINCIT \u201cdeep learned\u201d both single and multiple term topics, which they integrated into a query likelihood language model for retrieval.", "bow": [["1978129078", "Tag systems and lag systems"], ["2126967273", "Multiple queries for large scale specific object retrieval"], ["2093390569", "A language modeling approach to information retrieval"], ["2068297964", "Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings"], ["2045865594", "Context- and Content-aware Embeddings for Query Rewriting in Sponsored Search"]], "pp": [["1978129078", "Tag systems and lag systems"], ["2126967273", "Multiple queries for large scale specific object retrieval"], ["2093390569", "A language modeling approach to information retrieval"], ["2068297964", "Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings"], ["2045865594", "Context- and Content-aware Embeddings for Query Rewriting in Sponsored Search"]]}, {"context": "There may even be insufficient behaviour data to learn a click-based embedding CIT or a translation model MAINCIT .", "bow": [["2106630408", "Click chain model in web search"], ["2402441596", "Click Models for Web Search"], ["2154739689", "Efficient multiple-click models in web search"], ["1992549066", "An experimental comparison of click position-bias models"], ["2259177609", "DCM bandits: learning to rank with multiple clicks"]], "pp": [["2106630408", "Click chain model in web search"], ["2402441596", "Click Models for Web Search"], ["2102394389", "Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives"], ["2154739689", "Efficient multiple-click models in web search"], ["1992549066", "An experimental comparison of click position-bias models"]], "np": [["2270593706", "Integrating and Evaluating Neural Word Embeddings in Information Retrieval"], ["2035363410", "A syntactic tree matching approach to finding similar questions in community-based qa services"], ["2534253848", "Optimizing Statistical Machine Translation for Text Simplification"], ["2055981215", "Word Embedding based Generalized Language Model for Information Retrieval"], ["2102394389", "Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives"]]}, {"context": " As mentioned in Section REF , there is a parallel between this experiment setup and the telescoping CIT evaluation strategy, and has been used often in recent literature (e.g., MAINCIT ).", "bow": [["2094965692", "The method of creative telescoping"], ["1672862537", "Parameterized telescoping proves algebraic independence of sums"], ["2087893470", "A refined difference field theory for symbolic summation"], ["2145236745", "Telescopers for rational and algebraic functions via residues"], ["2166347079", "New Methods in Automatic Extracting"]], "pp": [["2094965692", "The method of creative telescoping"], ["1672862537", "Parameterized telescoping proves algebraic independence of sums"], ["2087893470", "A refined difference field theory for symbolic summation"], ["2145236745", "Telescopers for rational and algebraic functions via residues"], ["2166347079", "New Methods in Automatic Extracting"]]}, {"context": "Beyond word co-occurrence, recent studies have also explored learning text embeddings from clickthrough data MAINCIT , session data CIT , query prefix-suffix pairs CIT , via auto-encoders CIT , and for sentiment classification CIT and for long text CIT .", "bow": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]], "pp": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]], "np": [["2047221353", "Optimizing search engines using clickthrough data"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "Convolutional neural networks have also been shown to be highly effective for natural language processing and have achieved excellent results in information retrieval MAINCIT , semantic parsing CIT , sentence modeling CIT and other traditional natural language processing tasks CIT .", "bow": [["2120615054", "A Convolutional Neural Network for Modelling Sentences"], ["1423339008", "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"], ["179875071", "Recurrent neural network based language model"], ["1869752048", "Grammar as a foreign language"], ["2158899491", "Natural Language Processing (Almost) from Scratch"]], "pp": [["2042980227", "LDA-based document models for ad-hoc retrieval"], ["2168859760", "Statistical Language Models for Information Retrieval A Critical Review"], ["2099868020", "Concept-Based Information Retrieval Using Explicit Semantic Analysis"], ["2108168165", "Implicit user modeling for personalized search"], ["2401004062", "Finding optimal Bayesian network structures with constraints learned from data"]], "np": [["1990280147", "Dynamic personalized pagerank in entity-relation graphs"], ["2099868020", "Concept-Based Information Retrieval Using Explicit Semantic Analysis"], ["2108168165", "Implicit user modeling for personalized search"], ["2401004062", "Finding optimal Bayesian network structures with constraints learned from data"], ["2042980227", "LDA-based document models for ad-hoc retrieval"]]}, {"context": "In order to visualize the arrangement of SKUs on a larger scale, we resort to dimensionality reduction techniques, and find that t\u2013SNE (stochastic neighborhood embedding) is a suitable tool MAINCIT to reveal the structure hidden in the data.", "bow": [["2120934134", "Constructing arrangements of lines and hyperplanes with applications"], ["63678985", "Visualizing High-Dimensional Data Using t-SNE"], ["2187089797", "Visualizing Data using t-SNE"], ["2015696135", "An optimal algorithm for intersecting line segments in the plane"], ["212904614", "Multivariate Statistical Analysis."]], "pp": [["2120934134", "Constructing arrangements of lines and hyperplanes with applications"], ["63678985", "Visualizing High-Dimensional Data Using t-SNE"], ["2187089797", "Visualizing Data using t-SNE"], ["2015696135", "An optimal algorithm for intersecting line segments in the plane"], ["212904614", "Multivariate Statistical Analysis."]]}, {"context": "TABLE Model Architecture The model architecture shown in Figure REF , follows MAINCIT and CIT .", "bow": [["2162366870", "A classification and comparison framework for software architecture description languages"], ["2157418168", "Service-oriented computing: concepts, characteristics and directions"], ["2498153988", "The World Wide Web"], ["2556833785", "Designing Neural Network Architectures using Reinforcement Learning"], ["2293909219", "Algebraic statistics"]], "pp": [["2162366870", "A classification and comparison framework for software architecture description languages"], ["2157418168", "Service-oriented computing: concepts, characteristics and directions"], ["2498153988", "The World Wide Web"], ["2556833785", "Designing Neural Network Architectures using Reinforcement Learning"], ["2293909219", "Algebraic statistics"]]}, {"context": " H.3.3Information Search and RetrievalRetrieval Models Introduction Deep neural networks aim to mimick the multiple layers of neurons that operate in the human brain in order to learn how to solve a wide range of interesting problems, like identifying photos MAINCIT or responding to web search queries CIT .", "bow": [["1999653836", "Complex brain networks: graph theoretical analysis of structural and functional systems"], ["2212676342", "Steps Toward Deep Kernel Methods from Infinite Neural Networks"], ["2146693559", "The Human Connectome: A Structural Description of the Human Brain"], ["2305488978", "Probabilistic brains: knowns and"], ["2044770804", "Scale-free brain functional networks"]], "pp": [["1999653836", "Complex brain networks: graph theoretical analysis of structural and functional systems"], ["2212676342", "Steps Toward Deep Kernel Methods from Infinite Neural Networks"], ["2146693559", "The Human Connectome: A Structural Description of the Human Brain"], ["2305488978", "Probabilistic brains: knowns and"], ["2044770804", "Scale-free brain functional networks"]]}, {"context": "Recent papers have explored in detail the SG and CBOW training methodology CIT and its connection to other approaches for learning word embeddings such as explicit vector space representations MAINCIT , matrix factorization CIT and density-based representations CIT .", "bow": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "pp": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "np": [["2611511037", "Computable Analysis"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "Only a handful of studies have relied solely on the content of a single tweet to infer its location MAINCIT .", "bow": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["2252215182", "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"], ["2167686542", "Mining user similarity based on location history"]], "pp": [["2188871609", "A Multi-Indicator Approach for Geolocalization of Tweets"], ["1797988549", "TweetCred: Real-Time Credibility Assessment of Content on Twitter"], ["2371282818", "Tweet2Vec: Character-Based Distributed Representations for Social Media"], ["2252215182", "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"], ["2167686542", "Mining user similarity based on location history"]]}, {"context": "Bo et al. MAINCIT did focus on a broader geographical area, including 3.7k cities all over the world.", "bow": [["2161291053", "Growth, innovation, scaling, and the pace of life in cities"], ["2104371740", "Multipath Sparse Coding Using Hierarchical Matching Pursuit"], ["2017921654", "GeoMF: joint geographical modeling and matrix factorization for point-of-interest recommendation"], ["1573897183", "Unsupervised Feature Learning for RGB-D Based Object Recognition"], ["1978393907", "The Origins of Scaling in Cities"]], "pp": [["2161291053", "Growth, innovation, scaling, and the pace of life in cities"], ["2104371740", "Multipath Sparse Coding Using Hierarchical Matching Pursuit"], ["2017921654", "GeoMF: joint geographical modeling and matrix factorization for point-of-interest recommendation"], ["1573897183", "Unsupervised Feature Learning for RGB-D Based Object Recognition"], ["1978393907", "The Origins of Scaling in Cities"]]}, {"context": "Recent papers have explored in detail the SG and CBOW training methodology CIT and its connection to other approaches for learning word embeddings such as explicit vector space representations CIT , matrix factorization MAINCIT and density-based representations CIT .", "bow": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "pp": [["880548201", "Self-paced learning for matrix factorization"], ["2086325844", "MDL4BMF: Minimum Description Length for Boolean Matrix Factorization"], ["1985915435", "Alternating direction method of multipliers for non-negative matrix factorization with the beta-divergence"], ["2020098476", "Scalable Coordinate Descent Approaches to Parallel Matrix Factorization for Recommender Systems"], ["2047779269", "NMF-KNN: Image Annotation Using Weighted Multi-view Non-negative Matrix Factorization"]], "np": [["1481507124", "Diagonal and Low-Rank Matrix Decompositions, Correlation Matrices, and Ellipsoid Fitting"], ["2086325844", "MDL4BMF: Minimum Description Length for Boolean Matrix Factorization"], ["880548201", "Self-paced learning for matrix factorization"], ["1985915435", "Alternating direction method of multipliers for non-negative matrix factorization with the beta-divergence"], ["2040969041", "Iterative estimation of constrained rank-one matrices in noise"]]}, {"context": "In the case of major depressive disorder, recent efforts range from characterizing linguistic phenomena associated with depression MAINCIT and its subtypes e.g., postpartum depression CIT , to identifying specific depressive symptoms CIT e.", "bow": [["2066806488", "Trajectories of depression: unobtrusive monitoring of depressive states by means of smartphone mobility traces analysis"], ["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["2473822343", "Shifts to Suicidal Ideation from Mental Health Content in Social Media"], ["29935650", "Modeling Spread of Disease from Social Interactions"]], "pp": [["2066806488", "Trajectories of depression: unobtrusive monitoring of depressive states by means of smartphone mobility traces analysis"], ["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["2473822343", "Shifts to Suicidal Ideation from Mental Health Content in Social Media"], ["29935650", "Modeling Spread of Disease from Social Interactions"]]}, {"context": "Beyond word co-occurrence, recent studies have also explored learning text embeddings from clickthrough data CIT , session data CIT , query prefix-suffix pairs CIT , via auto-encoders CIT , and for sentiment classification MAINCIT and for long text CIT .", "bow": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]], "pp": [["2031998113", "Techniques and applications for sentiment analysis"], ["2293236424", "Cross-modality Consistent Regression for Joint Visual-Textual Sentiment Analysis of Social Multimedia"], ["2252215182", "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"], ["2250374929", "An Empirical Study on the Effect of Negation Words on Sentiment"], ["2250879510", "Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification"]], "np": [["2031998113", "Techniques and applications for sentiment analysis"], ["2250374929", "An Empirical Study on the Effect of Negation Words on Sentiment"], ["2252215182", "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"], ["2167660864", "Co-Training for Cross-Lingual Sentiment Classification"], ["2284289336", "A C-LSTM Neural Network for Text Classification"]]}, {"context": " 8.5in 11in pdfauthor = Anonymous, pdftitle = A Dual Embedding Space Model for Document Ranking, pdfsubject = A Dual Embedding Space Model for Document Ranking, pdfkeywords = H.3 [Information Storage and Retrieval], pdfcreator = LaTeX with hyperref package, pdfproducer = pdflatex to appearThis paper is an extended evaluation and analysis of the model proposed by CIT to appear in WWW'16, April 11 - 15, 2016, Montreal, Canada. Copyright 2016 by the author(s). A Dual Embedding Space Model for Document Ranking 4 Bhaskar Mitra Microsoft Cambridge, UK bmitra@microsoft.com Eric Nalisnick University of California Irvine, USA enalisni@uci.edu Nick Craswell, Rich Caruana Microsoft Redmond, USA nickcr, rcaruana@microsoft.com A fundamental goal of search engines is to identify, given a query, documents that have relevant text. This is intrinsically difficult because the query and the document may use different vocabulary, or the document may contain query words without being relevant. We investigate neural word embeddings as a source of evidence in document ranking. We train a word2vec embedding model on a large unlabelled query corpus, but in contrast to how the model is commonly used, we retain both the input and the output projections, allowing us to leverage both the embedding spaces to derive richer distributional relationships. During ranking we map the query words into the input space and the document words into the output space, and compute a query-document relevance score by aggregating the cosine similarities across all the query-document word pairs. We postulate that the proposed Dual Embedding Space Model (DESM) captures evidence on whether a document is about a query term in addition to what is modelled by traditional term-frequency based approaches. Our experiments show that the DESM can re-rank top documents returned by a commercial Web search engine, like Bing, better than a term-matching based signal like TF-IDF. However, when ranking a larger set of candidate documents, we find the embeddings-based approach is prone to false positives, retrieving documents that are only loosely related to the query. We demonstrate that this problem can be solved effectively by ranking based on a linear mixture of the DESM and the word counting features. H.3Information Storage and RetrievalH.3.3 Information Search and Retrieval Keywords: Document ranking; Word embeddings; Word2vec Introduction FIGURE TABLE Identifying relevant documents for a given query is a core challenge for Web search. For large-scale search engines, it is possible to identify a very small set of pages that can answer a good proportion of queries CIT . For such popular pages, clicks and hyperlinks may provide sufficient ranking evidence and it may not be important to match the query against the body text. However, in many Web search scenarios such query-content matching is crucial. If new content is available, the new and updated documents may not have click evidence or may have evidence that is out of date. For new or tail queries, there may be no memorized connections between the queries and the documents. Furthermore, many search engines and apps have a relatively smaller number of users, which limits their ability to answer queries based on memorized clicks. There may even be insufficient behaviour data to learn a click-based embedding CIT or a translation model CIT . In these cases it is crucial to model the relationship between the query and the document content, without click data. When considering the relevance of document body text to a query, the traditional approach is to count repetitions of query terms in the document. Different transformation and weighting schemes for those counts lead to a variety of possible TF-IDF ranking features. One theoretical basis for such features is the probabilistic model of information retrieval, which has yielded the very successful TF-IDF formulation BM25 CIT . As noted by CIT , the probabilistic approach can be restricted to consider only the original query terms or it can automatically identify additional terms that are correlated with relevance. However, the basic commonly-used form of BM25 considers query terms only, under the assumption that non-query terms are less useful for document ranking. In the probabilistic approach, the 2-Poisson model forms the basis for counting term frequency CIT . The stated goal is to distinguish between a document that is about a term and a document that merely mentions that term. These two types of documents have term frequencies from two different Poisson distributions, such that documents about the term tend to have higher term frequency than those that merely mention it. This explanation for the relationship between term frequency and aboutness is the basis for the TF function in BM25 CIT . The new approach in this paper uses word occurrences as evidence of aboutness, as in the probabilistic approach. However, instead of considering term repetition as evidence of aboutness it considers the relationship between the query terms and all the terms in the document. For example, given a query term \u201cyale\u201d, in addition to considering the number of times Yale is mentioned in the document, we look at whether related terms occur in the document, such as \u201cfaculty\u201d and \u201calumni\u201d. Similarly, in a document about the Seahawks sports team one may expect to see the terms \u201chighlights\u201d and \u201cjerseys\u201d. The occurrence of these related terms in sufficient numbers is a way to distinguish between documents that merely mention Yale or Seahawks and the documents that are about the university or about the sports team. With this motivation, in Section we describe how the input and the output embedding spaces learned by a word2vec model may be jointly particularly attractive for modelling the aboutness aspect of document ranking. Table REF gives some anecdotal evidence of why this is true. If we look in the neighbourhood of the IN vector of the word \u201cyale\u201d then the other IN vectors that are close correspond to words that are functionally similar or of the same type, e.g., \u201charvard\u201d and \u201cnyu\u201d. A similar pattern emerges if we look at the OUT vectors in the neighbourhood of the OUT vector of \u201cyale\u201d. On the other hand, if we look at the OUT vectors that are closest to the IN vector of \u201cyale\u201d we find words like \u201cfaculty\u201d and \u201calumni\u201d. We use this property of the IN-OUT embeddings to propose a novel Dual Embedding Space Model (DESM) for document ranking. Figure REF further illustrates how in this Dual Embedding Space model, using the IN embeddings for the query words and the OUT embeddings for the document words we get a much more useful similarity definition between the query and the relevant document centroids. The main contributions of this paper are, A novel Dual Embedding Space Model, with one embedding for query words and a separate embedding for document words, learned jointly based on an unlabelled text corpus. We propose a document ranking feature based on comparing all the query words with all the document words, which is equivalent to comparing each query word to a centroid of the document word embeddings. We analyse the positive aspects of the new feature, preferring documents that contain many words related to the query words, but also note the potential of the feature to have false positive matches. We empirically compare the new approach to a single embedding and the traditional word counting features. The new approach works well on its own in a telescoping setting, re-ranking the top documents returned by a commercial Web search engine, and in combination with word counting for a more general document retrieval task. Distributional Semantics for IR In this section we first introduce the Continuous Bag-of-Words (CBOW) model made popular by the software Word2Vec CIT . Then, inspired by our findings that distinctly different topic-based relationships can be found by using both the input and the output embeddings jointly \u2013 the latter of which is usually discarded after training \u2013 we propose the Dual Embedding Space Model (DESM) for document ranking. Continuous Bag-of-Words While many word embedding models have been proposed recently, the Continuous Bag-of-Words (CBOW) and the Skip-Gram (SG) architectures proposed by CIT are arguably the most popular (perhaps due to the popularity of the software Word2Vechttps://code.google.com/p/word2vec/, which implements both). Although here we will concentrate exclusively on the CBOW model, our proposed IR ranking methodology is just as applicable to vectors produced by SG, as both models produce qualitatively and quantitatively similar embeddings. The CBOW model learns a word's embedding via maximizing the log conditional probability of the word given the context words occurring within a fixed-sized window around that word. That is, the words in the context window serve as input, and from them, the model attempts to predict the center (missing) word. For a formal definition, let FORMULA be a FORMULA -dimensional, real-valued vector representing the FORMULA th context word FORMULA appearing in a FORMULA -sized window around an instance of word FORMULA , which is represented by a vector FORMULA . The model `predicts' word FORMULA by adapting its representation vector such that it has a large inner-product with the mean of the context word vectors. Training CBOW requires minimization of the following objective FORMULA where FORMULA and FORMULA represents the training corpus. Notice that the probability is normalized by summing over all the vocabulary, which is quite costly when training on web-scale data. To make CBOW scalable, CIT proposed the following slightly altered negative sampling objective: FORMULA where FORMULA is the Sigmoid function and FORMULA is the number of negative sample words drawn either from the uniform or empirical distribution over the vocabulary. All our experiments were performed with the negative sampling objective. FIGURE A crucial detail often overlooked when using Word2Vec is that there are two different sets of vectors (represented above by FORMULA and FORMULA respectively and henceforth referred to as the IN and OUT embedding spaces), which correspond to the FORMULA and FORMULA weight matrices in Figure REF . By default, Word2Vec discards FORMULA at the end of training and outputs only FORMULA . Subsequent tasks determine word-to-word semantic relatedness by computing the cosine similarity: FORMULA Dual Embedding Space Model FIGURE TABLE A key challenge for term-matching based retrieval is to distinguish whether a document merely references a term or is about that entity. See Figure REF for a concrete example of two passages that contain the term \"Albuquerque\" an equal number of times although only one of the passages is about that entity. The presence of the words like \"population\" and \"metropolitan\" indicate that the left passage is about Albuquerque, whereas the passage on the right just mentions it. However, these passages would be indistinguishable under term counting. The semantic similarity of non-matched terms (i.e. the words a TF feature would overlook) are crucial for inferring a document's topic of focus\u2013its aboutness. Due to its ability to capture word co-occurrence (i.e. perform missing word prediction), CBOW is a natural fit for modelling the aboutness of a document. The learnt embedding spaces contain useful knowledge about the distributional properties of words, allowing, in the case of Figure REF , an IR system to recognize the city-related terms in the left document. With this motivation, we define a simple yet, as we will demonstrate, effective ranking function we call the Dual Embedding Space Model: FORMULA where FORMULA Here FORMULA is the centroid of all the normalized vectors for the words in the document serving as a single embedding for the whole document. In this formulation of the DESM, the document embeddings can be pre-computed, and at the time of ranking, we only need to sum the score contributions across the query terms. We expect that the ability to pre-compute a single document embedding is a very useful property when considering runtime efficiency. IN-IN vs. IN-OUT CIT noted, \"Not all neural embeddings are born equal\". As previously mentioned, the CBOW (and SG) model contains two separate embedding spaces (IN and OUT) whose interactions capture additional distributional semantics of words that are not observable by considering any of the two embeddings spaces in isolation. Table REF illustrates clearly how the CBOW model \"pushes\" the IN vectors of words closer to the OUT vectors of other words that they commonly co-occur with. In doing so, words that appear in similar contexts get pushed closer to each other within the IN embedding space (and also within the OUT embedding space). Therefore the IN-IN (or the OUT-OUT) cosine similarities are higher for words that are typically (by type or by function) similar, whereas the IN-OUT cosine similarities are higher for words that co-occur often in the training corpus (topically similar). This gives us at least two variants of the DESM, corresponding to retrieval in the IN-OUT space or the IN-IN spaceIt is also possible to define FORMULA and FORMULA , but based on limited experimentation we expect them to behave similar to FORMULA and FORMULA , respectively.. FORMULA FORMULA In Section , we show that the FORMULA is a better indication of aboutness than BM25, because of its knowledge of the word distributional properties, and FORMULA , since topical similarity is a better indicator of aboutness than typical similarity. Modelling document aboutness We perform a simple word perturbation analysis to illustrate how the DESM can collect evidence on document aboutness from both matched and non-matched terms in the document. In Table REF , we consider five small passages of text. The first three passages are about Cambridge, Oxford and giraffes respectively. The next two passages are generated by replacing the word \"giraffe\" by the word \"Cambridge\" in the passage about giraffes, and vice versa. We compute the FORMULA and the FORMULA scores along with the term frequencies for each of these passages for the query term \"cambridge\". As expected, all three models score the passage about Cambridge highly. However, unlike the term frequency feature, the DESM seem robust towards keyword stuffinghttps://en.wikipedia.org/wiki/Keyword_stuffing, at least in this specific example where we replace the word \"giraffe\" with \"cambridge\" in the passage about giraffes, but the DESMs still score the passage relatively low. This is exactly the kind of evidence that we expect the DESM to capture that may not be possible by simple term counting. On the other hand, both the DESMs score the passage about Oxford very highly. This is expected because both these passages contain many words that are likely to co-occur with the word \"cambridge\" in the training corpus. This implies that the DESM features are very susceptible to false positive matches and can only be used either in conjunction with other document ranking features, such as TF-IDF, or for re-ranking a smaller set of candidate documents already deemed at least somewhat relevant. This is similar to the telescoping evaluation setup described by CIT , where multiple nested rankers are used to achieve better retrieval performance over a single ranker. At each stage of telescoping, a ranker is used to reduce the set of candidate documents that is passed on to the next. Improved performance is possible because the ranker that sees only top-scoring documents can specialize in handling such documents, for example by using different feature weights. In our experiments, we will see the DESM to be a poor standalone ranking signal on a larger set of documents, but performs significantly better against the BM25 and the LSA baselines once we reach a small high-quality candidate document set. This evaluation strategy of focusing at ranking for top positions is in fact quite common and has been used by many recent studies (e.g., CIT ). Dot product vs. cosine similarity In the DESM formulation (Equation REF ) we compute the cosine similaritiy between every query word and the normalized document centroid. The use of cosine similarity (as opposed to, say, dot-product) is motivated by several factors. Firstly, much of the existing literature CIT on CBOW and SG uses cosine similarity and normalized unit vectors (for performing vector algebra for word analogies). As the cosine similarity has been shown to perform well in practice in these embedding spaces we adopt the same strategy here. A secondary justification can be drawn based on the observations made by CIT that the length of the non-normalized word vectors has a direct relation to the frequency of the word. In information retrieval (IR), it is well known that frequently occurring words are ineffective features for distinguishing relevant documents from irrelevant ones. The inverse-document frequency weighting is often used in IR to capture this effect. By normalizing the word vectors in the document before computing the document centroids, we are counteracting the extra influence frequent words would have on the sum. Training corpus Our CBOW model is trained on a query corpusWe provide the IN and OUT word embeddings trained using word2vec on the Bing query corpus at http://research.microsoft.com/projects/DESM. consisting of 618,644,170 queries and a vocabulary size of 2,748,230 words. The queries are sampled from Bing's large scale search logs from the period of August 19, 2014 to August 25, 2014. We repeat all our experiments using another CBOW model trained on a corpus of document body text with 341,787,174 distinct sentences sampled from the Bing search index and a corresponding vocabulary size of 5,108,278 words. Empirical results on the performance of both the models are presented in Section . Out-of-vocabulary (OOV) words One of the challenges of the embedding models is that they can only be applied to a fixed size vocabulary. It is possible to explore different strategies to deal with out-of-vocab (OOV) words in the Equation REF In machine translation there are examples of interesting strategies to handle out-of-vocabulary words (e.g., CIT ). But we leave this for future investigation and instead, in this paper, all the OOV words are ignored for computing the DESM score, but not for computing the TF-IDF feature, a potential advantage for the latter. Document length normalization In Equation REF we normalize the scores linearly by both the query and the document lengths. While more sophisticated length normalization strategies, such as pivoted document length normalization CIT , are reasonable, we leave this also for future work. The Mixture Model The DESM is a weak ranker and while it models some important aspects of document ranking, our experiments will show that it's effective only at ranking at high positions (i.e. documents we already know are at least somewhat relevant). We are inspired by previous work in neural language models, for example by CIT , which demonstrates that combining a neural model for predicting the next word with a more traditional counting-based language model is effective because the two models make different kinds of mistakes. Adopting a similar strategy we propose a simple and intuitive mixture model combining DESM with a term based feature, such as BM25, for the non-telescoping evaluation setup described in Section REF . We define the mixture model MM(Q, D) as, FORMULA To choose the appropriate value for FORMULA , we perform a parameter sweep between zero and one at intervals of 0.01 on the implicit feedback based training set described in Section REF . Experiments TABLE We compare the retrieval performance of DESM against BM25, a traditional count-based method, and Latent Semantic Analysis (LSA), a traditional vector-based method. We conduct our evaluations on two different test sets (explicit and implicit relevance judgements) and under two different experimental conditions (a large collection of documents and a telescoped subset). Datasets All the datasets that are used for this study are sampled from Bing's large scale query logs. The body text for all the candidate documents are extracted from Bing's document index. Explicitly judged test set This evaluation set consists of 7,741 queries randomly sampled from Bing's query logs from the period of October, 2014 to December, 2014. For each sampled query, a set of candidate documents is constructed by retrieving the top results from Bing over multiple scrapes during a period of a few months. In total the final evaluation set contains 171,302 unique documents across all queries which are then judged by human evaluators on a five point relevance scale (Perfect, Excellent, Good, Fair and Bad). Implicit feedback based test set This dataset is sampled from the Bing logs from the period of the September 22, 2014 to September 28, 2014. The dataset consists of the search queries submitted by the user and the corresponding documents that were returned by the search engine in response. The documents are associated with a binary relevance judgment based on whether the document was clicked by the user. This test set contains 7,477 queries and the 42,573 distinct documents. Implicit feedback based training set This dataset is sampled exactly the same way as the previous test but from the period of September 15, 2014 to September 21, 2014 and has 7,429 queries and 42,253 distinct documents. This set is used for tuning the parameters for the BM25 baseline and the mixture model. Experiment Setup We perform two distinct sets of evaluations for all the experimental and baseline models. In the first experiment, we consider all documents retrieved by Bing (from the online scrapes in the case of the explicitly judged set or as recorded in the search logs in the case of the implicit feedback based sets) as the candidate set of documents to be re-ranked for each query. The fact that each of the documents were retrieved by the search engine implies that they are all at least marginally relevant to the query. Therefore, this experimental design isolates performance at the top ranks. As mentioned in Section REF , there is a parallel between this experiment setup and the telescoping CIT evaluation strategy, and has been used often in recent literature (e.g., CIT ). Note that by having a strong retrieval model, in the form of the Bing search engine, for first stage retrieval enables us to have a high confidence candidate set and in turn ensures reliable comparison with the baseline BM25 feature. In our non-telescoped experiment, we consider every distinct document in the test set as a candidate for every query in the same dataset. This setup is more in line with the traditional IR evaluation methodologies, where the model needs to retrieve the most relevant documents from a single large document collection. Our empirical results in Section will show that the DESM model is a strong re-ranking signal, but as a standalone ranker, it is prone to false positives. Yet, when we mix our neural model (DESM) with a counting based model (BM25), good performance is achieved. For all the experiments we report the normalized discounted cumulative gain (NDCG) at different rank positions as a measure of performance for the different models under study. Baseline models We compare the DESM models to a term-matching based baseline, in BM25, and a vector space model baseline, in Latent Semantic Analysis (LSA) CIT . For the BM25 baseline we use the values of 1.7 for the FORMULA parameter and 0.95 for the FORMULA parameter based on a parameter sweep on the implicit feedback based training set. The LSA model is trained on the body text of 366,470 randomly sampled documents from Bing's index with a vocabulary size of 480,608 words. Note that unlike the word2vec models that train on word co-occurrence data, the LSA model by default trains on a word-document matrix. Results TABLE FIGURE FIGURE Table REF shows the NCDG based performance evaluations under the telescoping setup. On both the explicitly judged and the implicit feedback based test sets the FORMULA performs significantly better than the BM25 and the LSA baselines, as well as the FORMULA model. Under the all documents as candidates setup in Table REF , however, the DESMs (both IN-IN and IN-OUT) are clearly seen to not perform well as standalone document rankers. The mixture of FORMULA (trained on queries) and BM25 rectifies this problem and gives the best NDCG result under the non-telescoping settings and demonstrates a statistically significant improvement over the BM25 baseline. Figure REF illustrates that the FORMULA is the most discriminating feature for the relevant and the irrelevant documents retrieved by a first stage retrieval system. However, BM25 is clearly superior in separating out the random irrelevant documents in the candidate set. The mixture model, unsurprisingly, has the good properties from both the FORMULA and the BM25 models. Figure REF shows the joint distribution of the scores from the different models which further reinforces these points and shows that the DESM and the BM25 models make different errors. We do not report the results of evaluating the mixture models under the telescoping setup because tuning the FORMULA parameter under those settings on the training set results in the best performance from the standalone DESM models. Overall, we conclude that the DESM is primarily suited for ranking at top positions or in conjunction with other document ranking features. Interestingly, under the telescoping settings, the LSA baseline also shows some (albeit small) improvement over the BM25 baseline on the implicit feedback based test set but a loss on the explicitly judged test set. With respect to the CBOW's training data, the DESM models with the embeddings trained on the query corpus performs significantly better than the models trained on document body text across different configurations. We have a plausible hypothesis on why this happens. Users tend to choose the most significant terms that they expect to match in the target document to formulate their search queries. Therefore in the query corpus, one may say that, the less important terms from the document corpus has been filtered out. Therefore when training on the query corpus the CBOW model is more likely to see important terms within the context window compared to when trained on a corpus of document body text, which may make it a better training dataset for the Word2vec model. Related Work Term based IR For an overview of lexical matching approaches for information retrieval, such as the vector space, probabilistic and language modelling approach, see CIT . In Salton's classic vector space model CIT queries and documents are represented as sparse vectors in a vector space of dimensionality |V|, where V is the word vocabulary. Elements in the vector are non-zero if that term occurs. Documents can be ranked in descending order of cosine similarity with the query, although a wide variety of weighting and similarity functions are possible CIT . In contrast to the classical vector space model, LSA CIT , PLSA CIT and LDA CIT learn dense vector representations of much lower dimensionality. It has been suggested that these models perform poorly as standalone retrieval models CIT unless combined with other TF-IDF like features. In our approach the query and documents are also low dimensional dense vectors. We learn 200-dimensional neural word embeddings, and generate document vectors as the centroids of all the word vectors. CIT suggested that term correlation data is less sparse than term-document matrix and hence may be more effective for training embeddings. The probabilistic model of information retrieval leads to the development of the BM25 ranking feature CIT . The increase in BM25 as term frequency increases is justified according to the 2-Poisson model CIT , which makes a distinction between documents about a term and documents that merely mention that term. Those two types of document have term frequencies from two different Poisson distributions, which justifies the use of term frequency as evidence of aboutness. By contrast, the model introduced in this paper uses the occurrence of other related terms as evidence of aboutness. For example, under the 2-Poisson model a document about Eminem will tend to mention the term `eminem' repeatedly. Under our all-pairs vector model, a document about Eminem will tend to contain more related terms such as `rap', `tracklist' and `performs'. Our experiments show both notions of aboutness to be useful. Neural embeddings for IR The word embeddings produced by the CBOW and SG models have been shown to be surprisingly effective at capturing detailed semantics useful for various Natural Language Processing (NLP) and reasoning tasks, including word analogies CIT . Recent papers have explored in detail the SG and CBOW training methodology CIT and its connection to other approaches for learning word embeddings such as explicit vector space representations CIT , matrix factorization CIT and density-based representations CIT . MAINCIT evaluated neural word embeddings against traditional word counting approaches and demonstrated the success of the former on a variety of NLP tasks.", "bow": [["2405884322", "Query Expansion with Locally-Trained Word Embeddings"], ["2250460709", "Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content"], ["2143196462", "Exploring Session Context using Distributed Representations of Queries and Reformulations"], ["2515351093", "Embedding-based Query Language Models"], ["2466445867", "Using Word Embeddings for Automatic Query Expansion."]], "pp": [["2405884322", "Query Expansion with Locally-Trained Word Embeddings"], ["2250460709", "Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content"], ["2143196462", "Exploring Session Context using Distributed Representations of Queries and Reformulations"], ["2515351093", "Embedding-based Query Language Models"], ["2466445867", "Using Word Embeddings for Automatic Query Expansion."]]}, {"context": "More recently, social media has been leveraged to monitor social risks such as prescription drug and smoking behaviors CIT as well as a variety of mental health disorders including suicidal ideation CIT , attention deficient hyperactivity disorder MAINCIT and major depressive disorder CIT .", "bow": [["2473822343", "Shifts to Suicidal Ideation from Mental Health Content in Social Media"], ["2250553926", "Quantifying Mental Health Signals in Twitter"], ["2003834798", "Tracking suicide risk factors through Twitter in the US."], ["2402700", "Predicting Depression via Social Media"], ["2252191003", "From ADHD to SAD: Analyzing the Language of Mental Health on Twitter through Self-Reported Diagnoses"]], "pp": [["2473822343", "Shifts to Suicidal Ideation from Mental Health Content in Social Media"], ["2250553926", "Quantifying Mental Health Signals in Twitter"], ["2003834798", "Tracking suicide risk factors through Twitter in the US."], ["2402700", "Predicting Depression via Social Media"], ["2252191003", "From ADHD to SAD: Analyzing the Language of Mental Health on Twitter through Self-Reported Diagnoses"]]}, {"context": "However, more recent works MAINCIT have shown that there does not seem to be one embedding approach that is best for all tasks.", "bow": [["2250376704", "Semantically Smooth Knowledge Graph Embedding"], ["2743104969", "metapath2vec: Scalable Representation Learning for Heterogeneous Networks"], ["2604942799", "Community Preserving Network Embedding"], ["2540057053", "From Node Embedding To Community Embedding."], ["2136040699", "Graph Embedding and Extensions: A General Framework for Dimensionality Reduction"]], "pp": [["46000818", "Search for sparse active inputs: a review"], ["2528305538", "Error bounds for approximations with deep ReLU networks"], ["2080645379", "Fast and exact geometric analysis of real algebraic plane curves"], ["2608858501", "Pixelwise Instance Segmentation with a Dynamically Instantiated Network"], ["2098902711", "Boolean compressed sensing: LP relaxation for group testing"]], "np": [["46000818", "Search for sparse active inputs: a review"], ["2080645379", "Fast and exact geometric analysis of real algebraic plane curves"], ["2098902711", "Boolean compressed sensing: LP relaxation for group testing"], ["2528305538", "Error bounds for approximations with deep ReLU networks"], ["2608858501", "Pixelwise Instance Segmentation with a Dynamically Instantiated Network"]]}, {"context": "Interesting is also the study of Mitra et al. MAINCIT , who used deep learning for reranking (with success) and ranking (with less success).", "bow": [["2060206980", "Partial and approximate symmetry detection for 3D geometry"], ["2121127625", "Discriminative Reranking for Natural Language Parsing"], ["1995137594", "Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search"], ["2125712079", "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking"], ["2000431947", "A cascade ranking model for efficient ranked retrieval"]], "pp": [["2060206980", "Partial and approximate symmetry detection for 3D geometry"], ["2121127625", "Discriminative Reranking for Natural Language Parsing"], ["1995137594", "Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search"], ["2125712079", "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking"], ["2000431947", "A cascade ranking model for efficient ranked retrieval"]]}, {"context": "We found it much more effective for reranking top-retrieved documents, than for ranking all documents in the index, similarly to MAINCIT .", "bow": [["2121127625", "Discriminative Reranking for Natural Language Parsing"], ["2007842132", "Copy detection mechanisms for digital documents"], ["105935508", "Link-PLSA-LDA: A New Unsupervised Model for Topics and Influence of Blogs."], ["2149427297", "Learning to Rank for Information Retrieval"], ["2148212498", "Search Engines: Information Retrieval in Practice"]], "pp": [["2121127625", "Discriminative Reranking for Natural Language Parsing"], ["2007842132", "Copy detection mechanisms for digital documents"], ["105935508", "Link-PLSA-LDA: A New Unsupervised Model for Topics and Influence of Blogs."], ["2149427297", "Learning to Rank for Information Retrieval"], ["2148212498", "Search Engines: Information Retrieval in Practice"]]}, {"context": "In MAINCIT , a RNN-based method is proposed to model the sequential session behaviors for more accurate recommendation.", "bow": [["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2128280268", "Session types revisited"], ["2040105559", "Multiparty asynchronous session types"], ["2080718905", "Fundamentals of session types"]], "pp": [["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2128280268", "Session types revisited"], ["2040105559", "Multiparty asynchronous session types"], ["2080718905", "Fundamentals of session types"]]}, {"context": "The seven examined methods are listed as below: FORMULA Popularity: the simple personalized re-ranking baseline provided by the official site; FORMULA KNN: the typical item-based collaborative filtering recommendation algorithm CIT ; FORMULA LFM: state-of-the-art Latent Factor Model CIT , which is mainly designed to address the sparsity problem; FORMULA BPR: the typical bayesian personalized ranking method with the implicit feedback CIT ; FORMULA S-RNN: a typical session-based recommendation method with recurrent neural networks MAINCIT ; FORMULA SIE: the proposed solution that directly use the class probabilities from session information embedding part for ranking; FORMULA ListRank: the proposed solution considers both the session information embedding and deep ListNet ranking.", "bow": [["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["2140310134", "BPR: Bayesian personalized ranking from implicit feedback"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2128280268", "Session types revisited"], ["2040105559", "Multiparty asynchronous session types"]], "pp": [["2474765392", "Multi-Rate Deep Learning for Temporal Recommendation"], ["2251592786", "Normalizing tweets with edit scripts and recurrent neural embeddings"], ["2337199865", "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex"], ["2292919134", "Representation of linguistic form and function in recurrent neural networks"], ["43858554", "Finding Structure in time-Cognitive Science"]], "np": [["2251592786", "Normalizing tweets with edit scripts and recurrent neural embeddings"], ["2337199865", "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex"], ["1823409095", "Temporal classification: extending the classification paradigm to multivariate time series"], ["2292919134", "Representation of linguistic form and function in recurrent neural networks"], ["2474765392", "Multi-Rate Deep Learning for Temporal Recommendation"]]}, {"context": "Having Twitter as a new kind of data source, researchers have looked into the development of tools for real-time trend analytics CIT or early detection of newsworthy events CIT , as well as into analytical approaches for understanding the sentiment expressed by users towards a target MAINCIT , or public opinion on a specific topic CIT .", "bow": [["2122369144", "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series"], ["1956559956", "Introduction to Modern Information Retrieval"], ["1517771839", "Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields"], ["2108646579", "Sentiment Analysis and Opinion Mining"], ["2128721751", "Twitinfo: aggregating and visualizing microblogs for event exploration"]], "pp": [["2122369144", "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series"], ["1956559956", "Introduction to Modern Information Retrieval"], ["1517771839", "Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields"], ["2108646579", "Sentiment Analysis and Opinion Mining"], ["2128721751", "Twitinfo: aggregating and visualizing microblogs for event exploration"]]}, {"context": "Researchers have attempted to infer attributes of Twitter users such as age CIT , gender MAINCIT , political orientation CIT or a range of social identities CIT .", "bow": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]], "pp": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]]}, {"context": "nearest neighbor search, quantization, source coding, high dimensional indexing, large databases Introduction Approximate nearest neighbors (ANN) search methods MAINCIT are required to handle large databases, especially for computer vision CIT and music retrieval CIT applications.", "bow": [["2427881153", "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"], ["2131929066", "On the Difficulty of Nearest Neighbor Search"], ["2167931879", "Anti-sparse coding for approximate nearest neighbor search"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"]], "pp": [["2427881153", "An optimal algorithm for approximate nearest neighbor searching fixed dimensions"], ["2131929066", "On the Difficulty of Nearest Neighbor Search"], ["2167931879", "Anti-sparse coding for approximate nearest neighbor search"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["2086504823", "Scalable Nearest Neighbor Algorithms for High Dimensional Data"]], "np": [["2008995227", "Approximate Nearest Subspace Search"], ["2171790913", "Kernelized locality-sensitive hashing for scalable image search"], ["1502916507", "Similarity Search in High Dimensions via Hashing"], ["2293597654", "Spectral Hashing"], ["2124509324", "Product Quantization for Nearest Neighbor Search"]]}, {"context": "The methods of MAINCIT , which embeds the vector into a binary space, better satisfies the memory constraint.", "bow": [["2014279255", "Approximation of zonoids by zonotopes"], ["1530572757", "Embeddings and extensions in analysis"], ["2044902313", "Memory coherence in shared virtual memory systems"], ["1905029509", "Metric embeddings with relaxed guarantees"], ["2342395274", "Recurrent Memory Networks for Language Modeling"]], "pp": [["2014279255", "Approximation of zonoids by zonotopes"], ["1530572757", "Embeddings and extensions in analysis"], ["2044902313", "Memory coherence in shared virtual memory systems"], ["1905029509", "Metric embeddings with relaxed guarantees"], ["2342395274", "Recurrent Memory Networks for Language Modeling"]]}, {"context": "But as shown in CIT , this post verification is also important for methods based on binary MAINCIT or quantized codes CIT , as the ranking provided on output of the large scale search is significantly improved when verifying the few first hypotheses.", "bow": [["2341106557", "Quantized consensus"], ["2130025446", "In transition from global to modular temporal reasoning about programs"], ["2148575324", "Polynomial Codes Over Certain Finite Fields"], ["2166966427", "Perfect binary codes: constructions, properties, and enumeration"], ["1771040533", "Three-weight cyclic codes and their weight distributions"]], "pp": [["2341106557", "Quantized consensus"], ["2130025446", "In transition from global to modular temporal reasoning about programs"], ["2148575324", "Polynomial Codes Over Certain Finite Fields"], ["2166966427", "Perfect binary codes: constructions, properties, and enumeration"], ["1771040533", "Three-weight cyclic codes and their weight distributions"]]}, {"context": "An approximate distance FORMULA between a query FORMULA and a database vector is computed as FORMULA The approximate nearest neighbor NNFORMULA of FORMULA is obtained by minimizing this distance estimator: FORMULA which is an approximation of the exact distance calculation FORMULA Note that, in contrast with the binary embedding method of MAINCIT , the query FORMULA is not converted to a code: there is no approximation error on the query side.", "bow": [["2099397891", "Modeling and querying moving objects"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["1491105865", "Nearest-neighbor methods in learning and vision : theory and practice"], ["2170741935", "Generating query substitutions"], ["2091208704", "Approximate distance oracles with constant query time"]], "pp": [["2099397891", "Modeling and querying moving objects"], ["2169351022", "An optimal algorithm for approximate nearest neighbor searching"], ["1491105865", "Nearest-neighbor methods in learning and vision : theory and practice"], ["2170741935", "Generating query substitutions"], ["2091208704", "Approximate distance oracles with constant query time"]]}, {"context": " Lee et al. CIT used deep learning for fusing multiple result lists, while Liu et al. MAINCIT used deep neural networks for both web search ranking and query classification.", "bow": [["2557283755", "Deep Learning"], ["2519373937", "Learning Recursive Filters for Low-Level Vision via a Hybrid Neural Network"], ["2150341604", "Deep Learning: Methods and Applications"], ["2072128103", "Learning Deep Architectures for AI"], ["2253171278", "Deep Ranking for Person Re-Identification via Joint Representation Learning"]], "pp": [["2519373937", "Learning Recursive Filters for Low-Level Vision via a Hybrid Neural Network"], ["1529906358", "Greedy localization and color-coding: improved matching and packing algorithms"], ["1803059841", "Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields"], ["2129326773", "Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors"], ["2185208392", "The Tweets They Are a-Changin\u2019: Evolution of Twitter Users and Behavior"]], "np": [["1529906358", "Greedy localization and color-coding: improved matching and packing algorithms"], ["156707654", "ZuBuD Zurich Buildings Database for Image Based Recognition"], ["2012855423", "A linear algorithm for 2-bend embeddings of planar graphs in the two-dimensional grid"], ["2185208392", "The Tweets They Are a-Changin\u2019: Evolution of Twitter Users and Behavior"], ["2473415337", "Multi-scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation"]]}, {"context": "They utilize layers with convolving filters that are applied to local features MAINCIT originally invented for computer vision.", "bow": [["77200240", "Simplifying convnets for fast learning"], ["2096691069", "A Biologically Inspired System for Action Recognition"], ["2137278143", "Do Convnets Learn Correspondence"], ["1961126707", "Propagated image filtering"], ["2139427956", "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition"]], "pp": [["2127025496", "One Point Isometric Matching with the Heat Kernel"], ["2130646036", "Surface feature detection and description with applications to mesh matching"], ["2007206727", "Scale-invariant heat kernel signatures for non-rigid shape recognition"], ["2167912153", "Pooling in image representation: The visual codeword point of view"], ["2012385660", "Intrinsic Shape Matching by Planned Landmark Sampling"]], "np": [["2127025496", "One Point Isometric Matching with the Heat Kernel"], ["2130646036", "Surface feature detection and description with applications to mesh matching"], ["2007206727", "Scale-invariant heat kernel signatures for non-rigid shape recognition"], ["2167912153", "Pooling in image representation: The visual codeword point of view"], ["2012385660", "Intrinsic Shape Matching by Planned Landmark Sampling"]]}, {"context": "The joint distribution for each ordered partition can then be composed using a variant of the Plackett-Luce model MAINCIT CIT , substituting object potentials by the partition potential.", "bow": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2079914085", "Efficient inference with cardinality-based clique potentials"], ["1499709881", "A Synthesis on Partition Refinement: A Useful Routine for Strings, Graphs, Boolean Matrices and Automata"]], "pp": [["43928053", "The Analysis of Permutations"], ["2094536313", "Individual choice behavior : a theoretical analysis"], ["2319178748", "Individual Choice Behavior"], ["2102456611", "Minimax-optimal Inference from Partial Rankings"], ["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"]], "np": [["43928053", "The Analysis of Permutations"], ["2094536313", "Individual choice behavior : a theoretical analysis"], ["2319178748", "Individual Choice Behavior"], ["2102456611", "Minimax-optimal Inference from Partial Rankings"], ["2049540116", "Efficient Bayesian Inference for Generalized Bradley\u2013Terry Models"]]}, {"context": "The novelty lies in the rigorous examination of probabilistic models over ordered partitions, extending earlier work in discrete choice theory CIT MAINCIT CIT .", "bow": [["2052069020", "System synthesis with morphological clique problem: fusion of subsystem evaluation decisions"], ["2151083897", "Abandoning objectives: Evolution through the search for novelty alone"], ["2081798681", "Newsjunkie: providing personalized newsfeeds via analysis of information novelty"], ["2119476348", "Cooperative mobile robotics: antecedents and directions"], ["2114865067", "A resource-allocating network for function interpolation"]], "pp": [["2052069020", "System synthesis with morphological clique problem: fusion of subsystem evaluation decisions"], ["2151083897", "Abandoning objectives: Evolution through the search for novelty alone"], ["2081798681", "Newsjunkie: providing personalized newsfeeds via analysis of information novelty"], ["2119476348", "Cooperative mobile robotics: antecedents and directions"], ["2114865067", "A resource-allocating network for function interpolation"]]}, {"context": " Another line of reasoning is largely associated with the discrete choice theory (e.g. see MAINCIT ), which assumes that each object has an intrinsic worth which is the basis for the ordering between them.", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"], ["1965499304", "Modelling disease outbreaks in realistic urban social networks"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"], ["1965499304", "Modelling disease outbreaks in realistic urban social networks"]]}, {"context": "Subsequently, Luce MAINCIT and Plackett CIT extended this model to multiple objects.", "bow": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2099353528", "Limit results on pattern entropy"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"]], "pp": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2099353528", "Limit results on pattern entropy"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"]]}, {"context": "This process continues until there is no more object to be selected.This process resembles the generative process of Plackett-Luce discrete choice model MAINCIT CIT , except we apply on partitions rather than single element.", "bow": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2125708105", "Tying process model quality to the modeling process: the impact of structuring, movement, and speed"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"]], "pp": [["853679117", "Assortment Planning Under the Multinomial Logit Model with Totally Unimodular Constraint Structures"], ["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2125708105", "Tying process model quality to the modeling process: the impact of structuring, movement, and speed"]], "np": [["853679117", "Assortment Planning Under the Multinomial Logit Model with Totally Unimodular Constraint Structures"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "Yet the need to privately access large public databases is pressing: for example Certificate Transparency MAINCIT , which strengthens TLS certificate issuing, requires clients to look up certificates for sites they have visited, resulting in privacy concerns.", "bow": [["2338858629", "Certificate transparency"], ["2078872626", "Enhanced Certificate Transparency and End-to-End Encrypted Mail."], ["2266218113", "The Transport Layer Security (TLS) Protocol Version 1.2"], ["2130867912", "The SSL landscape: a thorough analysis of the x.509 PKI using active and passive measurements"], ["2104899073", "Analysis of the HTTPS certificate ecosystem"]], "pp": [["2338858629", "Certificate transparency"], ["2078872626", "Enhanced Certificate Transparency and End-to-End Encrypted Mail."], ["2266218113", "The Transport Layer Security (TLS) Protocol Version 1.2"], ["2130867912", "The SSL landscape: a thorough analysis of the x.509 PKI using active and passive measurements"], ["2104899073", "Analysis of the HTTPS certificate ecosystem"]], "np": [["2078872626", "Enhanced Certificate Transparency and End-to-End Encrypted Mail."], ["2338858629", "Certificate transparency"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "As in some distributed systems CIT MAINCIT , each node has a clock which ticks according to a rate 1 exponential distribution.", "bow": [["2018093989", "Clock synchronization for wireless sensor networks: a survey"], ["2137536023", "Joint distributed synchronization and positioning in UWB ad hoc networks using TOA"], ["2109691981", "Logical time in distributed computing systems"], ["2118537895", "Distributed Clock Synchronization for Wireless Sensor Networks Using Belief Propagation"], ["2024045435", "Estimation and removal of clock skew from network delay measurements"]], "pp": [["2018093989", "Clock synchronization for wireless sensor networks: a survey"], ["2137536023", "Joint distributed synchronization and positioning in UWB ad hoc networks using TOA"], ["2109691981", "Logical time in distributed computing systems"], ["2118537895", "Distributed Clock Synchronization for Wireless Sensor Networks Using Belief Propagation"], ["2024045435", "Estimation and removal of clock skew from network delay measurements"]]}, {"context": ".5in 11in pdfauthor = Anonymous, pdftitle = A Dual Embedding Space Model for Document Ranking, pdfsubject = A Dual Embedding Space Model for Document Ranking, pdfkeywords = H.3 [Information Storage and Retrieval], pdfcreator = LaTeX with hyperref package, pdfproducer = pdflatex to appearThis paper is an extended evaluation and analysis of the model proposed by MAINCIT to appear in WWW'16, April 11 - 15, 2016, Montreal, Canada.", "bow": [["40976687", "A Survey of Text Clustering Algorithms"], ["1876495223", "String retrieval for multi-pattern queries"], ["2165612380", "A vector space model for automatic indexing"], ["1854885336", "Ranking with submodular valuations"], ["1557882449", "Testing Probability Distributions Underlying Aggregated Data"]], "pp": [["40976687", "A Survey of Text Clustering Algorithms"], ["1876495223", "String retrieval for multi-pattern queries"], ["2165612380", "A vector space model for automatic indexing"], ["1854885336", "Ranking with submodular valuations"], ["1557882449", "Testing Probability Distributions Underlying Aggregated Data"]]}, {"context": "RF is an ensemble classifier that aggregates the votes from decision trees for classification MAINCIT .", "bow": [["2015786472", "Simultaneous Information and Energy Transfer in Large-Scale Networks with/without Relaying"], ["1587157779", "Data Mining with Decision Trees: Theory and Applications"], ["1487563773", "Representing Knowledge in A-Prolog"], ["2100805904", "Popular ensemble methods: an empirical study"], ["2151485489", "Well-founded and stable semantics of logic programs with aggregates"]], "pp": [["2015786472", "Simultaneous Information and Energy Transfer in Large-Scale Networks with/without Relaying"], ["1587157779", "Data Mining with Decision Trees: Theory and Applications"], ["1487563773", "Representing Knowledge in A-Prolog"], ["2100805904", "Popular ensemble methods: an empirical study"], ["2151485489", "Well-founded and stable semantics of logic programs with aggregates"]]}, {"context": "OOB error is known to be an unbiased estimation of test set classification error MAINCIT .", "bow": [["98862427", "OUT-OF-BAG ESTIMATION"], ["2054640142", "Estimation of the mean of a multivariate normal distribution"], ["1995404822", "A posteriori error estimation in finite element analysis"], ["2060823036", "Signal Estimation With Additive Error Metrics in Compressed Sensing"], ["2133147107", "An optimal control approach to a posteriori error estimation in finite element methods"]], "pp": [["98862427", "OUT-OF-BAG ESTIMATION"], ["2098345921", "On Coreference Resolution Performance Metrics"], ["2116219421", "Stability-based validation of clustering solutions"], ["2099294566", "Relations between entropy and error probability"], ["2054640142", "Estimation of the mean of a multivariate normal distribution"]], "np": [["2098345921", "On Coreference Resolution Performance Metrics"], ["2116219421", "Stability-based validation of clustering solutions"], ["2099294566", "Relations between entropy and error probability"], ["2129000925", "Probability of error, equivocation, and the Chernoff bound"], ["2104752854", "Metric Learning by Collapsing Classes"]]}, {"context": "Furthermore we tried running Long Short Term Memory (LSTM) recurrent neural networks which are shown to outperform other recurrent neural network algorithms specifically for language modelling MAINCIT .", "bow": [["2064675550", "Long short-term memory"], ["2402268235", "LSTM Neural Networks for Language Modeling."], ["2026243162", "LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework"], ["2513222501", "Framewise phoneme classification with bidirectional LSTM and other neural network architectures"], ["2110485445", "Finding Structure in Time"]], "pp": [["2056590938", "Connectionist language modeling for large vocabulary continuous speech recognition"], ["2064675550", "Long short-term memory"], ["2123893795", "Probabilistic top-down parsing and language modeling"], ["2154137718", "A survey on the application of recurrent neural networks to statistical language modeling"], ["2402268235", "LSTM Neural Networks for Language Modeling."]], "np": [["2056590938", "Connectionist language modeling for large vocabulary continuous speech recognition"], ["2123893795", "Probabilistic top-down parsing and language modeling"], ["1710422233", "Lexical cohesion computed by thesaural relations as an indicator of the structure of text"], ["2107370612", "Latent concept expansion using markov random fields"], ["2071940869", "A Language Model Approach to Keyphrase Extraction"]]}, {"context": "More recently, social media has been leveraged to monitor social risks such as prescription drug and smoking behaviors CIT as well as a variety of mental health disorders including suicidal ideation MAINCIT , attention deficient hyperactivity disorder CIT and major depressive disorder CIT .", "bow": [["2473822343", "Shifts to Suicidal Ideation from Mental Health Content in Social Media"], ["2250553926", "Quantifying Mental Health Signals in Twitter"], ["2003834798", "Tracking suicide risk factors through Twitter in the US."], ["2402700", "Predicting Depression via Social Media"], ["2252191003", "From ADHD to SAD: Analyzing the Language of Mental Health on Twitter through Self-Reported Diagnoses"]], "pp": [["2473822343", "Shifts to Suicidal Ideation from Mental Health Content in Social Media"], ["2003834798", "Tracking suicide risk factors through Twitter in the US."], ["2250553926", "Quantifying Mental Health Signals in Twitter"], ["2402700", "Predicting Depression via Social Media"], ["2252191003", "From ADHD to SAD: Analyzing the Language of Mental Health on Twitter through Self-Reported Diagnoses"]], "np": [["2473822343", "Shifts to Suicidal Ideation from Mental Health Content in Social Media"], ["2003834798", "Tracking suicide risk factors through Twitter in the US."], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": " In the case of major depressive disorder, recent efforts range from characterizing linguistic phenomena associated with depression CIT and its subtypes e.g., postpartum depression MAINCIT , to identifying specific depressive symptoms CIT e.g., depressed mood.", "bow": [["2066806488", "Trajectories of depression: unobtrusive monitoring of depressive states by means of smartphone mobility traces analysis"], ["2166347079", "New Methods in Automatic Extracting"], ["2008803468", "Diurnal and Seasonal Mood Vary with Work, Sleep, and Daylength Across Diverse Cultures"], ["2473822343", "Shifts to Suicidal Ideation from Mental Health Content in Social Media"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"]], "pp": [["2066806488", "Trajectories of depression: unobtrusive monitoring of depressive states by means of smartphone mobility traces analysis"], ["2166347079", "New Methods in Automatic Extracting"], ["2008803468", "Diurnal and Seasonal Mood Vary with Work, Sleep, and Daylength Across Diverse Cultures"], ["2473822343", "Shifts to Suicidal Ideation from Mental Health Content in Social Media"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"]]}, {"context": " rightsretained 10.475/1234 123-4567-24-567/08/06 2017 2017 rightsretained SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17)August 7\u201311, 2017, Shinjuku, Tokyo, Japan Session-aware Information Embedding for E-commerce Product Recommendation Chen WuFORMULA , Ming YanFORMULA , Luo Si FORMULA FORMULA Search BU, Alibaba Group, 969 West Wenyi Road, Hangzhou 311121, China {wuchen.wc, ym119608, luo.si}@alibaba-inc.com Most of the existing recommender systems assume that user's visiting history can be constantly recorded. However, in recent online services, the user identification may be usually unknown and only limited online user behaviors can be used. It is of great importance to model the temporal online user behaviors and conduct recommendation for the anonymous users. In this paper, we propose a list-wise deep neural network based architecture to model the limited user behaviors within each session. To train the model efficiently, we first design a session embedding method to pre-train a session representation, which incorporates different kinds of user search behaviors such as clicks and views. Based on the learnt session representation, we further propose a list-wise ranking model to generate the recommendation result for each anonymous user session. We conduct quantitative experiments on a recently published dataset from an e-commerce company. The evaluation results validate the effectiveness of the proposed method, which can outperform the state-of-the-art significantly. <ccs2012> <concept> <conceptid>10010520.10010553.10010562</conceptid> <conceptdesc>Computer systems organization Embedded systems</conceptdesc> <conceptsignificance>500</conceptsignificance> </concept> <concept> <conceptid>10010520.10010575.10010755</conceptid> <conceptdesc>Computer systems organization Redundancy</conceptdesc> <conceptsignificance>300</conceptsignificance> </concept> <concept> <conceptid>10010520.10010553.10010554</conceptid> <conceptdesc>Computer systems organization Robotics</conceptdesc> <conceptsignificance>100</conceptsignificance> </concept> <concept> <conceptid>10003033.10003083.10003095</conceptid> <conceptdesc>Networks Network reliability</conceptdesc> <conceptsignificance>100</conceptsignificance> </concept> </ccs2012> [500]Computer systems organization Embedded systems [300]Computer systems organization Redundancy Computer systems organization Robotics [100]Networks Network reliability Introduction Nowadays, hundreds of millions of people use the e-commerce website for satisfying their daily demands. Due to some privacy reason, more and more users now prefer to surf the e-commerce website anonymously without login in. For example, in the released dataset from real search engine of DIGNETIA https://competition.codalab.org/competitions/11161, about 62.3% of all user sessions are non-logged users. For these users, there exist no historical behavior records for them and it is difficult for the recommender system to provide accurate recommendation due to a lack of adequate user behaviors. However, to find the right product, users in e-commerce website usually leave valuable online footprints when searching for it. For example, they may trigger multiple queries within each session or conversation to find a better result and click different products of the same category back and forth to compare the prices. Although limited, these instant search behaviors provide valuable information to understand the user's search intention, which can also help the system to better adapt to the user's subsequent information needs. Therefore, the aim of this paper is to generate recommendations for the anonymous users in e-commerce website, by leveraging the user's temporal behavioral information within the current session, such as clicks, views and purchases. Traditional recommender systems mainly rely on the long-term user behavior history for recommendation, which follow the idea of collaborative filtering CIT . However, the instant online user-item interactions are not well considered, especially in the e-commerce search scenario. The challenge lies in two-fold: (1) the available user online information is sparse and limited; (2) different kinds of user behaviors exist in the e-commerce website, such as clicks, views and purchases, how to combine all the different user behaviors for a better user understanding. To address the above challenges, we propose a list-wise deep learning model for solution. First, we present a session embedding method to better represent each session with all the available user online behaviors. Different kinds of user behaviors are separately embedded and are combined together with multi-layer deep neural networks. Based on the learnt session representation, a dependent list-wise ranking model is then used to calculate the relevance between the session and the candidate products. To accelerate the training process, the pre-train mechanism is also introduced in list-wise ranking. The main contributions of this paper can be summarized as: 1) we propose to address the cold-start recommendation problem for the anonymous users in e-commerce scenario by leveraging the diverse online search behaviors within each session; 2) a list-wise deep neural network based framework with pre-training mechanism is designed to model the user's online session behaviors for recommendation; 3) quantitative experiments on a real-world dataset demonstrate the effectiveness of the proposed method over the state-of-the-art recommendation methods. Related Work Research on recommender systems has been studied for years, in which collaborative filtering is one of the most popular techniques in this field. It relies on the users' collaborative behaviors on items for recommendation, among which the typical KNN CIT and latent factor models CIT are the traditional state-of-the-art methods. To further base the recommendation on the implicit feedback, a generic learning algorithm BPR CIT is proposed and applied to the typical KNN and MF methods. Recently, deep learning has also been applied to the recommender systems. For example, CIT presents a wide and deep learning method to jointly combine the id features and continuous features. CIT proposes a neural collaborative filtering method to better model the implicit feedback. Traditional collaborative filtering-based methods usually face the problem of data sparsity and will fail for the cold-start users. The proposed solution aims to leverage the instant online search behaviors within session for recommendation, which helps to address the new user problem. Some recent research also attempts to model the user's sequential session behaviors. In CIT , a RNN-based method is proposed to model the sequential session behaviors for more accurate recommendation. MAINCIT provides a dynamic recurrent model for next basket recommendation.", "bow": [["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["2469952266", "Improved Recurrent Neural Networks for Session-based Recommendations"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2474765392", "Multi-Rate Deep Learning for Temporal Recommendation"], ["2128280268", "Session types revisited"]], "pp": [["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["2469952266", "Improved Recurrent Neural Networks for Session-based Recommendations"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2474765392", "Multi-Rate Deep Learning for Temporal Recommendation"], ["2128280268", "Session types revisited"]]}, {"context": "For example, MAINCIT presents a wide and deep learning method to jointly combine the id features and continuous features.", "bow": [["2113957760", "Estimating the intrinsic dimension of data with a fractal-based method"], ["2030876210", "Data dimensionality estimation methods: a survey"], ["2076462394", "Audio-visual speech recognition using deep learning"], ["104211377", "Learning invariant feature hierarchies"], ["2163922914", "Representation Learning: A Review and New Perspectives"]], "pp": [["2113957760", "Estimating the intrinsic dimension of data with a fractal-based method"], ["2030876210", "Data dimensionality estimation methods: a survey"], ["2076462394", "Audio-visual speech recognition using deep learning"], ["104211377", "Learning invariant feature hierarchies"], ["2163922914", "Representation Learning: A Review and New Perspectives"]]}, {"context": "Instead, we adopt an information theoretic metric for anonymity proposed in MAINCIT : Definition 2 Define the effective size FORMULA of an anonymity probability distribution as, FORMULA where FORMULA is the probability that a rating record is from user FORMULA .", "bow": [["2482903976", "Towards an information theoretic metric for anonymity"], ["2142229514", "Anonymity, Unobservability, and Pseudonymity - A Proposal for Terminology."], ["2146112227", "Hang with your buddies to resist intersection attacks"], ["1536141561", "Riposte: An Anonymous Messaging System Handling Millions of Users"], ["1812425403", "CSP and Anonymity"]], "pp": [["2482903976", "Towards an information theoretic metric for anonymity"], ["2142229514", "Anonymity, Unobservability, and Pseudonymity - A Proposal for Terminology."], ["2146112227", "Hang with your buddies to resist intersection attacks"], ["1536141561", "Riposte: An Anonymous Messaging System Handling Millions of Users"], ["1812425403", "CSP and Anonymity"]]}, {"context": "Researchers have attempted to infer attributes of Twitter users such as age CIT , gender CIT , political orientation MAINCIT or a range of social identities CIT .", "bow": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]], "pp": [["2017729405", "Classifying latent user attributes in twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["9292421", "Discriminating Gender on Twitter"], ["2013416264", "Democrats, republicans and starbucks afficionados: user classification in twitter"], ["1014449310", "Homophily and Latent Attribute Inference: Inferring Latent Attributes of Twitter Users from Neighbors"]], "np": [["2017729405", "Classifying latent user attributes in twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2013416264", "Democrats, republicans and starbucks afficionados: user classification in twitter"], ["1014449310", "Homophily and Latent Attribute Inference: Inferring Latent Attributes of Twitter Users from Neighbors"], ["68298479", "Classifying Political Orientation on Twitter: It\u2019s Not Easy!"]]}, {"context": " rightsretained 10.475/1234 123-4567-24-567/08/06 2017 2017 rightsretained SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17)August 7\u201311, 2017, Shinjuku, Tokyo, Japan Session-aware Information Embedding for E-commerce Product Recommendation Chen WuFORMULA , Ming YanFORMULA , Luo Si FORMULA FORMULA Search BU, Alibaba Group, 969 West Wenyi Road, Hangzhou 311121, China {wuchen.wc, ym119608, luo.si}@alibaba-inc.com Most of the existing recommender systems assume that user's visiting history can be constantly recorded. However, in recent online services, the user identification may be usually unknown and only limited online user behaviors can be used. It is of great importance to model the temporal online user behaviors and conduct recommendation for the anonymous users. In this paper, we propose a list-wise deep neural network based architecture to model the limited user behaviors within each session. To train the model efficiently, we first design a session embedding method to pre-train a session representation, which incorporates different kinds of user search behaviors such as clicks and views. Based on the learnt session representation, we further propose a list-wise ranking model to generate the recommendation result for each anonymous user session. We conduct quantitative experiments on a recently published dataset from an e-commerce company. The evaluation results validate the effectiveness of the proposed method, which can outperform the state-of-the-art significantly. <ccs2012> <concept> <conceptid>10010520.10010553.10010562</conceptid> <conceptdesc>Computer systems organization Embedded systems</conceptdesc> <conceptsignificance>500</conceptsignificance> </concept> <concept> <conceptid>10010520.10010575.10010755</conceptid> <conceptdesc>Computer systems organization Redundancy</conceptdesc> <conceptsignificance>300</conceptsignificance> </concept> <concept> <conceptid>10010520.10010553.10010554</conceptid> <conceptdesc>Computer systems organization Robotics</conceptdesc> <conceptsignificance>100</conceptsignificance> </concept> <concept> <conceptid>10003033.10003083.10003095</conceptid> <conceptdesc>Networks Network reliability</conceptdesc> <conceptsignificance>100</conceptsignificance> </concept> </ccs2012> [500]Computer systems organization Embedded systems [300]Computer systems organization Redundancy Computer systems organization Robotics [100]Networks Network reliability Introduction Nowadays, hundreds of millions of people use the e-commerce website for satisfying their daily demands. Due to some privacy reason, more and more users now prefer to surf the e-commerce website anonymously without login in. For example, in the released dataset from real search engine of DIGNETIA https://competition.codalab.org/competitions/11161, about 62.3% of all user sessions are non-logged users. For these users, there exist no historical behavior records for them and it is difficult for the recommender system to provide accurate recommendation due to a lack of adequate user behaviors. However, to find the right product, users in e-commerce website usually leave valuable online footprints when searching for it. For example, they may trigger multiple queries within each session or conversation to find a better result and click different products of the same category back and forth to compare the prices. Although limited, these instant search behaviors provide valuable information to understand the user's search intention, which can also help the system to better adapt to the user's subsequent information needs. Therefore, the aim of this paper is to generate recommendations for the anonymous users in e-commerce website, by leveraging the user's temporal behavioral information within the current session, such as clicks, views and purchases. Traditional recommender systems mainly rely on the long-term user behavior history for recommendation, which follow the idea of collaborative filtering CIT . However, the instant online user-item interactions are not well considered, especially in the e-commerce search scenario. The challenge lies in two-fold: (1) the available user online information is sparse and limited; (2) different kinds of user behaviors exist in the e-commerce website, such as clicks, views and purchases, how to combine all the different user behaviors for a better user understanding. To address the above challenges, we propose a list-wise deep learning model for solution. First, we present a session embedding method to better represent each session with all the available user online behaviors. Different kinds of user behaviors are separately embedded and are combined together with multi-layer deep neural networks. Based on the learnt session representation, a dependent list-wise ranking model is then used to calculate the relevance between the session and the candidate products. To accelerate the training process, the pre-train mechanism is also introduced in list-wise ranking. The main contributions of this paper can be summarized as: 1) we propose to address the cold-start recommendation problem for the anonymous users in e-commerce scenario by leveraging the diverse online search behaviors within each session; 2) a list-wise deep neural network based framework with pre-training mechanism is designed to model the user's online session behaviors for recommendation; 3) quantitative experiments on a real-world dataset demonstrate the effectiveness of the proposed method over the state-of-the-art recommendation methods. Related Work Research on recommender systems has been studied for years, in which collaborative filtering is one of the most popular techniques in this field. It relies on the users' collaborative behaviors on items for recommendation, among which the typical KNN CIT and latent factor models CIT are the traditional state-of-the-art methods. To further base the recommendation on the implicit feedback, a generic learning algorithm BPR CIT is proposed and applied to the typical KNN and MF methods. Recently, deep learning has also been applied to the recommender systems. For example, CIT presents a wide and deep learning method to jointly combine the id features and continuous features. MAINCIT proposes a neural collaborative filtering method to better model the implicit feedback.", "bow": [["2469952266", "Improved Recurrent Neural Networks for Session-based Recommendations"], ["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["2474765392", "Multi-Rate Deep Learning for Temporal Recommendation"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2171960770", "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions"]], "pp": [["2469952266", "Improved Recurrent Neural Networks for Session-based Recommendations"], ["2062792908", "Two notions of sub-behaviour for session-based client/server systems"], ["2474765392", "Multi-Rate Deep Learning for Temporal Recommendation"], ["1784055113", "Language Primitives and Type Discipline for Structured Communication-Based Programming"], ["2171960770", "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions"]]}, {"context": "Beyond word co-occurrence, recent studies have also explored learning text embeddings from clickthrough data CIT , session data CIT , query prefix-suffix pairs CIT , via auto-encoders MAINCIT , and for sentiment classification CIT and for long text CIT .", "bow": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]], "pp": [["1485516007", "The Myriad Virtues of Subword Trees"], ["2121252285", "A Space-Economical Suffix Tree Construction Algorithm"], ["119539871", "A new method for on-line string searches"], ["2059513841", "On-line construction of suffix trees"], ["2071262136", "String matching in Lempel-Ziv compressed strings"]]}, {"context": "Quantifying that leakage is therefore necessary and we propose a game-based differential privacy definition MAINCIT to evaluate and minimize the risk introduced.", "bow": [["2610955953", "Differential privacy"], ["2088517895", "Privacy against statistical inference"], ["2077217970", "A firm foundation for private data analysis"], ["1526797722", "Towards privacy for social networks: a zero-knowledge based definition of privacy"], ["2074006684", "A Statistical Framework for Differential Privacy"]], "pp": [["2610955953", "Differential privacy"], ["2088517895", "Privacy against statistical inference"], ["2077217970", "A firm foundation for private data analysis"], ["1526797722", "Towards privacy for social networks: a zero-knowledge based definition of privacy"], ["2074006684", "A Statistical Framework for Differential Privacy"]], "np": [["2610955953", "Differential privacy"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "Differential Privacy definitions and mechanisms were first presented in 2006 MAINCIT to enable safe interactions with statistical databases.", "bow": [["2116974465", "Mind the duality gap: safer rules for the Lasso"], ["1485110750", "Differential Privacy for Statistics: What we Know and What we Want to Learn"], ["2231059058", "A dynamic screening principle for the Lasso"], ["2109426455", "Differential privacy: a survey of results"], ["19177142", "The Differential Privacy Frontier"]], "pp": [["2116974465", "Mind the duality gap: safer rules for the Lasso"], ["1485110750", "Differential Privacy for Statistics: What we Know and What we Want to Learn"], ["2231059058", "A dynamic screening principle for the Lasso"], ["2109426455", "Differential privacy: a survey of results"], ["19177142", "The Differential Privacy Frontier"]]}, {"context": "For FORMULA information about the query selected leaks at a non-negligible rate, and users should rate-limit recurring or correlated queries as for other differentially private mechanisms MAINCIT .", "bow": [["2399682466", "Private Convex Optimization for Empirical Risk Minimization with Applications to High-dimensional Regression."], ["2124500503", "Faster private release of marginals on small databases"], ["1586209290", "Iterative constructions and private data release"], ["2121386703", "Compressive mechanism: utilizing sparse representation in differential privacy"], ["2119219356", "Differentially private data release through multidimensional partitioning"]], "pp": [["2399682466", "Private Convex Optimization for Empirical Risk Minimization with Applications to High-dimensional Regression."], ["2124500503", "Faster private release of marginals on small databases"], ["1586209290", "Iterative constructions and private data release"], ["2121386703", "Compressive mechanism: utilizing sparse representation in differential privacy"], ["2119219356", "Differentially private data release through multidimensional partitioning"]]}, {"context": " See below for our analysis of this naive dummy requests mechanism.. Similarly, as the Tor network CIT grows it becomes untenable to broadcast information about all servers to all clients, and a private querying mechanism CIT will have to be implemented to prevent attacks based on partial knowledge of the network MAINCIT .", "bow": [["2156410527", "Low-resource routing attacks against tor"], ["1545050218", "Shining Light in Dark Places: Understanding the Tor Network"], ["2031925412", "Users get routed: traffic correlation on tor by realistic adversaries"], ["2092085685", "How much anonymity does network latency leak"], ["2112178441", "Locating hidden servers"]], "pp": [["2156410527", "Low-resource routing attacks against tor"], ["1545050218", "Shining Light in Dark Places: Understanding the Tor Network"], ["2031925412", "Users get routed: traffic correlation on tor by realistic adversaries"], ["2092085685", "How much anonymity does network latency leak"], ["2112178441", "Locating hidden servers"]]}, {"context": "The structure of scientific collaboration networks including the shortest paths, weighted networks, and centrality was studied CIT CIT MAINCIT .", "bow": [["2763120769", "Scientific collaboration networks. II. Shortest paths, weighted networks, and centrality."], ["2127405101", "A Graph-theoretic perspective on centrality"], ["1967570846", "The centrality index of a graph"], ["1971937094", "A set of measures of centrality based on betweenness"], ["1986310535", "Rethinking centrality: Methods and examples\u2606"]], "pp": [["2763120769", "Scientific collaboration networks. II. Shortest paths, weighted networks, and centrality."], ["2127405101", "A Graph-theoretic perspective on centrality"], ["1967570846", "The centrality index of a graph"], ["1971937094", "A set of measures of centrality based on betweenness"], ["1986310535", "Rethinking centrality: Methods and examples\u2606"]]}, {"context": "Recent papers have explored in detail the SG and CBOW training methodology MAINCIT and its connection to other approaches for learning word embeddings such as explicit vector space representations CIT , matrix factorization CIT and density-based representations CIT .", "bow": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "pp": [["2429488962", "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations"], ["1614298861", "Efficient Estimation of Word Representations in Vector Space"], ["1951325712", "Boosting Named Entity Recognition with Neural Character Embeddings"], ["2125076245", "RC-NET: A General Framework for Incorporating Knowledge into Word Representations"], ["2296194829", "Two/Too Simple Adaptations of Word2Vec for Syntax Problems."]], "np": [["2131479143", "Multi-Task Learning for Classification with Dirichlet Process Priors"], ["2156163116", "Best practices for convolutional neural networks applied to visual document analysis"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."]]}, {"context": "Rich-Club Connectivity The rich-club connectivity MAINCIT measures how tightly the high-degree nodes, rich nodes, interconnect with themselves.", "bow": [["2075984423", "Spectra of \"real-world\" graphs: beyond the semicircle law."], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2018438786", "On the minimum node degree and connectivity of a wireless multihop network"], ["2039302875", "Connectivity properties of a packet radio network model"], ["2138561727", "The critical transmitting range for connectivity in sparse wireless ad hoc networks"]], "pp": [["2075984423", "Spectra of \"real-world\" graphs: beyond the semicircle law."], ["2155353872", "The rich-club phenomenon in the Internet topology"], ["2018438786", "On the minimum node degree and connectivity of a wireless multihop network"], ["2039302875", "Connectivity properties of a packet radio network model"], ["2138561727", "The critical transmitting range for connectivity in sparse wireless ad hoc networks"]]}, {"context": "The complexity of this problem is super-exponential with respect to number of objects (FORMULA ) because both the number of partitions and their order are unknown - it grows exponentially as FORMULA MAINCIT .", "bow": [["2078762661", "The Number of Partitions of a Set"], ["2063302961", "Asymptotic Formula\u00e6 in Combinatory Analysis"], ["2110877857", "Multiple Non-Redundant Spectral Clustering Views"], ["2579352881", "Unsupervised Image-to-Image Translation with Generative Adversarial Networks."], ["1482303427", "Approximation of Relations"]], "pp": [["2078762661", "The Number of Partitions of a Set"], ["2063302961", "Asymptotic Formula\u00e6 in Combinatory Analysis"], ["2110877857", "Multiple Non-Redundant Spectral Clustering Views"], ["2579352881", "Unsupervised Image-to-Image Translation with Generative Adversarial Networks."], ["1482303427", "Approximation of Relations"]]}, {"context": "If we consider all the possible values of FORMULA , the size of our state space is FORMULA which is also known in combinatorics as the Fubini's number MAINCIT .", "bow": [["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["187895097", "Combinatorics on words: a tutorial"], ["2113137767", "An experimental comparison of min-cut/max- flow algorithms for energy minimization in vision"], ["106665802", "Polynomial-Time Algorithms for Energy Games with Special Weight Structures"], ["2162894853", "On the index of Simon's congruence for piecewise testability"]], "pp": [["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["187895097", "Combinatorics on words: a tutorial"], ["2113137767", "An experimental comparison of min-cut/max- flow algorithms for energy minimization in vision"], ["106665802", "Polynomial-Time Algorithms for Energy Games with Special Weight Structures"], ["2162894853", "On the index of Simon's congruence for piecewise testability"]]}, {"context": "Its asymptotic behaviour can also be shown MAINCIT to approach FORMULA as FORMULA where we note that FORMULA , and thus it grows much faster than FORMULA .", "bow": [["2058232210", "Intelligent Packet Dropping for Optimal Energy-Delay Tradeoffs in Wireless Downlinks"], ["2136661843", "Asymptotically optimal agents"], ["2155921792", "Encouraging behavioral diversity in evolutionary robotics: An empirical study"], ["1511751337", "Large sample estimation and hypothesis testing"], ["1596607002", "Digital processing of random signals: theory and methods"]], "pp": [["2058232210", "Intelligent Packet Dropping for Optimal Energy-Delay Tradeoffs in Wireless Downlinks"], ["2136661843", "Asymptotically optimal agents"], ["2155921792", "Encouraging behavioral diversity in evolutionary robotics: An empirical study"], ["1511751337", "Large sample estimation and hypothesis testing"], ["1596607002", "Digital processing of random signals: theory and methods"]]}, {"context": "Having Twitter as a new kind of data source, researchers have looked into the development of tools for real-time trend analytics CIT or early detection of newsworthy events CIT , as well as into analytical approaches for understanding the sentiment expressed by users towards a target CIT , or public opinion on a specific topic MAINCIT .", "bow": [["2122369144", "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series"], ["1956559956", "Introduction to Modern Information Retrieval"], ["1517771839", "Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields"], ["2108646579", "Sentiment Analysis and Opinion Mining"], ["2128721751", "Twitinfo: aggregating and visualizing microblogs for event exploration"]], "pp": [["2122369144", "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series"], ["1956559956", "Introduction to Modern Information Retrieval"], ["1485536830", "Using Reinforcement Learning to Spider the Web Efficiently"], ["1517771839", "Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields"], ["2108646579", "Sentiment Analysis and Opinion Mining"]], "np": [["1485536830", "Using Reinforcement Learning to Spider the Web Efficiently"], ["100047375", "Parallel algorithm configuration"], ["1000617247", "Constrained 1-Spectral Clustering"], ["10021998", "Loss Functions for Discriminative Training of Energy-Based Models."], ["100239005", "Towards effective event detection, tracking and summarization on microblog data"]]}, {"context": "Jaro-Winkler: Jaro-Winkler distance MAINCIT of two strings.", "bow": [["41404523", "String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage."], ["2128600649", "Probabilistic linkage of large public health data files"], ["2028400828", "Vertex-to-vertex pursuit in a graph"], ["2102443632", "Learning string-edit distance"], ["1974745849", "Counting linear extensions"]], "pp": [["2128600649", "Probabilistic linkage of large public health data files"], ["41404523", "String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage."], ["46452414", "The state of record linkage and current research problems"], ["1646278814", "A comparison of string distance metrics for name-matching tasks"], ["2028400828", "Vertex-to-vertex pursuit in a graph"]], "np": [["2128600649", "Probabilistic linkage of large public health data files"], ["46452414", "The state of record linkage and current research problems"], ["1646278814", "A comparison of string distance metrics for name-matching tasks"], ["2001496424", "A guided tour to approximate string matching"], ["100047375", "Parallel algorithm configuration"]]}, {"context": "The joint distribution for each ordered partition can then be composed using a variant of the Plackett-Luce model CIT MAINCIT , substituting object potentials by the partition potential.", "bow": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2079914085", "Efficient inference with cardinality-based clique potentials"], ["1499709881", "A Synthesis on Partition Refinement: A Useful Routine for Strings, Graphs, Boolean Matrices and Automata"]], "pp": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2079914085", "Efficient inference with cardinality-based clique potentials"], ["1499709881", "A Synthesis on Partition Refinement: A Useful Routine for Strings, Graphs, Boolean Matrices and Automata"]]}, {"context": "The novelty lies in the rigorous examination of probabilistic models over ordered partitions, extending earlier work in discrete choice theory CIT CIT MAINCIT .", "bow": [["2052069020", "System synthesis with morphological clique problem: fusion of subsystem evaluation decisions"], ["2151083897", "Abandoning objectives: Evolution through the search for novelty alone"], ["2081798681", "Newsjunkie: providing personalized newsfeeds via analysis of information novelty"], ["2119476348", "Cooperative mobile robotics: antecedents and directions"], ["2114865067", "A resource-allocating network for function interpolation"]], "pp": [["2052069020", "System synthesis with morphological clique problem: fusion of subsystem evaluation decisions"], ["2151083897", "Abandoning objectives: Evolution through the search for novelty alone"], ["2081798681", "Newsjunkie: providing personalized newsfeeds via analysis of information novelty"], ["2119476348", "Cooperative mobile robotics: antecedents and directions"], ["2114865067", "A resource-allocating network for function interpolation"]]}, {"context": "Subsequently, Luce CIT and Plackett MAINCIT extended this model to multiple objects.", "bow": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2099353528", "Limit results on pattern entropy"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"]], "pp": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2099353528", "Limit results on pattern entropy"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"]]}, {"context": "This process continues until there is no more object to be selected.This process resembles the generative process of Plackett-Luce discrete choice model CIT MAINCIT , except we apply on partitions rather than single element.", "bow": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2125708105", "Tying process model quality to the modeling process: the impact of structuring, movement, and speed"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"]], "pp": [["2029414932", "A REDUCTION FORMULA FOR NORMAL MULTIVARIATE INTEGRALS"], ["2121808626", "The Choice Axiom after Twenty Years"], ["1948584817", "Fast and accurate inference of Plackett-Luce models"], ["2125708105", "Tying process model quality to the modeling process: the impact of structuring, movement, and speed"], ["1978865199", "Games and Decisions: Introduction and Critical Survey"]]}, {"context": "Another widely used technique relies on the social network that a user is connected to, in order to infer a user's location from that of their followers and followees MAINCIT .", "bow": [["2076219102", "TwitterRank: finding topic-sensitive influential twitterers"], ["2019177758", "Analyzing user modeling on twitter for personalized news recommendations"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"], ["2140535046", "Location prediction in social media based on tie strength"]], "pp": [["2076219102", "TwitterRank: finding topic-sensitive influential twitterers"], ["2019177758", "Analyzing user modeling on twitter for personalized news recommendations"], ["2112421114", "Parallel and distributed graph cuts by dual decomposition"], ["1495515969", "Inferring the Location of Twitter Messages Based on User Relationships"], ["2140535046", "Location prediction in social media based on tie strength"]]}, {"context": " Other adaptive algorithms, e.g., ADAM CIT , ADADELTA MAINCIT gave similar results.", "bow": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["6908809", "ADADELTA: An Adaptive Learning Rate Method"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]], "pp": [["2166347079", "New Methods in Automatic Extracting"], ["2138952379", "Large-Margin Learning of Submodular Summarization Models"], ["1519275474", "A Note on Well-Covered Graphs"], ["6908809", "ADADELTA: An Adaptive Learning Rate Method"], ["2091058873", "A new dual to the G\u00e1cs-K\u00f6rner common information defined via the Gray-Wyner system"]]}, {"context": "Researchers have attempted to infer attributes of Twitter users such as age CIT , gender CIT , political orientation MAINCIT or a range of social identities CIT .", "bow": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]], "pp": [["2017729405", "Classifying latent user attributes in twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["9292421", "Discriminating Gender on Twitter"], ["2013416264", "Democrats, republicans and starbucks afficionados: user classification in twitter"], ["1014449310", "Homophily and Latent Attribute Inference: Inferring Latent Attributes of Twitter Users from Neighbors"]], "np": [["2017729405", "Classifying latent user attributes in twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2013416264", "Democrats, republicans and starbucks afficionados: user classification in twitter"], ["1014449310", "Homophily and Latent Attribute Inference: Inferring Latent Attributes of Twitter Users from Neighbors"], ["68298479", "Classifying Political Orientation on Twitter: It\u2019s Not Easy!"]]}, {"context": "Researchers have attempted to infer attributes of Twitter users such as age CIT , gender MAINCIT , political orientation CIT or a range of social identities CIT .", "bow": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]], "pp": [["2017729405", "Classifying latent user attributes in twitter"], ["9292421", "Discriminating Gender on Twitter"], ["184758014", "A Machine Learning Approach to Twitter User Classification"], ["2153802318", "On the entropy of sums"], ["2250194349", "Gender Inference of Twitter Users in Non-English Contexts"]]}]