(1999) ,  GREF) as well as  <scope> maximum entropy models </scope>  (e . g .   ,  GTREF)  <scope> in particular have shown a large degree of success for WSD ,  and have established challenging state-of-the-art benchmarks .  </scope>
(2005) used the semantic compatibility information ,  and TREF  <scope> used automatically discovered patterns integrated with semantic relatedness information </scope>  ,  while REF employed semantic class knowledge acquired from the Penn Treebank . 
(2)  <scope> We note that these posterior probabilities can be computed efficiently for some alignment models such as  </scope> the HMM (GREF) ,   <scope> Models 1 and 2  </scope> (TREF) . 
According to (TREF) ,  the  <scope> usage of two instances acts as a disambiguator and leads to much more accurate semantic class extraction </scope>  compared to (REF) . 
A comparison of unlexicalised PCFG parsing (K ? ubler ,  2005) trained and evaluated on the  <scope> German NEGRA </scope>  (TREF) and the T ? uBaD/Z (REF)  <scope> treebanks </scope>  using LoPar (REF) shows a difference in parsing results of about 16% ,  using the PARSEVAL metric (REF) . 
Active learning has been successfully applied to various natural language tasks such as parsing (REF) ,  POS tagging (REF) and <scope> providing large amounts of annotations for common natural language processing tasks such as word sense disambiguation </scope>  (TREF) . 
Adapting unlexicalized parsers appears to be equally difficult :  REF adapt <scope> the unlexicalized parser of  </scope> TREF to Chinese ,  but even after significant efforts on choosing category splits ,  only modest performance gains are reported . 
Adapting unlexicalized parsers appears to be equally difficult :  TREF  <scope> adapt the unlexicalized parser of REF to Chinese ,  but even after significant efforts on choosing category splits ,  only modest performance gains are reported .  </scope>
Additionally ,  a broader comparison with point-wise predictors (REF) as well as  <scope> Viterbi-based probabilistic models  </scope> (GTREF) in large-scale comparative studies is warranted . 
A detailed trace of the system's execution on this sentence can be found in REF ,  and <scope> a short description of the program's behavior can be found in  </scope> TREF . 
AEPs are derived from the concept of an extended projection in lexicalized tree adjoining grammars (LTAG) (REF) ,  with the addition of  <scope> alignment information that is based on work in synchronous LTAG </scope>  (TREF) . 
al (2007) ,  REF ,  and TREF  <scope> train a maximum entropy model </scope>  and REF train a voted perceptron algorithm to correct preposition errors . 
Algorithm  <scope> This algorithm was first implemented for the MUC-6 FASTUS system  </scope> (TREF) ,  and produced one of the top scores (a recall of 59% and precision of 72%) in the MUC-6 Coreference Task ,  which evaluated systems' ability to recog47 nize coreference among noun phrases (REF) . 
Algorithms for subsentential alignment have been developed as well as granularities of the charact <scope> e </scope> r (TREF) ,  word (GREF) ,  collocation (REF) ,  and specially segmented (REF) levels . 
All of these corpora were also used by (REF) and  <scope> BNC was used in similar settings by  </scope> (TREF) . 
All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (GREF) or  <scope> by allowing a lexicon model to hold phrases </scope>  (TREF) . 
Also ,  we considered some of the features designed by (REF) :  First and Last Word/POS in Constituent ,  Subcategorization ,   <scope> Head Word of Prepositional Phrases and the Syntactic Frame feature from  </scope> (TREF) . 
Alternative realizations are scored using  <scope> integrated n-gram and perceptron models </scope>  (TREF) ,  where the latter includes syntactic features from REF normal form model as well as discriminative n-gram features (REF) . 
Although Chinese zero anaphora has been widely studied in the linguistics research (GREF) ,   <scope> only a small body of prior work in computational linguistics deals with Chinese zero anaphora resolution </scope>  (GTREF) . 
Although differences prevent a direct comparison ,  TREF  <scope> achieved an accuracy of 59 . 4% on 694 verb occurrences using their baseline method </scope>  ,  REF achieved 82% accuracy on time-stamping clauses for a single type of event on 172 clauses ,  and Mani et al . 
Although  <scope> the BLEU </scope>  (TREF)  <scope> score from Finnish to English is 21 . 8 </scope>  ,  the score in the reverse direction is reported as 13 . 0 which is one of the lowest scores in 11 European languages scores (REF) . 
Among them ,  uncertainty sampling (REF)  <scope> is the most well-known and the simplest sample selection method which only needs one classifier </scope>  (TREF) . 
An additional research area which is relevant to this paper is the work on joint structure modeling (GREF) and  <scope> stacked classification (GTREF) in natural language processing .  </scope>
And as we expected ,   <scope> the feature that represents whether the word is in the hedge list or not is very useful especially in hedge cue finding ,  indicating that methods based on a hedge cue lists </scope>  (TREF) or keyword selection (REF)  <scope> are quite significant way to accomplish such tasks </scope>  . 
And  <scope> researchers in multi-document text summarization </scope>  (TREF) ,  information extraction (REF) ,  and question answering (GREF) <scope> have focused on identifying and exploiting paraphrases in the context of recognizing redundancies ,  alternative formulations of the same meaning ,  and improving the performance of question answering systems .  </scope>
(An example is presented below ;   <scope> see </scope>  TREF and especially Section 2 . 10 of REF  <scope> for further discussion </scope> ) . 
Another advantage of generative models is that they do not suffer from the label bias problems (REF) ,  <scope> which is a potential problem for conditional or deterministic history-based models ,  such as </scope>  (TREF) . 
Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e . g .  REF) ,  often  <scope> using approaches based in synchronous parsing </scope>  (e . g .  TREF) . 
Another thread related to our work includes  <scope> extracting from text corpora paraphrases </scope>  (TREF) and inference rules ,  e . g .  TEASE 1 (REF) and DIRT (REF) . 
A number of successful studies emerged thereafter for other natural language processing tasks ,  such as text classification (REF) ,  noun phrase chunking (REF) ,  parsing (REF) and  <scope> reference or relation resolution  </scope> (GTREF) . 
Approaches have been proposed recently towards getting better word alignment and thus better TTS templates ,  such as encoding syntactic structure information into the HMM-based word alignment model REF ,  and  <scope> build62 ing a syntax-based word alignment model  </scope> TREF<div class="tooltip fixed active" style="left :  1017px ;  top :  3794px ;  display :  block ;  "><span style="color : black"><tref>May and Knight (2007)</tref></div></span> with TTS templates . 
Approaches to tackle assertion classification can be roughly organized into following classes :  rule based models (REF) ,  statistical models (REF) ,   <scope> machine learning  </scope> (TREF) ,  though most contributions can be seen as a combination of these (REF) . 
As an alternative to the often used sourcechannel approach (REF) ,   <scope> we directly model the posterior probability Pr(e I 1 | f J 1 )  </scope> (TREF) . 
As comparison ,  REF used seed sets consisting of 7 words in their word valence annotation experiments ,  while TREF  <scope> used minimal seed sets consisting of only one positive and one negative word (excellent and poor) in his experiments on review classification .  </scope>
As gold standard data for supertagger evaluation we have used supertagged GENIA data (REF) ,  <scope> annotated by  </scope> TREF . 
As methodologies deriving well-formedness of a sentence we use supertagging (REF) with lightweight dependency analysis (LDA)1 (REF) ,  link grammars (REF) and a  <scope> maximumentropy (ME) based chunk parser  </scope> (TREF) . 
A somewhat tighter definition of co-occurrence ,  which nevertheless yields a semantic distance measure ,  serves as the basis of a method that captures irregular inflectional transformations in REF . 1 TREF  <scope> employ distributions over adjacent words (yielding a syntactic distance metric) to improve the precision of their conflation sets .  </scope>
As reported in (REF) ,  <scope> EM ,  as has been used in  </scope> (TREF)  <scope> to estimate rule probabilities in derivation forests ,  is an iterative procedure and prefers shorter derivations containing large rules over longer derivations containing small rules .  </scope>
As shown by (TREF) ,   <scope> some of these features can be combined with lexical information to improve segmentation performance (although in a supervised manner) </scope>  ,  and (REF) show some success in broadcast news segmentation using only these kinds of non-lexical features . 
As such ,  it emphasizes robustness at Web scale ,  without taking advantage of existing speci cation languages for representing events and temporal expressions occurring in text (REF) ,  and forgoing the potential bene ts of more complex methods that  <scope> extract temporal relations from relatively clean text collections </scope>  (TREF) . 
As to the tasks involved in our scenario ,   <scope> several papers address AL for NER </scope>  (GTREF) and syntactic parsing (GREF) . 
A total of 26875 synsets in WordNet 1 . 7 . 1 and a total of 25940 synsets are augmented with SemCor examples . 5 5 Experiments and Results 5 . 1 Data  <scope> We experiment with all the standard data sets ,  namely ,  </scope>  Senseval 2 (SV2) (M .  REF) ,   <scope> Senseval 3 (SV3)  </scope> (TREF) ,  and SEMEVAL (SM) (REF) English All Words data sets . 
A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews) ,  the most popular being decision lists (REF) and  <scope> naive Bayesian classifiers </scope>  (GTREF) . 
A wide variety of semantic and syntactic criteria were used to produce the SENSEVAL-2 groupings (GREF) ;   <scope> this data covers all senses of 411 nouns ,  519 verbs ,  and 257 adjectives ,  and has been used as gold standard sense clustering data in previous work </scope>  (GTREF)2 . 
Because of the cost and difficulty in treebank construction ,  researchers have also investigated the utilization of unannotated text ,  including the  <scope> unsupervised parsing which totally uses unannotated data </scope>  (GTREF) ,  and the semisupervised parsing which uses both annotated and unannotated data (GREF) . 
Because our target is biomedical texts ,   <scope> we re-trained a parser  </scope> (REF)  <scope> with the GENIA treebank  </scope> (TREF) ,  and also applied a bidirectional part-ofspeech tagger (REF) trained with the GENIA treebank as a preprocessor . 
Besides being used in SMT ,  it is also used in translation lexicon building (REF) ,  transfer rule learning (REF) ,  example-based machine translation (REF) ,  etc .  <scope> In previous alignment methods ,  some researches modeled the alignments as hidden parameters in a  </scope> statistical translation model (GREF) or  <scope> directly modeled them given the sentence pairs  </scope> (TREF) . 
Besides being used in SMT ,  it is also used in translation lexicon building (REF) ,  transfer rule learning (REF) ,  example-based machine translation (REF) ,  etc .   <scope> In previous alignment methods ,  some researches modeled the alignments as hidden parameters in a statistical translation model </scope>  (GTREF) or directly modeled them given the sentence pairs (REF) . 
Besides contextual features ,  the vectors can also be represented by verb-noun relations (REF) ,   <scope> syntactic dependency  </scope> (GTREF) ,  co-occurrence (REF) ,  conjunction and appositive features (REF) . 
Besides vector averaging (GREF) ,  that can model distributional meaning of sentences ,   <scope> recent distributional compositional models focus on finding distributional vectors of word pairs </scope>  (GTREF) . 
BioCreative (REF) and FlySlip (TREF)  <scope> also comprise texts in the biomedical domain ,  annotated for gene entity mentions in articles from the human and fruit fly literature ,  respectively .  </scope>
Both generative (GREF) and <scope> discriminative training </scope>  (GTREF) <scope> algorithms have been proposed .  </scope>
Both  <scope> generative </scope>  (GTREF) and discriminative training (GREF)  <scope> algorithms have been proposed .  </scope>
Both the GENIA corpus (REF) and  <scope> the BioIE cytochrome P450 corpus </scope>  (TREF)  <scope> come with named entity annotations that include a proportion of chemicals ,  and at least a few abstracts that are recognisable as chemistry abstracts .  </scope>
By contrast ,  explicit syntax approaches seek to directly model the relations learned from parsed data ,  including  <scope> models between source trees and target trees </scope>  (GTREF) ,  source trees and target strings (GREF) ,  or source strings and target trees (GREF) . 
By habit ,  most systems for automatic role-semantic analysis have used Pennstyle constituents (REF) produced by REF or TREF  <scope> parsers </scope>  . 
Cognates have been employed for sentence and word alignment in bitexts (GREF) ,  improving statistical machine translation models (Al-REF) ,  and inducing  <scope> translation lexicons  </scope> (TREF) . 
Compared to term-counting approaches ,  machine learning approaches usually achieve much better performance (GREF) ,  and have been adopted to more complicated scenarios ,  such as domain adaptation (REF) ,   <scope> multi-domain learning </scope>  (TREF) and semi-supervised learning (GREF) for sentiment classification . 
Compared to term-counting approaches ,  machine learning approaches usually achieve much better performance (GREF) ,   <scope> and have been adopted to more complicated scenarios ,  such as domain adaptation </scope>  (TREF) ,  multi-domain learning (REF) and semi-supervised learning (GREF) for sentiment classification . 
Conceptual relations can be used to define measures of semantic distance (GREF) ,  but topic relatedness is not well captured by wordnet relations ,  and  <scope> this is a fundamental parameter to estimate sense similarity in many NLP applications </scope>  (TREF) . 
Consortium (REF) ,  <scope> AT&T  </scope> (TREF) ,  and ATR have been making their own research effort (REF) . 
Constraints filter the candidate list of improbable antecedents ,  while preferences encourage selection of antecedents that are more recent ,  frequent ,  etc .  Implementation of constraints and preferences can be based on empirical insight (REF) ,  or  <scope> machine learning from a reference88 annotated corpus </scope>  (TREF) . 
Contextual roles (REF) ,   <scope> semantic relations </scope>  (TREF) ,  semantic roles (GREF) ,  and animacy (REF)  <scope> have also been exploited to improve coreference resolution .  </scope>
Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (GREF) ,  phrasal translation (GREF) ,  target word selection (GREF) ,  domain word translation (GREF) ,   <scope> sense disambiguation </scope>  (GTREF) ,  and even recently for query translation in cross-language IR as well (REF) . 
Corpus-based or example-based MT (GREF) and  <scope> statistical MT </scope>  (TREF) systems provide the easiest customizability ,  since users have only to supply a collection of source and target sentence pairs (a bilingual corpus) . 
Co-training has been applied to a number of NLP applications ,  including POS-tagging (REF) ,   <scope> parsing  </scope> (TREF) ,  word sense disambiguation (REF) ,  and base noun phrase detection (REF) . 
Crucially ,  our work distinguishes itself from previous hierarchical models in that  <scope> it does not rely on any cubic-time parsing algorithms such as CKY (used in ,  e . g .  ,  </scope>  (TREF)) or the Earley algorithm (used in (REF)) . 
Data For our experiments we use the MultextEast parallel corpus (REF) which has been used before for  <scope> multilingual learning </scope>  (GTREF) . 
Dialogue act (DA) tagging is a means of capturing the function of a given utterance relative to an encompassing discourse ,  and has been proposed variously as a means of enhancing <scope> dialogue summarisation  </scope> (TREF) ,  and tracking commitments and promises in email (GREF) ,  as well as being shown to improve speech recognition accuracy (REF) . 
Different methods of dividing the range of values into clusters have been used ,  e . g .  REF used the Lloyd-Max algorithm ,  whilst TREF  <scope> use the simpler Binning method to quantize probabilities ,  and show that the LMs so produced out-perform those produced using the Lloyd-Max method on a phrase-based machine translation task .  </scope>
Due to the absence of adequate training data for intersection ,   <scope> our approach to the task is unsupervised ,  similar to previous work in fusion  </scope> (GTREF) and sentence compression (GREF) . 
Due to the developments of large annotated corpora with predicate-argument and coreference relations (e . g .  , (REF)) and with case frames ,   <scope> several works using statistical models have been proposed to solve these two tasks simultaneously </scope>  (GTREF) . 
Either  <scope> pruning </scope>  (GTREF) or lossy randomizing approaches (REF<input type="hidden" value="0" name="ref[9975][2]"><input type="checkbox" id="cb9975-2%.
Elsewhere  <scope> I argue that FIG points the way to accurate and flexible word choice </scope>  (TREF) ,  producing naturalsounding output for machine translation (REF) ,  and modeling the key aspects of the human language production process (REF) . 
Examples are  <scope> movie reviews </scope>  (TREF) ,  opinions (REF) ,  customer reviews (REF) or multiple aspects of restaurants (REF) . 
Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (REF) ,  the EMU Speech Database System (REF) ,  <scope> the General Architecture for Text Engineering  </scope> (TREF) ,  the Maxent Package for Maximum Entropy Models (REF) ,  and the Annotation Graph Toolkit (REF) . 
Exponential family models are a mainstay of modern statistical modeling (REF) and  <scope> they are widely and successfully used for example in text classification  </scope> (TREF) . 
Extensive frameworks were proposed for iterative discovery of any pre-specified (e . g .  ,  (GREF)) and  <scope> unspecified </scope>  (e . g .  ,  (GTREF))  <scope> relation types </scope>  . 
Few recent attempts on related (though different) tasks were made to classify (REF) and  <scope> label  </scope> (TREF) distributional similarity output using lexical-syntactic patterns ,  in a pipeline architecture . 
Figure 1 :  Example of a Lets Go dialogue ,  from (REF) use an older version of the  <scope> Olympus SDS infrastructure  </scope> (TREF) ,  and the other half uses Olympus II . 
Figure 3 :   <scope> One of the grammar alternatives for a unit The text planner is implemented as a Functional Unification Grammar </scope>  (TREF) in FUF (REF) . 
Finally ,   <scope> some researchers have argued that the Parseval metrics </scope>  (TREF)  <scope> are too forgiving with respect to certain errors and that an evaluation based on syntactic dependencies ,  for which scores are typically lower ,  is a better test of parser performance </scope>  (GREF) . 
Finally ,   <scope> the algorithm </scope>  first described by REF and  <scope> used for data-driven parsing by  </scope> TREF ,   <scope> is complete but has quadratic complexity even in the best case </scope>  . 
Finally ,  we use REF generalization of REF   a standard metric employed for inter-annotator 7The MUC scoring algorithm (TREF) was omitted because it led to an unjustifiably high MUC F-score ( . 920) for the ONE TOPIC baseline . 
Finally ,  we would like to investigate the incorporation of unsupervised methods for WSD ,  such as the heuristically-based methods of (REF) and (REF) ,  <scope> and the theoretically purer bootstrapping method of  </scope> (TREF) . 
First ,   <scope> we seek to go beyond the kind of bag-ofwords features employed in earlier systems </scope>  (GTREF) ,  and attempt to exploit deep semantic features beyond the work of REF . 
First ,  some of the measures make use of WordNet relations (REF) ,  and second ,   <scope> some use the temporal ordering provided by the happens-before relation in VerbOcean  </scope> (TREF) . 
First ,  we considered single sentences as documents ,  and tokens as sentences (we define a token as a sequence of characters delimited by 1In our case ,  the score we seek to globally maximize by dynamic programming is not only taking into account the  <scope> length criteria described in </scope>  (TREF) but also a cognate-based one similar to (REF) . 
Following the relevance formula presented in (REF) ,  we defined 2 feature types :  (1) the most relevant domain ,  and (2) a list of domains above a predefined threshold 3  .  Other experiments using domains from SUMO ,  the EuroWordNet 1 The PoS tagging was performed with the <scope> fnTBL toolkit </scope>  (TREF) . 
<FONT style="BACKGROUND-COLOR :  yellow">Sample similarity in the multi-way sentiment detection setting has previously been considered by using Support Vector Machines (SVMs) in conjunction with a metric labeling metaalgorithm</FONT> (<SPAN id=s45734 . r1 class="trefstyle ref45734">TREF<DIV style="DISPLAY :  none" class="tooltip fixed"><SPAN style="COLOR :  black">Pang and Lee ,  2005</TREF></SPAN></DIV></SPAN><INPUT name=ref[45734][1] value=0 type=hidden><INPUT id=cb45734-1 name=ref[45734][1] value=1 type=checkbox>) ;  by taking a semisupervised graph-based learning approach (<SPAN id=s45734 . r2 class="refstyle ref45734">REF<DIV style="DISPLAY :  none" class="tooltip fixed">Goldberg and Zhu ,  2006</REF></DIV></SPAN><INPUT name=ref[45734][2] value=0 type=hidden><INPUT id=cb45734-2 name=ref[45734][2] value=1 type=checkbox>) ;  and by using optimal stacks of SVMs (<SPAN id=s45734 . r3 class="refstyle ref45734">REF<DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  1065px ;  LEFT :  311px" class="tooltip fixed">Koppel and Schler ,  2006</REF></DIV></SPAN><INPUT name=ref[45734][3] value=0 type=hidden><INPUT id=cb45734-3 name=ref[45734][3] value=1 type=checkbox>) . 
<FONT style="BACKGROUND-COLOR :  yellow">The semantic similarity between words is computed based on Wu and Palmers measure </FONT>(<SPAN id=s57958 . r1 class="trefstyle ref57958">TREF<DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  1669px ;  LEFT :  127px" class="tooltip fixed"><SPAN style="COLOR :  black">Wu and Palmer ,  1994</TREF></SPAN></DIV></SPAN><INPUT name=ref[57958][1] value=0 type=hidden><INPUT id=cb57958-1 name=ref[57958][1] value=1 type=checkbox>) <FONT style="BACKGROUND-COLOR :  yellow">using WordNet </FONT>(<SPAN id=s57958 . r2 class="refstyle ref57958">REF<DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  1669px ;  LEFT :  365px" class="tooltip fixed">Fellbaum ,  1998</REF></DIV></SPAN><INPUT name=ref[57958][2] value=0 type=hidden><INPUT id=cb57958-2 name=ref[57958][2] value=1 type=checkbox>) . 1 The similarity between contiguous sentences will be used to capture the dependency for CRFs . 
For core roles ,  only agent and patient are consistently defined across different predicates ,  e . g .   <scope> in the popular PropBank </scope>  (TREF) and the derived version evaluated in the REF and 2005 shared tasks ,  as ARG0 and ARG1 . 
For determining whether an opinion sentence is positive or negative ,  we have used seed words similar to those produced by (REF) and  <scope> extended them to construct a much larger set of semantically oriented words with a method similar to that proposed by </scope>  (TREF) . 
For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (REF) and sections 1-270 of the  <scope> Chinese Treebank (CTB) </scope>  (TREF) respectively . 
For evaluation we use a state-of-the-art baseline system (Moses) (REF)  <scope> which works with a log-linear interpolation of feature functions optimized by MERT </scope>  (TREF) . 
For example ,  for constituent syntactic parsing (GREF) in Chinese ,   <scope> in addition to the most popular treebank Chinese Treebank (CTB) </scope>  (TREF) ,   <scope> there are also other treebanks  </scope> such as Tsinghua Chinese Treebank (TCT) (REF) . 
For example ,  hierarchical (REF) and  <scope> syntax-based </scope>  (TREF)  <scope> systems have recently improved in both accuracy and scalability .  </scope>
For example ,  in psychology ,  REF report that psychologists have largely abandoned synonymy in favour of semantic similarity .   <scope> In addition ,  work in automatic lexical acquisition is based on the proposition that distributional similarity correlates with semantic similarity </scope>  (GTREF) . 
For example ,  in this work we use  <scope> loglikelihood ratio  </scope> (TREF) to determine the SoA between a word sense and co-occurring words ,  and cosine to determine the distance between two DPWSs log likelihood vectors (REF) . 
For example ,  <scope> for constituent syntactic parsing </scope>  (GTREF) in Chinese ,  in addition to the most popular treebank Chinese Treebank (CTB) (REF) ,  there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (REF) . 
For example ,   <scope> previous work has addressed low-level tasks such as tagging a free-style corpus with </scope>  part-of-speech information (REF) ,   <scope> aligning a bilingual corpus </scope>  (GTREF) ,  and producing a list of collocations (REF) . 
For example ,   <scope> we can infer the number of nonterminals with a nonparametric Bayesian model  </scope> (TREF) ,  infer the model more robustly based on a Markov chain Monte Carlo inference (REF) ,  and use probabilistic grammar models other than PCFGs . 
For example ,  some researchers extract paraphrases from multiple translations of the same foreign novel (GREF) ,  while some  <scope> others make use of comparable news articles that report on the same event within a small time interval </scope>  (GTREF) . 
For example ,  the constrained optimization method of (REF) relies on approximations of sensitivity (which they call CA) and specificity2 (their CR) ;   <scope> related techniques </scope>  (GTREF)  <scope> rely on approximations of true positives ,  false positives ,  and false negatives ,  and ,  indirectly ,  recall and precision </scope>  . 
For example ,  the tree-based model proposed in (REF) used a phrasal decoder for sub-clause translation ,  and recently ,  TREF <scope> reduced a TAGbased translation model to a CFG-based model by applying all possible adjunction operations offline and stored the results as rules ,  which were then used by an existing syntax-based decoder .  </scope>
For example ,  [TREF] and [REF]  <scope> describe both plan-based methods ,  where a vague and partial description is produced initially ,  which is narrowed and ultimately confirmed in the subsequent discourse .  </scope>
For example ,  TREF <scope> apply the Lemur IR toolkit </scope>  ,  REF use the BM25 similarity measure ,  and REF use cosine similarity . 
For example ,  TREF  <scope> developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (REF) .  </scope>
For example TREF  <scope> use a maximum entropy classifier to build a model of correct preposition usage ,  with 7 million instances in their training set </scope>  ,  and REF use memory-based learning ,  with 10 million sentences in their training set . 
For example ,  two high-accuracy systems are those described in (TREF) ,   <scope> achieving 60 . 4% accuracy with no task-specific information </scope>  ,  and (REF) ,  which achieves 61 . 2% task-dependent accuracy ,  i . e .  when able to use the specific task labels as input . 
For example ,  unsupervised systems of (REF) and (REF) leverage word distributions in general and/or word-specific corpus for detecting erroneous usages while (REF) and (TREF)  <scope> use Web as a corpus </scope>  . 
For instance ,  BLEU and ROUGE (REF) are based on n-gram precisions ,   <scope> METEOR </scope>  (TREF) and STM (REF)  <scope> use word-class or structural information </scope>  ,  REF leverages on paraphrases ,  and TER (REF) uses edit-distances . 
For instance ,  the classical Dale and Reiter algorithms compute purely conjunctive formulas ;  van REF extends this language by adding the other propositional connectives ,  whereas TREF  <scope> extends it by allowing existential quantification .  </scope>
For nominals ,  the best semantic F1 score was 0 . 7664 (REF) ;  however  <scope> this score is not directly comparable to the NomBank SRL results of </scope>  TREF <scope> or the results in this paper due to a focus on different aspects of the problem (see the end of section 5 . 2 for details) .  </scope>
For  <scope> German to English </scope>  (TREF) and Dutch to English (REF) this reordering involves moving some long-distance dependencies closer together ,  such as clause-final participles and verb-second auxiliaries . 
For the MER training (TREF) ,  <scope> Koehns MER trainer </scope>  (REF)  <scope> is modified for our system .  </scope>
For this purpose ,  (REF) adopted a variable memory Markov model proposed by (REF) ,  (REF) applied selective extension of features to POS tagging ,  and (TREF)  <scope> expanded context of D2-gram models to find errors in syntactic anno634 tation .  </scope>
Fortunately ,  the state of the art in broad-coverage (REF) and  <scope> unsupervised (TREF) dependency parsing allows us to treat dependency parsing merely as a preprocessing step .  </scope>
FrameNethas ashorter historyin NLPapplications thanWordNet ,  butlately more and more researchers have been demonstrating its potential to improve the quality of question answering (TREF) and recognizing textual entailment (REF) . 
FrameNethas ashorter historyin NLPapplications thanWordNet ,  butlately more and more researchers have been <FONT style="BACKGROUND-COLOR :  yellow">demonstrating its potential to improve the quality of question answering</FONT> (<SPAN id=s9718 . r1 class="trefstyle ref9718">TREF<DIV style="DISPLAY :  none" class="tooltip fixed"><SPAN style="COLOR :  black"><TREF>Shen and Lapata ,  2007</TREF></SPAN></DIV><DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  844px ;  LEFT :  209px" class="tooltip fixed"><SPAN style="COLOR :  black">Shen and Lapata ,  2007</TREF></SPAN></DIV></SPAN><INPUT name=ref[9718][1] value=0 type=hidden><INPUT id=cb9718-1 name=ref[9718][1] value=1 CHECKED type=checkbox>) and recognizing textual entailment (<SPAN id=s9718 . r2 class="refstyle ref9718">REF<DIV style="DISPLAY :  none" class="tooltip fixed"><REF>Burchardt et al .  ,  2009</REF></DIV><DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  844px ;  LEFT :  593px" class="tooltip fixed">Burchardt et al .  ,  2009</REF></DIV></SPAN><INPUT name=ref[9718][2] value=0 type=hidden><INPUT id=cb9718-2 name=ref[9718][2] value=1 CHECKED type=checkbox>) . 
F <scope> or the automatic detection of the semantic relations encoded by genitives ,  the boundary constructed by the Semantic Scattering model is more apppropriate than a tree cut ,  like the ones used for verb disambiguation </scope>  (TREF) (REF) and constructed using the Minimum Description Length model (REF) . 
Generally ,   <scope> constraints were used to support syntactic disambiguation </scope>  (TREF) or to generate acceptable sentences (REF) . 
Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (GREF) ,  but recently discriminative models attract more attention due to their superior accuracy (GREF) and  <scope> adaptability to new grammars and languages  </scope> (TREF) . 
Gildea and Jurafsky (TREF)  <scope> describe a statistical approach for semantic role labelling using data collected from FrameNet by analysing a number of features such as phrase type ,  grammatical function ,  position in the sentence </scope>  ,  etc .  Shi and Mihalcea (REF) propose a rule-based approach for semantic parsing using FrameNet and WordNet . 
GIZA +  + /RW  <scope> As a second baseline ,  we used an off-the-shelf statistical MT system ,  consisting of the ISI ReWrite Decoder  </scope> (TREF) together with a translation model produced by GIZA +  +  (REF) and a language model produced by the CMU Statistical Language Modeling Toolkit (REF) . 
(GREF)) <scope> and are used in the Charniak </scope>  (TREF) ,  Berkeley (REF) and Stanford (REF) parsers ,  as well as in the parser used for the experiments in this paper ,  an in-house implementation of the Berkeley parser . 
[GREF]) ,   <scope> training on a corpus of one type and then applying the tagger to a corpus of a different type usually results in a tagger with low accuracy </scope>  [TREF] . 
GREFuse the web to generate corpora for languages where electronic resources are scarce ,  while TREF  <scope> describes a method for mining the web for bilingual texts .  </scope>
(GTREF)  <scope> apply fuzzy techniques for integrating source syntax into hierarchical phrasebased systems </scope>  (REF) . 
(GTREF)  <scope> differs from earlier work in that a substantially larger set of SF types are considered </scope>  ;  (REF) use an EM algorithm to learn subcategorization as a result of learning rule probabilities ,  and ,  in tnrn ,  to improve parsing accuracy by applying the verb SFs obtained . 
(GTREF)  <scope> have been developed using PCKimmo </scope>  (REF) ,   <scope> a morphological parser based on the two-level model </scope>  . 
(GTREF)  <scope> have successfully applied DAP for the learning of hyponyms and hypernyms of is-a relations and report improvements over (REF) and (REF) .  </scope>
Hand-tuned weights were used in (REF) and  <scope> a linear classifier was used in  </scope> (TREF) to combine the extracted features . 
Here are some examples in the literature of mismatched comparisons :   TREF  <scope> concluded their system significantly outperformed that of REF .  </scope>
However ,   <scope> generative models make complicated and poorly justified independence assumptions and estimations ,  so we might expect better performance from discriminatively trained models ,  as has been shown for other tasks like </scope>  document classification (REF) and  <scope> shallow parsing </scope>  (TREF) . 
However ,   <scope> most previous work has mainly focused on </scope>  reviews (GREF) ,   <scope> news resources </scope>  (TREF) ,  and multi-domain adaptation (GREF) . 
However ,  this is a notoriously hard problem (REF) :  Task-based evaluations with human experimental subjects are time-consuming and expensive ,  and  <scope> corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the systems output is inferior </scope>  (TREF) . 
IBM constraints (REF) ,  lexical word reordering model (REF) ,  and  <scope> inversion transduction grammar (ITG) constraints </scope>  (GTREF) belong to this type of approach . 
IBM model 3 ,  4 ,  5 (REF) and TREF  <scope> are another kind of related works that allow 1-to-n alignments ,  but they rarely questioned if such alignments exist in word units level ,  that is ,  they rarely questioned word as basic translational unit .  </scope>
IE systems typically focus on  <scope> information about events that are relevant to a specific domain ,  such as </scope>  terrorism (GREF) ,   <scope> management succession </scope>  (GTREF) ,  or job announcements (GREF) . 
In addition ,  REF used homophones (e . g .   ,  cellar~seller) and TREF created homographs by stripping accents from French and Spanish words . 
In addition ,   <scope> we investigate some models </scope>  of grammaticality (REF) and  <scope> coherence </scope>  (GTREF) f <scope> rom prior work </scope>  (Sections 3 . 6 to 3 . 9) . 
In addition ,  several other methods have used Wordnet (REF) for connecting  <scope> semantically related words  </scope> (GTREF) . 
In addition to their use in machine translation (GREF) ,   <scope> translation models can be applied to machineassisted translation  </scope> (GTREF) ,  cross-lingual information retrieval (SIGIR ,  1996) ,  and gisting of World Wide Web pages (REF) . 
In addition to the MT-based method ,  researchers have also investigated other methods for paraphrase generation ,  such as  <scope> the pattern-based methods </scope>  (GTREF) ,  thesaurus-based methods (GREF) ,  and NLG-based methods (GREF) . 
In addition ,  various researchers have explored the use of hard linguistic constraints on the source side ,  e . g .  via  chunking noun phrases and translating them separately (REF) ,  or by  <scope> performing hard reorderings of source parse trees in order to more closely approximate target-language word order </scope>  (GTREF) . 
In bilingual terminology mining from specialized comparable corpora ,  the terminology reference lists are often composed of a hundred words (180 SWTs in (REF) and  <scope> 97 SWTs in </scope>  (TREF)) . 
In consequence ,  agent/patient nominals tend to realise fewer arguments  the average in FrameNet is 1 . 46 arguments ,  compared to 1 . 74 PropBank Verbs (Carreras and M`arquez ,  2005) 80% Nouns (REF) 73% FrameNet Verbs (REF)  <scope> 72% Nouns </scope>  (TREF) 64% Table 1 :  F-Scores for supervised SRL (end-to-end) for events/results . 
In contrast ,  the work of (REF) and (TREF)  <scope> directly sets as a goal the assessment of sentence-level fluency ,  regardless of content .  </scope>
In contrast to the approach presented in this section ,  the work described in (REF) is set in <scope> an untyped feature system using DATR to define inheritance networks with path-value equations  </scope> (TREF) . 
In fact ,  in this paper ,  we use clusters generated with the REF dataset ,  a superset of  <scope> the CoNLL03 NER dataset </scope>  (GTREF) . 
In general ,  dependency parsers (GREF) ,   <scope> seem to suffer more from this domain change than constituency parsers </scope>  (GTREF) . 
In Heemans POS language model (REF) ,  <scope> the joint probability of word sequence and associated POS sequence was estimated directly ,  which has been demonstrated to be superior to the conditional probability previously used in the class-based models  </scope> (TREF) . 
In order to overcome this ,   <scope> several methods are proposed ,  including minimally-supervised learning methods  </scope> (e . g .   ,  (GTREF)) ,  and active learning methods (e . g .   ,  (GREF)) . 
In order to select this subset of derivations ,  two alternatives have been considered :  froln  <scope> structural information content in a bracketed corpus </scope>  (GTREF) ,  and from statistical information content in the kbest derivations (REF) . 
In particular ,  generation from grammars with recursions whose well-foundedness relies on lexical information will terminate ;  <scope> top-down generation regimes such as those of  </scope> TREF or REF <scope> lack this property </scope>  ;  further discussion can be found in Section 2 . 1 . 
In particular ,  we test different variants of the classical A* algorithm (REF) ,   <scope> which has met with success in Penn Treebank parsing with context-free grammars </scope>  (GTREF) . 
In particular ,  we use variants of existing search optimization (REF) and ranking algorithms (REF) to train our run-time component to find good outputs within a specified time window ;   <scope> see also  </scope> (GTREF) . 
In previous studies ,  <scope> seed lexicons vary between 16 , 000  </scope> (REF)  <scope> and 65 , 000 </scope>  (REF)  <scope> entries ,  a typical size being around 20 , 000 </scope>  (GTREF) . 
In principle ,  the comparison can be done via coselection of extracted sentences (GREF) ,  by  <scope> string-based surface measures </scope>  (GTREF) ,  or by subjective judgements of the amount of information overlap (DUC ,  2002) . 
In (REF) ,  it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models ,  measuring  <scope> compositionally built sentence vectors against a benchmark dataset such as that provided by  </scope> TREF<div class="tooltip fixed active" style="left :  901px ;  top :  9649px ;  display :  block ;  "><span style="color : black"><tref>Mitchell and Lapata (2008)</tref></div></span> . 
In (REF) ,   <scope> different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA +  +  </scope>  (TREF) . 
In (REF) ,   <scope> the authors proposed a method to integrate the IBM translation model 2 </scope>  (TREF)  <scope> with an ASR system .  </scope>
In (REF) ,   <scope> we established that the task is not trivially transferable to Hebrew ,  but reported that SVM based chunking </scope>  (TREF)  <scope> performs well .  </scope>
In short ,  we believe REF only intended that tense and aspect could signal discourse segment boundaries ,  and thus indirectly influence pronoun interpretation ,  while TREF  <scope> claims that a change in time scale ,  aspectual class ,  or other temporal characteristics could signal a new discourse segment ,  and thus ,  indirectly influence pronoun interpretation .  </scope>
Instead of manually selecting what goes into the history ,  as is usually done ( <scope> see </scope>  (TREF)  <scope> for an exception </scope> ) ,  we manually select how to linguistically meaningfully map Treebank structures onto dependency relations by the use of mapping patterns adapted from (REF) . 
Instead ,   <scope> they choose to rely on the intuition of annotators </scope>  (Rens Blog Emotion Corpus ,  RBEC ,  TREF) or authors (Mishnes blog emotion corpus ,  REF) . 
In structural example-based machine translation systems (GREF) ,  examples in the knowledge base are normally annotated with their constituency (REF) or  <scope> dependency structures  </scope> (GTREF) ,  which allows the corresponding relations between source and target sentences to be established at the structural level . 
In the aspect of decoding ,  beam-search (GTREF) and partial dynamic-programming (REF<input type="hidden" value="0" name="ref[11917][2]"><input type="checkbox" id="cb11917-2" name="ref[11917][2]".
In the context of headline generation ,  simple statistical models are used for aligning documents and headlines (GREF) ,  based on <scope> IBM Model 1 </scope>  (TREF) . 
In the future works ,  aiming to analyze the rule contributions and the redundances issues using the presented rule classification based on some real translation systems ,   <scope> we plan to implement some synchronous grammar based syntax translation models such as the one presented in (REF) or in </scope>  (TREF) . 
In the latter case ,  the correctness of the final dependency tree is ensured by :  (a) selecting entire trees proposed by the base parsers (REF) ;  or (b) <scope> re-parsing the pool of dependencies proposed by the base models  </scope> (TREF) . 
In the same spirit and facilitated by the release of the REF Task 18 data1 ,  based on the Pilot Arabic Propbank ,   <scope> a preliminary SRL system exists for Arabic2 </scope>  (GTREF) . 
In the same year ,  REF described a project aimed directly at the extraction of general world knowledge from Treebank text ,  and TREF  <scope> providedthe results of hand-assessment of the resulting propositions .  </scope>
In the tradition of REF ,   <scope> a characterization of the power of STAGs in terms of bimorphims was developed by </scope>  TREF . 
In this paper ,  we only used bigram-subtrees and the limited form of trigram-subtrees ,  though in theory we can use k-gram-subtrees ,  which are limited in the same way as our trigram subtrees ,  in (k-1)th-order MST parsing models mentioned in REF or use <scope> grandparenttype trigram-subtrees in parsing models of  </scope> TREF<div class="tooltip fixed active" style="left :  1400 . 5px ;  top :  12552px ;  display :  block ;  "><span style="color : black"><tref>Carreras (2007)</tref></div></span> . 
In this perspective ,  String Kernel (SK) proposed in (REF) and  <scope> the Syntactic Tree Kernel (STK) </scope>  (TREF)  <scope> allow for modeling structured data in high dimensional spaces .  </scope>
In this section ,   <scope> we propose a new greedy algorithm which can be seen as an extension of </scope>  (REF) and (TREF)  <scope> algorithms </scope>  as it allows polysemous words to belong to different chains thus breaking the one-word/oneconcept per document paradigm (REF) . 
In (TREF) ,  (REF) the significance of an association (x , y) is measured by the mutual information I(x , y) ,  i . e .  the probability of observing x and y together ,  compared with the probability of observing x and y independently . 
It can not only maintain the strength of phrase translation in traditional phrase-based models (GREF) ,  but also  <scope> characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models  </scope> (GTREF) . 
It combines online Peceptron learning (REF) with a  <scope> parsing model based on the Eisner algorithm  </scope> (TREF) ,  extended so as to jointly assign syntactic and semantic labels . 
It consists of the second order parsing algorithm of REF ,   <scope> the non-projective approximation algorithm </scope>  (TREF) ,  the passiveaggressive support vector machine ,  and a feature extraction component . 
It has been used in a variety of difficult classification tasks such as part-of-speech tagging (REF) ,  prepositional phrase attachment (REF) and  <scope> named entity tagging </scope>  (TREF) ,  and achieves state of the art performance . 
It has inspired many  <scope> discriminative approaches to the compression task </scope>  (GTREF) and has been extended to languages other than English (see REF) . 
It is unrealistic to compute the partition factor of the formula directly ;   <scope> therefore ,  the factor has been </scope>  computed by dynamic programing (GREF) or  <scope> approximated by the n-best list of highly probable alignments </scope>  (GTREF) . 
Its core modules were updated as well :   <scope> Minipar was replaced with the Charniak-Johnson reranking parser </scope>  (TREF) ,  Named Entity identification was added ,  and the BE extraction is conducted usingasetofTregexrules(REF) . 
Joint n-gram models (GREF) have been widely applied to string  <scope> transduction problems  </scope> (GTREF) . 
Kennedy and REF describe an  <scope> algorithm for anaphora resolution based on  </scope> TREF  <scope> approach but without employing deep syntactic parsing .  </scope>
Language modeling (REF) ,  noun-clustering (REF) ,   <scope> constructing syntactic rules for SMT  </scope> (TREF) ,  and finding analogies (REF)  <scope> are examples of some of the problems where we need to compute relative frequencies .  </scope>
Lastly ,   <scope> rule composition and different amounts of lexicalization </scope>  (GTREF) or context modeling (REF)  <scope> have been successful with other models .  </scope>
Lexical chains have been used as an intermediate representation of text for various tasks such as au111 tomatic text summarisation (GREF) ,  malapropism detection and correction (Hirst and St-REF) ,  and  <scope> hypertext construction  </scope> (TREF) . 
Lexical cohesion (REF) forms the backbone of many verbal segmentation algorithms ,   <scope> on the theory that segmentation boundaries should be placed where the distribution of words changes </scope>  (TREF) . 
Lexicalization can increase parsing performance dramatically for English (GREF) ,  and <scope> the lexicalized model proposed by </scope>  REF  <scope> has been successfully applied to </scope>  Czech (REF) and  <scope> Chinese </scope>  (TREF) . 
Lightweight linguistic approaches such as METEOR (TREF) ,  MaxSim (REF) ,  wpF and wpBleu (REF)  <scope> exploit a limited range of linguistic information that is relatively cheap to acquire and to compute ,  including lemmatization ,  part-ofspeech (POS) tagging ,  and synonym dictionaries .  </scope>
Like Construe/TIS [REF] ,  the work derives from ,  and coordinates with ,  NLP efforts ,  but  <scope> the system primarily uses a lexicosemantic pattern matcher for categorization </scope>  [TREF] . 
Like the Penn English Discourse Treebank (GREF) ,  the  <scope> CDTB project adopts the general idea presented in </scope>  (GTREF)  <scope> where discourse connectives are considered to be predicates that take abstract objects such as propositions ,  events and situations as their arguments .  </scope>
Links are generally allowed only in a same level or between two adjacent levels ,  like in hypertexts (REF) made of three specified levels (documents ,  terms ,  concepts) ,  or  <scope> in Multi-Level Association Graphs </scope>  (TREF)  <scope> in which there is no constraint on the number of levels .  </scope>
LT-XML (REF) ,   <scope> XCES  </scope> (TREF) and many other are examples for XML-based or XML-supporting software architectures for natural language processing . 
Many machine learning techniques have been successfully applied to chunking tasks ,  such as Regularized Winnow (REF) ,  SVMs (REF) ,  CRFs (REF) ,   <scope> Maximum Entropy Model  </scope> (TREF) ,  Memory Based Learning (REF) and SNoW (REF) . 
Many other complexity metrics have beensuggestedasmutuallycontributingtoreading difficulty ;  for example ,  entropy reduction (REF) ,  bigram probabilities (REF) ,  and  <scope> split-syntactic/lexical versions of other metrics </scope>  (TREF) . 
Many research groups are making progress toward efficient customization ,  such as BBN (REF) ,  NYU (REF) ,  SRI (REF< ! -- <scope> UMass  </scope> (TREF)etc .  SRI  <scope> developed a specification language called FASTSPEC that automatically translates regular productions  </scope> written by the developer into finite state machines (REF< ! --<span class='refstyle ref31161' id='s31161 . r9'-->REF&gt ; ) . 
Maximum entropy estimation for translation of individual words dates back to REF ,  and  <scope> the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroducedundertherubricofwordsensedisambiguation and generalized to substrings  </scope> (GTREF) . 
McDonald has even argued for extending the model to a large number of components (REF) ,  and  <scope> several systems have indeed added an additional component between the planner and the linguistic component  </scope> (GTREF) . 
Mention heads are determined by parsing the given mention span with the Stanford parser (REF) and using the Collins head rules (REF) ;  TREF  <scope> showed that using syntactic heads strongly outperformed a simple rightmost headword rule .  </scope>
Meteor  ,  as well as several other proposed metrics such as GTM (REF) ,  TER (REF) and <scope> CDER  </scope> (TREF)  <scope> aim to address some of these weaknesses </scope>  . 
Metrics  <scope> All experiments are evaluated in terms of the commonly-used </scope>  Pk (REF) and  <scope> WindowDiff (WD) </scope>  (TREF)  <scope> scores </scope>  . 
Minimum error rate training (REF) was run on each system afterwards and  <scope> the BLEU score </scope>  (TREF)  <scope> was calculated on the test set .  </scope>
Moreover ,   <scope> several methods using a latent topic model have been proposed </scope>  (REF ;  Arora and Ravin30 dran ,  2008 ;  GTREF) . 
Moreover ,  the methodology does not depend on the way candidate correspondence points are generated ,  i . e .  <scope> although we used homographs with equal frequencies ,  we could have also bootstrapped the process using cognates (Michel REF) or a small bilingual lexicon to identify equivalents of words or expressions  </scope> (TREF ;  Pascale Fung and GREF) . 
More recently ,  however ,  TREF  <scope> showed that a  1 Conditional maximum entropy models (REF) provide somewhat of a counter-example ,  but there ,  too ,  many kinds of global and non-local features are difficult to use (REF) .  </scope>
More recently ,  REF  <scope> proposed an algorithm for anaphora resolution that is actually a modified and extended version of the one developed by </scope>  TREF . 
More recent research are concerned with capturing structural similarity between conjuncts using thesauri and corpora (REF) ,  or  <scope> web-based statistics  </scope> (TREF) . 
More recent work includes REF who combines distributional and morphological information ,  and TREF  <scope> who uses a hidden Marcov model in combination with co-clustering .  </scope>
Moses provides BLEU (K . REF) and NIST (REF) ,  but  <scope> Meteor </scope>  (GTREF) and TER (REF)  <scope> can easily be used instead .  </scope>
Most of the previous work on statistical machine translation ,  as exemplified in (REF) ,  employs  <scope> wordalignment algorithm (such as GIZA +  +  </scope>  (TREF)) <scope> that provides local associations between source words and target words .  </scope>
Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (REF) or  <scope> word-level system combination  </scope> (GTREF) . 
Most semi-automated approaches have met with limited success (REF)  <scope> and supervised learning models have tended to outperform dictionary-based classi cation schemes </scope>  (TREF) . 
Mutual information has been positively used in  <scope> many NLP tasks such as collocation analysis </scope>  (TREF) ,  terminology extraction (REF) ,  and word sense disambiguation (REF) . 
Not limited to parsing ,   <scope> supertags can be used for </scope>  NP chunking (REF) ,  <scope> semantic role labeling </scope>  (TREF) and machine translation (GREF)  <scope> to explore rich syntactic information contained in them .  </scope>
One approach for compactly increasing capacity is to automatically induce intermediate features through the composition of non-linearities ,  for example SVMs with a non-linear kernel (REF) ,  inducing compound features in a CRF (REF) ,   <scope> neural networks  </scope> (GTREF) ,  and boosting decision trees (REF) . 
One can imagine the same techniques coupled with more informative probability distributions ,  such as lexicalized PCFGs (REF) ,  or even grammars not based upon literal rules ,  but  <scope> probability distributions that describe how rules are built up from smaller components </scope>  (GTREF) . 
One exception is Marcus work (REF) (see also TREF  <scope> for constructing discourse structures for individual sentences </scope> ) . 
One is to find unknown words from corpora and put them into a dictionary (e . g .   ,  (REF)) ,  and  <scope> the other is to estimate a model that can identify unknown words correctly (e . g .  </scope>   ,  (GTREF)) . 
One is to find unknown words from corpora and put them into a dictionary (e . g .   ,  (REF)) ,  and  <scope> the other is to estimate a model that can identify unknown words correctly </scope>  (e . g .   ,  (GTREF)) . 
One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (REF) ,   <scope> MIRA for dependency parsing </scope>  (TREF) ,  exponentiated gradient algorithms (REF) ,  stochastic gradient for constituency parsing (REF) ,  just to name a few . 
On newswire data the (REF) system had an F score of 54 . 7 and the (TREF) <scope> system had an F score of 50 . 1 .  </scope>
OpinionFinder (REF) (Version 1 . 4)  :   <scope> We used </scope>  the  + /labels assigned by its contextual polarity classifier (REF) to create  + /states and  <scope> the MPQASD tags produced by its Direct Subjective and Speech Event Identifier </scope>  (TREF) <scope> to produce mental (M) states .  </scope>
Other NLG systems that extract and summarise information have been developed in other research ,  particularly by CoGenTex ;   <scope> their systems include ,  for example ,  FOG  </scope> (TREF) ,  which produced weather reports ;  LFS (REF) ,  which summarised employment statistics ;  and Joyce (REF) ,  which summarised software designs from a security perspective . 
Other work has looked at  <scope> techniques for learning phrasal patterns likely to contain slot fillers </scope>  (GTREF) or contain information semantically similar to a set of seed examples (REF) . 
Our approach focuses on the strong influence of domain for WSD (REF) and <scope> the benefits of focusing on words salient to the domain  </scope> (TREF) . 
Our data preparation follows (REF) :  the training data is a parallel corpus of 28 . 3M words on the English side ,  from which  <scope> we extracted 24 . 7M tree-to-string rules using the algorithm of </scope>  (TREF) ,  and trained a Chinese trigram model on the Chinese side . 
Our model considers two sets of features :  Feature Set 1 (FS1) :   <scope> features used in the work reported in </scope>  (TREF) and (REF)  ;  and Feature Set 2 (FS2) :  a novel set of features introduced in this paper . 
Our work goes beyond the simple co-occurrence features (REF) and the  <scope> limited extracted information (e . g .  biographical information in </scope>  (TREF)  <scope> that is relatively scarce in web data </scope> ) using the broad range of relational information with the support of information extraction tools . 
Parsing is performed using the usual pipeline approach ,  first with the TreeTagger analyzer (REF) and then with a  <scope> state-of-the-art dependency parser </scope>  (TREF) . 
Part-of-speech information was incorporated in ve systems :  four of them utilized domainspecialized part-of-speech taggers (GREF) and  <scope> the other utilized generalpurpose taggers ( </scope> TREF) . 
People have proposed different syntactic rules to pre-reorder SOV languages ,  either based on a constituent parse tree (GTREF) or dependency parse tree (REF) . 
plus higherlevel shorthand languages such as lexc (<SPAN id=s59169 . r1 class="refstyle ref59169">REF<DIV style="DISPLAY :  none" class="tooltip fixed">Karttunen ,  1993</REF></DIV></SPAN><INPUT name=ref[59169][1] value=0 type=hidden><INPUT id=cb59169-1 name=ref[59169][1] value=1 type=checkbox>) ,  twolc (<SPAN id=s59169 . r2 class="refstyle ref59169">REF<DIV style="DISPLAY :  none" class="tooltip fixed">Karttunen and Beesley ,  1992</REF></DIV></SPAN><INPUT name=ref[59169][2] value=0 type=hidden><INPUT id=cb59169-2 name=ref[59169][2] value=1 type=checkbox>) and <FONT style="BACKGROUND-COLOR :  yellow">Replace Rules </FONT>(<SPAN id=s59169 . r3 class="trefstyle ref59169">GTREF<DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  1376px ;  LEFT :  -53px" class="tooltip fixed">Karttunen ,  1995</REF> ;  <REF>Karttunen and Kempe ,  1995</REF> ;  <SPAN style="COLOR :  black"><TREF>Karttunen ,  1996</TREF></SPAN> ;  </DIV></SPAN><INPUT name=ref[59169][3] value=0 type=hidden><INPUT id=cb59169-3 name=ref[59169][3] value=1 type=checkbox>) . 
P means precision ,  R means recall and F is the F1-measure (all is in % percentage metrics) ;  Charniak represents the parser of (REF) ,   <scope> Berkeley represents the parser of </scope>  (TREF) ,  Comb .  represents the combination of the two parsers . 
Previous approaches ,  e . g .  ,  (REF) and (REF) ,  have all used the  <scope> Brown algorithm for clustering  </scope> (TREF) . 
P <scope> revious research has shown that RST trees </scope>  can play a crucial role in building natural language generation systems (GREF) and text summarization systems (REF) ;   <scope> can be used to increase the naturalness of machine translation outputs </scope>  (TREF) ;  and can be used to build essayscoring systems that provide students with discourse-based feedback (REF) . 
P <scope> revious work has generally relied on two approaches to representation </scope>  :  explicitly handcrafted features (e . g .   ,  in REF) or  <scope> features defined through kernels </scope>  (e . g .   ,  see TREF) . 
Recent work has explored incorporating eye gaze into automated language understanding such as  <scope> automated speech recognition </scope>  (GTREF) ,  automated vocabulary acquisition (GREF) ,  attention prediction (GREF) . 
(REF) . 2 From the word-to-word alignments ,  the system extracts a phrase table (REF) and <scope> hierarchical reordering model  </scope> (TREF) . 
(REF) achieves the best result in the AS corpus ,  and (TREF)  <scope> performs best in the remaining three corpora .  </scope>
REF also submitted two contrastive metrics :   <scope> bleusp4114 ,  a modified version of BLEU-S </scope>  (TREF) <scope>  ,  with tuned n-gram weights </scope>  ,  and bleusp ,  with constant weights . 
REF and ,  more or less ,  TREF ,   <scope> suggest that the property set for an entity includes all the relations in which it is involved (even non unary ones) ,  and no others .  </scope>
(REF) and (TREF)  <scope> address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A[YZ] one would compute the acceptability of A[XZ] .  </scope>
REF approximated by ignoring all sentences of length greater than some maximum ,   <scope> and the contrastive estimation </scope>  of TREF  <scope> approximates the partition function with a set E c onomic discr e pancies </scope>  A N ar e V gr owing V Figure 2 :  Monolingual MRF tag model (REF) of automatically distorted training examples which are compactly represented in WFSTs . 
REF describe <scope> a non-deterministic implementation to the dependency parser outlined by  </scope> TREF ,   <scope> where they apply an n-best beam search strategy .  </scope>
(REF) describe three error-checking measures used in the construction of NomBank ,  and  <scope> the use of inter-annotator agreement as a quality control measure for corpus construction is discussed at some length in  </scope> (GTREF) . 
REF developed three different approaches to the problem that use counting methods and clustering algorithms ,  and TREF  <scope> expands upon Shaw and Hatzivassiloglous work .  </scope>
REF employ a contrastive estimation tech1 <scope> As </scope>  (TREF)  <scope> point out ,  unsupervised tagging accuracy varies wildly depending on the dictionary employed .  </scope>
REF extended the  <scope> weakly supervised machine learning methodology of </scope>  TREF by applying feature selection to reduce the number of candidate keywords ,  by using limited manual supervision to filter the features ,  and by extending the feature representation with bigrams and trigrams . 
REF have also shown that  <scope> there is a correlation between cohesion-defined textual segments and hierarchical ,  intentionally defined segments </scope>  (TREF) . 
REF have showed that WordNet-based approaches do not always outperform simple frequency-based models ,  and a number of techniques have been recently proposed which may offer ideas for refining our  <scope> current unsupervised approach  </scope> (GTREF) . 
REF have showed that WordNet-based approaches do not always outperform simple frequency-based models ,  and  <scope> a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach </scope>  (GTREF) . 
REF improve parsing accuracy for MaltParser by projectivizing training data and applying an inverse transformation to the output of the parser ,   <scope> while  </scope>  <scope> REF </scope>  <scope> apply post-processing to the output of Charniaks parser </scope>  (TREF) . 
REF ,  in contrast ,  used data which was explicitly created for his experiments ;  TREF  <scope> used texts which subjects had written for a previous experiment </scope>  ;  and REF used both data and texts that were created for his experiments . 
REF ,  in his parse-reranking experiments ,  used his Model 2 parser (REF) with a beam width of 103 together with a cell limit of 100 to obtain k-best lists ;  the average number of parses obtained per sentence was 29 . 2 ,  the maximum ,  101 . 7 TREF  <scope> use coarse-tone parsing on top of the </scope>  REF  <scope> parser and get 50-best lists for section 23 </scope>  . 
REF manually adds positive/negative arguing information to entries in a  <scope> sentiment lexicon from  </scope> (TREF) and uses these as arguing features . 
REF ,  Pang and Lee extended their earlier work in (REF) to determine a reviewers evaluation with respect to  <scope> multiscales  </scope> (TREF) . 
REF parser uses a simple PCFG to prune the chart for a richer model ;   <scope> and TREF added a discriminatively trained reranker to the end of that pipeline .  </scope>
REF proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass .  TREF <scope> added additional model terms to balance the cost of long and short derivations in a syntactic alignment model .  </scope>
REF provided examples of these domains ,  for instance ,  <scope> meeting  </scope> (TREF) and flight simulation control (REF) . 
REF reported Kappa agreement scores of between 0 . 11 and 0 . 35 across 6 annotators ,  TREF  <scope> reported 0 . 38 on telephone conversation and 0 . 37 on lecture speech ,  using 3 annotators ,  </scope>  and REF reported 0 . 32 on meeting data . 
REF reported that a strong trend exists recently in applying  <scope> machine learning (ML) techniques such as Support Vector Machine (SVM)  </scope> (GTREF) and Conditional Random Field (CRF) (REF) to NER ,  which can address these issues . 
REF reports positive results ,  TREF  <scope> argue that AL based on uncertainty sampling may face serious performance degradation when labeled data is reused for training a classifier different from the one employed during AL </scope>  .  For committee-based AL ,  however ,  there is a lack of work on reusability . 
REF  <scope> adopted a simple translation model from IBM model 1 </scope>  (GTREF) and applied it to QA . 
(REF)  <scope> constructed an EDR-based dependency parser by using a similar method to Collins' </scope>  (TREF) . 
(REF)  <scope> describes an approach that uses syntactic information from the source side to derive reordering subtrees ,  which can then be used within a data-oriented translation (DOT) MT system ,  similar in framework to </scope>  (TREF) . 
REF  <scope> describe the supertagger ,  which uses log-linear models to define a distribution over the lexical category set for each local five-word context containing the target word  </scope> (TREF) . 
(REF)  <scope> discusses possible con icts between named entity analyses and syntactic structure  </scope> and (TREF) discusses a con ict between discourse structure and syntactic structure . 
(REF)  <scope> expanded on the work of  </scope> (TREF)  <scope> by adding 41 lexical ,  semantic and grammatical features .  </scope>
REF  <scope> explore the framework developed in </scope>  TREF <scope>  ,  and seek ways to minimize the time required by the heap manipulations necessary in this scheme .  </scope>
(REF) <scope> follow </scope>  (TREF)  <scope> in using the noisy channel model ,  by decomposing the translation decisions modeled by the translation model into different types ,  and inducing probability distributions via maximum likelihood estimation over each decision type .  </scope>
REF  <scope> found that they could effectively integrate prosodic informa821 tion in the form of this simplified three class ToBI encoding when parsing spontaneous speech by using a prosodically enriched PCFG model with latent annotations (PCFG-LA) </scope>  (GTREF) <scope> to rescore n-best parses produced by a baseline PCFG-LA model without prosodic enrichment .  </scope>
REF  <scope> gives a more covering description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging ,  such as PARTS </scope>  (TREF) and VOLSUNGA (REF) . 
(REF)  <scope> investigated syntactic alternations of verbs and their nominalized forms which occurred in the PennBioIE corpus </scope> (TREF) ,   <scope> whilst keeping PASs of the PASBio in their minds .  </scope>
REF  <scope> reported that a strong trend exists recently in applying machine learning (ML) techniques such as Support Vector Machine (SVM) </scope>  (GTREF) and Conditional Random Field (CRF) (REF)  <scope> to NER ,  which can address these issues .  </scope>
(REF)  <scope> uses Hearsts patterns  </scope> (TREF)  <scope> to learn semantic class instances and class groups by acquiring contexts around the pattern .  </scope>
REF  <scope> use the Arabic morphological analyzer MADA </scope>  (TREF)  <scope> to segment the Arabic source ;  they propose various segmentation schemes .  </scope>
(REF)  <scope> use transformationbased error-driven learning </scope>  (TREF) to derive disambiguation rules based on simple context information (e . g .  right and left adjacent words or POSs) . 
REF ,   <scope> we also modify this parser to output a packed forest for each sentence ,  which can be pruned by the marginal probability-based insideoutside algorithm </scope>  (GTREF) . 
REF show that <scope> Model 1 </scope>  (TREF)  <scope> probabilities of the word pairs inside and outside a span pair are useful .  </scope>
REF used feature support cutoffs and early stopping to stop overfitting of the model ,  and TREF  <scope> contends that including low support features harms a maximum entropy model </scope>  ,  our results show that low support features are useful in a regularized maximum entropy model . 
(REF) uses  <scope> perceptron </scope>  (TREF) to generate word candidates with both word and character features . 
(REF) use syntactic constituents for the PSCFG nonterminal set and (REF) take advantage of CCG (REF) categories ,  while (TREF)  <scope> uses a single generic nonterminal .  </scope>
SAMT (REF) and  <scope> Tree Sequence Alignment </scope>  (TREF)  <scope> are proposed to amend this problem by allowing non-constituent phrases to be extracted .  </scope>
<scope> (2004) conduct experiments with </scope>  model 2 of Collins parser (REF) and  <scope> the Stanford parser </scope>  (TREF)  <scope> on two Italian treebanks </scope>  . 
<scope> (2004) reported results for English using their automatically acquired rst sense heuristic on </scope>  SemCor (REF) and  <scope> the SENSEVAL-2 English all words dataset </scope>  (TREF) . 
<scope> Above the phrase level </scope>  ,  some models perform no reordering (GREF) ,   <scope> some have a simple distortion model that reorders phrases independently of their content </scope>  (GTREF) ,  and some ,  for example ,  the Alignment Template System (GREF) ,  hereafter ATS ,  and the IBM phrase-based system (GREF) ,  have phrase-reordering models that add some lexical sensitivity . 
<scope> A context (position) can be a word-window </scope>  (TREF) ,  sentence (REF) ,  or a certain position in the dependencyparse tree (GREF) . 
<scope> Active learning has been applied to a number of natural language processing tasks like  </scope> POS tagging (REF) ,  NER (GREF) ,   <scope> syntactic parsing </scope>  (GTREF) ,  Word Sense Disambiguation (GREF) and morpheme glossing for language documentation (REF) . 
<scope> Active learning has been applied to a number of natural language processing tasks like </scope>  POS tagging (REF) ,  NER (GREF) ,  syntactic parsing (GREF) ,  <scope> Word Sense Disambiguation </scope>  (GTREF) and morpheme glossing for language documentation (REF) . 
<scope> Actually ,  discriminative probabilistic latent variable models (DPLVMs) have recently become popular choices for performing a variety of tasks with sub-structures ,  e . g .  </scope>  ,  vision recognition (REF) ,  syntactic parsing (REF) ,  and  <scope> syntactic chunking </scope>  (TREF) . 
<scope> Actually ,  the best individual system </scope>  (TREF)  <scope> in </scope>  REF  <scope> chunking shared task for the English language </scope>  (REF)  <scope> used </scope>  the same HMM-based tagging engine . 
<scope> Additional linguistic knowledge sources such as dependency trees or parse trees were used in </scope>  (TREF) and (REF) . 
<scope> Additionally ,  we used the Jiang&Conrath (J&C) distance  </scope> (REF)  <scope> computed with wn :  : similarity package  </scope> (TREF)  <scope> to measure the similarity between T and H </scope>  .  This similarity is also used to define the texthypothesis word overlap kernel (WOK) . 
<scope> Additional techniques such as </scope>  semi-supervised learning (REF) and  <scope> parser combination </scope>  (TREF)  <scope> do achieve accuracies equal to or higher than ours ,  but their results are not directly comparable to ours since they have access to extra information like unlabeled data .  </scope>
<scope> Additional techniques such as semi-supervised learning </scope>  (TREF) and parser combination (REF)  <scope> do achieve accuracies equal to or higher than ours ,  but their results are not directly comparable to ours since they have access to extra information like unlabeled data .  </scope>
<scope> A detailed description of the popular translation/alignment models IBM-1 to IBM-5 (REF) ,  as well as the Hidden-Markov alignment model (HMM) (REF) can be found in </scope>  (TREF) . 
<scope> A detailed description of the popular translation/alignment models IBM-1 to IBM-5 </scope>  (REF) ,   <scope> as well as the Hidden-Markov alignment model (HMM) </scope>  (TREF)  <scope> can be found in </scope>  (REF) . 
<scope> A diverse array of NLP research in the past few years has used Wikipedia ,  such as </scope>  for word sense disambiguation (REF) ,  classification (Gantner and Schmidt-REF) ,  machine translation (REF) ,  coreference resolution (GREF) ,  sentence extraction for summarization (REF) ,  information retrieval (REF) ,  and semantic role labeling (TREF) ,  to name a few . 
<scope> A feasibility study of the parsers  </scope> CASS (REF) and  <scope> FIDDITCH  </scope> (TREF)  <scope> showed that coding FICUs on this data could be automated ,  n Subjectivity in coding the infer feature was eliminated by providing operational definitions of 11 Specifications for adapting the parser and incorporating it into coding software were formulated ,  but never implemented .  </scope>
<scope> A few examples already in print :  a number of projects investigating stochastic parsing have used </scope>  either the POS-tagged materials (GREF) or  <scope> the skeletally parsed corpus </scope>  (GTREF) . 
<scope> A few exceptions are the hierarchical (possibly syntax-based) transduction models </scope>  (GTREF) and the string transduction models (REF) . 
<scope> A few exceptions are the hierarchical (possibly syntax-based) transduction models  </scope> (GTREF) and the string transduction models (REF) . 
<scope> Affixes </scope>  have been shown to be useful in part-of-speech tagging (GREF) and  <scope> have been used in the </scope>  Charniak (REF) ,  Stanford (REF) and  <scope> Berkeley </scope>  (TREF) <scope> parser </scope> s . 
<scope> AFGs are too large to fully extract explicitly ;  researchers therefore </scope>  either work with a tractable subset of the fragments (GREF) or  <scope> use a PCFG reduction like that of  </scope> TREF ,   <scope> in which each treebank node token Xi is given its own unique grammar symbol .  </scope>
<scope> A firstpass approach to resolving such cases of overgeneration (based on aspectual features) is discussed in </scope>  TREF and in more detail in REF . 
<scope> After analyzing the results presented in the first and second Bakeoffs </scope>  ,  (REF) and (TREF) ,   <scope> we created a new Chinese word segmentation system named as Achilles that consists of four modules mainly :  Regular expression extractor ,  dictionary-based Ngram segmentation ,  CRF-based subword tagging </scope>  (REF) ,   <scope> and confidence-based segmentation .  </scope>
<scope> After an attempt with a context heterogeneity measure </scope>  (TREF)  <scope> for identifying word translations ,  Fung based her later work also on the co-occurrence assumption </scope>  (GREF) . 
<scope> After Linguistic Data Consortium (LDC) released the Chinese Treebank (CTB) </scope>  developed at UPenn (REF) ,   <scope> various statistical Chinese parsers </scope>  (GTREF)  <scope> have been built </scope>  . 
<scope> After parsing ,  we re-extracted the leaf nodes of the parse trees and statistically word-aligned the corpus using a multi-threaded implementation </scope>  (TREF)  <scope> of the GIZA +  +  program (REF) .  </scope>
<scope> A large ,  high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including </scope>  semantic role labeling (REF) ,  pronoun resolution (REF) ,  textual inference (REF) ,   <scope> word-sense disambiguation </scope>  (TREF) ,  and many more . 
<scope> A large ,  high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling  </scope> (TREF) ,  pronoun resolution (REF) ,  textual inference (REF) ,  word-sense disambiguation (REF) ,  and many more . 
<scope> A large number of text processing applications have already employed techniques for automatic subjectivity analysis ,  including automatic expressive text-to-speech synthesis </scope>  (TREF) ,  text semantic analysis (GREF) ,  tracking sentiment timelines in on-line forums and news (GREF) ,  mining opinions from product reviews (REF) ,  and question answering (REF) . 
<scope> Algorithms for the generation of referring expressions commonly use this as a starting point ,  proposing that properties are organized </scope>  in some linear order (REF) or  <scope> weighted order </scope>  (TREF)  <scope> as input .  </scope>
<scope> Algorithms light in linguistic theories but rich in available training data have been successfully applied to several applications such as machine translation </scope>  (TREF) ,  information extraction (REF) ,  and question answering (REF) . 
<scope> Alignment spaces can emerge from generative stories  </scope> (TREF) ,  from syntactic notions (REF) ,  or they can be imposed to create competition between links (REF) . 
<scope> All of the commonly used statistical parsers available on the web such as </scope>  the Collins(/Bikel) (REF)  <scope> Charniak-Johnson </scope> (TREF) ,  and Petrov-Klein (REF) ,  parsers  <scope> use context-free dynamic programming algorithms so they work bottom up on the entire sentence .  </scope>
<scope> All submitted runs were evaluated with the automatic metrics :  ROUGE </scope>  (TREF) ,  which calculates the proportion of n-grams shared between the candidate summary and the reference summaries ,  and Basic Elements (REF) ,  which compares the candidate to the models in terms of head-modifier pairs . 
<scope> All the meetings have been transcribed and annotated with dialog acts (DA) </scope>  (TREF) ,  topics ,  and extractive summaries (REF) . 
<scope> All the optimization tricks proposed in </scope>  (TREF)  <scope> for Suffix Arrays can be used here </scope>  ,  especially the optimization proposed in (REF) . 
<scope> Also ,  as with any generative model ,  it should be easy to improve the parsers accuracy with </scope>  discriminative reranking ,  such as discriminative retraining techniques (REF) or  <scope> data-defined kernels  </scope> (TREF) ,  with or even without the introduction of any additional linguistic features . 
<scope> Also ,  several models are proposed to address the problem of improving generative models with small amount of manual data ,  including Model 6 </scope>  (TREF) and the model proposed by REF and its extension called LEAF aligner (REF) . 
<scope> Alternate formulations of sentiment summarization are possible ,  </scope>   <scope> including </scope>  aspect-based summarization (REF) ,  abstractive summarization (REF) or  <scope> related tasks such as opinion attribution  </scope> (TREF) . 
<scope> Alternative approaches have either avoided defining axioms for mutual belief ,  e . g .  REF or defined it as a primitive operator without reference to simple beliefs ,  e . g .  </scope>  TREF . 
<scope> Alternatives that appear compatible with the present approach are quantitier movement </scope>  (TREF) ,  type-ralsing at LF (REF) ,  or the use of disambiguated quantifers in the derivation itself (REF) . 
<scope> Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings </scope>  (TREF) ,  we again propose the use of vector space methods from IR ,  which can be easily extended to the summarization task (REF) :  1 . 
<scope> Although more exotic forms of event recognition exist at varying levels of analysis </scope>  (such as within the abductive reasoning mechanism of SRI's TACITUS system (REF) ,  in a thesaurus-based lexical cohesion algorithm (REF) and <scope> in a semantic network  </scope> (TREF)) ,   <scope> template merging is the most used method .  </scope>
<scope> Although the effect of lexicalization has been discussed in several studies recently </scope>  (GTREF) ,  <scope> it is often investigated as an all-or-nothing affair </scope>  , except for a few studies that analyze the distributions of lexical items , for example , GREF . 
<scope> Although the interpretation of quantifiers and negation is a traditional research area in computational linguistics </scope>  (GREF) ,  <scope> their generation is much less studied  </scope> (TREF) . 
<scope> Although the interpretation of quantifiers and negation is a traditional research area in computational linguistics </scope>  (GTREF) ,  <scope> their generation is much less studied (REF) .  </scope>
<scope> Although there are an exponential number of parse trees for a given query ,  this space can be sampled efficiently using dynamic programming </scope>  (GTREF) In order to apply CLC to Web-scale data ,  we implement an efficient parallel approximate Gibbs sampler in the MapReduce framework REF . 
<scope> Although there is now a sizable literature on </scope>  trace and function-tag insertion algorithms (GREF) ,  and  <scope> integrated parsing with function tags or null elements </scope>  (GTREF) ,   <scope> such approaches typically require additional preor postprocessing steps that are likely to add further noise and errors to the parser output .  </scope>
<scope> Although there is now a sizable literature on trace and function-tag insertion algorithms  </scope> (GTREF) ,  and integrated parsing with function tags or null elements (GREF) ,  such approaches typically require additional preor postprocessing steps that are likely to add further noise and errors to the parser output . 
<scope> Although the work by  </scope> TREF  <scope> uses Maximum Entropy to create their ranking-based model ,  we adopt the Ranking SVM algorithm </scope>  (REF) ,  which learns a weight vector to rank candidates for a given partial ranking of each referent . 
<scope> Although to a lesser extent ,  measures of word relatedness have also been applied on other languages ,  including German </scope>  (GTREF) ,  Chinese (REF) ,  Dutch (REF) and others . 
<scope> Among others Co-Training was applied to </scope>  document classification (REF) ,   <scope> namedentity recognition  </scope> (TREF) ,  noun phrase bracketing (REF) ,  and statistical parsing (REF) . 
<scope> Among others ,  it can be composed with transducers that encode :   correction rules for the most frequent tagging errors which are automatically generated </scope>  (GTREF) or manually written (REF) ,  <scope> in order to significantly improve tagging accuracy 2 .  </scope>
<scope> Among the applications of Model 1 are </scope>  segmenting long sentences into subsentental units for improved word alignment (REF) ,  extracting parallel sentences from comparable corpora (REF) ,  bilingual sentence alignment (REF) ,  aligning syntactictree fragments (REF) ,  and  <scope> estimating phrase translation probabilities </scope>  (TREF) . 
<scope> Among the applications of Model 1 are </scope>  segmenting long sentences into subsentental units for improved word alignment (REF) ,   <scope> extracting parallel sentences from comparable corpora </scope>  (TREF) ,  bilingual sentence alignment (REF) ,  aligning syntactictree fragments (REF) ,  and estimating phrase translation probabilities (REF) . 
<scope> Among the methods that have been proposed for specifying lexical relationships are </scope>  natural language description and rules (REF) ,  distributional means (REF) ,  sample term pairs (REF) ,  relationship instances (REF) and <scope> pattern clusters  </scope> (TREF) . 
<scope> Among the most common knowledge acquisition approaches are those based on lexical patterns </scope>  (GTREF) and clustering (GREF) . 
<scope> Among them ,  we select the following subset </scope>  :  (a) Phrase Type ,  Predicate Word ,  Head Word ,  134 Position and Voice as defined in (REF) ;  (b) Partial Path ,  No Direction Path ,  Head Word POS ,  First and Last Word/POS in Constituent and SubCategorization as proposed in (REF) ;  and (c)  <scope> Syntactic Frame as designed in </scope>  (TREF) . 
<scope> Among the various </scope>  knowledge-based (GREF) and  <scope> data-driven </scope>  (GTREF)  <scope> word sense disambiguation methods that have been proposed to date ,  supervised systems have been constantly observed as leading to the highest performance .  </scope>
<scope> A more general and comprehensive overview of state-of-the-art in anaphora resolution is given in </scope>  (REF) and also in (TREF) . 
<scope> An algorithm for computing lexical chains was first given by </scope>  (TREF)  <scope> using the Rogets Thesaurus </scope>  (REF) . 
<scope> Analogous techniques for tree-structured translation models involve either allowing each nonterminal to generate both terminals and other nonterminals </scope>  (GTREF) ,  or ,  given a constraining parse tree ,  to flatten it (GREF) . 
<scope> Analogous techniques for tree-structured translation models involve </scope>  either  <scope> allowing each nonterminal to generate both terminals and other nonterminals </scope>  (GTREF) ,  or ,  given a constraining parse tree ,  to flatten it (GREF) . 
<scope> An alternative paradigm ,  Open IE ,  pioneered by </scope>  the TextRunner system (REF) and  <scope> the preemptive IE in  </scope> (TREF) ,   <scope> aims to handle an unbounded number of relations and run quickly enough to process Webscale corpora .  </scope>
<scope> Analysis systems that are based on unification grammars can be classified into two groups from the viewpoint of the ways feature structures are represented :  (a) those using labeled ,  directed graphs </scope>  (TREF) and (b) those using first-order terms (GREF) . 
<scope> An analysis based on actual data allows us to establish a clear set of patterns and sub-groups ,  for example to decide whether we require  </scope> either Producer-For-Product (REF) or Artist-for-Artform (TREF) ,  or both of them . 
<scope> Ananiadou et al .  proposed C-value </scope>  (TREF) and NC-value (REF)  <scope> which count how independently the given compound noun is used in the given corpus .  </scope>
<scope> An average improvement of 0 . 52 BLEU  </scope> (TREF)  <scope> score </scope>  and 2 . 05 TER (REF) score over 5 test sets for a typical phrase-based translation system ,  Moses (REF) ,  <scope> validated the effectiveness of our experim </scope> ents . 
<scope> Another approach is taken by two other commonly used metrics ,  METEOR  </scope> (TREF) and TER (REF) . 
<scope> Another interesting approach to this problem was using supertagging </scope>  (GTREF) ,   <scope> which was originally developed for lexicalized tree adjoining grammars (LTAG) </scope>  (REF) . 
<scope> Another interesting approach to this problem was using supertagging  </scope> (GTREF) ,  which was originally developed for lexicalized tree adjoining grammars (LTAG) (REF) . 
<scope> Another interesting direction is applying our approach to extracting translation rules with hierarchical structures such as </scope>  hierarchical phrases (REF) and  <scope> tree-to-string rules </scope>  (GTREF) . 
<scope> Another related line of work is automated ontology construction ,  which aims to create lexical hierarchies based on semantic classes (e . g .  ,  </scope>  (GREF)) <scope>  ,  and learning semantic relations such as meronymy </scope>  (GTREF) . 
<scope> Another research line is to use humanannotated corpora as training data ,  e . g .  ,  </scope>  the RST Bank (REF) used by (REF) ,   <scope> adhoc annotations used by </scope>  ( ? ) ,  (TREF) ,  and the GraphBank (REF) used by (REF) . 
<scope> Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages ,  through heuristic projection </scope>  (GTREF) or constraints in learning (GREF) or inference (REF) . 
<scope> Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages ,  through  </scope> heuristic projection (GREF) or constraints in learning (GREF) or <scope> inference </scope>  (TREF) . 
<scope> Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages ,  through </scope>  heuristic projection (GREF) or  <scope> constraints in learning </scope>  (GTREF) or inference (REF) . 
<scope> Another thread related to our work includes extracting from text corpora paraphrases </scope>  (REF)  <scope> and inference rules ,  e . g .  TEASE 1  </scope> (TREF) and DIRT (REF) . 
<scope> A number of feature-based methods have been tried ,  including Bayesian classifiers </scope>  (GTREF) ,  decision lists (REF) ,  and knowledge-based approaches (REF) . 
<scope> A number of knowledge-rich </scope>  [GTREF] and knowledge-poor [GREF] methods have been proposed for recognizing when words are similar . 
<scope> A number of relation extraction kernels have been proposed ,  including  </scope> dependency tree kernels (REF) ,   <scope> shortest dependency path kernels  </scope> (TREF) and more recently convolution tree kernels (GREF) . 
<scope> A number of </scope>  knowledge-rich [GREF] and  <scope> knowledge-poor </scope>  [GTREF]  <scope> methods have been proposed for recognizing when words are similar .  </scope>
<scope> A number of separate functionalities included in this system have been developed and used either in commercial word processing software packages or in NLP projects </scope>  (e . g .   ,  the translator's tools described in REF ,   ;   <scope> and the developer environments IRACQ </scope>  (TREF) ,  LUKE (REF) or ONTOS (REF) ,  among many others) . 
<scope> A number of statistical surface realizers have been described ,  notably the </scope>  FERGUS (REF) and HALogen systems (REF) ,  as well as  <scope> experiments in </scope>  (TREF) . 
<scope> A number of studies have investigated sentiment classification at document level ,  e . g .  ,   </scope> (GTREF) ,  and at sentence level ,  e . g .  ,  (GREF) ;  however ,  the accuracy is still less than desirable . 
<scope> A number of studies have investigated sentiment classification </scope>  at document level ,  e . g .  ,  (GREF) ,  and  <scope> at sentence level ,  e . g .  ,  </scope>  (GTREF) ;   <scope> however ,  the accuracy is still less than desirable .  </scope>
<scope> A number of supervised learning approaches have extracted such informationabout verbs from corpora ,  including their </scope>  argument roles (REF) ,  selectional preferences (REF) ,  and  <scope> lexical semantic classification (i . e .   ,  grouping verbs according to their argument structure properties) </scope>  (GTREF) . 
<scope> A particularly simple way of doing this is to use unseoped logical forms where qmmtifiers are left in situ (silnilar to the representations used by </scope>  REF ,   <scope> or to Quasi Logical Form </scope>  (TREF)) . 
<scope> A possible drawback of U-DOP ,  however ,  is the statistical inconsistency of its estimator </scope>  (TREF) which is inherited from the DOP1 model (REF) . 
<scope> Applications to </scope>  CCG parsing (REF) and  <scope> LFG parsing </scope>  (GTREF)  <scope> demonstrated that feature forest models attained higher accuracy than other models .  </scope>
<scope> A related thread of research is work on automatic transliteration ,  where training sets are typically used to compute probabilities for mappings in  </scope> weighted finite state transducers (Al-GREF) or  <scope> source-channel models  </scope> (GTREF) . 
<scope> A representative sample of modern syntax-based systems includes models based on </scope>  bilingual synchronous grammar (REF) ,   <scope> parse tree-to-string translation models </scope>  (TREF) and nonisomorphic tree-to-tree mappings (REF) . 
<scope> As a baseline ,  we use an IBM Model 4 </scope>  (TREF) system3 with a greedy decoder4 (REF) . 
<scope> A sample of these includes methods to extract linguistic resources </scope>  (GTREF) ,  retrieve useful information in response to user queries (GREF) and mine/discover knowledge latent in the Web (REF) . 
<scope> As an alternative ,  researchers have tried to acquire paraphrases from large-scale web corpora </scope>  (GTREF) or directly based on web mining (REF) . 
<scope> As a point of comparison ,  we also trained an IBM-4 translation model with the GIZA +  +  toolkit  </scope> (TREF) ,   <scope> using the combined bi-phrase building and training sets </scope>  ,  and translated the test set using the ReWrite decoder (REF)5 . 
<scope> As a promising approach to solve the problem of vocabulary gap ,  SMT has been widely exploited in many applications </scope>  such as information retrieval (GREF) ,  image and video annotation (REF) ,  question answering (GREF) ,  query expansion and rewriting (GREF) ,  summarization (REF) ,   <scope> collocation extraction </scope>  (GTREF) and paraphrasing (GREF) . 
<scope> As a promising approach to solve the problem of vocabulary gap ,  SMT has been widely exploited in many applications such as </scope>  information retrieval (GREF) ,  image and video annotation (REF) ,  question answering (GREF) ,  query expansion and rewriting (GREF) ,  summarization (REF) ,  collocation extraction (GREF) and  <scope> paraphrasing </scope>  (GTREF) . 
<scope> As a promising approach to solve the problem of vocabulary gap ,  SMT has been widely exploited in many applications such as </scope>  information retrieval (GREF) ,  image and video annotation (REF) ,  question answering (GREF) ,   <scope> query expansion and rewriting </scope>  (GTREF) ,  summarization (REF) ,  collocation extraction (GREF) and paraphrasing (GREF) . 
<scope> As a representative ,  </scope>  TREF ,  and REF  <scope> looked for modal adjectives (e . g .  necessary) or cognitive verbs (e . g .  It is thought that ) in a set of patterned constructions .  </scope>
<scope> As a result of this ,  different task on aligments in statistical machine translation have been proposed in the last few years </scope>  (HLTNAACL 2003 (TREF) and ACL 2005 (REF)) . 
<scope> As a result ,  they are being used in a variety of applications </scope>  ,  such as question answering (REF) ,  speech recognition (REF) ,  language modeling (REF) ,  language generation (REF) and ,   <scope> most notably ,  machine translation </scope>  (GTREF) . 
<scope> As a result ,  they have to resort to  </scope> either heuristics (GREF) or <scope> separate ordering models </scope>  (GTREF)  <scope> to control the word order of translations .  </scope>
<scope> As a side product ,  we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques </scope>  (GTREF) and parent annotation techniques (REF)  <scope> is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora .  </scope>
<scope> As a side product ,  we find empirical evidence to suggest that the effectiveness of  </scope> rule lexicalization techniques (GREF) and <scope> parent annotation techniques  </scope> (TREF) is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora . 
<scope> A second important line of research has focused on word segmentation ,  which is useful for languages like German ,  which are rich in compound words that are spelled concatenated </scope>  (GTREF) ,  or like Arabic ,  Turkish ,  Finnish ,  and ,  to a lesser extent ,  Spanish and Italian ,  where clitics often attach to the preceding word (REF) . 
<scope> A second potentially interesting direction would be to incorporate other semantic resources such as lexical patterns </scope>  (TREF) or Wikipedia-generated gazetteers (REF) . 
<scope> As for the identifying method available in general discourses ,  the centering theory </scope> (GTREF) and the property sharing theory(REF)  <scope> are proposed .  </scope>
<scope> A similar approach at the level of unigram matching has been used by the well-known METEOR metric </scope>  (TREF) ,   <scope> which proved its qualities during the previous MT evaluation task in 2008 </scope>  (REF) . 
<scope> A similar task has been approached in a variety of perspectives ,  including term clustering </scope>  (GTREF) and term categorization (REF) . 
<scope> A simpler ,  related idea of penalizing distortion from some ideal matching pattern can be found in the </scope>  statistical translation (GREF) and  <scope> word alignment </scope>  (GTREF)  <scope> models .  </scope>
<scope> As in the work of (REF) and </scope>  (TREF) ,   <scope> to avoid calculating excessively optimistic values ,  constituents bearing the O label are not counted in for computing overall precision ,  recall and F-score .  </scope>
<scope> As lexical/semantic information </scope>  ,  REF used about 50 semantic categories ,  while TREF  <scope> used lexicai forms of words .  </scope>
<scope> As more and more conversation data becomes available ,  researchers have investigated automated processing of conversation data to acquire useful information ,  for example ,  related to opinions </scope>  (GTREF) ,  biographic attributes (REF) ,  social networks (REF) ,  and agreements and disagreements between participants (REF) . 
<scope> As noted earlier ,  this strategy is characteristic of the systems that participated in the SemEval task on classifying semantic relations between nominals (REF) ,  such as </scope>  TREF . 
<scope> As noted ,  the most successful work has used </scope>  edit distance (REF) or  <scope> bag-of-words features to measure sentence similarity ,  along with shallow syntactic features  </scope> (GTREF) . 
<scope> As one reviewer noted ,  CCG formulation of the syntax-phonology interface moved from autonomous prosodic types </scope>  (TREF)  <scope> to syntax-directed prosodic features </scope>  (REF) . 
<scope> As pointed out by (TREF) ,  these approaches either try to correct individual words (and will fail to correct Him Clijsters to Kim Clijsters) or employ features based on relatively wide context windows </scope>  1In contrast ,  80% of misspelled words in general text are due to single typographical errors as found by (REF) . 
<scope> As regards evaluation ,  NLG systems have been evaluated e . g .  by using human judges to assess the quality of the texts produced </scope>  (GTREF) ;  by comparing the systems performance to that of humans (REF) ;  or through task efficacy measures ,  i . e .  ,  measuring how well the users so ,  and the distribution of topics and of evaluations is too broad to be telling . 
<scope> As shown in the following parts of this paper ,  it works very well with the existing techniques ,  such as </scope>  rule composing (REF) ,   <scope> SPMT models </scope>  (TREF) and rule extraction with kbest parses (REF) . 
<scope> As the state-of-the-art ,  </scope>  TREF  <scope> applied the convolution tree kernel </scope>  (REF)  <scope> and achieved comparable performance with a state-of-theart linear kernel  </scope> (REF)  <scope> on the 5 relation types in the ACE RDC 2003 corpus .  </scope>
<scope> As the third test set we selected all tokens of the Brown corpus part of the Penn Treebank </scope>  (TREF) ,  a selected portion of the original one-million word Brown corpus (REF) ,  a collection of samples of American English in many different genres ,  from sources printed in 1961 ;  we refer to this test set as BROWN . 
<scope> As well as the sentiment expressions leading to evaluations ,  there are many semantic aspects to be extracted from documents which contain writers opinions ,  such as subjectivity </scope>  (TREF) ,  comparative sentences (REF) ,  or predictive expressions (REF) . 
<scope> As with the previous work of </scope>  (GTREF) ,  we propose a feature driven statistical question classifier (REF) . 
<scope> A topic that is increasingly studied in distributional semantics is the semantics of adjectives </scope>  ,  both in isolation (REF) and  <scope> in compositional adjective-noun phrases </scope>  (GTREF) . 
<scope> A tree kernel </scope>  (TREF)  <scope> is used to exploit the deep syntactic processing obtained using the Charniak parser </scope>  (REF) . 
<scope> Attempts have been made to remedy this deficit through various techniques ,  including </scope>  modelbuilding (REF) and <scope> the addition of semantic axioms  </scope> (TREF) . 
<scope> At the intersection of these lines of work ,  discriminative ITG models have also been proposed ,  including one-to-one alignment models </scope>  (TREF) and block models (REF) . 
<scope> At the same time ,  grammar theoreticians have proposed various generative synchronous grammar formalisms for MT ,  such as Synchronous Context Free Grammars (S-CFG) </scope>  (TREF) or Synchronous Tree Adjoining Grammars (S-TAG) (REF) . 
<scope> At the same time ,  many high-performance parsers have focused on symbol refinement approaches ,  wherein PCFG independence assumptions are weakened not by increasing rule sizes but by subdividing coarse treebank symbols into many subcategories either using structural annotation </scope>  (GTREF) or lexicalization (GREF) . 
<scope> At the Senseval competitions (GREF) ,  the choice of a sense inventory also frequently presented problems ,  spurring the efforts to create coarser-grained sense inventories </scope>  (GTREF) . 
<scope> Authors of </scope>  (TREF) applied the PageRank algorithm (REF) for keyword extraction using a simpler graph representation (undirected unweighted graphs) ,  and show that their results compare favorably with results on established benchmarks of manually assigned keywords . 
<scope> Automatic alignment can be performed using different algorithms such as the EM algorithm </scope>  (GTREF) or HMM based alignment (GREF) . 
<scope> A variety of approaches exist for determining the salient sentences in the text :  </scope>  statistical techniques based on word distribution (REF) ,   <scope> symbolic techniques based on discourse structure </scope>  (TREF) ,  and semantic relations between words (REF) . 
<scope> A variety of automatic MT evaluation metrics have been developed over the years ,  including </scope>  BLEU (REF) ,  NIST (REF) ,  METEOR (exact) (REF) ,   <scope> GTM  </scope> (TREF) ,  and TER (REF) . 
<scope> A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews) ,  the most popular being </scope>  decision lists (REF) and naive Bayesian classifiers (GTREF) . 
<scope> A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews) ,  the most popular being </scope>  decision lists (REF) and  <scope> naive Bayesian classifiers </scope>  (GTREF) . 
<scope> A variety of learning-based systems have been trained and tested  </scope> on the former (GREF) ,  on the latter (GREF) ,  <scope> or on both  </scope> (GTREF) . 
<scope> Aware of this problem ,  Resnik and Yarowsky suggest creating the sense distance matrix based on results in experimental psychology such as  </scope> REF or TREF . 
<scope> A wide range of contextual information ,  such as </scope>  surrounding words (GREF) ,   <scope> dependency or case structure </scope>  (GTREF) ,  and dependency path (GREF) ,   <scope> has been utilized for similarity calculation ,  and achieved considerable success .  </scope>
<scope> A wide range of DA tag sets have been proposed ,  usually customised to a particular medium such as speech dialogue  </scope> (GTREF) ,  taskfocused email (GREF) or instant messaging (REF) . 
<scope> A wide variety of machine learning methods have been applied to this problem ,  including </scope>  Hidden Markov Models (REF) ,  Maximum Entropy methods (GREF) ,  Decision Trees (REF) ,  Conditional Random Fields (REF) ,  Class-based Language Model (REF) ,   <scope> Agent-based Approach </scope>  (TREF) and Support Vector Machines . 
<scope> A wide variety of machine learning methods have been applied to this problem ,  including  </scope> Hidden Markov Models (REF) ,  Maximum Entropy methods (GREF) ,  Decision Trees (REF) ,  Conditional Random Fields (REF) ,   <scope> Class-based Language Model </scope>  (TREF) ,  Agent-based Approach (REF) and Support Vector Machines . 
<scope> A wide variety of machine learning methods have been applied to this problem ,  including </scope>  Hidden Markov Models (REF) ,  Maximum Entropy methods (GREF) ,  Decision Trees (REF) ,   <scope> Conditional Random Fields </scope>  (TREF) ,  Class-based Language Model (REF) ,  Agent-based Approach (REF) and Support Vector Machines . 
<scope> A wide variety of machine learning methods have been applied to this problem ,  including  </scope> Hidden Markov Models (REF) ,  Maximum Entropy methods (GREF) ,   <scope> Decision Trees  </scope> (TREF) ,  Conditional Random Fields (REF) ,  Class-based Language Model (REF) ,  Agent-based Approach (REF) and Support Vector Machines . 
<scope> Based on </scope>  (TREF) and (REF) ,  <scope> we consider four types of structures (as shown in Table 5) during sentiment phrase extraction .  </scope>
<scope> Based on the idea of translation ,  we use word alignment models (WAM) </scope>  (TREF) in statistical machine translation (SMT) (REF) <scope> and propose a unified framework for keyphrase extraction :  (1) From a collection of translation pairs of two languages ,  WAM learns translation probabilities between the words in the two languages .  </scope>
<scope> Based on them ,  many state-of-art English parser were built ,  including </scope>  the well-known Collins parser (REF) ,  Charniak parser (REF) and  <scope> Berkeley parser  </scope> (TREF) . 
<scope> Bayesian inference ,  the approach in this paper ,  have recently been applied to several unsupervised learning problems in NLP </scope>  (GTREF) as well as to other tasks in SMT such as synchronous grammar induction (REF) and learning phrase alignments directly (REF) . 
<scope> Because of the cost and difficulty in treebank construction ,  researchers have also investigated the utilization of unannotated text ,  including the unsupervised parsing which totally uses unannotated data </scope>  (GTREF) ,  and the semisupervised parsing which uses both annotated and unannotated data (GREF) . 
<scope> Because of the cost and difficulty in treebank construction ,  researchers have also investigated the utilization of unannotated text ,  including the unsupervised parsing which totally uses unannotated data </scope>  (GTREF) ,  and the semisupervised parsing which uses both annotated and unannotated data (<span class="refstyle ref49636" id="s49636 . r2">GREF<%.
<scope> Because of this property ,  vector space models have been used successfully both in computational linguistics </scope>  (GTREF) and in cognitive science (GREF) . 
<scope> Beyond WordNet (REF) ,  a wide range of resources has been developed and utilized ,  including extensions to WordNet </scope>  (GTREF) and resources based on automatic distributional similarity methods (GREF) . 
<scope> BioInfer contains uncollapsed gold-standard parses while the BioNLP09 Shared Task corpus includes parses ,  in the collapsed representation ,  generated by the parser of </scope>  TREF  <scope> using the model of REF .  </scope>
<scope> Bolstered by the success of </scope>  (REF) ,  (REF) and  <scope> especially  </scope> (TREF) ,   <scope> we believe there is great promise the incorporation of word-sense into a probabilistic parsing model .  </scope>
<scope> Bolstered by the success of </scope>  (TREF) ,  (REF) and especially (REF) ,   <scope> we believe there is great promise the incorporation of word-sense into a probabilistic parsing model .  </scope>
<scope> Both left-corner strategy </scope>  (GTREF) and head-corner strategy (GREF)  <scope> were employed in incremental parsing .  </scope>
<scope> Both parsing is formulated as a single-stage word-pair classification problem ,  and the latter is carried out by a search through the NomBank </scope>  (REF)  <scope> or the PropBank </scope>  (TREF)1 . 
<scope> Both the IOB2 representation  </scope> (TREF) and the Start/End representation (REF)  <scope> are popular .  </scope>
<scope> Both the linguistic approach </scope>  (GTREF) and the ML based approach (GREF) use gazetteer lists . 
<scope> Both were 5gram models with modified Kneser-Ney smoothing ,  lossily compressed using a perfect-hashing scheme similar to that of  </scope> TREF but using minimal perfect hashing (REF) . 
<scope> Bridging anaphora (REF) represents a special part of the general problem of anaphora resolution ,  which has been studied and discussed for various languages and domains </scope>  (GTREF) . 
<scope> Bridging anaphora  </scope> (TREF)  <scope> represents a special part of the general problem of anaphora resolution </scope>  ,  which has been studied and discussed for various languages and domains (GREF) . 
<scope> Building upon </scope>  the large body of research to improve tagging performance for various languages using various models (e . g .  ,  (GREF)) and  <scope> the recent work on PCFG grammars with latent annotations </scope>  (GTREF) ,  <scope> we will investigate the use of fine-grained latent annotations for Chinese POS tagging .  </scope>
<scope> Building upon the large body of research to improve tagging performance for various languages using various models </scope>  (e . g .  ,  (GTREF)) and the recent work on PCFG grammars with latent annotations (GREF) ,  we will investigate the use of fine-grained latent annotations for Chinese POS tagging . 
<scope> But CCG-Std cannot handle the generalization of type raising that has been used in accounting for various linguistic phenomena including </scope>  :  coordination and extraction (GREF) ,   <scope> prosody </scope>  (TREF) ,  and quantifier scope (REF) . 
<scope> By habit ,  most systems for automatic role-semantic analysis have used Pennstyle constituents </scope>  (TREF) produced by REF or REF parsers . 
<scope> By using only the information available in a training corpus ,  the methods we explore are applicable to annotation error detection for either hand-annotated or automatically-parsed corpora and can also provide insights for </scope>  parse reranking (e . g .  ,  REF) or  <scope> parse revision </scope>  (TREF) . 
<scope> By validate with development data ,  we found that C=1 . 25 and use the E-BIES representation method </scope>  (TREF)  <scope> yields better accuracy than B-BIES (REF) .  </scope>
<scope> By viewing parse trees from diverse perspectives ,  we can either use functions on bracket structures of parse trees ,  as in  </scope> (TREF) ,  or use functions on head-dependent relations by first transforming constituency trees into dependency trees ,  as in (REF) . 
<scope> Capitalizing on the strength of the phrase-based approach </scope>  ,  REF  <scope> introduced a hierarchical phrase-based statistical MT system ,  Hiero ,  which achieves significantly better translation performance than Pharaoh  </scope> (TREF) ,   <scope> which is a state-of-the-art phrasebased statistical MT system .  </scope>
<scope> CCA has been used in bilingual lexicon extraction from </scope>  comparable corpora (REF) and  <scope> monolingual corpora </scope>  (TREF) . 
<scope> Charniak </scope>  (REF)  <scope> developed a state-of-the-art statistical CFG parser and then built an effective language model based on it </scope>  (TREF) . 
<scope> Chunking For NP chunking ,  [REF] used data extracted from section 15-18 of the WS . J as a fixed train set and section 20 as a fixed test set ,  the same data as </scope>  [TREF] . 
<scope> Clause restructuring performed with hand-crafted reordering rules for German-toEnglish and Chinese-to-English tasks are presented in  </scope> (TREF) and (REF) ,  respectively . 
<scope> Closely related proposals have been applied in syntactic disambiguation  </scope> (GTREF) and to automatic acquisition of more KatzFodoresque selection restrictions in the form of weighted disjunctions (REF) . 
<scope> Clusters are created by means </scope>  of distributional techniques in (REF) ,  while in (TREF)  <scope> low level synonim sets in WordNet </scope>  are used . 
<scope> Collocation extraction methods have been used not only for English ,  but for many other languages :  </scope>  French (REF) ,  German (REF) and  <scope> Japanese  </scope> (TREF) ,  to cite but those . 
<scope> Combination techniques have earlier been applied to various applications including machine translation </scope>  (TREF) ,  part-of-speech tagging (REF) and base noun phrase identification (REF) . 
<scope> CommandTalk </scope>  (TREF) ,  Circuit Fix-It Shop (REF) and Tl : tAINS-96 (GREF)  <scope> are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .  </scope>
<scope> Compared to term-counting approaches ,  machine learning approaches usually achieve much better performance </scope>  (GTREF) ,   <scope> and have been adopted to more complicated scenarios </scope>  ,  such as domain adaptation (REF) ,  multi-domain learning (REF) and semi-supervised learning (GREF) for sentiment classification . 
<scope> Complex linguistic tools such as </scope>  terminological extractors (REF) ,  parsers (REF) or  <scope> lemmatizers </scope>  (TREF)  <scope> are sometimes used .  </scope>
<scope> Concerning relatedness measure ,  additional corpus-based measures such as Web-based measures </scope>  (TREF) or measures based on syntactic relations (REF) <scope> could appear to be useful for improving the ranking of the extracted relations .  </scope>
<scope> Concerning the second step ,  there is some work that uses ,  for scoring ,  </scope>  additional information such as rules for cognate recognition (REF) or  <scope> sentence-aligned parallel corpora and a translation model ,  as in the full system described by  </scope> TREF . 
<scope> Conditional random fields (CRFs) </scope>  (REF)  <scope> further improve the performance </scope>  (GTREF)  <scope> by performing whole-sequence normalization to avoid label-bias and length-bias problems .  </scope>
<scope> Consequently ,  there is a recent common trend towards enriching the current models with some extra knowledge as the new approaches of factored translation models </scope>  (TREF) or syntax-based (or syntaxaugmented) MT systems (GREF) . 
<scope> Co-occurrence information between neighboring words and words in the same sentence has been used in </scope>  phrase extraction (GREF) ,  phrasal translation (GREF) ,  target word selection (GREF) ,  domain word translation (GREF) ,   <scope> sense disambiguation  </scope> (GTREF) ,  and even recently for query translation in cross-language IR as well (REF) . 
<scope> Co-occurrence information between neighboring words and words in the same sentence has been used in </scope>  phrase extraction (GREF) ,   <scope> phrasal translation </scope>  (GTREF) ,  target word selection (GREF) ,  domain word translation (GREF) ,  sense disambiguation (GREF) ,  and even recently for query translation in cross-language IR as well (REF) . 
<scope> Co-selection measures include precision and recall of co-selected sentences ,  relative utility </scope>  (TREF) ,  and Kappa (GREF) . 
<scope> Co-training has been applied to a number of NLP applications ,  including POS-tagging </scope>  (TREF) ,  parsing (REF) ,  word sense disambiguation (REF) ,  and base noun phrase detection (REF) . 
<scope> CRFs have been applied with impressive empirical results </scope>  to the tasks of named entity recognition (GREF) ,  part-of-speech (PoS) tagging (REF) ,   <scope> noun phrase chunking </scope>  (TREF) and extraction of table data (REF) ,  among other tasks . 
<scope> CRFs have been previously applied to other tasks such as </scope>  name entity extraction (REF) ,  table extraction (REF) and  <scope> shallow parsing  </scope> (TREF) . 
<scope> CRFs have been shown to perform well in a number of natural language processing applications ,  such as  </scope> POS tagging (REF) ,  shallow parsing or NP chunking (REF) ,  and  <scope> named entity recognition </scope>  (TREF) . 
<scope> CRFs have been shown to perform well on a number of NLP problems such as </scope>  shallow parsing (REF) ,  table extraction (REF) ,  and  <scope> named entity recognition </scope>  (TREF) . 
<scope> DATR has been applied to problems in inflectional and derivational morphology </scope>  (GREF) ,  lexical semantics (REF) ,   <scope> morphonology </scope>  (TREF) ,  prosody (REF) and speech (REF) . 
<scope> Decision trees ,  recently used in NLP basic tasks such as tagging and parsing  </scope> (REF :  GTREF) ,   <scope> are suitable for performing this task .  </scope>
<scope> Deeper syntax ,  e . g .  phrase or dependency structures ,  has been shown useful in generative models  </scope> (GTREF) ,  heuristic-based models (GREF) and even for syntactically motivated models such as ITG (GREF) . 
<scope> Dependency parsers have been tested on parsing sentences in English </scope>  (GTREF) as well as many other languages (REF) . 
<scope> Dependency trees capture grammatical structures that can be useful in several language processing tasks such as information extraction  </scope> (TREF) and machine translation (REF) . 
<scope> Dependency trees capture grammatical structures that can be useful in several language processing tasks such as </scope>  information extraction (REF) and  <scope> machine translation </scope>  (TREF) . 
<scope> Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction </scope>  (TREF) ,  paraphrase acquisition (REF) and machine translation (REF) . 
<scope> Depending on the application and domain at hand ,  the items to be ordered may vary greatly from propositions </scope>  (GREF)  <scope> to trees </scope>  (REF)  <scope> or sentences  </scope> (GTREF) . 
<scope> Depending on the type of input ,  these efforts can be divided into two broad categories </scope>  :  the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (GREF) ,  and  <scope> the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string </scope>  (GTREF) . 
<scope> Depending on the type of output ,  these models can be divided into two categories :   </scope> the constituentoutput systems (GREF) and  <scope> dependencyoutput systems </scope>  (GTREF) . 
<scope> Deriving lexical relatedness between terms has been a topic of interest with applications in </scope>  word sense disambiguation (REF) ,  paraphrasing (REF) ,  question answering (REF) ,  and  <scope> machine translation </scope>  (TREF)  <scope> to name a few .  </scope>
<scope> Despite ME theory and its related training algorithm </scope>  (REF) <scope> do not set restrictions on the range of feature functions1 ,  popular </scope>  NLP text books (REF) and <scope> research papers </scope>  (TREF)  <scope> seem to limit them to binary features .  </scope>
<scope> Determining methods for ordering modifiers prenominally and investigating the factors underlying modifier ordering have been areas of considerable research ,  including work in natural language processing </scope>  (GTREF) ,  linguistics (GREF) ,  and psychology (GREF) . 
<scope> Dialogue acts have been used to benefit tasks such as machine translation </scope>  (TREF) and the automatic detection of dialogue games (REF) . 
<scope> Different from the bilingual parsing </scope>  (GTREF)  <scope> that improves parsing performance with bilingual constraints </scope>  ,  and the bilingual grammar induction (GREF) that induces grammar from parallel text ,   <scope> the syntax projection aims to project the syntactic knowledge from one language to another .  </scope>
<scope> Differing from the interpolation smoothing algorithm used in </scope> (TREF) ,  both p(T) and p(W|T) were smoothed by back-off methods(REF) . 
<scope> Discourse disentanglement is the task of dividing a  </scope> conversation thread (GREF) or  <scope> document thread  </scope> (TREF)  <scope> into a set of distinct sub-discourses .  </scope>
<scope> Discourse segmentation is thought to facilitate </scope>  automatic summarization (GREF) ,  information retrieval (REF) ,  anaphora resolution (REF) and  <scope> question answering </scope>  (TREF) . 
<scope> Discovering Lexical Association in Text A 13 million word sample of Associated Press new stories from 1989 were automatically parsed by the Fidditch parser </scope>  (REF) <scope>  ,  using Church's part of speech analyzer as a preprocessor </scope>  (TREF) . 
<scope> Distance measures for CCG Our distance measures are related to those  </scope> proposed by REF ,   <scope> which are appropriate for binary trees (unlike those of  </scope> TREF) . 
<scope> Distributional analyses of large corpora have been shown to produce nuanced models of lexical similarity </scope>  (e . g .  TREF) as well as contextsensitive thesauri for a given domain (REF) . 
<scope> DITE or frame-based ,  with no explicit dialogue model ,  but task is explicitly represented as a frame  </scope> or a form (REF) ,  a task description table (REF) ,  a topic forest (REF) ,  <scope> or an agenda  </scope> (TREF) ,  etc .  Both system and user may take the initiative . 
<scope> Due in large part to the creation of biomedical treebanks </scope>  (GTREF) and rapid progress of data-driven parsers (GREF) ,   <scope> there are now fast ,  robust and accurate syntactic parsers for text in the biomedical domain </scope>  . 
<scope> Due to the importance of WN for NLP tasks ,  substantial research was done on direct or indirect automated extension of the English WN </scope>  (e . g .  ,  (TREF)) or WN in other languages (e . g .  ,  (REF)) . 
<scope> During the last decade ,  several machine learning methods for coreference resolution have been developed </scope>  ,  from local pairwise classifiers (REF)  <scope> to global learning methods </scope>  (GTREF) ,  from simple morphological ,  grammatical features to more liguistically rich features on syntactic structures and semantic relations (GREF) . 
<scope> During the past few years ,  verbal SRL has dominated the research on SRL with the availability of  </scope> FrameNet (REF) ,   <scope> PropBank </scope>  (TREF) ,  and the consecutive CoNLL shared tasks (REF & 2005) in English language . 
<scope> During the testing phase ,  we used automatically assigned POS and chunk tags by Tsuruoka ? s tagger4( </scope> TREF) and YamCha chunker5(REF) . 
<scope> Dynamic lexicon are used by a numer of NERC systems such as ones described in </scope>  (TREF) ,  (REF) and (REF) . 
<scope> Earlier studies by GREFusing the Negra treebank </scope>  (TREF)  <scope> reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negras flat constituent structures .  </scope>
<scope> Earlier work in a noncorpus based </scope>  (TREF) as well as corpus-based setting (REF)  <scope> attests to the usefulness of combining knowledge sources for inferring temporal relations .  </scope>
<scope> Early approaches included algorithms for efficiently calculating string prefix probabilities </scope>  (GTREF) and approaches to exploit such algorithms to produce n-gram models (GREF) . 
<scope> Early examples of the use of sparsity in natural language processing include maximum entropy classification </scope>  (TREF) ,  language modeling (REF) ,  and incremental parsing (REF) . 
<scope> EFOG is also related to FO-TAG </scope>  (TREF) and the HPSG approach (GREF)  <scope> in extending the domain of applicability of LP rules .  </scope>
<scope> Eigenvector centrality in particular has been successfully applied to many different types of networks ,  including </scope>  hyperlinked web pages (GREF) ,  lexical networks (GREF) ,  and  <scope> semantic networks </scope>  (TREF) . 
<scope> Endemic structural ambiguity ,  which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence ,  can be greatly reduced by adding empirically derived probabilities to grammar rules </scope>  (GTREF) and by computing statistical measures of lexical association (REF) . 
<scope> Especially ,  when incorporated with forest ,  the correspondent forest-based tree-to-string models </scope>  (GTREF) ,   <scope> tree-to-tree models </scope>  (REF)  <scope> have achieved a promising improvements over correspondent treebased systems </scope>  . 
<scope> Essentially there are different ways to estimate distributional similarity between two words </scope>  (TREF) ,  and the one we propose to use is confusion probability (REF) . 
<scope> Essentially ,  we follow </scope>  TREF  <scope> in using a rich ontology </scope>  and a representation scheme that makes explicit all the individuals and abstract objects (i . e .   ,  propositions ,  facts/beliefs ,  and eventualities) (REF) involved in the LF interpretation of an utterance . 
<scope> Estimated clues are derived from the parallel data using ,  for example ,  measures of co-occurrence (e . g .  </scope>  the Dice coefficient (REF)) ,  statistical alignment models (e . g .  IBM models from statistical machine translation (REF)) ,  or  <scope> string similarity measures (e . g .  the longest common sub-sequence ratio  </scope> (TREF)) . 
<scope> Evaluated on a Chinese-to-English translation task ,  our approach </scope>  improves translation quality over a popular PSCFG baselinethe hierarchical model of REF and  <scope> performs on par with the model of  </scope> TREF ,   <scope> using heuristically generated labels from parse trees .  </scope>
<scope> Evaluating the algorithm on the output of Charniaks parser </scope>  (TREF) and the Penn treebank (REF)  <scope> shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity </scope>  . 
<scope> Evaluationofinductionmethodsisdifficult ,  dueto </scope>  the variety of corpora and tagsets in existence (see discussion in REF) and  <scope> the variety of potential purposes for induced categories </scope>  (e . g .  ,  GTREF) . 
<scope> Evaluation Results For the evaluation of our system ,  we used a number of widely accepted automatic metrics ,  namely </scope>  BLEU (REF) ,   <scope> METEOR </scope>  (TREF) ,  TER (REF) and inverse F-Score based on token-level precision and recall . 
<scope> Even unsupervised attempts to learn semantic roles have required </scope>  a pre-defined set of roles (REF) and  <scope> often a hand-labeled seed corpus  </scope> (GTREF) . 
<scope> Examples are the Penn Treebank </scope>  (GTREF)  <scope> annotated at the University of Pennsylvania </scope>  ,  the Negra corpus (REF) developed in Saarbriicken ,  the Verbmobil treebank~ (REF) annotated in Tiibingen *The work presented here was done as part of a project in SFB 441 "Linguistic Data Structures" at the University of Tiibingen . 
<scope> Examples include </scope>  RTE (GREF) ;  string matching (REF) ;  transliteration (REF) ;  and  <scope> paraphrase identification </scope>  (GTREF) . 
<scope> Examples include </scope>  summarization (REF) ,   <scope> question answering </scope>  (TREF) and machine translation (REF) . 
<scope> Examples include </scope>  text summarisation (REF) ,   <scope> subtitle generation from spoken transcripts </scope>  (TREF) and information retrieval (REF) . 
<scope> Examples include </scope>  the Cambridge Learners Corpus2 used in (REF) ,  and  <scope> TOEFL data ,  used in </scope>  (TREF) . 
<scope> Examples include using bootstrapping to amplify small seed sets of example outputs (GREF) ,  leveraging existing databases that overlap with the text  </scope> (GTREF) ,  and learning general domain-independent knowledge bases by exploiting redundancies in large web and news corpora (GREF) . 
<scope> Examples of AL used in language engineering include </scope>  named entity recognition (GREF) ,  text categorization (GREF) ,   <scope> part-of-speech tagging </scope>  (TREF) ,  and parsing (GREF) . 
<scope> Examples of classifier-based IE systems are </scope>  SRV (REF) ,  HMM approaches (REF) ,  ALICE (REF) ,  and  <scope> Relational Markov Networks </scope>  (TREF) . 
<scope> Examples of classifier-based IE systems are </scope>  SRV (REF) ,  HMM approaches (REF) ,   <scope> ALICE </scope>  (TREF) ,  and Relational Markov Networks (REF) . 
<scope> Examples of metameasures based on this criterion are ORANGE </scope>  (TREF) and KING (REF) . 
<scope> Examples of such approaches have exploited techniques including integer linear programming (ILP)  </scope> (TREF) ,  label propagation (REF) ,  and minimum cuts (REF) . 
<scope> Examples of such models and their implementation are the  </scope> informationstate-update approach (an implemented system is described in (REF)) ,  or  more  <scope> linguisticallyorientedapproachesliketheadjacency-pair models or intentional models such as GROSZ and SIDNERs  </scope> (see (TREF)) . 
<scope> Examples of this line of research include Model 6 </scope>  (TREF) and the EMD training approach proposed by REF and its extension called LEAF aligner (REF) . 
<scope> Except for the more simple ,  although not unproblematic ,  extensions to PATR-II like the ones proposed by  </scope> TREF ,  most of these efforts have their root in the work by Rounds ,  Kasper and Moshier (GREF) ,  who give the proof of the existence of a sound ,  although non classical ,  logical interPretation for disjunctive and negative feature specifications . 
<scope> Existing CSD approaches focus on a particular class of language phenomena (especially negation or hedging) and use hand-crafted rules </scope>  (TREF) or a supervised learning approach that exploits corpora manually annotated at the token-level for a particular type of content shifter (REF) . 
<scope> Existing DOP models converge to STSGs that either (i) give all subtrees of the observed trees nonzero weights </scope>  (GTREF) ,  or (ii) give only the largest possible fragments nonzero weights (GREF) . 
<scope> Existing handannotated corpora like SemCor </scope>  (TREF) ,   <scope> which is annotated with WordNet senses </scope>  (REF)  <scope> allow for a small improvement over the simple most frequent sense heuristic ,  as attested in the allwords track of the last Senseval competition </scope>  (REF) . 
<scope> Existing handannotated corpora </scope>  like SemCor (REF) ,  which is annotated with WordNet senses (REF)  <scope> allow for a small improvement over the simple most frequent sense heuristic ,  as attested in the allwords track of the last Senseval competition </scope>  (TREF) . 
<scope> Experiments We have conducted a series of lexical acquisition experiments with the above algorithm on largescale English corpora ,  e . g .  ,  </scope>  the Brown corpus [REF] and  <scope> the PTB WSJ corpus </scope>  [TREF] . 
<scope> Expressions that co-occur with the given expression ,  such as adjacent words </scope>  (GTREF) ,  and modifiers/modifiees (GREF) ,  have so far been examined . 
<scope> Extending a technique presented in </scope>  (REF)  <scope> and adopted in </scope>  (GTREF)  <scope> for function labels </scope>  ,   <scope> we split some part-of-speech tags into tags marked with semantic role labels .  </scope>
<scope> Extensions range from part of speech tagging </scope>  (GTREF) to shallow syntactic analysis (GREF) and full-blown parsing (REF) . 
<scope> Extensions to Hiero Several authors describe extensions to Hiero ,  to incorporate additional syntactic information </scope>  (GTREF) ,  or to combine it with discriminative latent models (REF) . 
<scope> Extensions to Hiero Several authors describe extensions to Hiero ,  to incorporate additional syntactic information  </scope> (GTREF) ,  or to combine it with discriminative latent models (REF) . 
<scope> Extensions to PARSIFAL have been researched independently including </scope>  the parsing of ungrammatical sentences in PARAGRAM (REF) ,  <scope> the resolution of lexical ambiguities in ROBIE </scope>  (TREF) ,  and the acquiring of syntactic rules from examples in LPARSIFAL (REF) . 
<scope> Feature function scaling factors m are optimized based on a maximum likelihood approach </scope>  (TREF) or on a direct error minimization approach (REF) . 
<scope> Figure 2 :  Multimodal Architecture 2 . 2 Multimodal Integration and Understanding Our approach to integrating and interpreting multimodal inputs  </scope> (GTREF) is an extension of the finite-state approach previously proposed (GREF) . 
<scope> Finally ,  many other projects in machine learning of natural language use input that is annotated in some way ,  either with part-of-speech tags </scope>  (GTREF) or with syntactic brackets (REF) . 
<scope> Finally ,  many other projects in machine learning of natural language use input that is annotated in some way ,  </scope>  either with part-of-speech tags (GREF) or  <scope> with syntactic brackets  </scope> (TREF) . 
<scope> Finally ,  the idea of LMS is similar to the techniques for sentence selection based on rare n-gram co-occurrences used in </scope>  machine translation (REF) and  <scope> syntactic parsing </scope>  (TREF) . 
<scope> Finally ,  the parameters  i of the log-linear model (18) are learned by minimumerror-rate training </scope>  (TREF) ,   <scope> which tries to set the parameters so as to maximize the BLEU score </scope>  (REF)  <scope> of a development set .  </scope>
<scope> Finally ,  we are investigating several avenues for using this system output for Machine Translation (MT) including :  (1) aiding word alignment for other MT system  </scope> (TREF) ;  and (2) aiding the creation various MT models involving analyzed text ,  e . g .  ,  (GREF) . 
<scope> Finally ,  we test the utility of the extended Treebank for training statistical models on two tasks :  NP bracketing </scope>  (GTREF) and full parsing (REF) . 
<scope> Finally ,  we would like to investigate the incorporation of unsupervised methods for WSD ,  such as the heuristically-based methods of </scope>  (TREF) and (REF) ,  and the theoretically purer bootstrapping method of (REF) . 
<scope> First ,  there is some evidence suggesting that standoff annotation and embedded XML are the two most highly preferred corpus annotation formats ,  and second ,  these formats are employed by the two largest extant curated biomedical corpora </scope>  ,  GENIA (REF) and  <scope> BioIE </scope>  (TREF) . 
<scope> First ,  two estimates of importance on words have been used very successfully both in generic and query-focused summarization :  </scope>  frequency (GREF) and  <scope> loglikelihood ratio </scope>  (GTREF) . 
<scope> First we extended the baseline vocabulary with words from a small in-domain training corpus (REF) ,  and then we used N-grams with these new words in our web queries ,  e . g .  wireless mikes like ,  I know that recognizer for a meeting transcription task </scope>  (TREF) . 
<scope> First ,  we observe without details that we can easily achieve this by starting instead with the algorithm of  </scope> REF ,  <scope> 20 rather than </scope>  TREF ,  and again refusing to add long tree dependencies . 
<scope> First we used the C&C Combinatory Categorial Grammar (CCG) parser5 (C&C) by </scope>  TREF  <scope> using the biomedical model described in </scope>  REF  <scope> which was trained on GTB .  </scope>
<scope> Following common practice </scope>  (GTREF) <scope>  ,  we employed a coarse-to-fine procedure to prune away unlikely candidate arcs ,  as described by REF .  </scope>
<scope> Following the previous work in relation extraction </scope>  (GTREF) ,   <scope> we use the standard convolution tree kernel (REF) to count the number of common sub-trees as the structural similarity between two parse trees .  </scope>
<scope> Following the two-stage prediction aggregation methods  </scope> (TREF) ,  such pre-labeled results ,  together with other conventional features used by the state-of-the-art NER systems ,  are fed into a linear Conditional Random Fields (CRF) (REF) model ,  which conducts fine-grained tweet level NER . 
<scope> Following this discriminative approach ,  techniques for efficiency were investigated for </scope>  estimation (GREFMalouf and van REF) and  <scope> parsing </scope>  (GTREF) . 
<scope> For algorithms whose packed representations are graphs ,  such as </scope>  Hidden Markov Models and other nitestate methods ,   <scope> Ratnaparkhis MXPARSE parser </scope>  (TREF) ,  and many stack-based machine translation decoders (GREF) ,  the k-best paths problem is well-studied in both pure algorithmic context (see (REF) and (REF) for surveys) and NLP/Speech community (GREF) . 
<scope> For all our parsing experiments ,  we use the REF version of the two-stage parser reported in </scope>  TREF . 
<scope> For Arabic ,  the gold segmentation was obtained using a highly accurate Arabic morphological analyzer </scope>  (TREF) ;  for Hebrew ,  from a Bible edition distributed by Westminster Hebrew Institute (REF) . 
<scope> For a second set of parsing experiments ,  we used the WSJ portion of the Penn Tree Bank </scope>  (TREF) and Helmut Schmids enrichment program tmod (REF) . 
<scope> For classi cation ,  we use a maximum entropy model </scope>  (TREF) ,  from the logistic regression package in Weka (REF) ,  with all default parameter settings . 
<scope> For comparison ,  we use the MT training program ,  GIZA +  +  </scope>  (TREF) ,  the phrase-base decoder ,  Pharaoh (REF) ,  and the wordbased decoder ,  Rewrite (REF) . 
<scope> For determining whether an opinion sentence is positive or negative ,  we have used seed words similar to those produced by </scope>  (TREF) and extended them to construct a much larger set of semantically oriented words with a method similar to that proposed by (REF) . 
<scope> For each differently tokenized corpus ,  we computed word alignments by a HMM translation model  </scope> (TREF) and by a word alignment refinement heuristic of grow-diagfinal (REF) . 
<scope> For each training direction ,  we run GIZA +  +  </scope>  (REF) ,   <scope> specifying 5 iterations of Model 1 ,  4 iterations of the HMM model </scope>  (TREF) ,   <scope> and 4 iterations of Model 4 .  </scope>
<scope> For each training direction ,  we run GIZA +  +  </scope>  (TREF) <scope>  ,  specifying 5 iterations of Model 1 ,  4 iterations of the HMM model </scope>  (REF) <scope>  ,  and 4 iterations of Model 4 .  </scope>
<scope> For English ,  we used the Penn Treebank </scope>  (TREF)  <scope> in our experiments </scope>  and the tool Penn2Malt7 to convert the data into dependency structures using a standard set of head rules (REF) . 
<scope> Forest-based translation frameworks ,  which make use of packed parse forests on the source and/or target language side(s) ,  are an increasingly promising approach to syntax-based SMT ,  being both algorithmically appealing  </scope> (TREF) and empirically successful (GREF) . 
<scope> For evaluation of the given task ,  we have incorporated evaluation techniques based on current evaluation techniques used in machine translation ,  BLEU </scope>  (TREF) and METEOR (REF) . 
<scope> For evaluation of the given task ,  we have incorporated evaluation techniques based on current evaluation techniques used in machine translation </scope>  ,  BLEU (REF) and METEOR (TREF) . 
<scope> For evaluation we use a state-of-the-art baseline system (Moses)  </scope> (TREF) which works with a log-linear interpolation of feature functions optimized by MERT (REF) . 
<scope> For example ,  210 by using dependency-based kernels  </scope> (GTREF) or syntactic kernels (GREF) or by including the word categories and their POS tags in the subsequences . 
<scope> For example ,  for constituent syntactic parsing (GREF) for Chinese ,  in addition to the most popular treebank Chinese Treebank (CTB) </scope>  (TREF) ,   <scope> there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (REF) .  </scope>
<scope> For example ,  in our previous work </scope>  (TREF) ,   <scope> we have used a statistical translation memory of phrases in conjunction with a statistical translation model (REF) .  </scope>
<scope> For example ,  it meant that simple word alignment models like IBM models 1 and 2  </scope> (TREF) and the HMM model (REF)  <scope> came many weeks after HMMs were introduced in the context of part-of-speech tagging .  </scope>
<scope> For example ,  it meant that simple word alignment models like  </scope> IBM models 1 and 2 (REF) and <scope> the HMM model  </scope> (TREF) <scope> came many weeks after HMMs were introduced in the context of part-of-speech tagging .  </scope>
<scope> For example ,  SCFs have been used for verb disambiguation and classification </scope>  (Schulte im GTREF) and SCCs for semantic role labeling (GREF) . 
<scope> For example </scope>  ,  Chinese-English (REF) ,   <scope> Swedish-English-Polish </scope>  (TREF) ,  and Chinese-Korean (REF) . 
<scope> For example ,  </scope>  REF apply the Lemur IR toolkit ,  REF use the BM25 similarity measure ,  and TREF  <scope> use cosine similarity </scope>  . 
<scope> For example ,   </scope> REF  <scope> presents an analysis of Spanish verbal gaps based on the Minimal Generalization Learner ( </scope> TREF) . 
<scope> For example ,   </scope> (TREF)  <scope> described an adaptive CWS system </scope>  ,  and (REF) employed a conditional random field model for sequence segmentation . 
<scope> For example ,  </scope>  TREF  <scope> filtered out bilingual term pairs with low monolingual frequencies (those below 100 times) ,  </scope>  while REF restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words . 
<scope> For example ,  the dialogue act representation can be used to provide a broad range of text-planning inputs for a stochastic sentence planner in the travel domain  </scope> (TREF) ,  or to represent the systems dialogue strategies for reinforcement learning (REF ;  Schef er and REF) . 
<scope> For example ,  the discourse use of and is defined as 'parallelism' in </scope>  TREF ,  'a marker of addition' or 'sequential continuity' in REF ,  and 'conjunction' in REF . 
<scope> For example ,  the works by </scope>  REF ,  and TREF  <scope> use graph measures to extensively analyze WordNet properties .  </scope>
<scope> For instance ,  functional words in one language tend to correspond to functional words in another language </scope>  (TREF) ,  and the syntactic dependency of words in each language can help the alignment process (REF) . 
<scope> For instance ,  one can formulate the sentence extraction task as </scope>  a sequence labeling problem similar to (REF) ,   <scope> or use a more expressive graphical model such as in </scope>  (GTREF) . 
<scope> For instance ,  some methods employ a hand-coded similarity function </scope>  (TREF) ,  while others assume knowledge of the phonetic mapping or require parallel cognate pairs to learn a similarity function (<span class="refstyle ref31467" id="s31467 . r2">REF<div class="tooltip fixed" style="display :  none ;  "><ref>Bouchard et al .  ,  2007</ref></div%3.
<scope> For learning multilingual subjectivity ,  the literature tentatively concludes that translating lexicon is less dependable in terms of preserving subjectivity than corpus translation </scope>  (GTREF) ,  and though corpus translation results in modest performance degradation ,  it provides a viable approach because no manual labor is required (GREF) . 
<scope> For learning ,  we are using the Timbl memory based learning algorithm </scope>  (REF) ,   <scope> which was previously found useful for the task of word sense disambiguation </scope>  (TREF) . 
<scope> Formalisms exemplifying each of these knowledge types are </scope>  REF scripts ,  REF schemas ,  and TREF  <scope> domain-specific schemas ,   </scope> respectively . 
<scope> For microplanning ,  we have implemented the algorithm for reference planning described in  </scope> [TREF] and the aggregation algorithm described in [REF] . 
<scope> For more specific lexical relationships like relationships between </scope>  verbs (REF) ,  nominals (GREF) or  <scope> meronymy subtypes </scope>  (TREF)  <scope> there is still little agreement which important relationships should be defined .  </scope>
<scope> For more specific lexical relationships like relationships between verbs </scope>  (TREF) ,  nominals (GREF) or meronymy subtypes (REF)  <scope> there is still little agreement which important relationships should be defined .  </scope>
<scope> For nominal and pronoun mentions ,  there are several well-studied anaphora cues </scope>  ,  including centering (REF) ,  nearness (REF) ,  and deterministic constraints ,   <scope> which have all been utilized in prior coreference work </scope>  (GTREF) . 
<scope> For our core English experiments we report results on the entire Penn .  Treebank </scope>  (REF) ,  while for other languages we use the corpora made available for the CoNLL-X Shared Task (TREF) . 
<scope> For parse forest pruning (REF) ,  we utilize the Margin-based pruning algorithm presented in </scope>  (TREF) . 
<scope> For right-branching structures ,  the left-corner ancestor is the parent </scope>  ,  conditioning on which has been found to be beneficial (REF) ,   <scope> as has conditioning on the left-corner child </scope>  (TREF) . 
<scope> For SCFG ,  </scope>  TREF  <scope> showed that the exponent in the time complexity of parsing algorithms must grow at least as fast as the square root of the rule rank </scope>  ,  and REF tightened this bound to be linear in the rank . 
<scope> For simplicity ,  we extract class labels using the seed-based approach proposed by </scope>  REF <scope> (in particular  </scope> REF) <scope> which generalizes </scope>  TREF . 
<scope> For such a genre ,  a </scope>  domain-dependent bottom-up planner (REF) or  <scope> opportunistic planner  </scope> (TREF)  <scope> suits better than a domainindependent top-down planner .  </scope>
<scope> For the BioNLP09 shared task it was observed that the Bikel parser9  </scope> (TREF) ,  which used a non-biomedical model and can be argued that it uses the somewhat dated Collins parsing model (REF) ,  did not contribute towards event extraction performance as strongly as other parses supplied for the same data . 
<scope> For the research we present here ,  we have taken the lexicon of a HPSG-based grammars developed in the LKB platform </scope>  (REF)  <scope> for Spanish ,  similarly to the work </scope>  of TREF . 
<scope> For the segmentation task ,  we compare to BayesSeg </scope>  (REF) , 10  <scope> a Bayesian topic-based segmentation model that outperforms previous segmentation approaches </scope>  (GTREF) . 
<scope> For the system combination task ,  we first use the minimum Bayes-risk (MBR) </scope>  (TREF)  <scope> decoder to select the best hypothesis as the alignment reference for the confusion network (CN) </scope>  (REF) . 
<scope> For the task ,  many machine learning methods have been proposed ,  including </scope>  supervised methods (GREF) ,  semisupervised methods (GREF) ,  and  <scope> unsupervised method </scope>  (TREF) . 
<scope> For the training of the SMT models ,  standard word alignment  </scope> (TREF) and language modeling (REF) tools  <scope> were used </scope>  . 
<scope> For this purpose ,  our system is based on a combination of subword-based tagging method  </scope> (TREF) and accessor variety-based new word recognition method (REF) . 
<scope> For this purpose ,  we use a recursive spectral partitioning algorithm ,  a variant of spectral clustering (REF) which obtains an average Vmeasure </scope>  (TREF)  <scope> of 0 . 93 when clustering just pyramid contributors labeled by their SCUs .  </scope>
<scope> For this reason ,  using the Web has proved successful in several other fields of NLP ,  e . g .  ,  </scope>  machine translation (REF) and  <scope> bigram frequency estimation  </scope> (TREF) . 
<scope> For transition-based models ,  the trend is to alleviate error propagation by abandoning greedy ,  deterministic inference in favor of beam search with globally normalized models for scoring transition sequences </scope>  ,  either generative (GREF) or  <scope> conditional </scope>  (GTREF) . 
<scope> For years ,  the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics ,  such as </scope>  BLEU (REF) ,  NIST (Doddington , 2002) , METEOR(REF) ,  PER (REF) ,   <scope> CDER  </scope> (TREF) and WER (Nie en et al .   ,  2000) . 
<scope> Frequently </scope>  ,  production systems (REF) ,   <scope> semantic chunkers </scope>  (TREF) or simple word-to-concept lexica (REF)  <scope> are employed for this task .  </scope>
<scope> From sentence-to-sentence aligned corpora ,  symbolic </scope>  (TREF) ,  statistical (REF) ,  or hybrid techniques (REF) <scope> are used for word and expression alignments .  </scope>
<scope> From the parsing perspective ,  the redundancy of analyses can be controlled by </scope>  (1) grammar rewriting (REF) ,  (2) checking the chart for PAS equivalence (GREF) ,  (3) making the processor parsimonious on using long-distance compositions (REF) ,  or (4)  <scope> parsing into normal forms </scope>  (GTREF) . 
<scope> From this viewpoint ,  research on paraphrasing has adapted techniques fostered in the literature of machine translation </scope>   <scope> (MT) ,  such as </scope>  transformation algorithms (GREF) ,   <scope> corpus-based techniques for paraphrase pattern acquisition </scope>  (GTREF) ,  and fluency measurements (GREF) . 
<scope> Fujitsu </scope>  (TREF) ,  Hitachi ,  Toshiba (REF) ,  and NEC (REF) ,   <scope> among others ,  support large projects generally concentrating on the translation of computer manuals </scope>  . 
<scope> Further linguistic markup is added using </scope>  the morpha lemmatiser (REF) and <scope> the C&C named entity tagger </scope>  (TREF) <scope> trained on the data from MUC-7 .  </scope>
<scope> Furthermore ,  as a standalone sentence compression system it yields state of the art performance ,  comparable to </scope>  TREF  <scope> discriminative model </scope>  and superior to Hedge Trimmer (REF) ,  a less sophisticated deterministic system . 
<scope> Furthermore ,  CRFs have been used successfully in information extraction </scope>  (TREF) ,  named entity recognition (GREF) and sentence parsing (REF) . 
<scope> Furthermore ,  most implementations </scope>  either contain no evaluation (GREF) or  <scope> report results on the development data </scope>  (TREF) . 
<scope> Furthermore ,  statistical generation systems </scope>  (GTREF)  <scope> could use  as a means of directly optimizing information ordering ,  much in the same way MT systems optimize model parameters using BLEU as a measure of translation quality </scope>  (REF) . 
<scope> Furthermore the new features on the edges are introduced by formula (6) ,  which is then tuned on the development set using MERT </scope>  (TREF) in the log-linear model (REF) . 
<scope> Furthermore ,  these systems have tackled the problem at different levels of granularity ,  from the document level (REF) ,  sentence level (GREF) ,  phrase level (GREF) ,  as well as the speaker level in debates </scope>  (TREF) . 
<scope> Furthermore ,  these systems have tackled the problem at different levels of granularity ,  from the document level </scope>  (TREF) ,  sentence level (GREF) ,  phrase level (GREF) ,  as well as the speaker level in debates (REF) . 
<scope> Furthermore ,  those techniques </scope>  have largely relied on different voting schemes in the past (GREF) ,  and  <scope> only more recently have started using actual posteriors from the underlying models </scope>  (GTREF) . 
<scope> Furthermore ,  training corpora for information extraction are typically annotated with domain-specific tags ,  in contrast to general-purpose annotations such as part-of-speech tags or noun-phrase bracketing (e . g </scope>  .   ,  the Brown Corpus [REF] and  <scope> the Penn Treebank </scope>  [TREF]) . 
<scope> Furthermore ,  we consider an F-score measure that is adapted from </scope>  dependency-based parsing (REF) and <scope> sentence-condensation  </scope> (TREF) . 
<scope> Furthermore ,  we note that recent work indicates that verb alternations may also play a role in automatic processing of language for applied tasks ,  such as question-answering  </scope> (TREF) ,  detection of text relations (REF) ,  and determination of verb-particle constructions (REF) . 
<scope> Further recent refinements in methods for readability prediction include using machine learning methods such as </scope>  Support Vector Machines (REF) ,  <scope> log-linear models  </scope> (TREF) ,  k-NN classifiers and combining semantic and grammatical features (REF) . 
<scope> Further work will also include making use of a lexical resource external to the treebank </scope>  (GTREF) and investigating clustering techniques to reduce data sparseness (REF) . 
<scope> Future work could explore softening constraints  </scope> (GTREF) ,  perhaps using features (REF ;  Berg-REF) or by learning to associate different settings with various marks :  Simply adding a hidden tag for ordinary versus divide types of punctuation (REF) may already usefully extend our model . 
<scope> GE constraints have also been used to leverage labeled features in </scope>  document classification [REF] and  <scope> information extraction </scope>  [GTREF] ,  and to incorporate linguistic prior knowledge into dependency grammar induction [REF] . 
<scope> GE constraints have also been used to leverage labeled features in </scope>  document classification [REF] and  <scope> information extraction </scope>  [GTREF] ,  <scope> and to incorporate linguistic prior knowledge into dependency grammar induction </scope>  [REF] . 
<scope> Generalized expectation (GE) </scope>  (GTREF)  <scope> is a recently proposed framework for incorporating prior knowledge into the learning of conditional random fields (CRFs) </scope>  (REF) . 
<scope> Generative alignment models like </scope>  the HMM model (REF) and  <scope> IBM models 4 and above </scope>  (GTREF)  <scope> directly model correlations between alignments of consecutive words (at least on one side) .  </scope>
<scope> Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing </scope>  (GTREF) ,  but recently discriminative models attract more attention due to their superior accuracy (GREF) and adaptability to new grammars and languages (REF) . 
<scope> German Lemmas were predicted by </scope>  TreeTagger (REF) ,  PoS and morphology by RFTagger (REF) ,  and  <scope> dependency information by MaltParser </scope>  (TREF) . 
<scope> GIZA +  +  (REF) is the most widely used implementation of the IBM Models and the 2 HMM model </scope>  (TREF) . 
<scope> GIZA +  +  </scope>  (TREF)  <scope> was employed to train word alignments </scope>  ,  language models have been created with the SRILM toolkit (REF) . 
<scope> Grammar transformation techniques such as linguistically inspired non-terminal annotations </scope>  (GTREF) and latent variable grammars (GREF)  <scope> have increased the grammar size |G| from a few thousand rules to several million in an explicitly enumerable grammar ,  or even more in an implicit grammar .  </scope>
<scope> GTM </scope>  (TREF) and the variants of ROUGE (REF)  <scope> concentrate on matched longest common substring and discontinuous substring of translation output according to the human references </scope>  . 
<scope> Handcrafted rules  </scope> (TREF)  <scope> as well as language models also have been utilized to generate fluent compressions </scope>  (GREF) . 
<scope> Harvested lexical resources ,  such as </scope>  concept lists (REF) ,  facts (REF) and  <scope> semantic relations </scope>  (TREF) could be then successfully used in different frameworks and applications . 
<scope> He has achieved state-of-the art results by applying M . E .  to parsing </scope>  (TREF) ,  part-of-speech tagging (REF) ,  and sentence-boundary detection (REF) . 
<scope> Hence ,  we use the Maximum Entropy (MaxEnt) </scope>  (TREF) and Conditional Random Fields (CRFs) (REF) approaches to build statistical models for structural event detection . 
<scope> Here we will focus on some of the most widely used ,  namely </scope>  BLEU (REF) ,  NIST (REF) ,   <scope> METEOR  </scope> (TREF) and WER (REF) . 
<scope> Hierarchical Bayesian modelling has recently gained notable popularity in many core areas of natural language processing ,   </scope> from morphological segmentation (REF) to  <scope> opinion modelling </scope>  (TREF) . 
<scope> Hoffman </scope>  ,  in her thesis (GREF) ,   <scope> has used the MultisetCombinatory Categorial Grammar formalism </scope>  (TREF) ,   <scope> an extension of Combinatory Categorial Grammar to handle free word order languages ,  to develop a generator for Turkish .  </scope>
<scope> However ,  alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA +  +  </scope>  (TREF) or by local maximization (REF) . 
<scope> However ,  as Kay remarked (e . g .  in </scope>  /TREF/) ,  <scope> phonology was actually what was done in </scope>  /REF/ . 
<scope> However ,  as  </scope> TREF  <scope> argue ,  the sense distinctions between the literal and metonymical meanings of a word are not of a topical 3With SVD ,  I set the number of dimensions to 300 ,  </scope>  as in REF . 
<scope> However ,  automatic Chinese word segmentation and tagging has been recognized as a very difficult task </scope>  (TREF) ,  for the following reasons :  First ,  Chinese text provides few cues for word boundaries (GREF) and part-ofspeech (POS) information . 
<scope> However ,  each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with  </scope> seed patterns (GREF) ,  pattern-based rules (REF) ,  relation keywords (REF) ,  or  <scope> word pairs exemplifying relation instances </scope>  (GTREF) . 
<scope> However ,  for the particular case of Chinese source inputs ,  although the DE construction has been addressed for both PBSMT and HPBSMT systems in  </scope> (GREF) ,  <scope> as indicated b </scope> y (TREF) ,  there are still lots of unexamined structures that imply source-side reordering ,  especially in the nondeterministic approach . 
<scope> However ,  in </scope>  TREF as well as REF  <scope> we find that human can assign the binary distinction to word senses with a high level of reliability .  </scope>
<scope> However ,  most question answering systems target factoid questions  the research of non-factoid questions started only a few years ago but limited to several kinds ,  such as </scope>  definitional questions (REF) and  <scope> questions asking for biographies </scope>  (TREF) . 
<scope> However ,  some of the systems restrict the search for arguments only to the immediate clause </scope>  (GTREF) and others use the clause hierarchy to guide the exploration of the sentence (GREF) . 
<scope> However ,  the coverage of WordNet is still limited in many regions (even well-studied ones such as the concepts and instances below Animals and People) ,  as noted by researchers </scope>  such as (REF) and (TREF)  <scope> who perform automated semantic class learning .  </scope>
<scope> However ,  the maximum entrop </scope> y (REF) <scope> was found to yield higher accuracy than nave Bayes in a subsequent comparison by </scope>  TREF ,   <scope> who used a different subset of either Senseval-1 or Senseval-2 English lexical sample data .  </scope>
<scope> However ,  there are other linguistic phenomena </scope>  ,  such as alignment (REF) ,  consistency (REF) ,  and variation ,   <scope> which influence peoples assessment of </scope>  discourse (REF) and  <scope> generated output </scope>  (GTREF) . 
<scope> However ,  these unsupervised methodologies show a major drawback by extracting quasi-exact or even exact match pairs of sentences as they rely on classical string similarity measures such as </scope>  the Edit Distance in the case of (REF) and  <scope> Word N-gram Overlap for </scope>  (TREF) . 
<scope> However ,  they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies ,  such as </scope>  the state-of-the-art phrasebased system (REF) ,   <scope> syntax-based systems </scope>  (GTREF) . 
<scope> However ,  they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies ,  such as </scope>  the state-of-the-art phrasebased system (REF) ,   <scope> syntax-based systems  </scope> (GTREF) . 
<scope> However ,  this sense can vary across corpora (REF) ,  and assuming a single sense is inadequate for an important group of medium and high frequency verbs whose distribution of senses in balanced corpus data is flat rather than zipfian </scope>  (TREF) . 
<scope> However ,  to the best of our knowledge only the model of  </scope> TREF ,  which is based on a Conditional Random Field (CRF) (REF) ,   <scope> can compute word indices pairs directional posterior probabilities ,   </scope> like those computed by the HMM models . 
<scope> However ,  training data has either been small </scope>  (TREF) ,  only partly annotated (REF) ,  or artificially created (GREF) . 
<scope> However ,  we can learn to attribute some similarity between </scope>  (TREF)  <scope> and the second publication using the text in (REF) </scope>  . 
<scope> If they can not ,  we perform word alignment using some publicly available tools </scope>  ,  such as Giza +  +  (REF) or  <scope> Berkeley Aligner </scope>  (GTREF) . 
<scope> If words can be defined with concepts in a hierarchical structure ,  it is possible to measure the meaning similarity between words with </scope>  an information measure based on WordNet (REF) ,  or  <scope> structure level information based on a thesaurus </scope>  (TREF) . 
<scope> ILP models have been successfully applied in several natural language processing tasks ,  including </scope>  relation extraction (REF) ,   <scope> semantic role labeling  </scope> (TREF) and the generation of route directions (REF) . 
<scope> Impressive results have been achieved culminating in the state-of-the-art parser of  </scope> TREF  <scope> which has been used as the parser for the Pascal Rich Textual Entailment Challenge entry of </scope>  REF . 
<scope> In addition ,  corpus-based stochastic modelling of lexical patterns </scope>  (see TREF) may provide information about word sense frequency of the kind advocated since (REF) . 
<scope> In addition ,  if our results were fully automated ,  they could also be used to enhance the ability of understanding systems to recognize discourse structure ,  which in turn improves tasks such as information retrieval </scope>  (TREF) and plan recognition (REF) . 
<scope> In addition ,  it has a relatively simple method of computing possible quantifier scoping ,  drawing from the approaches by </scope>  (REF) and (TREF) . 
<scope> In addition to the original PropBank by Palmer et al .  ,  numerous PropBanks have been developed for languages other than English (e . g .  Chinese </scope>  (TREF) and Arabic (REF)) . 
<scope> In addition to these basic types ,  several studies deal with the discovery and labeling of more specific relation sub-types ,  including inter-verb relations </scope>  (TREF) and nouncompound relationships (REF) . 
<scope> In addition ,  unlike other feature-based methods ,  such as </scope>  CRFs (REF) ,   <scope> MaxEnt </scope>  (TREF) or SVM (REF) ,   <scope> the JSCM model directly computes model probabilities using maximum likelihood estimation (REF) .  </scope>
<scope> In addition we intend to employ classic Information Extraction techniques (described below) such as named entity recognition </scope>  (TREF) in order to pre-process the text ,  as the identification of complex terms such as proper names ,  dates ,  numbers ,  etc ,  allows to reduce data sparseness in learning (REF) . 
<scope> In all experiments ,  we use the backoff smoothing method of </scope>  (TREF) ,   <scope> with additive smoothing </scope>  (REF)  <scope> for the lexical probabilities </scope>  . 
<scope> In a previous paper (REF) ,  we presented a first comparison between the two previous role sets on the SemEval-2007 Task 17 corpus </scope>  (TREF) . 
<scope> In a previous study using the same video recordings but a different ,  simpler scheme </scope>  (TREF) ,   <scope> facial displays could only be associated with single leaf nodes (i . e .   ,  words) </scope>  ;  that is ,  in the terminology of REF ,  all motions were considered to be batons rather than underliners . 
<scope> In computational linguistics ,  a multitude of tasks is sensitive to selectional preferences ,  such as  </scope> the resolution of ambiguous attachments (REF) ,  word sense disambiguation (REF) ,   <scope> semantic role labelling </scope>  (TREF) ,  or testing the applicability of inference rules (REF) . 
<scope> In contrast ,  previously proposed supervised approaches have used segmented training sets ranging from 1000-5000 sentences (REF) to 190 , 000 sentences </scope>  (TREF) . 
<scope> In contrast ,  the idea of bootstrapping for relation and information extraction was first proposed in (REF) ,  and successfully applied to </scope>  the construction of semantic lexicons (REF) ,  named entity recognition (REF) ,  extraction of binary relations (REF) ,  and  <scope> acquisition of structured data for tasks such as Question Answering </scope>  (GTREF) . 
<scope> In contrast ,  the majority of the previous work </scope>  either ignores this contextual information (GREF) or  <scope> makes limited use of the 85 discourse structure hierarchy by flattening it </scope>  (TREF) (Section 5) . 
<scope> In corpus linguistics ,  cognates have been used for bitext alignment </scope>  (GTREF) ,  and for extracting lexicographically interesting wordpairs from multilingual corpora (REF) . 
<scope> Incremental topdown and left-corner parsing </scope>  (GTREF)  <scope> and head-driven parsing </scope>  (REF)  <scope> approaches have directly used generative PCFG models as language models .  </scope>
<scope> In current knowledge harvesting algorithms ,  seeds are chosen </scope>  either at random (GREF) ,  by picking the top N most frequent terms of the desired class (GREF) ,  or  <scope> by asking experts  </scope> (TREF) . 
<scope> Indeed ,  our methods were inspired by past work on variational decoding </scope>  for DOP (REF) and  <scope> for latent-variable parsing </scope>  (TREF) . 
<scope> Indeed ,  using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing </scope>  (GTREF) and relation extraction (GREF) . 
<scope> Indeed ,  using tree kernel methods to mine structured knowledge has shown success in some NLP applications like </scope>  parsing (REF) ,   <scope> semantic role labeling  </scope> (GTREF) ,  relation extraction (REF) ,  pronoun resolution (REF) and question classification (REF) . 
<scope> In each case ,  results for a comparison system  the Giza +  +  IBM Model 4 implementation </scope>  (TREF)  <scope> with the ReWrite decoder </scope>  (REF)   <scope> are included as a benchmark </scope>  . 
<scope> In English ,  the average use of comma per sentence is 0 . 869 </scope>  (TREF) 1 ~1 . 04(REF) ,  and in Chinese it is 1 . 79 2 ,  which is one and a half to two more times as it is used in English . 
<scope> In fact ,  although the concept is not new </scope>  (TREF) ,  state of the art frameworks such as (GREF) are using flat semantic representations ,  taht is formulas with no embedded structures (see (REF) for details about flatness) ,  which simplify the syntactic-semantic interface . 
<scope> In fact ,  participants in that competition from  </scope> the University of Durham (REF) and from  <scope> SRA  </scope> (TREF)  <scope> report that gazetteers did not make that much of a difference to their system .  </scope>
<scope> In fact ,  some structured prediction algorithms ,  such as the MIRA algorithm used in dependency parsing </scope>  (TREF) and MT (REF) uses iterative sets of N-best lists in its training process . 
<scope> Infinitive Verbs (e . g .  ,  to eat) are also in controversy ,  as </scope>  in (REF) the verb is the head while  <scope> in </scope>  (GTREF)  <scope> the to token is the head .  </scope>
<scope> In function labeling level ,  EXT that signifies degree ,  amount of the predicates should be grouped into adverbials like in the work of </scope>  (REF)  <scope> and  </scope> (TREF<div class="tooltip fixed active" style="left :  654 . 5px ;  top :  8699px ;  display :  block ;  "><span style="color : black"><tref>Merlo and Musillo ,  2005</tref></div></span>) . 
<scope> In future work ,  we hope to improve the performance of our repair correction component by incorporating acoustic/prosodic techniques for repair detection </scope>  (REF) (TREF) (O'REF) . 
<scope> In future work ,  we intend to examine how such grammars can be learned in conjunction with the language itself ;  extending </scope>  research on learning task models (REF) and  <scope> work on learning PCFGs </scope>  (TREF) with our own work on unsupervised language learning . 
<scope> In future work we plan to experiment with richer representations ,  e . g </scope>  .  including long-range n-grams (REF) ,   <scope> class n-grams </scope>  (TREF) ,  grammatical features (REF) ,  etc' . 
<scope> In hierarchical phrase-based translation </scope>  (TREF)  <scope> a weighted synchronous context-free grammar is induced from parallel text ,  </scope>  the search is based on CYK +  parsing (REF) and typically carried out using the cube pruning algorithm (REF) . 
<scope> In HPSG ,  the most extensive grammars are those of </scope>  English (REF) ,  German (REF) ,  and Japanese (GTREF) . 
<scope> in importance of various sentiment markers have crystallized in two main approaches :  automatic assignment of weights based on some statistical criterion </scope>  ((GTREF) ,  and others) or manual annotation (REF) . 
<scope> In linguistics ,  expectations ,  in the form of selectional restrictions and selectional preferences </scope>  ,  have long been used in semantic theories (GREF) ,   <scope> and more recently induced from corpora  </scope> (GTREF) . 
<scope> In MUC-7 ,  these kinds of temporal instances are called TIMEX </scope>  (TREF) ,  and it has known that TIMEX can be easily extracted by using FSA (REF) . 
<scope> In natural language processing ,  label propagation has been used for </scope>  document classification (REF) ,  word sense disambiguation (GREF) ,  and  <scope> sentiment categorization </scope>  (TREF) . 
<scope> In NLP ,  clustering has been used for virtually every semiand unsupervised task ,  including  </scope> POS tagging (REF) ,   <scope> labeled parse tree induction </scope>  (TREF) ,  verb-type classification (Schulte im REF) ,  lexical acquisition (GREF) ,  multilingual document * Both authors equally contributed to this paper . 
<scope> In NLP ,  such methods have been applied to tasks such as </scope>  POS tagging (REF) ,   <scope> word sense disambiguation </scope>  (TREF) ,  parsing (REF) ,  and machine translation (REF) . 
<scope> In NLP ,  vector space models </scope>  have featured most prominently in information retrieval (REF) ,  but  <scope> have also been used for </scope>  ontology learning (GREF) and  <scope> word sense-related tasks </scope>  (GTREF) . 
<scope> In numerous articles the usefulness of this data and software ensemble has been demonstrated (e . g .  ,  </scope>  for word sense disambiguation (REF) ,  the analysis of noun phrase conjuncts (REF) ,  or  <scope> the resolution of coreferences </scope>  (TREF)) . 
<scope> In order to avoid negative values ,  we normalized the correlation score ,  obtaining the penalty NSCP :  2)1( /NSCP  + =                     (17) Normalized Kendalls correlation penalty (NKCP) :   this is adopted from </scope>  (TREF) and (REF) . 
<scope> In order to extract the linguistic features necessary for the model ,  all sentences were first automatically part-of-speech-tagged using a maximum entropy tagge </scope> r (REF)  <scope> and parsed using the Collins parser </scope>  (TREF) . 
<scope> In order to further improve the reordering performance ,  many structure-based methods are proposed ,  including the reordering model in hierarchical phrase-based SMT systems </scope>  (TREF) and syntax-based SMT systems (GREF) . 
<scope> In order to proceed with POS-tagging ,  such various methods as </scope>  Hidden Markov Models (HMM) ;  Memorybased models (REF) ;   <scope> Transformationbased Learning (TBL) </scope>  (TREF) ;  Maximum Entropy ;  decision trees (REF) ;  Neural network (REF) ;   <scope> and so on can be used .  </scope>
<scope> In order to tackle this problem ,  a number of soft pattern matching approaches which aim to improve recall with minimized precision loss have been applied to the linear vector pattern models by introducing a probabilistic model  </scope> (TREF) or a sequence alignment algorithm (REF) . 
<scope> In our approach ,  all documents are preprocessed with </scope>  a syntactic phrase chunker (REF) and  <scope> the EXERT 1 system </scope>  (GTREF) ,  a named-entity detection and co-reference resolution system that was developed for the ACE 2 project . 
<scope> In our corpus study </scope>  (TREF)  <scope> we found that our subjects did better at ideutifying discourse-new descriptions all together (K= . 68) than they did at distinguish 'unfamiliar' from 'larger situation' </scope>  (REF) cases (K =  . 63) . 
<scope> In our experimentations ,  SVM light  </scope> (<span class="refstyle ref44132" id="s44132 . r1">REF<div class="tooltip fixed" style="left :  433 . 5px ;  top :  15088px ;  display :  none ;  "><ref>Joachims ,  1998<%.
<scope> In our experiments ,  we follow REF in using the well-known log-likelihood ratio G 2 </scope>  (TREF) . 
<scope> In our experiments we use the ASSERT parser </scope>  (TREF) ,   <scope> an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments ,  which are output as PropBank arguments </scope>  (REF) . 
<scope> In our future work ,  we would like to investigate more sophisticated clustering methods that would lead to global optimization ,  e . g .  ,  by keeping a large search space </scope>  (TREF) or using integer programming (REF) . 
<scope> In our implementation ,  we built a tree kernelbased SVM model using </scope>  SVM-Light 5 (REF) and  <scope> Tree Kernel Tools 6  </scope> (TREF) . 
<scope> In our implementation ,  we use </scope>  the binary SVMLight (REF) and  <scope> Tree Kernel Tools  </scope> (TREF) . 
<scope> In our prior work  </scope> (REF) ,  <scope> we examined whether techniques used for predicting the helpfulness of product reviews </scope>  (TREF)  <scope> could be tailored to our peer-review domain ,  where the definition of helpfulness is largely influenced by the educational context of peer review .  </scope>
<scope> In particular ,  resources annotated with the surface realization of semantic roles ,  </scope>  like FrameNet (REF) or PropBank (REF)  <scope> have shown to convey an improvement in several NLP tasks ,   </scope> from question answering (REF)  <scope> to textual entailment  </scope> (TREF) and shallow semantic parsing (REF) . 
<scope> In particular ,  </scope>  mutual information (GREF) and  <scope> other statistical methods such as </scope>  (TREF) and frequency-based methods such as (REF)  <scope> exclude infrequent phrases because they tend to introduce too much noise .  </scope>
<scope> In particular ,  the well-defined semantic role labeling (SRL) task has been drawing increasing attention in recent years due to its importance in natural language processing (NLP) applications ,  such as </scope>  question answering (REF) ,  information extraction (REF) ,  and  <scope> co-reference resolution </scope>  (TREF) . 
<scope> In particular ,  the well-defined semantic role labeling (SRL) task has been drawing increasing attention in recent years due to its importance in natural language processing (NLP) applications ,  such as </scope>  question answering (REF<input type="hidden" value="0" name="ref[41084]%.
<scope> In particular ,  we have used </scope>  EuroWordNet in a Cross-Language Text Retrieval (CLTR) application (REF) and  <scope> a number of CLTR experiments </scope>  (GTREF) ,   <scope> confirming that it is crucial to apply certain sense clusters to Wordnet (WN) and EuroWordNet (EWN) to take real advantage of them in Information Retrieval applications .  </scope>
<scope> In previous research ,  expression level annotation of opinions was extensively investigated on newspaper articles </scope>  (GTREF) and on meeting dialogs (GREF) . 
<scope> In previous studies ,  seed lexicons vary between 16 , 000  </scope> (TREF) and 65 , 000 (REF) entries ,  a typical size being around 20 , 000 (GREF) . 
<scope> In previous work </scope>  (TREF) ,   <scope> I described a Maximum Entropy/Minimum Divergence (MEMD) model (REF) for p(w[hi ,  s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 (REF) .  </scope>
<scope> In previous work ,  Walker and Passonneau de ned the DATE scheme ,  and labelled the system utterances in the </scope>  REF  <scope> data collection of 663 dialogues from nine participating Communicator systems  </scope> (GTREF) . 
<scope> In previous work ,  we tested the DOP method on a cleaned-up set of analyzed part-of-speech strings from the Penn Treebank </scope>  (TREF) ,  achieving excellent test results (REF ,  b) . 
<scope> In Question Answering ,  it has been used to establish that a certain sentence found in the corpus can serve as a suitable ,  albeit implicit answer to a query  </scope> (REF) ,  (REF) ,  (TREF) . 
<scope> In recent years ,  graph-based ranking methods have been investigated for document summarization ,  such as TextRank </scope>  (GTREF) and LexPageRank (REF) . 
<scope> In recent years ,  MAP adaptation has been successfully applied to </scope>  n-gram language models (REF) and  <scope> lexicalized PCFG models </scope>  (TREF) . 
<scope> In recent years ,  NomBank </scope>  (TREF)  <scope> has provided a set of about 200 , 000 manually annotated instances of nominalizations with arguments ,  giving rise to supervised machinelearned approaches </scope>  such as (REF) and (REF) ,  which perform fairly well in the overall task of classifying deverbal arguments . 
<scope> In recent years ,  the availability of annotated corpora </scope>  ,  such as TimeBank (REF) ,   <scope> has triggered the use of machinelearning methods for temporal analysis </scope>  (GTREF) . 
<scope> In recent years ,  the availability of large human-labeled corpora such as PropBank </scope>  (TREF) and FrameNet (REF)  <scope> has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language texts .  </scope>
<scope> In recent years there has been a substantial amount of work on term extraction ,  including semantic class learning  </scope> (GTREF) ,  relation acquisition between entities (GREF) ,  and creation of concept lists (REF) . 
<scope> In recent years there have been many studies on text classification using </scope>  general methods (GREF)  <scope> semi-structured texts </scope>  (TREF) ,  and XML classification (REF) . 
<scope> In (REF) ,  </scope>  (TREF) <scope> the significance of an association (x , y) is measured by the mutual information I(x , y) ,  i . e .  the probability of observing x and y together ,  compared with the probability of observing x and y independently .  </scope>
<scope> In related joint work </scope>  (Katz-REF) and (TREF) ,  <scope> it is shown that the framework can be used to optimize reordering components automatically .  </scope>
<scope> In related work </scope>  (TREF) ,   <scope> we compare the performance of the Blum and Mitchell cotraining algorithm with that of two existing singleview bootstrapping algorithms </scope>   self-training with bagging (REF) and EM (REF)  on coreference resolution ,   <scope> and show that single-view weakly supervised learners are a viable alternative to co-training for the task .  </scope>
<scope> In  </scope> (TREF) <scope> and  </scope> (TREF)  <scope> (P&M) ,  we presented a heuristic framework for feature selection in kernel spaces that selects features based on the components of the weight vector ,  vectorw ,  optimized by Support Vector Machines (SVMs) .  </scope>
<scope> In  </scope> (TREF) ,   <scope> an extension of the PrefixSpan algorithm </scope>  (REF) <scope> is used to efficiently mine the features in a low degree polynomial kernel space .  </scope>
<scope> In </scope>  (TREF)  <scope> dependency graph features are exploited </scope>  ,  and in (REF) syntactic features are employed for relation extraction . 
<scope> In  </scope> (TREF) ,   <scope> different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus  </scope> using GIZA +  +  (REF) . 
<scope> In </scope>  (TREF) ,   <scope> the use of Voted Perceptron (VP) </scope>  (REF)  <scope> for the parse reranking problem has been described .  </scope>
<scope> In similar work on blogs ,  it is shown that </scope>  detecting topics (GREF) and <scope> sentiment </scope>  (TREF)  <scope> in the blogosphere can help identify influential bloggers (GREF) and mine opinions about products (REF) .  </scope>
<scope> Instance weighting </scope>  (TREF) and active learning (REF)  <scope> are also employed in domain adaptation .  </scope>
<scope> Instead of just focusing our attention on keyword queries ,  as is often done in previous work </scope>  (GTREF) ,  we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and whquestions ,  which often pose a challenge for IR applications (GREF) . 
<scope> Instead we carry out extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics :  BLEU </scope>  (TREF) ,  METEOR (REF) ,  NIST (REF) ,  WER ,  PER and TER (REF) . 
<scope> In surface realisation ,  we aim to optimise the tradeoff between </scope>  alignment and consistency (GREF) on the one hand ,  and  <scope> variation (to improve text quality and readability) on the other hand </scope>  (GTREF)  <scope> in a 50/50 distribution .  </scope>
<scope> Integer linear programming was used in many NLP tasks </scope>  (REF) ,   <scope> for its power in both expressing and approximating various inference problems ,  especially in parsing </scope>  (GTREF) . 
<scope> In terms of granularity ,  this task has been investigated from building word level sentiment lexicon </scope>  (GTREF) to detecting phrase-level (GREF) and sentence-level (GREF) sentiment orientation . 
<scope> In that paper ,  QG was applied to word alignment and has since found applications in question answering </scope>  (TREF) ,  paraphrase detection (REF) ,  and machine translation (REF) . 
<scope> In the area of statistical machine translation (SMT) ,  recently a combination of the BLEU evaluation metric (REF) and the bootstrap method for statistical significance testing (REF) has become popular </scope>  (GTREF) . 
<scope> In the domain of frame semantics ,  previous work has sought to extend the coverage of FrameNet by exploiting resources like VerbNet ,  WordNet ,  or Wikipedi </scope> a (GTREF) ,  and projecting entries and annotations within and across languages (GREF) . 
<scope> In the field of natural language processing this approach has been applied for example in </scope>  parsing (<span class="refstyle ref64438" id="s64438 . r1">REF<div class="tooltip fixed" style%3.
<scope> In the first approach ,  heuristic rules are used to find the dependencies  </scope> (TREF) or penalties for label inconsistency are required to handset ad-hoc (REF) . 
<scope> In the first instance ,  a specialised methodology is proposed to (automatically) learn a particular linguistic property such as </scope>  verb subcategorisation (e . g .  REF) or  <scope> noun countability </scope>  (e . g .  TREF) ,   <scope> and little consideration is given to the applicability of that method to more general linguistic properties .  </scope>
<scope> In the geometric interpolation above ,  the weight n controls the relative veto power of the n-gram approximation and can be tuned using MERT </scope>  (TREF) or a minimum risk procedure (REF) . 
<scope> In the HMM community it 4In prior work involving factored syntax models lexicalized ( </scope> TREF) and bilingual (REF)fpoe , 1 was applied ,  and the asymptotic runtime went to O(n5) and O(n7) . 
<scope> In the last decade ,  several systems that integrate NLG techniques for AAC systems have been developed  </scope> ((TREF) ,  (REF) for example) . 
<scope> In the literature ,  Domain Models have been </scope>  introduced to represent ambiguity and variability (REF) and <scope> successfully exploited in many NLP applications ,  such as Word Sense Disambiguation </scope>  (TREF) ,  Text Categorization and Term Categorization . 
<scope> In the monolingual setting </scope>  ,  TREF  <scope> showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words) </scope>  ,  and REF describe an MRF-based monolingual POS induction model that uses features . 
<scope> In the original work </scope>  (REF) ,   <scope> the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh </scope>  (TREF) . 
<scope> In the original work  </scope> (TREF) ,   <scope> the target-side rule selection is analogous to the model in traditional phrase-based SMT system </scope>  such as Pharaoh (REF) . 
<scope> In the prior work on extraction pattern acquisition ,  the representation model of the patterns was based on </scope>  a fixed set of pattern templates (REF) ,  or  <scope> predicate-argument relations ,  such as subject-verb ,  and object-verb </scope>  (TREF) . 
<scope> In these works ,  the definitions are analyzed using </scope>  either  <scope> a parser  </scope> (TREF) or a pattern matcher (REF)  <scope> into semantic relations .  </scope>
<scope> In the spoken dialog systems ,  these are manually designed for various purposes including dialog modeling </scope>  (GTREF) ,  search space reduction (REF) ,  domain knowledge (REF) ,  and user simulation (REF) . 
<scope> In the WSD work involving the use of context ,  we can find two approaches :  one that uses few strong contextual evidences for disambiguation purposes ,  as exemplified by </scope>  (TREF) ;  and the other that uses weaker evidences but considers a combination of a number of them ,  as exemplified by (REF) . 
<scope> In this approach ,  source  </scope> (GTREF) ,  target (GREF) ,  or both side (GREF)  <scope> tree structures are used for model training .  </scope>
<scope> In this framework </scope>  ,  a whole grammar is not acquired from scratch(REF) or  <scope> an initial grammar does not need to be assumed </scope> (TREF) . 
<scope> In this paper ,  we reach a reasonable compromise by showing how the methodology of </scope>  [TREF]  <scope> supplements the evaluation efforts of [REF] using a similar (yet by necessity smaller) dataset .  </scope>
<scope> In this paper we report case-insensitive Bleu scores (REF) ,  unless otherwise stated ,  calculated with the NIST tool ,  and caseinsensitive Meteor-ranking scores ,  without WordNet </scope>  (TREF) . 
<scope> In this paper we report case-insensitive Bleu scores </scope>  (TREF) ,   <scope> unless otherwise stated ,  calculated with the NIST tool </scope>  ,  and caseinsensitive Meteor-ranking scores ,  without WordNet (REF) . 
<scope> In this paper we report on a particular experience of automatic extraction of an English grammar from the WSJ corpus of the Penn Treebank (PTB) </scope>  (TREF)1 using Tree Adjoining Grammar (TAGs ,  (REF)) . 
<scope> In this paper we show how nonmonotonic operations can also be user-defined by </scope>  applying default logic (REF) and  <scope> generalizing previous results on nonmonotonic sorts </scope>  (TREF) . 
<scope> In this section ,  we relate our approach to the HPSG based approach presented in [REF] ,  </scope>  to the statistical and semi-statistical strategies used in [REF] and in [REF] and  <scope> to the constraint based approach described in </scope>  [TREF] . 
<scope> In this vein ,  the REF shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (REF) ) and semantic dependencies (extracted both from PropBank </scope>  (TREF)  <scope> c2008 </scope>  . 
<scope> Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications ,  including information retrieval </scope>  (GTREF) ,  machine translation (GREF) ,  and speech synthesis (REF) . 
<scope> Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications ,  including  </scope> information retrieval (GREF) ,   <scope> machine translation  </scope> (GTREF) ,  and speech synthesis (REF) . 
<scope> It adopts an SVM model </scope>  (TREF) <scope>  ,  which obtains the best performance </scope>  in REF Task 11 :  English Lexical Sample Task via English-Chinese Parallel Text . 
<scope> It also contains tools for </scope>  tuning these models using minimum error rate training (REF) and  <scope> evaluating the resulting translations using the BLEU score </scope>  (TREF) . 
<scope> It can not only maintain the strength of phrase translation in traditional phrase-based models </scope>  (GTREF) ,   <scope> but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models </scope>  (GREF) . 
<scope> It differs from the many approaches where (1) is defined by a stochastic synchronous grammar </scope>  (GTREF) and from transfer-based systems defined by context-free grammars (REF) . 
<scope> It does support k-best parsing </scope>  ,  but ,  following Collins parse-reranking work (REF) (see also Section 5 . 1 . 2) ,  <scope> it accomplishes this by simply abandoning dynamic programming ,  i . e .  ,  no items are considered equivalent </scope>  (TREF) . 
<scope> It has been aligned </scope>  on the sentence level by JAPA (REF) ,  and  <scope> further on the word level by GIZA +  +   </scope> (TREF) . 
<scope> It has been applied to problems such as  </scope> word-sense disambiguation (REF) ,  web-page classification (REF) ,  <scope> namedentity recognition </scope>  (TREF) and automatic construction of semantic lexicon (REF) . 
<scope> It has been applied to various NLP/IE tasks ,  including </scope>  named entity recognition (REF) and <scope> parse selection </scope>  (TREF)  <scope> with rather impressive results in reducing the amount of annotated training data .  </scope>
<scope> It has been successfully used for a wide range of practical tasks ,  such as </scope>  data-driven parsing (GREF) ,   <scope> wide-coverage semantic construction </scope>  (TREF) ,  and the modelling of syntactic priming (REF) . 
<scope> It has been successfully used for a wide range of practical tasks ,  such as  </scope> data-driven parsing (GREF) ,  wide-coverage semantic construction (REF) ,  and  <scope> the modelling of syntactic priming </scope>  (TREF) . 
<scope> It has been used for a variety of tasks ,  such as </scope>  wide-coverage parsing (GREF) ,  sentence realization (REF) ,   <scope> learning semantic parsers </scope>  (TREF) ,  dialog systems (REF) ,  grammar engineering (GREF) ,  and modeling syntactic priming (REF) . 
<scope> It has been used for a variety of tasks ,  such as wide-coverage parsing </scope>  (GTREF) ,  sentence realization (REF) ,  learning semantic parsers (REF) ,  dialog systems (REF) ,  grammar engineering (GREF) ,  and modeling syntactic priming (REF) . 
<scope> It has been used in a variety of difficult classification tasks such as </scope>  part-of-speech tagging (REF) ,   <scope> prepositional phrase attachment </scope>  (TREF) and named entity tagging (REF) ,  and achieves state of the art performance . 
<scope> It has been used in the area of natural language interface for database (NLIDB) </scope>  (REF)  <scope> and a TREC QA system for the purpose of matching the user query with the appropriate answer types at syntax/semantic level  </scope> (GTREF) . 
<scope> It has been used to describe the morphology of  </scope> Syriac (REF) ,  Akkadian (REF) and  <scope> Arabic Dialects </scope>  (TREF) . 
<scope> It has been widely used to adapt language models for speech recognition and other applications ,  for instance </scope>  using cross-domain topic mixtures ,  (REF) ,  dynamic topic mixtures (REF) ,  <scope> hierachical mixtures </scope>  (TREF) ,  and cache mixtures (Kuhn and REF) . 
<scope> It has potential uses for a variety of tasks including machine translation </scope>  (TREF) and question answering (REF) . 
<scope> It has shown promise in improving the performance of many tasks such as name tagging </scope>  (TREF) ,  semantic class extraction (REF) ,  chunking (REF) ,  coreference resolution (REF) and text classification (REF) . 
<scope> It is an online training algorithm and has been successfully used in many NLP tasks ,  such as POS tagging </scope>  (TREF) ,  parsing (REF) ,  Chinese word segmentation (GREF) ,  and so on . 
<scope> It is a very powerful technique already used for NLP applications such as </scope>  information retrieval (REF) and  <scope> text segmentation </scope>  (TREF) and ,  more recently ,  multiand single-document summarization . 
<scope> It is not surprising then that stable averages of quality judgements ,  let alone high levels of agreement ,  are hard to achieve ,  as has been observed for </scope>  MT (GREF) ,  text summarisation (REF) ,  and  <scope> language generation </scope>  (TREF) . 
<scope> It is slightly disappointing that the book is angled almost exclusively towards machine-learning approaches to IE ,  and consequently omits any mention of some of the leading tools in the eld such as GATE  </scope> (TREF) and KIM (REF) ,  which are based on knowledge-engineering approaches . 
<scope> It is structured as a cross-language named entity recognition task similar to those outlined in (REF) and </scope>  (TREF) . 
<scope> It is structured as a cross-language named entity recognition task similar to those outlined in </scope>  (TREF) and (REF) . 
<scope> It is worth mentioning that the f-scores reported in </scope>  (TREF)  <scope> are about 1% less than the results in (REF) .  </scope>
<scope> It is worth noting that REF has employed </scope>  REF discourse theory and  <scope> Veins Theory </scope>  (TREF)  <scope> to identify and remove candidate antecedents that are not referentially accessible to an anaphoric pronoun in his heuristic pronoun resolvers .  </scope>
<scope> It maintains a discourse state that consists of a goal stack with similarities to the plan stack of </scope>  REF <scope> and the attentional state of </scope>  TREF . 
<scope> It </scope>  was first introduced as a means of reducing parser ambiguity by REF in the context of the LTAG formalism ,  and  <scope> has since been applied in a similar context within the CCG formalism </scope>  (TREF) . 
<scope> It seems that give is emphasizing the role of a book as a gift ,  read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of </scope>  (TREF) as well as (REF) and purchase denotes the more general purpose of a book ,  i . e .  to be bought . 
<scope> It should be noted that BFP makes use of Centering Rule 2 </scope>  (TREF) ,  LRC does not use the transition generated or Rule 2 in steps 4 and 5 since Rule 2's role in pronoun resolution is not yet known (see REF for a critique of its use by BFP) . 
<scope> It shows the equivalence of a large fragment of MRSs and a corresponding fragment of normal dominance constraints ,  which in turn is equivalent to a large fragment of Hole Semantics  </scope> (REF)  <scope> as proven in  </scope> (TREF) . 
<scope> It uses existing NLG components ,  including the Exemplars text planning framework </scope>  (TREF) and the RealPro syntactic realizer (REF) . 
<scope> It was first used for unlabeled dependency parsing by </scope>  TREF  <scope> (for Japanese) </scope>  and REF (for English) . 
<scope> I will discuss one example of the decoding approach in more detail in Section 3 ,  chooser & inquiry semantics developed for and used in the Penman text generation system (e . g .   </scope>  ,  GTREF) ,  and one example of the encoding approach in Section 4 ,  the theory of situation-specific semantic systems (REF) modelled in Patten.
<scope> KneserNey discounted LMs were built from monolingual corpora using </scope>  the SRILM toolkit (REF) ,  while bilingual corpora were aligned at the wordlevel using GIZA +  +  (REF) or its  <scope> multi-threaded version MGIZA +  +  </scope>  (TREF) for the large corpora UN and giga . 
<scope> Knowledge of cognates is useful for a number of applications ,  including sentence alignment </scope>  (TREF) and learning translation lexicons (GREF) . 
<scope> Language modeling </scope>  (TREF) ,  noun-clustering (REF) ,  constructing syntactic rules for SMT (REF) ,  and finding analogies (REF)  <scope> are examples of some of the problems where we need to compute relative frequencies .  </scope>
<scope> Last ,  we include all the nouns and verbs used in the  </scope> REF  <scope> WSI Task  </scope> (TREF) ,   <scope> which are used in our evaluation .  </scope>
<scope> Later heuristic metrics such as METEOR </scope>  (TREF) and TER (REF)  <scope> account for both precision and recall ,  but their relative weights are difficult to determine manually </scope>  . 
<scope> Lexical cues of differing complexities have been used ,  including single words and Ngrams (e . g .   ,  </scope>  (GTREF)) ,  as well as phrases and lexico-syntactic patterns (e . g ,  (GREF)) . 
<scope> Li and Roth demonstrated that their shallow parser ,  trained to label shallow constituents </scope>  along the lines of the well-known REF task (REF) ,   <scope> outperformed the Collins parser in correctly identifying these constituents in the Penn Wall Street Journal (WSJ) Treebank </scope>  (TREF) . 
<scope> Linguistic encoding schemes like ToBI (REF) have also been used </scope>  for sentence boundary detection (GREF) ,  as well as  <scope> for parsing </scope>  (GTREF) . 
<scope> Log-likelihood ratio (G2)  </scope> (TREF) with respect to a large reference corpus ,  Web 1T 5-gram Corpus (REF) ,   <scope> is used to capture the contextually relevant nouns .  </scope>
<scope> Many bootstrapping algorithms have been proposed for a variety of tasks </scope>  :  word sense disambiguation (GREF) ,  information extraction (GREF) ,  named entity recognition (REF) ,  part-of-speech tagging (REF) ,  and  <scope> statistical parsing </scope>  (GTREF) . 
<scope> Many bootstrapping algorithms have been proposed for a variety of tasks :  word sense disambiguation </scope>  (GTREF) ,  information extraction (GREF) ,  named entity recognition (REF) ,  part-of-speech tagging (REF) ,  and statistical parsing (GREF) . 
<scope> Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or a tagged corpus with the instances of each possible sense ,  e . g .  ,  </scope>  supervised sense disambiguation (REF) ,  and  <scope> semi-supervised sense disambiguation </scope>  (TREF) . 
<scope> Many corpus based statistical methods have been proposed to solve this problem ,  including  </scope> supervised learning algorithms (GREF) ,  <scope> weakly supervised learning algorithms  </scope> (GTREF) ,  unsupervised learning algorithms (or word sense discrimination) (GREF) ,  and knowledge based algorithms (GREF) . 
<scope> Many data-driven techniques have been proposed for letter-to-phoneme conversion systems ,  including pronunciation by analogy </scope>  (TREF) ,  constraint satisfaction (Van REF) ,  Hidden Markov Model (REF) ,  decision trees (REF) ,  and neural networks (REF) . 
<scope> Many machine learning approaches have been proposed to attack this problem ,  including </scope>  supervised (GREF) ,   <scope> semisupervised </scope>  (GTREF) ,  and unsupervised methods (GREF) . 
<scope> Many machine learning approaches have been proposed to attack this problem ,  including supervised </scope>  (GTREF) ,  semisupervised (GREF) ,  and unsupervised methods (GREF) . 
<scope> Many machine learning approaches have been proposed to attack this problem ,  including supervised </scope>  (GTREF) ,  semisupervised (GREF) ,  and unsupervised  <scope> methods </scope>  (GREF) . 
<scope> Many machine learning approaches have been proposed to attack this problem ,  </scope>  including supervised (GREF) ,  semisupervised (GREF) ,  and  <scope> unsupervised methods </scope>  (GTREF) . 
<scope> Many machine learning methods have been proposed to address this problem ,  e . g .  ,  supervised learning algorithms </scope>  (GTREF) ,  semi-supervised learning algorithms (GREF) ,  and unsupervised learning algorithms (REF) . 
<scope> Many machine-learning methods ,  such as </scope>  HMM (REF) ,  CRF (REF) ,   <scope> Maximum Entropy </scope>  (TREF) ,   <scope> have been exploited in segmentation task .  </scope>
<scope> Many machine learning techniques have been successfully applied to chunking tasks ,  such as Regularized Winnow </scope>  (REF) ,  <scope> SVMs  </scope> (TREF) ,  CRFs (REF) ,  Maximum Entropy Model (REF) ,  Memory Based Learning (REF) and SNoW (REF) . 
<scope> Many methods have been proposed to deal with this task ,  including supervised learning algorithms  </scope> (GTREF) ,  semi-supervised learning algorithms (GREF) ,  and unsupervised learning algorithm (REF) . 
<scope> Many models have been successfully applied to sequence labeling problems ,  such as </scope>  maximumentropy (REF) ,  conditional random fields (CRF) (REF) and  <scope> perceptron </scope>  (TREF) . 
<scope> Many NLP applications could benefit from identifying linguistic hedges ,  e . g .   </scope> question answering systems (REF) ,  information extraction from biomedical documents (GREF) ,  and <scope> deception detection  </scope> (TREF) . 
<scope> Many NLP systems use the output of supervised parsers </scope>  (e . g .  ,  (REF) for QA ,  (REF) for IE ,  (REF) for SRL ,  (REF) for Textual Inference and (TREF)  <scope> for MT </scope> ) . 
<scope> Many reordering constraints have been used for word reorderings ,  such as </scope>  ITG constraints (REF) ,  IBM constraints (REF) and  <scope> local constraints </scope>  (TREF) . 
<scope> Many reordering constraints have been used for word reorderings ,  such as </scope>  ITG constraints (REF) ,   <scope> IBM constraints  </scope> (TREF) and local constraints (REF) . 
<scope> Many research efforts utilize machine learning (ML) approaches ;  such as  </scope> support vector machines (GREF) ,  perceptrons (REF) ,  the SNoW learning architecture (REF) ,  EMbased clustering (REF) ,  transformation-based learning (REF) ,  memory-based learning (REF) ,  and <scope> inductive learning </scope>  (TREF) . 
<scope> Many researchers  </scope> ((TREF) ,  (REF) <scope>  ,  showed that lexical and syntactic information is very useful for predicateargument recognition tasks ,  such as semantic roles .  </scope>
<scope> Many researchers that followed this approach relied mostly on hand-coded rules </scope>  (REF) ,  (TREF) . 
<scope> Many statistical metrics have been proposed ,  including  </scope> pointwise mutual information (MI) (REF) ,  mean and variance ,  hypothesis testing (t-test ,  chisquare test ,  etc . ) ,   <scope> log-likelihood ratio (LR) </scope>  (TREF) ,  statistic language model (Tomokiyo ,  et al ,  2003) ,  and so on . 
<scope> Many systems that deal with </scope>  sentence compression (GREF) and  <scope> fusion  </scope> (GTREF) ,   <scope> do not take into account the specificity of the original or desired sentence </scope>  . 
<scope> Many systems that deal with sentence compression </scope>  (GTREF) and fusion (GREF) ,   <scope> do not take into account the specificity of the original or desired sentence .  </scope>
<scope> Many theories on linguistic salience have been developed ,  including how the salience of entities affects </scope>  the form of referring expressions as in the Givenness Hierarchy (REF) and  <scope> the local coherence of discourse as in the Centering Theory </scope>  (TREF) . 
<scope> Marcu </scope>  (TREF)  <scope> uses rhetorical structure analysis to guide the selection of text segments for the summary </scope>  ;  similarly Teufel and Moens (REF) analyze argumentative structure of discourse to extract appropriate sentences . 
<scope> Matching </scope>  using stems and synonyms (REF<input type="hidden" value="0" name="ref[70902]%5.
<scope> Maximum entropy (ME) models have been used in </scope>  bilingual sense disambiguation ,  word reordering ,  and sentence segmentation (REF) ,  parsing ,  POS tagging and PP attachment (REF) ,  machine translation (REF) ,  and  <scope> FrameNet classification </scope>  (TREF) . 
<scope> Maximum-entropy models have been used for Chinese </scope>  character-based parsing (GREF) and  <scope> POS tagging </scope>  (TREF) . 
<scope> Meanwhile ,  some learning algorithms ,  like </scope>  maximum likelihood for conditional log-linear models (REF) ,  unsupervised models (REF) ,  and  <scope> models with hidden variables </scope>  (GTREF) ,   <scope> require summing over the scores of many structures to calculate marginals .  </scope>
<scope> Measures of distance in the WordNet hierarchy such as JCN have been widely used for </scope>  WSD (REF) as well as  <scope> the information contained in the structure of the hierarchy </scope>  (TREF) <scope> which has been used for backing off when training a supervised system .  </scope>
<scope> Methods differ in complexity from simple ones using lexicosyntactic patterns </scope>  (TREF) <scope> to more complicated techniques based on distributional similarity (REF) .  </scope>
<scope> Methods of selecting spelling and correction pairs with </scope>  maximum entropy model (REF) or  <scope> similarity functions </scope>  (GTREF) <scope> have been developed .  </scope>
<scope> Methods such as </scope>  (TREF) ,  (REF) and (REF)  <scope> employ a synchronous parsing procedure to constrain a statistical alignment .  </scope>
<scope> Minimizing word sense ambiguity by focusing on a specific domain was later seen in the work of REF ,  who performed hierarchical clustering using output from their KNEXT-like system first described in </scope>  (TREF) . 
<scope> MIRA has been used successfully for both sequence analysis </scope>  (TREF) and dependency parsing (TREF) . 
<scope> Model F score SVM combination 94 . 39%  </scope> (TREF) CRF 94 . 38% Generalized winnow 93 . 89% (REF) Voted perceptron 94 . 09% MEMM 93 . 70% Table 2 :  NP chunking F scores 5 Results All the experiments were performed with our Java implementation of CRFs , designed to handle millions of features ,  on 1 . 7 GHz Pentium IV processors with Linux and IBM Java 1 . 3 . 0 . 
<scope> Models of coherence have been used </scope>  to impose an order on sentences for multidocument summarization (REF) ,  to evaluate the quality of human-authored essays (REF) ,  and  <scope> to insert new information into existing documents </scope>  (TREF) . 
<scope> Models of that form include </scope>  hidden Markov models (GREF) as well as discriminative tagging models based on maximum entropy classification (GREF) ,   <scope> conditional random fields </scope>  (GTREF) ,  and large-margin techniques (GREF) . 
<scope> Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types </scope>  ,  such as the Swedish dependency treebank derived from REF ,   <scope> where the extra part-of-speech features are largely redundant </scope>  (TREF) . 
<scope> Models that employ syntax or syntaxlike representations </scope>  (GTREF)  <scope> handle long-distance reordering better </scope>  than phrase-based systems (REF)  <scope> but often require constraints on the formalism or rule extraction method in order to achieve computational tractability .  </scope>
<scope> Modular pipeline architectures have a long history of use in text gen16 eration systems </scope>  (GTREF) ,  although recent work argues for the need for interaction between modules (GREF) . 
<scope> More complete discussions of M . E .  as applied to computational linguistics ,  including a description of the M . E .  estimation procedure can be found in </scope>  (TREF) and (Della REF) . 
<scope> More generally ,  this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features </scope>  (TREF) . 4 Chelba and REFdevelopedasimilaralgorithmforlattices . 
<scope> Moreover ,  it should be noted that our approach is at the same time very different to other purely web-based approaches such as askMSR  </scope> (TREF) and Aranea (REF) . 
<scope> Moreover ,  the deterministic dependency parser of REF ,  when trained on the Penn Treebank ,  gives a dependency accuracy that is almost as good as that of </scope>  GTREF . 
<scope> More recent algorithms in the same tradition </scope>  ,   <scope> including the </scope>  refined MBDP-1 of REF ,  the  <scope> WordEnds algorithm of  </scope> TREF ,  and the Hierarchical Dirichlet Process (HDP) algorithm of REF ,   <scope> share this limitation </scope>  . 
<scope> More recently and for knowledge discovery in molecular biology ,  more elaborated models were proposed by </scope>  (TREF) (REF) and by (REF)  <scope> for novelty-detection </scope>  . 
<scope> More recently ,  data-driven approaches have gained popularity and been applied </scope>  to article prediction in English (GREF) ,   <scope> to an array of Japanese learners errors in English </scope>  (TREF) ,  to verb errors (REF) ,  and to article and preposition correction in texts written by non-native ELLs (GREF) . 
<scope> More recently ,  data-driven approaches have gained popularity and been applied to </scope>  article prediction in English (GREF) ,  to an array of Japanese learners errors in English (REF) ,  to verb errors (REF) ,  and to  <scope> article and preposition correction in texts written by non-native ELLs </scope>  (GTREF) . 
<scope> More recently ,  summarizers using sophisticated postextraction strategies ,  such as revision </scope>  (GTREF) ,  and sophisticated grammar-based generation (REF) <scope> have also been presented .  </scope>
<scope> More recent work has shown that improvements can be made by modifying the basic HMM structure </scope>  (REF) ,  using better smoothing techniques or added constraints (REF) ,   <scope> or using a discriminative model rather than an HMM 744 </scope>  (TREF) . 
<scope> More recent work includes  </scope> TREF  <scope> who combines distributional and morphological information </scope>  ,  and REF who uses a hidden Marcov model in combination with co-clustering . 
<scope> More recent work use techniques from </scope>  graph theory (REF) and  <scope> machine learning </scope>  (GTREF)  <scope> in order to find patterns in vocabulary use .  </scope>
<scope> More recent work use techniques from  </scope> graph theory (REF) and  <scope> machine learning </scope>  (GTREF) <scope> in order to find patterns in vocabulary use .  </scope>
<scope> More sophisticated guessers further examine  </scope> the prefixes of unknown words (REF) and  <scope> the categories of contextual tokens </scope>  (TREF ;  Daelemans et aL ,  1996) . 
<scope> Morphological analysis has been used for improving Arabic-English translation </scope>  (TREF) ,  for SerbianEnglish translation (REF) as well as for Czech-English translation (REF) . 
<scope> Morphological analysis has been used for improving </scope>  Arabic-English translation (REF) ,  for SerbianEnglish translation (REF) as well as for  <scope> Czech-English translation </scope>  (TREF) . 
<scope> Morphological Extension Morphologically motivated add-ons are used e . g .  </scope>  in (REF) and (TREF)  <scope> to guess a more appropriate category distribution based on a words suffix or its capitalization for OOV words .  </scope>
<scope> Moses uses standard external tools for some of these tasks ,  such as GIZA +  +  </scope>  (TREF)  <scope> for word alignments </scope>  and SRILM (REF) for language modeling . 
<scope> Most existing work in Opinion Summarization </scope>  focus on predicting sentiment orientation on an entity (REF) (REF) or  <scope> attempt to generate aspect-based ratings for that entity </scope>  (REF) (REF)(TREF)(REF) . 
<scope> Most methods of coreference resolution ,  if providing a baseline ,  usually use a feature set similar to </scope>  (REF) or (TREF)  <scope> and do the feature extraction in the preprocessing stage .  </scope>
<scope> Most of the following works focused on </scope>  feature engineering (GREF) and  <scope> machine learning models </scope>  (GTREF) . 
<scope> Most of these previous works manually selected combination of features except for SVM with </scope>  polynomial kernel and (REF) a <scope> boosting-based re-ranking </scope>  (TREF) . 
<scope> Most of the unsupervised WSD work has been based on the vector space model ,  where </scope>  each example is represented by a vector of features (e . g .  the words occurring in the context) ,  and  <scope> the induced senses are either clusters of examples </scope>  (Schcurrency1utze ,  1998 ;  TREF) or clusters of words (REF) . 
<scope> Most of the work focused on </scope>  seeking better word alignment for consensus-based confusion network decoding (REF) or  <scope> word-level system combination  </scope> (GTREF) . 
<scope> Most of the work focused on seeking better word alignment for consensus-based confusion network decoding </scope>  (TREF) or word-level system combination (GREF) . 
<scope> Most of the work has focused on analyzing the content of movie or general product reviews ,  but there are also applications to other domains such as </scope>  debates (GREF) ,   <scope> news </scope>  (TREF) and blogs (GREF) . 
<scope> Most other researchers take either the HMM alignments </scope>  (REF)  <scope> or IBM Model 4 alignments </scope>  (TREF)  <scope> as input and perform post-processing </scope>  ,  whereas our model is a potential replacement for the HMM and IBM Model 4 . 
<scope> Most research views the task as a sequential labeling problem ,  using HMMs </scope>  (GTREF) and discriminative models (GREF) . 
<scope> Most research views the task as a sequential labeling problem ,  using </scope>  HMMs (GREF) and  <scope> discriminative models </scope>  (GTREF) . 
<scope> Most WSD systems have relied on hand-labeled training examples </scope>  (GTREF) or on dictionary glosses (GREF) or the WordNet hierarchy (REF)  <scope> to help make disambiguation choices .  </scope>
<scope> MS1 and the systems by </scope>  (REF) and (TREF)  <scope> consist of a training phase ,  where they form certain structures (such as rules ,  models ,  etc) .  </scope>
<scope> MS1 and the systems by </scope>  (TREF) and (REF)  <scope> consist of a training phase ,  where they form certain structures (such as rules ,  models ,  etc) .  </scope>
<scope> MST uses </scope>  Chu-LiuEdmonds (GREF) Maximum Spanning Tree algorithm for nonprojective parsing and  <scope> Eisner's algorithm for projective parsing </scope>  (TREF) . 
<scope> Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling </scope>  (GREF) or  <scope> alternative approaches to training </scope>  (GTREF) . 
<scope> Much work has addressed this problem </scope>  :  generative models for direct phrasal alignment (REF) ,   <scope> heuristic word-alignment combinations </scope>  (GTREF) ,  models with pseudoword collocations (GREF) ,  synchronous grammar based approaches (REF) ,  etc .  Most have a large state-space ,  using constraints and approximations for efficient inference . 
<scope> Much work in this area focused on evaluating reviews as being positive or negative either at the document level </scope>  (GTREF) or sentence level (GREF) . 
<scope> Multi-view learning has been successfully applied to a number of tasks in natural language processing (NLP) ,  including </scope>  text classification (GREF) ,   <scope> named entity classification  </scope> (TREF) ,  base noun phrase bracketing (REF) ,  and statistical parsing (GREF) . 
<scope> My semantic disambiguator  </scope> (REF)  <scope> is an evolution of a tool previously developed for both French and English at XRCE </scope>  (GTREF) . 
<scope> Nearly all previous work on unsupervised grammar induction has focused on learning </scope>  hierarchical phrase structure (GREF) or  <scope> dependency structure </scope>  (TREF) ;  we are aware of only one previous paper on unsupervised syntactic chunking (REF) . 
<scope> NETE discovery from comparable corpora using time series and transliteration model was proposed in </scope>  (TREF) ,  and extended for NETE mining for several languages in (REF) . 
<scope> Nivres parser has been tested for Swedish  </scope> (TREF) ,  English (REF) ,  Czech (REF) ,  Bulgarian (REF) and Chinese Cheng et al . 
<scope> NLP researchers have </scope>  used databases from dictionary publishers (GREF) ,  or  <scope> developed NLP resources </scope>  (COMLEX (REF) ,   <scope> XTAG </scope>  (TREF)) or used WordNet , (REF) or have switched to fully corpus-based strategies which need no lexicons . 
<scope> Non-local information </scope>  can be incorporated directly into the PCFG model (REF) ,  or  <scope> can be reconstructed in a post-processing step after PCFG parsing </scope>  (GTREF) . 
<scope> Non-probabilistic statistical classiers have also been applied to disambiguation in HPSG parsing :  voted perceptrons </scope>  (TREF) and support vector machines (REF) . 
<scope> Notable examples are </scope>  (REF) for unsupervised POS tagging and (TREF)  <scope> for unsupervised dependency parsing .  </scope>
<scope> Notably ,  </scope>  conditional estimation (GREF) ,   <scope> maximum margin estimation  </scope> (TREF) ,  and unsupervised contrastive estimation (REF)  <scope> have been applied to structured models .  </scope>
<scope> Note that cdb is a subset of the training corpus used in the REF shared task </scope>  (TREF) . 
<scope> Note that previous work </scope>  (GTREF)  <scope> has suggested the use of bootstrap tests </scope>  (REF)  <scope> for the calculation of confidence intervals for Bleu scores </scope>  . 
<scope> Note that unlike the constructions in </scope>  (REF) and (TREF)  <scope> no errors are possible for ngrams stored in the model .  </scope>
<scope> Notice that the best results in  </scope> (TREF) <scope> were obtained over the enriched version of the LKB ,  </scope>  i . e .  the combination of WordNet and extra information supplied by extendedWordNet (REF) . 
<scope> NVI(L , C) = 1log NVI(L , C) (8) V-measure ,  Vbeta and VI measure both completeness and homogeneity ,  no mapping between classes and clusters is needed  </scope> (TREF) and they are only dependent on the relative size of the clusters (REF) . 
<scope> Obtaining an explicit alignment ,  or a different way of bridging the language barrier ,  is an important step in many natural language processing (NLP) applications such as :  document retrieval </scope>  (GTREF) ,  Transliteration Mining (GREF) and Multilingual Web Search (GREF) . 
<scope> Obtaining an explicit alignment ,  or a different way of bridging the language barrier ,  is an important step in many natural language processing (NLP) applications such as :  </scope>  document retrieval (GREF) ,   <scope> Transliteration Mining </scope>  (GTREF<input type="hidden" va.
<scope> One advantage of automatically generated thesauruses </scope>  (GTREF)  <scope> over large-scale manually created thesauruses </scope>  such as WordNet (REF)  <scope> is that they might be tailored to a particular genre or domain .  </scope>
<scope> One approach for compactly increasing capacity is to automatically induce intermediate features through the composition of non-linearities ,  for example SVMs with a non-linear kernel </scope>  (TREF) ,  inducing compound features in a CRF (REF) ,  neural networks (GREF) ,  and boosting decision trees (REF) . 
<scope> One common approach involves the design of heuristic rules to identify specific types of (non-)anaphoric NPs such as </scope>  pleonastic pronouns (e . g .   ,  GREF) and  <scope> definite descriptions </scope>  (e . g .   ,  TREF) . 
<scope> One could use a sophisticated method for lemmatizing words  </scope> (e . g .  ,  GTREF) ,  but we would likely have to clean the lexicon later ;  as REF point out ,  it is difficult to automatically guess the entries for a word ,  without POS information . 
<scope> One especially interesting possibility is to apply multiple-pass techniques to formalisms that require >> O(n 3) parsing time ,  such as </scope>  Stochastic Bracketing Transduction Grammar (SBTG) (REF) and <scope> Stochastic Tree Adjoining Grammars (STAG) </scope>  (GTREF) . 
<scope> One exception to this is  </scope> (TREF) ,   <scope> where the target function space are the subsequential transducers </scope>  ,  for which a limit-identification algorithm exists (REF) . 
<scope> One is sample selection </scope>  (GTREF) <scope>  ,  a variant of active learning </scope>  (REF) ,   <scope> which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 .  </scope>
<scope> One is the statistical dependency transduction model </scope>  (GTREF) ,   <scope> a trainable generative statistical translation model using head transducers </scope>  (REF) . 
<scope> One of the most common approaches to performing SRL automatically is to use a statistical classifier trained on labeled corpora </scope>  (TREF) ,  with FrameNet (REF) and PropBank (REF) being the primary sources . 
<scope> One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing ,  e . g .   </scope> the deterministic parsing method of (REF) and  <scope> the minimum spanning tree parser of </scope>  (TREF) . 
<scope> One promising way to tackle the problem of error propagation is to explore joint learning which integrates evidences from multiple sources and captures mutual benefits across multiple components of a pipeline for all relevant subtasks simultaneously </scope>  (e . g .  ,  (TREF) ,  (REF) ,  (REF)) . 
<scope> One solution might be to apply an informed prior with a computationally tractable inference procedure  </scope> (e . g .  TREF or REF) . 
<scope> One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of </scope>  (REF) ,  (REF) ,  (REF) ,  and (TREF) . 
<scope> One way around this dif culty is to stipulate that all rules must be binary from the outset ,  as in </scope>  inversion-transduction grammar (ITG) (REF) and  <scope> the binary synchronous context-free grammar (SCFG) employed by the Hiero system </scope>  (TREF)  <scope> to model the hierarchical phrases .  </scope>
<scope> One way around this difficulty is to stipulate that all rules must be binary from the outset ,  as in Inversion Transduction Grammar (ITG) </scope>  (TREF) and the binary SCFG employed by the Hiero system (REF) <scope> to model the hierarchical phrases .  </scope>
<scope> On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications :  the discovery of semantic verb relations </scope>  (TREF) ,  the acquisition of entailment relations (REF) ,  and the discovery of concept-specific relationships (REF) . 
<scope> On the empty category prediction task ,  our parser outperforms the best previously reported system </scope>  (TREF)  <scope> by 0 . 7% reaching an f-score of 84 . 1% ,  although the general parsing accuracy of our unlexicalized parser is 3% lower than that of the parser used by </scope>  TREF . 
<scope> On the evaluation data in  </scope> (REF) ,   <scope> this system significantly outperformed MEMT </scope>  (TREF) ,   <scope> which was among the best-performing system combination tools at WMT 2010 </scope>  (Callison-REF) . 
<scope> On the other hand ,  Goldberg and Elhadad proposed splitSVM  </scope> (TREF)  <scope> for fast low-degree polynomial kernel classifiers ,  and applied it to transition based parsing </scope>  (REF) . 
<scope> On the other hand ,  high-quality treebanks such as the Penn Treebank </scope>  (TREF) and the Kyoto University text corpus (REF)  <scope> have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structure analysis .  </scope>
<scope> On the other hand ,  in text-based NER ,  better results are obtained using discriminative schemes such as </scope>  maximum entropy (ME) models (GREF) ,  support vector machines (SVMs) (REF) ,  and  <scope> conditional random fields (CRFs) </scope>  (TREF) . 
<scope> On the other hand </scope>  ,  both  <scope> BLEU </scope>  (TREF) and NIST (REF)  <scope> scores are higher for the baseline system (mteval-v11b . pl) .  </scope>
<scope> On the other hand ,  we scored lower on all-caps than BBN's Identifinder in the MUC-7 formal evaluation for reasons which are probably similar to the ones discussed in section 9 in the comparison of our mixed case performances </scope>  (REF) (TREF) . 
<scope> On the practical side ,  we have corpora with CCG derivations for </scope>  each sentence (REF) ,  a wide-coverage parser trained on that corpus (REF) and  <scope> a system for converting CCG derivations into semantic representations  </scope> (TREF) . 
<scope> OREF made use of a window size of five words and features from the Penn Treebank (PTB)  </scope> (TREF) and FrameNet (REF) to classify prepositions . 
<scope> Other approaches include the extraction of semantic preferences from  </scope> sense-annotated (REF) and raw corpora (REF) ,   <scope> as well as the disambiguation of dictionary glosses based on cyclic graph patterns  </scope> (TREF) . 
<scope> Other approaches use ensemble techniques by </scope>  either combining lexicon-based and corpus-based algorithms (REF) or  <scope> combining sentiment classification outputs from different experimental settings </scope>  (TREF) . 
<scope> Other corpus-based methods determine associations between words  </scope> (GTREF) ,  which yields a basis for computing thesauri ,  or dictionaries of terminological expressions and multiword lexemes (GREF) . 
<scope> Other interesting work that addresses a similar task as ours includes the work on </scope>  homophones (e . g .  ,  REF) ,  abbreviations with their definitions (e . g .  ,  REF) ,  abbreviations and acronyms in the medical domain (REF) ,  and  <scope> transliteration </scope>  (e . g .  ,  (GTREF)) . 
<scope> Other linear functions have been explored for MBR ,  including </scope>  Taylor approximations to the logarithm of BLEU (REF) and  <scope> counts of matching constituents </scope>  (TREF) ,   <scope> which are discussed further in Section 3 . 3 </scope>  . 
<scope> Other Machine Learning approaches (GTREF) make use of decision tree learning </scope> 4 ;  we used C4 . 5 (REF) . 
<scope> Other Machine Learning approaches </scope>  (GTREF)  <scope> make use of decision tree learning4 </scope>  ;  we used C4 . 5 (REF) . 
<scope> Other metrics are based on </scope>  computing lexical precision ,  e . g .  ,  BLEU (REF) and NIST (REF) ,  lexical recall ,  e . g .  ,  ROUGE (REF) and CDER (REF) ,  or  <scope> a balance between the two ,  e . g .  ,  GTM </scope>  (GTREF) ,  METEOR (REF) ,  BLANC (REF) ,  SIA (REF) ,  MAXSIM (REF) ,  and Ol (REF) . 
<scope> Other phrase-based models model the joint distribution P(e ,  f ) </scope>  (TREF) or made P(e) and P( f | e) into features of a log-linear model (REF) . 
<scope> Other potential uses of these algorithms include </scope>  better language modeling by building topic-based language models ,  improving NLP algorithms (e . g .  coreference resolution) ,  summarization ,  hypertext linking (REF) ,  <scope> automated essay grading </scope>  (TREF) and topic detection and tracking (TDT program committee ,  1998) . 
<scope> Other research directed towards improving the throughput of unification-based parsing systems has been concerned with the unification operation itself ,  which can consume up to 90% of parse time (e . g .  </scope>  TREF <scope> ) in systems using lexicalist grammar formalisms (e . g .  HPSG ;  REF) .  </scope>
<scope> Other researchers have focused on the generation of readable texts for readers with low basic skills  </scope> (TREF) ,  and for teaching foreign languages (REF) . 
<scope> Others are finer-grained and based e . g .  on argumentative zones  </scope> (GTREF) ,  qualitative dimensions (REF) or conceptual structure (REF) of documents . 
<scope> Other statistical systems that address word classification probleans do not  </scope> emphasize the use of linguistic knowledge and do not deal with a specific word class[REF] ,  or <scope>  </scope> do not  <scope> exploit as much linguistic knowledge as we do  </scope> [TREF] . 
<scope> Other studies have shown that when both speech and text are available to labelers ,  </scope>  segmentation is clearer (REF) and  <scope> reliability improves  </scope> (TREF) . 
<scope> Other techniques use clustering and/or similarity matrices based on word co-occurrences </scope>  (GTREF) ,  and still others use machine learning techniques to detect cue words ,  or hand-selected cue words to detect segment boundaries (GREF) . 
<scope> Other than transformation based systems the methods presented in this paper can be used for learning rules of  </scope> constraint grammars (REF) ,   <scope> phonological rule systems as in  </scope> (TREF) ,  and in general those grammatical systems using constraints represented by means of rewriting rules . 
<scope> Other well-known metrics are  </scope> WER (REF) ,  NIST (REF) ,  GTM (REF) ,  ROUGE (REF) ,   <scope> METEOR  </scope> (TREF) ,  and TER (REF) ,  just to name a few . 
<scope> Other well-known metrics are </scope>  WER (REF) ,  NIST (REF) ,  GTM (REF) ,   <scope> ROUGE </scope>  (TREF) ,  METEOR (REF) ,  and TER (REF) ,  just to name a few . 
<scope> Other work has looked at te </scope>  <scope> chniques for learning phrasal patterns likely to contain slot fillers </scope>  (GTREF) or contain information semantically similar to a set of seed examples (REF) . 
<scope> Other works describe systems that induce structures from corpora ,  but they use  </scope> tagged corpora (REF) ,  or  <scope> grammatical informations </scope>  (TREF) ,  or work with artificial samples (REF) . 
<scope> Other works describe systems that induce structures from corpora ,  but they use tagged corpora  </scope> (TREF) ,  or grammatical informations (REF) ,  or work with artificial samples (REF) . 
<scope> Other work uses human-annotated corpora ,  such as the RST Bank  </scope> (TREF) ,  used by REF ,  the GraphBank (REF) ,  used by Wellner et al . 
<scope> Our approach builds upon earlier work on corpus-based methods for </scope>  generating extraction patterns (REF) and  <scope> semantic lexicons </scope>  (TREF) . 
<scope> Our approach is to use finite-state approximations of long-distance dependencies ,  as they are described in </scope>  (REF) for Dependency Grammar (DG) and (TREF)  <scope> for Lexical Functional Grammar (LFG) .  </scope>
<scope> Our baseline is a factored phrase based SMT system that uses  </scope> the Moses toolkit (REF) for translation model training and decoding ,  GIZA +  +  (REF) for word alignment ,   <scope> SRILM  </scope> (REF) an KenLM (REF)  <scope> for language modelling and minimum error rate training  </scope> (TREF) to tune model feature weights . 
<scope> Our baseline is a factored phrase based SMT system that uses  </scope> the Moses toolkit (REF) for translation model training and decoding ,   <scope> GIZA +  +  </scope>  (TREF)  <scope> for word alignment </scope>  ,  SRILM (REF) an KenLM (REF) for language modelling and minimum error rate training (REF) to tune model feature weights . 
<scope> Our bootstrapping model can be viewed as a form of self-training (e . g .  ,  </scope>  (GTREF)) ,  and cross-category training is similar in spirit to co-training (e . g .  ,  (GREF)) . 
<scope> Our decoder </scope>  presently handles SCFGs of the kind extracted by Heiro (REF) ,  but  <scope> is easily extensible to more general SCFGs and closely related formalisms such as synchronous tree substitution grammars </scope>  (GTREF) . 
<scope> Our discrete ,  classificatio-nased approach </scope>  has the same goal as probabilistic methods for language modeling for automatic speech recognition (REF) ,  and  <scope> is also functionally equivalent to n-gram models with back-off smoothing </scope>  (TREF) . 
<scope> Our evaluator has two parts :  the Binomial Hypothesis Test </scope>  (TREF) and a back-off algorithm (REF) . 
<scope> Our first word representation is exactly the same as the one used in </scope>  (TREF) where words are clustered using the Brown algorithm (REF) . 
<scope> Our framework for rule extraction is thus most similar to the Stat-XFER system </scope>  (GTREF) and the tree-to-tree situation considered by REF . 
<scope> Our method is different from automated work on metaphor recognition such as  </scope> (TREF) and (REF) in that it includes nouns as parts of speech . 
<scope> Our method starts with word-level alignments of two sentences that are paraphrases ,  since the approach can be used with any alignment method from the MT (GREFfor example) or textual inference </scope>  (TREF ,  inter alia) literature in principle . 
<scope> Our method was applied to 23 million words of the WSJ that were automatically tagged with Ratnaparkhi's maximum entropy tagger </scope>  (TREF) and chunked with the partial parser CASS (REF) . 
<scope> Our model considers two sets of features :  Feature Set 1 (FS1) :  features used in the work reported in (REF) and </scope>  (TREF)  ;  and Feature Set 2 (FS2) :  a novel set of features introduced in this paper . 
<scope> Our models improve translation quality over the single generic label approach of </scope>  TREF and perform on par with the syntactically motivated approach from REF on the NIST large Chineseto-English translation task . 
<scope> Our monolingual model is most similar to previous work using counts from web-scale text ,  both for resolving coordination ambiguity </scope>  (GTREF) ,  and for syntax and semantics in general (GREF) . 
<scope> Our primary points of comparison are </scope>  the latent SVM training of REF ,  mentioned above ,  and <scope> the extensive set of local and nonlocal feature templates developed by </scope>  TREF <scope> for parse tree reranking .  </scope>
<scope> Our research is also close to semi-supervised IE pattern learners including </scope>  (REF) ,  (REF) ,  (TREF) ,  and many others . 
<scope> Our system ,  therefore ,  is based on a shallow-processing approach more radical even than that attempted by the  </scope> first advocate of this approach ,  REF ,  or by the  <scope> systems that participated in the MUC evaluations </scope>  (GTREF) ,  <scope> since we made no attempt to fine-tune the system to maximize performance on a particular domain .  </scope>
<scope> Our tool OIL2XSD </scope>  (TREF)  <scope> transforms an ontology written in OIL (REF) into an M3L compatible XML Schema definition .  </scope>
<scope> Our unsupervised approach follows a self training protocol </scope>  (GTREF) enhanced with constraints restricting the output space (GREF) . 
<scope> Our unusual focus means that we can not readily take advantage of software toolkits such as NLTK  </scope> (TREF) or Regulus (REF) . 
<scope> Our work builds upon Turneys work on semantic orientation </scope>  (TREF) and synonym learning (REF) ,  in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries . 
<scope> Our work is mostly influence by the work of </scope>  (GTREF) as well as by the work described in (GREF) . 
<scope> Out of these 41 papers ,  36 feature as a test set (with nfold validation) also for the sentence splitters RASP  </scope> (TREF) and the XML-aware sentence splitter developed by (REF) . 
<scope> Over the last decade ,   </scope> (<span class="trefstyle ref55823" id="s55823 . r1">GTREF<div class="tooltip fixed" style="left :  97 . 5px ;  top :  10368px ;  display :  none ;  %2.
<scope> Pair HMMs originate from work in Biological sequence analysis </scope>  (GREF)  <scope> from which variants were created and successfully applied in </scope>  cognate identification (REF) ,   <scope> Dutch dialect comparison </scope>  (TREF) ,  transliteration identification (REF) ,  and transliteration generation (REF) . 
<scope> Parameter tuning was done with minimum error rate training </scope>  (TREF) ,  which was used to maximize IBM BLEU-4 (REF) . 
<scope> Paraphrasesofthiskind have been shown to be useful </scope>  in applications such as machine translation (REF) and  <scope> as an intermediate step in inventory-based classification of abstract relations </scope>  (GTREF) . 
<scope> PARENT METHOD differs from conventional methods such as </scope>  TREF or REF ,   <scope> in the process of determining the parent node </scope>  . 
<scope> Parsers and applications usually refer to grammars built around a core of dependency concepts ,  but there is a great variety in the description of syntactic constraints ,  </scope>  from rules that are very similar to CFG productions (REF) to <scope> individual binary relations on words or syntactic categories </scope>  (TREF) (REF) . 
<scope> Parsing research has also begun to adopt discriminative methods from the Machine Learning literature ,  such as the perceptron </scope>  (GTREF) and the largemargin methods underlying Support Vector Machines (GREF) . 
<scope> Particularly ,  syntactically annotated corpora (treebanks) ,  such as </scope>  Penn Treebank (REF) ,   <scope> Negra Corpus </scope>  (TREF) and EDR Corpus (REF) ,   <scope> contribute to improve the performance of morpho-syntactic analysis systems .  </scope>
<scope> Part of the pronoun resolution performance here enables a preliminary comparison with the results reported in </scope>  (1) REF and (2) TREF . 
<scope> Past approaches have pruned spans  </scope> using IBM Model 1 probability estimates (REF) or  <scope> using agreement with an existing parse tree </scope>  (TREF) . 
<scope> Past research in a variety of NLP tasks has shown that parsers </scope>  (TREF) ,  chunkers (REF) ,  part-of-speech taggers (REF) ,  named-entity taggers (REF) ,  and word sense disambiguation systems (REF) all  <scope> suffer from a similar drop-off in performance on out-of-domain tests .  </scope>
<scope> Past research in a variety of NLP tasks ,  like parsing </scope>  (TREF) and chunking (REF) ,   <scope> has shown that systems suffer from a drop-off in performance on out-of-domain tests .  </scope>
<scope> Pattern-based approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen ,  either manually </scope>  (GTREF) or via automatic bootstrapping (GREF) . 
<scope> People have proposed different syntactic rules to pre-reorder SOV languages ,  either based on a constituent parse tree  </scope> (GTREF) or dependency parse tree (REF) . 
<scope> Performing 5-fold cross validation on the nwire and bnews corpora ,  </scope>  (TREF) and (REF)  <scope> reported F-measures of 71 . 5 </scope>  and 71 . 2 ,  respectively . 
<scope> Perhaps the most well-known method is </scope>  maximum marginal relevance (MMR) (REF) ,  as well as  <scope> cross-sentence informational subsumption </scope>  (TREF) ,  mixture models (REF) ,  subtopic diversity (REF) ,  diversity penalty (REF) ,  and others . 
<scope> Phoneme based models ,  such as </scope>  ,  the ones based on Weighted Finite State 421 Transducers (WFST) (REF) and  <scope> extended Markov window  </scope> (TREF) treat transliteration as a phonetic process rather than an orthographic process . 
<scope> Phonological rewrite-rules </scope>  (TREF) ,  two-level rules (REF) ,  syntactic disarnbiguation rules (Kar]sson et al 1994 ,  REF) ,  and part-of-speech assignment rules (GREF)  <scope> are examples of replacement in context of finite-state grammars .  </scope>
<scope> Phrase-based MT systems </scope>  are straightforward to train from parallel corpora (REF) and ,   <scope> like the original IBM models  </scope> (TREF) ,   <scope> benefit from standard language models built on large monolingual ,  target-language corpora </scope>  (REF) . 
<scope> Pointers ,  in various forms ,  allow one to efficiently represent infinite circular references ,  either directly </scope>  (TREF) ,  or indirectly (REF) . 
<scope> Popular approaches to text extraction essentially collapse interpretation and transformation into one step </scope>  ,  with generation either being ignored or consisting of postprocessing techniques such as sentence compression (GREF) or  <scope> sentence merging </scope>  (TREF) . 
<scope> Popular approaches to text extraction essentially collapse interpretation and transformation into one step ,  with generation either being ignored or consisting of postprocessing techniques such as sentence compression </scope>  (GTREF) or sentence merging (REF) . 
<scope> Popular clustering algorithms used prevalently in many NER systems are ,  for example ,  the combination of distributional and morphological similarity work of </scope>  (TREF) or the classic N-gram language model based clustering algorithm of (REF) . 
<scope> Previous approaches to (automatically) deriving a text's discourse structure either tried to rely on purely surface-based criteria (cuephrases </scope>  ,  (TREF)) or concentrated on inferences about representations of discourse units (GREF) . 
<scope> Previous research in automatic acquisition focuses primarily on the use of statistical techniques ,  such as </scope>  bilingual alignment (GREF) or  <scope> extraction of syntactic constructions from online dictionaries and corpora </scope>  (TREF) . 
<scope> Previous research in inducing sense rankings from an untagged corpus </scope>  (REF) ,   <scope> and inducing selectional preferences at the word level (for other applications) </scope>  (TREF)  <scope> will provide the starting point for research in this direction .  </scope>
<scope> Previous research in inducing sense rankings from an untagged corpus </scope>  (TREF) ,  and inducing selectional preferences at the word level (for other applications) (REF)  <scope> will provide the starting point for research in this direction .  </scope>
<scope> Previous sentiment-analysis work in different domains has considered inter-document similarity </scope>  (GTREF) or explicit 333 inter-document references in the form of hyperlinks (REF) . 
<scope> Previous uses of this model include language modeling </scope> (TREF) ,  machine translation(REF) ,  prepositional phrase attachment(REF) ,  and word morphology(Della REF) . 
<scope> Previous work has discussed the building of special-purpose classifiers which generate grammatical elements such as </scope>  prepositions (REF) ,  determiners (REF) and  <scope> case markers </scope>  (TREF)  <scope> with an eye toward improving MT output .  </scope>
<scope> Previous work has either assumed that </scope>  (1) features are not assigned to classes ,  but instead flagged for relevance to the task (GREF) ,  or (2) <scope> feature queries are posed just like instance queries :  a word is presented to the annotator ,  who must choose among the labels  </scope> (GTREF) . 
<scope> Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees </scope>  (GTREF) or dependency trees (REF) ,  which are obtained using a parser trained to determine linguistic constituency . 
<scope> Previous work has used functionality for the tasks of </scope>  contradiction detection (REF) ,   <scope> quantifier scope disambiguation </scope>  (TREF) ,  and synonym resolution (REF) . 
<scope> Previous work includes using </scope>  noun phrases (GREF) ,  words (REF) ,   <scope> n-grams  </scope> (GTREF) or proper nouns ,  multi-word terms and abbreviations (REF) . 
<scope> Previous work in this line includes speaker role detection </scope>  (e . g .   ,  GTREF) and speaker diarization (e . g .   ,  REF) . 
<scope> Previous work on building hybrid systems includes ,  among others </scope>  ,  approaches using reranking ,  regeneration with an SMT decoder (GREF) ,  and  <scope> confusion networks  </scope> (GTREF) . 
<scope> Previous work on building hybrid systems includes </scope>  ,  among others ,  approaches using reranking ,  regeneration with an SMT decoder (GREF) ,   <scope> and confusion networks </scope>  (GTREF) . 
<scope> Previous work on multimodal reference resolution includes the use of </scope>  a focus space model (REF) ,  the centering framework (REF) ,   <scope> context factors </scope>  (TREF) ,  and rules (REF) . 
<scope> Previous work on temporal inference has focused on </scope>  the automatic tagging of temporal expressions (e . g .   ,  REF) or on  <scope> learning the ordering of events from manually annotated data </scope>  (e . g .   ,  TREF) . 
<scope> Previous work on unsupervised grammar induction has made progress by exploiting information such as  </scope> gold-standard part of speech tags (e . g .  REF) or  <scope> punctuation  </scope> (e . g .  TREF) . 
<scope> Previous work on using the unordered set of surrounding words have used a much larger window ,  such as the 100-word window of  </scope> (TREF) ,  and the 2-sentence context of (REF) . 
<scope> Previous works dealing with translating telegraphic text ,  such as </scope>  (TREF) ,  (REF) requires to identify dependency relations among the tokens of the telegraphic input . 
<scope> Previous works have shown that supervised approaches to Word Sense Disambiguation which rely on sense annotated corpora </scope>  (GTREF)  <scope> outperform unsupervised </scope>  (<span class="refstyle ref59054" id="s59054 . r2">REF<div class="to.
<scope> Previous works that focus on multi-word translation correspondences from parallel corpora include </scope>  noun phrase correspondences (REF) ,  fixed/flexible collocations (REF) ,  n-gram word sequences of arbitrary length (REF) ,  non-compositional compounds (REF) ,   <scope> captoids </scope>  (TREF) ,  and named entities 1 . 
<scope> Prior work has explored the use of PBMT for paraphrasegeneration </scope> (TREF ;  Bannard and Callison-GREFCallison-GREF) However ,  since many researchers believe that PBMT has reached a performance ceiling ,  ongoing research looks into more structural approaches to statistical MT (GREF) . 
<scope> Projection-based approaches to multilingual annotation have proven adequate in various domains ,  including part-of-speech tagging  </scope> (TREF) ,  NP-bracketing (REF) ,  dependency analysis (REF) ,  and role semantic analysis (REF) . 
<scope> Projects involving learner corpora in analyzing and categorizing learner errors include NICT Japanese Learners of English (JLE) ,  the Chinese Learners of English Corpus </scope>  (TREF) and English Taiwan Learner Corpus (or TLC) (REF) . 
<scope> Promising candidates are random forest LMs </scope>  (TREF) ,  random cluster LMs (REF) and the neural network LM (REF) . 
<scope> Pronunciation by Analogy (PbA) is a datadriven method </scope>  (TREF)  <scope> for letter-to-phoneme conversion </scope>  which is used again by REF . 
<scope> PropBank (REF) is used as a gold-standard to inform these decisions ,  similar to the way that we use the </scope>  TREF  <scope> data .  </scope>
<scope> Proposed approaches to modelling NC semantics have used </scope>  semantic similarity (GREFO REF) and  <scope> paraphrasing </scope>  (GTREF) . 
<scope> Quasi-Logical Forms (QLFs) </scope>  (GTREF)  <scope> provide the semantic level of representation employed in the Core Language Engine (CLE) </scope>  (REF) . 
<scope> Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar ,  though dependency grammars have been used in most previous applications of QG </scope>  (GTREF) ,  including previous work in MT (GREF) . 
<scope> Ranking algorithms may be </scope>  rule-based (e . g .  ,  the one using a threshold on distributional similarity in (REF)) or  <scope> ML-based (e . g .  ,  the SVM model in </scope>  (TREF) for combining pattern-based and distributional features) . 
<scope> Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of  </scope> 72 . 5% agreement in the preparation of the English all-words test set at Senseval-3 (REF) and  <scope> 67 . 3% on the Open Mind Word Expert annotation exercise  </scope> (TREF) . 
<scope> Recent innovations have greatly improved the efficiency of language model integration through multipass techniques ,  such as forest reranking </scope>  (TREF) ,  local search (REF) ,  and coarse-to-fine pruning (GREF) . 
<scope> Recent innovations have greatly improved the efficiency of language model integration through multipass techniques ,  such as </scope>  forest reranking (REF) ,   <scope> local search </scope>  (TREF) ,  and coarse-to-fine pruning (GREF) . 
<scope> Recent investigations along this line includes using </scope>  word-disambiguation schemes (REF) and  <scope> non-overlapping bilingual word-clusters </scope>  (GTREF)  <scope> with particular translation models ,  which showed various degrees of success </scope>  . 
<scope> Recently ,  a Pearson correlation method was proposed to mine word pairs from comparable corpora (REF) ;  this idea is similar to the method used in </scope>  (TREF)  <scope> for sentence alignment .  </scope>
<scope> Recently ,  content features were also well studied ,  including </scope>  centroid (REF) ,  signature terms (TREF) and high frequency words (Nenkova e t al .  ,  2006) . 
<scope> Recently ,  dependency parsing has gained popularity as a simpler ,  computationally more efficient alternative to constituency parsing and has spurred several supervised learning approaches </scope>  (GTREF) as well as unsupervised induction (GREF) . 
<scope> Recently ,  graph-based methods have proved useful for a number of NLP and IR tasks such as </scope>  document re-ranking in ad hoc IR (REF) and  <scope> analyzing sentiments in text </scope>  (TREF) . 
<scope> Recently ,  graph-based ranking methods have been proposed for sentence ranking and scoring ,  such as LexRank </scope>  (TREF) and TextRank (REF) . 
<scope> Recently IE systems based on supervised learning paradigms such as </scope>  hidden Markov models (REF) ,  maximum entropy (REF) and  <scope> decision trees  </scope> (TREF)  <scope> have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past .  </scope>
<scope> Recently ,  machine learning approaches are widely used in NER ,  including </scope>  the hidden Markov model (GREF) ,  maximum entropy model (REF) ,  decision tree (REF) ,  transformation-based learning (REF) ,   <scope> boosting </scope>  (GTREF) ,  support vector machine (GREF) ,  memory-based learning (REF) . 
<scope> Recently ,  methods for training binary classifiers to maximize the F 1 -score have been proposed for  </scope> SVM (REF) and  <scope> LRM  </scope> (TREF) . 
<scope> Recently ,  more linguistically informed algorithms have been introduced </scope>  both for CE (REF) and  <scope> for ATR </scope>  (TREF) ,   <scope> which have been shown to outperform several of the statisticsonly metrics .  </scope>
<scope> Recently ,  new methods aim to assign fine-grained affect labels </scope>  based on various psychological theoriese . g .  ,  the MPQA project (REF)  <scope> based on literary theory and linguistics and work by </scope>  (TREF) based on the Appraisal framework (REF) . 
<scope> Recently ,  the idea of using fine-grained paraphrasing verbs for NC semantics has been gaining popularity  </scope> (GTREF) ;  there has also been a related shared task at SemEval-2010 (REF) . 
<scope> Recently ,  the integration of NLP systems with manually-built resources at the predicate argument-level ,  such as </scope>  FrameNet (REF) and  <scope> PropBank </scope>  (TREF)  <scope> has received growing interest </scope>  . 
<scope> Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption ,  including </scope>  both approximate methods (REF)andexactmethodsthroughinteger linear programming (REF) or  <scope> branch-and-bound algorithms </scope>  (TREF) . 
<scope> Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption ,  including </scope>  both approximate methods (REF)and <scope> exactmethodsthroughinteger linear programming </scope>  (TREF) or branch-and-bound algorithms (REF) . 
<scope> Recently ,  there have been several discriminative approaches at training large parameter sets including </scope>  (TREF) and (REF) . 
<scope> Recent research has addressed the challenges of  </scope> detecting decisions (REF) ,  action items (GREF) and  <scope> subjective sentences </scope>  (TREF) . 
<scope> Recent research has focused on non-English languages such as Spanish ,  Dutch ,  and German </scope>  (GTREF) ,  and on improving the performance of unsupervised learning methods (GREF) . 
<scope> Recent research including the use of loopy belief network </scope>  (TREF) ,  integer linear programming (REF) and an improved dynamic programming algorithm (REF)  <scope> can be seen as methods to incorporate nonlocal features into a graph-based model </scope>  . 
<scope> Recent research </scope>  [REF]  <scope> shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models ,  which use the same clusters for both words </scope>  [TREF] . 
<scope> Recent studies have also shown that WSD can bene t </scope>  machine translation (REF) and  <scope> information retrieval </scope>  (TREF) . 
<scope> Recent studies have shown that SMT systems can benefit from widening the annotation pipeline :  using packed forests instead of 1-best trees  </scope> (TREF) ,  word lattices instead of 1-best segmentations (REF) ,  and weighted alignment matrices instead of 1-best alignments (REF) . 
<scope> Recent studies have shown that SMT systems can benefit from widening the annotation pipeline :  using </scope>  packed forests instead of 1-best trees (REF) ,   <scope> word lattices instead of 1-best segmentations </scope>  (TREF) ,  and weighted alignment matrices instead of 1-best alignments (REF) . 
<scope> Recent studies in the difficult task of Word Sense Disambiguation </scope>  (REF ,   <scope> WSD) have shown the impact of the amount and quality of lexical knowledge </scope>  (REF) :   <scope> richer knowledge sources can be of great benefit to </scope>  both knowledge-lean systems (REF) and  <scope> supervised classifiers </scope>  (GTREF) . 
<scope> Recent work has applied Bayesian non-parametric models to anaphora resolution  </scope> (TREF) ,  lexical acquisition (REF) and language modeling (REF)  <scope> with good results .  </scope>
<scope> Recent work has applied Bayesian non-parametric models to </scope>  anaphora resolution (REF) ,  lexical acquisition (REF) and  <scope> language modeling </scope>  (TREF)  <scope> with good results </scope>  . 
<scope> Recent work has shown how to  </scope> define probability distributions over the parses of UBGs (REF) and <scope> efficiently estimate and use conditional probabilities for parsing  </scope> (TREF) . 
<scope> Recent work includes improved model variants  </scope> (e . g .   ,  GTREF) and applications such as web data extraction (REF) ,  scientific citation extraction (REF) ,  and word alignment (REF) . 
<scope> (REF) and </scope>  (TREF)  <scope> describe the use of syntactic features in reranking the output of a full translation system ,  but the syntactic features give very small gains .  </scope>
<scope> REF developed a top-down alternative along similar lines but based on a lexicalized and context-sensitive DP version of an efficient Earley parser </scope>  (GTREF) . 
<scope> Reference-free approaches to automatic MT quality assessment ,  based on Machine Learning techniques such as </scope>  classification (REF) ,  regression (REF) ,  and  <scope> ranking </scope>  (GTREF) ,   <scope> have a different focus compared to ours .  </scope>
<scope> REF extended the work of </scope>  TREF  <scope> for largescale anaphoricity determination by additionally detecting non-anaphoric instances of it using Minipars pleonastic category Subj .  </scope>
<scope> REF extracted collocations with Xtract </scope>  (TREF) and classified the collocations using the orientations of the words in the neighboring sentences . 
<scope> REF originally proposed the averaged parameter method ;  it was shown to give substantial improvements in accuracy for tagging tasks in </scope>  TREF . 
<scope> REF propose an approach that applies a modified version of the dependency path distributional similarity algorithm proposed by REF to the same monolingual parallel corpus (multiple translations of literary works) used by </scope>  TREF . 
<scope> REF propose an HMM framework capable of dealing with 1-to-n alignment ,  which is an extension of the original model of </scope>  (TREF) . 
<scope> REF recently argued that probabilistic Synchronous Tree Adjoining Grammars </scope>  (TREF)  <scope> have the right combination of properties that satisfy both linguists and empirical MT practitioners .  </scope>
<scope> REFs parser and its reimplementation and extension </scope>  by REF  <scope> have by now been applied to a variety of languages :  </scope>  English (REF) ,  Czech (REF) ,  German (REF) ,   <scope> Spanish </scope>  (TREF) ,  French (REF) ,  Chinese (REF) and ,  according to Dan Bikels web page ,  Arabic . 
<scope> REFs parser and its reimplementation and extension </scope>  by REF  <scope> have by now been applied to a variety of languages :   </scope> English (REF) ,  Czech (REF) ,   <scope> German  </scope> (TREF) ,  Spanish (REF) ,  French (REF) ,  Chinese (REF) and ,  according to Dan Bikels web page ,  Arabic . 
<scope> REF uses the discriminative perceptron algorithm </scope>  (TREF) <scope> to score whole character tag sequences ,  finding the best candidate by the global score .  </scope>
<scope> Regarding the WordNet expansion technique we use here ,  it is implemented on top of publicly available software4 ,  which has been successfully used in </scope>  word similarity (REF) and  <scope> word sense disambiguation </scope>  (TREF) . 
<scope> Regardless of whether it takes the form of  </scope> dictionaries (GREF) ,   <scope> thesauri </scope>  (GTREF) ,  bilingual corpora (GREF) ,  or hand-labeled training sets (GREF) ,  <scope> providing information for sense definitions can be a considerable burden .  </scope>
<scope> Related work on Indo-Aryan languages has mostly focused on the extraction of complex predicates ,  with the focus on </scope>  Hindi (GREF) and  <scope> Bengali </scope>  (GTREF) . 
<scope> Relevance between term candidate and sentences ,  referred to as CS ,  is calculated using the TV_HITS (Term Verification  HITS) algorithm proposed in  </scope> (TREF) based on  Hyperlink-Induced Topic Search (HITS) algorithm (REF) . 
<scope> Reordering approaches have given significant improvements in performance for translation </scope>  from French to English (REF) and <scope> from German to English </scope>  (TREF) . 
<scope> Reported work includes improved </scope>  model variants (e . g .  ,  REF) and applications such as web data extraction (REF) ,  scientific citation extraction (REF) ,  word alignment (REF) ,  and  <scope> discourselevel chunking </scope>  (TREF) . 
<scope> (Re)rankers have been successfully applied to numerous NLP tasks ,  such as parse selection  </scope> (GTREF) ,  parse reranking (GREF) ,  question-answering (REF) . 
<scope> Rerankers have been successfully used in syntactic parsing </scope>  (GTREF) and semantic role labeling (REF) . 
<scope> Reranking approaches have given improvements in accuracy on a number of NLP problems  </scope> including parsing (GREF) ,   <scope> machine translation  </scope> (GTREF) ,  information extraction (REF) ,  and natural language generation (REF) . 
<scope> Researchers have also successfully harvested relations between entities ,  such as is-a </scope>  (GTREF) and part-of (REF) . 
<scope> Researchers have explored error detection for manually tagged corpora in the context of pos-tagging </scope>  (GTREF) ,  dependency parsing (REF) ,  and text-classification (REF) . 
<scope> Researchers have explored the use of this kind of document segmentation to improve automated summarization  </scope> (GTREF) and automated genre detection (REF) . 
<scope> Researchers have explored this problem for a variety of reasons :  to argue empirically against the poverty of the stimulus </scope>  (TREF) ,  to use induction systems as a first stage in constructing large treebanks (van REF) ,  or to build better language models (GREF) . 
<scope> Researchers have focused on learning adjectives or adjectival phrases </scope>  (GTREF) and verbs (REF) <scope>  ,  but no previous work has focused on learning nouns .  </scope>
<scope> Researchers have harvested with varying success semantic lexicons </scope>  (TREF) and concept lists (REF) . 
<scope> Researchers have </scope>  used supervised and semi-supervised approaches (GREF) ,  and  <scope> explored rich features </scope>  (TREF) ,  kernel design (GREF) and inference algorithms (REF) ,   <scope> to detect predefined relations between NEs .  </scope>
<scope> Researchers have studied the problem at </scope>  the document level (e . g .  ,  GREF) sentence and clause level (GREF) ,   <scope> word level </scope>  (e . g .  ,  GTREF) ,  and attribute level (GREF) . 
<scope> Researchers have studied the problem at the document level </scope>  (e . g .  ,  GTREF) sentence and clause level (GREF) ,  word level (e . g .  ,  GREF) ,  and attribute level (GREF) . 
<scope> Researchers have worked to incorporate syntactic information into word alignments ,  resulting in improvements to both alignment quality </scope>  (GTREF) ,  and translation quality (GREF) . 
<scope> Research in the field of unsupervised and weakly supervised parsing ranges from </scope>  various forms of EM training (GREF) over bootstrapping approaches like selftraining (REF) to feature-based enhancements of discriminative reranking models (REF) and  <scope> the application of semisupervised SVMs </scope>  (TREF) . 
<scope> Research that benefited from this additional layering ranges from </scope>  question answering (REF) ,  to conversation summarization (REF) ,  and  <scope> text semantic analysis </scope>  (GTREF) . 
<scope> Resolving this dilemma has been the topic of several studies in pitch accent placement </scope>  (GTREF) and in prosodic boundary placement (GREF) . 
<scope> Responsiveness differs from other measures of summary content such as  </scope> SEE coverage (REF) and <scope> Pyramid scores </scope>  (TREF) <scope> in that it does not compare a peer summary against a set of known human summaries .  </scope>
<scope> Results are presented in terms of the standard BLEU metric  </scope> (TREF) ,  METEOR metric (REF) and a manual evaluation targeting subject span translation correctness . 
<scope> Results are reported on three metrics ,  BLEU  </scope> (TREF) ,  NIST (REF) and Meteor ranking scores (REF) based on truecased output . 
<scope> Results are reported on three metrics </scope>  ,  BLEU (REF) ,  NIST (REF) and  <scope> Meteor ranking scores </scope>  (TREF)  <scope> based on truecased output .  </scope>
( <scope> Salience-based relations are discovered using a technique first reported in  </scope> (TREF) which approximates a Centering Theory-style approach (REF) to the resolution of coreference . ).
<scope> Second-order co-occurrence vectors were </scope>  first introduced by REF for the task of word sense discrimination and  <scope> later extended by </scope>  TREF . 
<scope> Seeds may be chosen at random </scope>  (GTREF) ,  by picking the most frequent terms of the desired class (GREF) ,  or by asking humans (REF) . 
<scope> See ,  for example ,  NP chunkers utilizing conditional random fields </scope>  (TREF) and support vector machines (REF) . 
<scope> See for instance the following :  Dependency Unification Grammar </scope>  (TREF) ,  Functional Generative Description (REF) ,  Meaning Text Theory (REF) ,  Word Grammar (REF) . 
<scope> Semantic change </scope>  ,  defined as a change of one or more meanings of the word in time (REF) ,   <scope> is of interest to historical linguistics and is related to the natural language processing task of unknown word sense detection </scope>  (TREF) . 
<scope> Semantic class information has proven to be useful for many natural language processing tasks ,  including </scope>  information extraction (GREF) ,  anaphora resolution (REF) ,   <scope> question answering </scope>  (GTREF) ,  and prepositional phrase attachment (REF) . 
<scope> Semantic knowledge has also been extracted from WordNet and unannotated corpora for computing the semantic compatibility/similarity between two common nouns </scope>  (GTREF) as well as the semantic class of a noun (GREF) . 
<scope> Semantic parsing enables logic reasoning and is critical in many practical tasks </scope>  <scope>  ,  such as </scope>  speech understanding (REF) ,   <scope> question answering </scope>  (TREF) and advice taking (REF) . 
<scope> Semantic similarity measures are vital for various applications in natural language processing such as </scope>  word sense disambiguation (REF) ,  language modeling (REF) ,  synonym extraction (REF) and <scope> automatic thesaurus extraction </scope>  (TREF) . 
<scope> Semantic tags are assigned from on-line thesaura like WordNet  </scope> (REF) (TREF) ,  Roget's categories (REF) (REF) ,  the Japanese BGH (REF) ,  or assigned manually (REF) 1 . 
<scope> Sentence-level subjectivity detection ,  where training data is easier to obtain than for positive vs .  negative classification ,  has been successfully performed </scope>  using supervised statistical methods alone (REF) or  <scope> in combination with a knowledgebased approach </scope>  (TREF) . 
<scope> Several automatic sentence alignment approaches have been proposed based on </scope>  sentence length (REF) and  <scope> lexical information </scope>  (TREF) . 
<scope> Several kinds of POS taggers using </scope>  rule-based (e . g .   ,  REF) ,  statistical (e . g .   ,  REF) ,  memory-based (e . g .   ,  REF) ,  and  <scope> neural network </scope>  (e . g .   ,  TREF)  <scope> models have been proposed for some languages .  </scope>
<scope> Several methods have been proposed to address different aspects of the coverage problem ,  ranging from automatic data expansion and semi-supervised semantic role labelling  </scope> (GTREF) to systems which can infer missing word senses (GREF) . 
<scope> Several methods have been proposed to address different aspects of the coverage problem </scope>  ,  ranging from automatic data expansion and semi-supervised semantic role labelling (GREF) to  <scope> systems which can infer missing word senses </scope>  (GTREF) . 
<scope> Several ML techniques have been successfully used for the NER task of which Hidden Markov Model </scope>  (TREF) <scope>  ,  Maximum Entropy </scope>  (REF) <scope>  ,  Conditional Random Field </scope>  (REF)  <scope> are most common .  </scope>
<scope> Several restrictions on the adjunction operation for TAG have been proposed in the literature  </scope> (GTREF) (REF) . 
<scope> Several works have addressed the problems of representing temporal information in  </scope> natural language (GREF) ,   <scope> extracting and/or anchoring (normalizing) temporal and event related expressions  </scope> (GTREF) ,  and discovering the ordering of events (REF) . 
<scope> Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models :  </scope>  string-to-tree (GREF) ,   <scope> tree-to-string </scope>  (GTREF) ,  tree-to-tree (GREF) ,  and treelet (GREF)  <scope> techniques use syntactic information to inform the translation model .  </scope>
<scope> Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models :  </scope>  string-to-tree (GREF) ,   <scope> tree-to-string </scope>  (GTREF) ,  tree-to-tree (GREF) ,  and treelet (GREF) techniques use syntactic information to inform the translation model . 
<scope> Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models :  string-to-tree </scope>  (GTREF) ,  tree-to-string (GREF) ,  tree-to-tree (GREF) ,  and treelet (GREF)  <scope> techniques use syntactic information to inform the translation model .  </scope>
<scope> Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models :  string-to-tree  </scope> (GTREF) ,  tree-to-string (GREF) ,  tree-to-tree (GREF) ,  and treelet (GREF)  <scope> techniques use syntactic information to inform the translation model .  </scope>
<scope> Similar advances have been made in machine translation </scope>  (TREF) ,  speech recognition (REF) and named entity recognition (REF) . 
<scope> Similar applications have so far been largely confined to authorship identification ,  such as </scope>  (GREF) and  <scope> the identification of association rules </scope>  (GTREF) . 
<scope> Similar dominance mechanisms have been employed in various tree description formalisms </scope>  (GTREF) and TAG extensions (GREF) . 
<scope> Similarity-based methods have also been successfully applied word sense disambiguation </scope>  (TREF) and extraction of grammatical relations (REF) . 
<scope> Similarly ,  the minimally-supervised Espresso algorithm </scope>  (TREF)  <scope> is initialized with a single set that mixes seeds of heterogeneous types ,  such as leader-panel and oxygen-water ,  which respectively correspond to the member-of and sub-quantity-of relations in the taxonomy of REF .  </scope>
<scope> Similar motivation to study LVCs/SVCs (for SRL) is found within the scope of  </scope> Framenet (REF) and  <scope> Propbank </scope>  (TREF) . 
<scope> Similar techniques are used </scope>  in (GREF)  <scope> for socalled direct translation models instead of those proposed in </scope>  (TREF) . 
<scope> Simultaneously ,  mounting efforts have been directed towards SMT models employing linguistic syntax on the </scope>  source side (GREF) ,   <scope> target side </scope>  (GTREF) or both (GREF) . 
<scope> Since conceptual structures are used for example in text generation </scope>  (TREF) or knowledge-based machine translation (REF) ,   <scope> typed feature structures provide an attractive alternative to current procedural implementations .  </scope>
<scope> Since it loosely links the two sentences syntactic structures ,  QG is well suited for problems like word alignment for MT </scope>  (TREF) and question answering (REF) . 
<scope> Since neither </scope>  REF nor TREF  <scope> present their feature vectors in sufficient detail to perform the calculations ,  </scope>  I adopted a fairly standard set of 17 binary features from REF . 
<scope> Since paraphrases capture the variations of linguistic expressions while preserving the meaning ,  they are very useful in many applications ,  such as  </scope> machine translation (REF) ,   <scope> document summarization </scope>  (TREF) ,  and recognizing textual entailment (RTE) (REF) . 
<scope> Since part of the chunking errors could be caused by POS errors ,  we also compared the same baseNP chunker on the santo corpus tagged with i) the Brill tagger as used in </scope>  [TREF] ,  ii) the Memory-Based Tagger (MBT) as described in [REF] . 
<scope> Since (REF) </scope>  ,  who used a manually prepared set of initial lexical patterns in order to acquire relationships ,   <scope> numerous pattern-based methods have been proposed for the discovery of concepts from seeds </scope>  (GTREF) . 
<scope> Since Soon </scope>  (REF) <scope> started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving co-reference resolution problem ,  many machine learning-based systems have been built ,  using both supervised and ,  unsupervised learning methods </scope>  (TREF) . 
<scope> Since the BNC has been tagged automatically ,  using the CLAWS4 automatic tagger </scope>  (TREF) and the Template Tagger (REF) ,   <scope> the experiments in this article are artificial in the sense that they do not learn real part-of-speech tags but rather partof-speech tags as they are assigned by the automatic taggers .  </scope>
<scope> Since the success of phrase-based methods </scope>  (GTREF) ,  models based on formal syntax (REF) or linguistic syntax (GREF) have also achieved state-of-the-art performance . 
<scope> Since we aimed to measure the performance of coreference ,  the metrics used for evaluation are </scope>  the ECMF (REF) and  <scope> the MUC P ,  R and F scores </scope>  (TREF) . 
<scope> Since we aimed to measure the performance of coreference ,  the metrics used for evaluation are the ECMF </scope>  (TREF) and the MUC P ,  R and F scores (REF) . 
<scope> SMT decoders such as Moses </scope>  (REF)  <scope> may store the translation model in an efficient on-disk data structure </scope>  (TREF) ,  leaving almost the entire working memory for LM storage . 
<scope> So far ,  the annotation of discourse structure of documents has been applied primarily to identifying topical segments </scope>  (TREF) ,  inter-sentential relations (GREF) ,  and hierarchical analyses of small corpora (GREF) . 
<scope> Solutions to improve these results include modifying the first-stage grammar by annotating the category labels with local syntactic features as suggested in  </scope> (REF) and (TREF) <scope> as well as incorporating some level of lexicalization .  </scope>
<scope> Solvers have been proposed by </scope>  TREF ,  by REF using a hypergraph representation ,  and by Eisner et al . 
<scope> Some approaches have </scope>  used syntax at the core (GREF) while  <scope> others have integrated syntax into existing phrase-based frameworks </scope>  (GTREF) . 
<scope> Some approaches have used syntax at the core </scope>  (GTREF) while others have integrated syntax into existing phrase-based frameworks (GREF) . 
<scope> Some approaches have used WordNet for the generalization step </scope>  (GTREF) ,  others EM-based clustering (REF) . 
<scope> Some approaches simply report that a relation exists between two proteins but do not determine which relation holds  </scope> (GTREF) ,  while most others start with a list of interaction verbs and label only those sentences that contain these trigger words (GREF) . 
<scope> Some examples include </scope>  text categorization (REF) ,  base noun phrase chunking (REF) ,  part-of-speech tagging (REF) ,   <scope> spelling confusion set disambiguation </scope>  (TREF) ,  and word sense disambiguation (REF) . 
<scope> Some of them are based on </scope>  centering theory (GREF) ,  others on  <scope> Machine Learning </scope>  (GTREF) . 
<scope> Some of them assume that important sentences are located </scope>  at the beginning or end of paragraphs (REF) or  <scope> at positions that can be determined through training for each particular text genre </scope>  (TREF) . 
<scope> Some of these are mutual information </scope>  (TREF) ,  distributed frequency (REF) and Latent Semantic Analysis (LSA) model (REF) . 
<scope> Some of these techniques have been successfully applied for NLP tasks </scope>  :  word sense disambiguation (GREF) ,   <scope> sentiment analysis </scope>  (TREF) ,  and statistical machine translation (REF) ,  to name but a few . 
<scope> Some of these techniques have been successfully applied for NLP tasks :  word sense disambiguation </scope>  (GTREF) ,  sentiment analysis (REF) ,  and statistical machine translation (REF) ,   <scope> to name but a few </scope>  . 
<scope> Some of these techniques have been successfully applied for NLP tasks :  word sense disambiguation </scope>  (GTREF) ,  sentiment analysis (REF) ,  and statistical machine translation (REF) ,  to name but a few . 
<scope> Some of the work is not related to discourse at all (e . g .  ,  </scope>  lexical similarities (REF) ,   <scope> morphosyntactic similarities </scope>  (TREF) and word-based measures like TFIDF (REF)) . 
<scope> Some only predict primary stress markers </scope>  (GTREF) ,  while those that predict both primary and secondary stress generally achieve lower accuracy (GREF) . 
<scope> Some other works paid much attention to the robust SRL  </scope> (TREF) and post inference (REF) . 
<scope> Some parameter values can be learned  for example ,  </scope>  dictionaries (REF) and  <scope> regular expressions </scope>  (TREF) . 
<scope> Some philosophical ,  psychological and linguistic theories of irony and sarcasm are worth referencing as a theoretical framework :  the constraints satisfaction theory </scope>  (GTREF) ,  the role playing theory (REF) ,  the echoic mention framework (REF) and the pretence framework (REF) . 
<scope> Some previous work </scope>  (GTREF)  <scope> illustrated the effectiveness of using characters as tagging units </scope>  ,  while literatures (GREF) focus on employing lexical words or subwords as tagging units . 
<scope> Some recent research used comparable corpora to </scope>  re-score name transliterations (GREF) or <scope> mine new word translations  </scope> (GTREF) . 
<scope> Some researchers extract synonyms as paraphrases </scope>  (TREF) ,  while some others use looser definitions ,  such as hypernyms and holonyms (REF) . 
<scope> Some statistical NLG research has looked at subproblems of language generation ,  such as </scope>  ordering of NP premodifiers [GREF] ,  attribute selection in content planning [REF] ,  NP type determination [REF] ,  pronominalisation [REF] ,  and  <scope> lexical choice </scope>  [TREF] . 
<scope> Some summarization systems assume that the importance of a sentence is derivable from a rhetorical representation of the source text  </scope> (TREF) ,  while others leverage information from multiple texts to re-score the importance of conceptual units across all the sources (REF) . 
<scope> Some used (dependency) parsing to feed the models </scope>  (GREF) ;   <scope> the others utilized only part of speech information ,  e . g .  ,  </scope>  TREF . 
<scope> Specifically </scope>  ,  in (REF) the largest dataset contains texts from five authors ,   <scope> in </scope>  (TREF)  <scope> from three </scope>  ,  while in (REF) and (REF) from ten . 
<scope> Specifically ,  three linguistic realization packages ,  FUF/SURGE ( </scope> GTREF) ,  PENMAN/NIGEL (Penman group 1989) ,  and its descendant KPML/NIGEL (REF) ,  <scope> are widely used in the field .  </scope>
<scope> Specifically ,  we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of </scope>  GTREF(though not more recent models ,  such as REF or REF) . 
<scope> Specifically ,  we use adaptor grammars </scope>  (REF) ,   <scope> a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes </scope>  (TREF) ,  grammar induction (REF) ,  and named entity structure learning (REF) ,   <scope> to make supervised nave Bayes classification nonparametric in order to improve perspective modeling </scope>  . 
<scope> Specifically ,  we view stacked learning as a way of approximating non-local features in a linear model ,  rather than making empirically dubious independence </scope>  (TREF) or structural assumptions (e . g .  ,  projectivity ,  REF) ,  using search approximations (GREF) ,  solving a (generally NP-hard) integer linear program (REF) ,  or adding latent variables (REF) . 
<scope> Specifically ,  we view stacked learning as a way of approximating non-local features in a linear model ,  rather than </scope>  making empirically dubious independence (REF) or structural assumptions (e . g .  ,  projectivity ,  REF) ,  <scope> using search approximations </scope>  (GTREF) ,  solving a (generally NP-hard) integer linear program (REF) ,  or adding latent variables (REF) . 
<scope> Stable averages of human quality judgments ,  let alone high levels of agreement ,  are hard to achieve ,  as has been observed for </scope>  MT (GREF) ,  text summarization (REF) ,  and  <scope> NLG </scope>  (TREF) . 
<scope> State-ofthe-art methods for edit region detection such as  </scope> (GTREF)  <scope> model speech disfluencies as a noisy channel model </scope>  ,  though direct classification models have also shown promise (GREF) . 
<scope> State-of-the-art word alignment models ,  such as  </scope> IBM Models (REF) ,  HMM (REF) ,  and  <scope> the jointly-trained symmetric HMM </scope>  (TREF) ,   <scope> contain a large number of parameters (e . g .  ,  word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables .  </scope>
<scope> Statistical disambiguation such as </scope>  (REF) for PP-attachment or (GTREF)  <scope> for generative parsing greatly improve disambiguation ,  but as they model by imitation instead of by understanding ,  complete soundness has to remain elusive </scope>  . 
<scope> Statistical disambiguation such as </scope>  (TREF)  <scope> for PP-attachment </scope>  or (GREF) for generative parsing  <scope> greatly improve disambiguation ,  but as they model by imitation instead of by understanding ,  complete soundness has to remain elusive </scope>  . 
<scope> Statistical machine translation has been applied to a smorgasbord of NLP problems </scope>  ,  including question answering (REF) ,   <scope> semantic parsing and generation </scope>  (GTREF) ,  summarization (Daume III and REF) ,  generating bid-phrases in online advertising (REF) ,  spelling correction (REF) ,  paraphrase (GREF) and query expansion (REF) . 
<scope> Statistical machine translation has been applied to </scope>  a smorgasbord of NLP problems ,  including  <scope> question answering </scope>  (TREF) ,  semantic parsing and generation (GREF) ,  summarization (Daume III and REF) ,  generating bid-phrases in online advertising (REF) ,  spelling correction (REF) ,  paraphrase (GREF) and query expansion (REF) . 
<scope> Statistical methods have </scope>  proven their value in automatic speech recognition (REF) and have  <scope> recently been applied to </scope>  lexicography (REF) and to  <scope> natural language processing </scope>  (GTREF) . 
<scope> Statistical parsers have been developed for TAG </scope>  (GTREF) ,  LFG (GREF) ,  and HPSG (GREFMalouf and van REF) ,  among others . 
<scope> Stevenson and Gaizauskas </scope>  (TREF) and Goto and Renals (REF<input type="hidden" value=%2.
<scope> Still ,  it is in our next plans and part of our future work to embed in our model some of the interesting WSD approaches ,  like </scope>  knowledgebased (GREF) ,  <scope> corpus-based  </scope> (GTREF) ,  or combinations with very high accuracy (REF) . 
<scope> STSGs form a restricted subclass of Stochastic Tree Adjoining Grammars (henceforth ,  STAGs) </scope>  (GTREF) <scope>  ,  the difference being that STSGs only allow for substitution and not for adjunction </scope>  (REF) . 
<scope> Subsequent work has focused on designing better features and testing different classifiers ,  including </scope>  memory-based learning (REF) ,   <scope> decision tree learning </scope>  (GTREF) ,  and logistic regression (GREF) . 
<scope> Such a database can also be readily exploited for machine identification of cognates and recurrent sound correspondences to test algorithms for language family reconstruction </scope>  (GTREF) or to assist in the automatic identification of phonemic systems and ,  thereby ,  enhance relevant existing work (REF) . 
<scope> Such approaches include the use of PoS dictionaries by </scope>  sequential tagging models (GREF) ,  the use of labeled data from different languages (GREF) or  <scope> the (possibly indirect) assignment of labels to topics </scope>  (GTREF) . 
<scope> Such approach is in linewithcurrentresearchontheuseoftreekernels for natural language learning ,  e . g .  syntactic parsing re-ranking </scope>  (REF) ,  relation extraction (REF) and named entity recognition (GTREF) . 
<scope> Such coarse-grained inventories can be produced manually </scope>  from scratch (REF) or  <scope> by </scope>  automatically relating (REF) or  <scope> clustering </scope>  (GTREF)  <scope> existing word senses </scope>  . 
<scope> Such indicative phrases have been successfully applied in various tasks such as synonym extraction ,  hyponym extraction </scope>  (TREF) and fact extraction (REF) . 
<scope> Such methods involve many different sequence labeling models including  </scope> HMMs (REF) ,   <scope> maximum entropy (Maxent) models  </scope> (TREF) ,  and CRFs (REF) . 
<scope> Such models have been used as generative solutions to several other segmentation problems ,  ranging from word segmentation  </scope> (TREF) ,  to parsing (GREF) and machine translation (GREF) . 
<scope> Such techniques </scope>  either  <scope> do not use any information regarding the linguistic properties of MWEs </scope>  (TREF) ,  or mainly focus on their noncompositionality (REF) . 
<scope> Supertagging has since been effectively applied </scope>  to other formalisms ,  such as HPSG (GREF) ,  and  <scope> as an information source for tasks such as Statistical Machine Translation </scope>  (TREF) . 
<scope> Supertagging </scope>  was originally developed for Lexicalized Tree Adjoining Grammar (REF) ,  but  <scope> has been particularly successful for wide-coverage CCG parsing </scope>  (TREF) . 
<scope> Supertagging was first proposed for Lexicalized Tree Adjoining Grammar (LTAG) </scope>  (REF) ,   <scope> and then successfully applied to </scope>  Combinatory Categorial Grammar (CCG) (REF) and  <scope> Head-driven Phrase Structure Grammar (HPSG) </scope>  (TREF) . 
<scope> Supertagging was first proposed for Lexicalized Tree Adjoining Grammar (LTAG) </scope>  (TREF) ,  and then successfully applied to Combinatory Categorial Grammar (CCG) (REF) and Head-driven Phrase Structure Grammar (HPSG) (REF) . 
<scope> Supervised approaches  </scope> which make use of a small hand-labeled training set (GREF) typically  <scope> outperform unsupervised approaches </scope>  (GTREF) ,   <scope> but tend to be tuned to a speci c corpus and are constrained by scarcity of labeled data .  </scope>
<scope> Supervised approaches to the problem of keyphrase extraction include </scope>  the Naive Bayes-based KEA algorithms (REF) (REF) ,  decision tree-based and the genetic algorithm-based GenEx (REF) ,  and <scope> the probabilistic KL divergence-based language model  </scope> (TREF) . 
<scope> Supervised methods for taxonomy induction provide training instances with global semantic information about concepts  </scope> (TREF) and use bootstrapping to induce new seeds to extract further patterns (REF) . 
<scope> Supervised training methods already applied to PP attachment range from stochastic maximum likelihood </scope>  (TREF) or maximum entropy models (REF) to the induction of transformation rules (REF) ,  decision trees (REF) and connectionist models (REF) . 
<scope> Syntactic complexity is an obvious factor :  indee </scope> d (TREF) and (REF) also used syntactic features ,  such as parse tree height or the number of passive sentences ,  to predict reading grade levels . 
<scope> Systems based on perceptron have been shown to be competitive in NER and text chunking  </scope> (GTREF) We specify the model and the features with the LBJ (REF) modeling language . 
<scope> Systems for automatically determining the degree of semantic relatedness between two terms have traditionally either used a measurement based on </scope>  the distance between the terms within WordNet (GREF) ,  or  <scope> used co-occurrence statistics from a large corpus </scope>  (GTREF) . 
<scope> Systems such as the SRI Core Language Engine </scope>  (GTREF) ,  LUNAR (REF) ,  and TEAM (REF)  <scope> have employed scope critics that use heuristics to decide between alternative scopings .  </scope>
<scope> Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for </scope>  U-DOP reported in REF ,   <scope> the CCM model in </scope>  TREF ,  the DMV dependency model in REF and their combined model DMV + CCM . 
<scope> Table 1 shows the results of U-DOP in terms of UP ,  UR and F1 compared to the results of </scope>  the CCM model by REF ,   <scope> the DMV dependency learning model by </scope>  TREF  <scope> together with their combined model DMV + CCM .  </scope>
<scope> Table 1 shows the results of U-DOP in terms of UP ,  UR and F1 compared to the results of the CCM model by  </scope> TREF ,  the DMV dependency learning model by REF together with their combined model DMV + CCM . 
<scope> Table 3 lists the automatic metric scores for the newstest2010 test set ,  according to the BLEU </scope>  (TREF) and TER (REF)  <scope> metrics .  </scope>
<scope> Table 4 compares our baseline against </scope>  the state-of-the-art graph-based (REF) and  <scope> transition-based </scope>  (TREF) <scope> approaches ,  and confirms that our system performs at the same level with those stateof-the-art ,  and runs extremely fast in the deterministic mode (k=1) ,  and still quite fast in the beamsearch mode (k=16) .  </scope>
<scope> Table 4 compares our baseline against the state-of-the-art graph-based </scope>  (TREF) and transition-based (REF) approaches ,   <scope> and confirms that our system performs at the same level with those stateof-the-art ,  and runs extremely fast in the deterministic mode (k=1) ,  and still quite fast in the beamsearch mode </scope>  (k=16) . 
<scope> Table 5 shows results using the PARSEVAL measures ,  as well as results using the slightly more forgiving measures of </scope>  (REF) and (TREF) . 
<scope> Table 5 shows that the maximum entropy parser performs better than the parsers presented in </scope>  (REF) and (TREF) ~ ,   <scope> which have the best previously published parsing accuracies on the Wall St .  Journal domain </scope>  . 
<scope> Table 5 shows that the maximum entropy parser performs better than the parsers presented in  </scope> (TREF) and (REF) ~ ,  which have the best previously published parsing accuracies on the Wall St .  Journal domain . 
<scope> Table 6 shows 3An exception is </scope>  REF <scope>  ,  who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus </scope>  (TREF)  <scope> for testing .  </scope>
<scope> Taken together with </scope>  cube pruning (REF) ,   <scope> k-best tree extraction </scope>  (TREF) ,  and cube growing (Huang and REF) ,   <scope> these results provide evidence that lazy techniques may penetrate deeper yet into MT decoding and other NLP search problems .  </scope>
<scope> taxonomy files created from Webster's Seventh </scope>  /REF/  <scope> using techniques reported in  </scope> TREF ,  3 . 
<scope> Techniques developed for RTE have now been successfully applied in the domains of Question Answering </scope>  (TREF) and Machine Translation (REF) ,  (REF) . 
<scope> TE has been successfully applied to a variety of natural language processing applications ,  including information extraction </scope>  (TREF) and question answering (REF) . 
<scope> Temporal analysis has also been applied in </scope>  Question-Answering systems (GREF) ,  email classification (REF) ,  aiding the precision of Information Retrieval results (REF) ,  document summarisation (REF) ,   <scope> time stamping of event clauses </scope>  (TREF) ,  temporal ordering of events (REF) and temporal reasoning from text (GREF) . 
<scope> Temporal interpretation encompasses levels ranging from the syntactic to the lexico-semantic </scope>  (GTREF) and includes the characterization of temporal discourse in terms of rhetorical structure and pragmatic relations (GREF) . 
<scope> Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments </scope>  (TREF) and also discourse markers to identify discourse structure (REF) . 
<scope> Text excerpts are usually extracted through string matching </scope>  (GTREF) ,  sentence clustering (REF) ,  or through topic models (GREF) . 
<scope> Text simplification has also been shown to improve the performance of other natural language processing applications including semantic role labeling </scope>  (TREF) and relation extraction (REF) . 
<scope> That is ,  we attempt to make maximum use of surface information in performing a deep semantic task ,  in the same vein ,  e . g .  </scope>  ,  as REF for English verb classification and TREF  <scope> in disambiguating nominalisations .  </scope>
<scope> The acquisition of clues is a key technology in these research efforts ,  as seen in learning methods </scope>  for document-level SA (GREF) and for  <scope> phraselevel SA </scope>  (GTREF) . 
<scope> The algorithm does not include a mechanism for handling global focus </scope>  (TREF) ,  for centering within a discourse segment (GREF) ,  or for performing tense and aspect interpretation . 
<scope> The algorithm for the efficient evaluation of  for the syntactic tree kernel (STK) has been widely discussed in  </scope> (TREF) whereas its fast evaluation is proposed in (REF) ,  so we only describe the equations of the partial tree kernel (PTK) . 
<scope> The algorithm was also evaluated on </scope>  two other data sets ,   <scope> SENSEVAL-3 English all-words data </scope>  (TREF) and a subset of SemCor (REF) ,   <scope> although only fine-grained sense evaluations could be conducted on these test sets .  </scope>
<scope> The algorithm was reimplemented as soon as digital WordNet and Rogets became available  </scope> (TREF) and its complexity was improved (GREF) . 
<scope> The application of the method to a mid-size UBG of English ,  and largesize HPSGs of English and Japanese is described in </scope>  (REF) and (TREF) . 
<scope> The applications range from simple classification tasks such as text classification and history-based tagging </scope>  (TREF) to more complex structured prediction tasks such as partof-speech (POS) tagging (REF) ,  syntactic parsing (REF) and semantic role labeling (REF) . 
<scope> The approaches can be grouped into  </scope> corpus-based approaches (GREF) and  <scope> dictionary-based approaches </scope>  (GTREF) . 
<scope> The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including </scope>  automatic speech recognition (REF) ,  machine translation (GREF) ,  bilingual word alignment (REF) ,   <scope> andparsing </scope> (GTREF) . 
<scope> The approach is related ,  but not identical ,  to distributional similarity (for details ,  see </scope>  (TREF) and (REF)) . 
<scope> The approach of the top system </scope>  (TREF)  <scope> was to fit the model to minimize cost over sentences </scope>  ,  while the secondbest system (REF) trained the model to maximize performance over individual decisions in an incremental algorithm . 
<scope> The approach proposed by </scope>  TREF  <scope> is really only an extension of the full additive model of REF ,  the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression .  </scope>
<scope> The articles </scope>  (REF) ,  (TREF) and (REF)  <scope> are introducing algorithms for extracting parallel sentences and sub-sententional fragments from comparable corpora and using the automatically extracted parallel data for improving statistical machine translation algorithms performance .  </scope>
<scope> The availability of annotated corpora </scope>  like PropBank and FrameNet (REF)  <scope> have provided rapid development of research into SRL  </scope> (GTREF) . 
<scope> The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations ,  originally in parsing </scope>  (GTREF) ,  has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (GREF) . 
<scope> The baselines include </scope>  right attachment (where each word is attached to the word to its right) ,   <scope> MLE via EM </scope>  (TREF) ,  and empirical Bayes with Dirichlet and LN priors (REF) . 
<scope> The basic idea of using synchronous TAG for machine translation dates from the original definition </scope>  (TREF) ,  and has been pursued by several researchers (GREF) ,  but only recently in its probabilistic form (REF) . 
<scope> The basic idea of using synchronous TAG for machine translation </scope>  dates from the original definition (REF) ,  and  <scope> has been pursued by several researchers </scope>  (GTREF) ,  but only recently in its probabilistic form (REF) . 
<scope> The B&C scheme is similar to the original DepBank scheme </scope>  (REF) ,   <scope> but overall contains less grammatical detail ;   </scope> TREF  <scope> describes the differences .  </scope>
<scope> The beam search algorithm attempts to find the translation (i . e .   ,  hypothesis that covers all source words) with the minimum cost as in </scope>  (TREF) and (REF)  .  The distortion cost is added to the log-linear mixture of the hypothesis extension in a fashion similar to the language model cost . 
<scope> The benefit of semantic roles has already been demonstrated for a number of tasks ,  among others for </scope>  machine translation (REF) ,   <scope> information extraction </scope>  (TREF) ,  and question answering (REF) . 
<scope> The Berkeley parser </scope>  (TREF) <scope> is a latent-variable PCFG parser </scope>  ,  MSTParser (REF) is a graph-based dependency parser ,  and MaltParser (REF) is a transition-based dependency parser . 
<scope> The bestreported F1 constituency scores from this work for each language are 79 . 9% (Chinese </scope>  (TREF)) ,  81 . 0% (French (REF) ,  76 . 2% (German (REF)) ,  and 73 . 8% (Spanish (REF)) . 
<scope> The best results have been achieved using Support-Vector Machines placing the MaltParser very high in both the CoNNL shared tasks on dependency parsing in 2006 and 2007 </scope>  (GTREF) and it has been shown that SVMs are better for the task than Memory-based learning (REF) . 
<scope> The biomedical dataset of  </scope> (TREF)  <scope> has been annotated according to the version of AZ  </scope> developed for biology papers (REF) (with only minor modifications concerning zone names) . 
<scope> The BLEU metric </scope>  (TREF) and the closely related NIST metric (REF) along with WER and PER 48 <scope> have been widely used by many machine translation researchers .  </scope>
<scope> The chunks and their relations in the texts were analyzed by cabocha </scope>  (TREF) ,  and named entities were analyzed by the method of (REF) . 
<scope> The classification steps of most approaches vary in the choice of the classifier (e . g .  decision tree classifiers </scope>  (TREF) ,  maximum entropy classification (REF) ,  SVM classifiers (REF)) and the number of features used (Soon et al . 
<scope> The class labeling system in our experiment is IOB2 </scope>  (REF) ,   <scope> which is a variation of IOB </scope>  (TREF) . 
<scope> The combined training corpus from which we extracted our grammar consisted of 123 , 609 sentence pairs ,  which was then filtered for length and aligned using the GIZA +  +  implementation of IBM Model 4 </scope>  (TREF)  <scope> to obtain one-to-many alignments in either direction  </scope> and symmetrized using the grow-diag-final-and method (REF) . 
<scope> The complexities of 15 restricted alignment problems in two very different synchronous grammar formalisms of syntax-based machine translation ,  inversion transduction grammars (ITGs) </scope>  (TREF) and a restricted form of range concatenation grammars ((2 , 2)-BRCGs) (REF) ,  <scope> are investigated .  </scope>
<scope> The complexity of the probabilistic models needed to explain the hidden correspondence among words has necessitated the development of highly non-convex and difficult to optimize models ,  such as </scope>  HMMs (REF) and <scope> IBM Models 3 and higher </scope>  (TREF) . 
<scope> The component models were also used in other Senseval-3 tasks :  Semantic Role Labeling </scope>  (TREF) and the lexical sample tasks for Chinese and English ,  as well as the Multilingual task (REF) . 
<scope> The compound splitting procedure mainly follows the approach from </scope>  (REF) and (TREF) ,  so the emphasis is put on finding correct translations for compounds . 
<scope> The conclusions we draw can also be applied to more efficient parsers </scope>  (GTREF)  <scope> that produce other logical representations </scope>  (e . g .   ,  GREF) <scope> in more compact forests </scope>  (REF) . 
<scope> The conditional maximum entropy model in our implementation is based </scope>  on the one described in Section 2 . 5 in (REF) ,  and  <scope> features are the same as those described in </scope>  (TREF) . 
<scope> The conditional random fields (CRF) (REF) model has shown great benefits in similar applications of natural language processing such as </scope>  part-of-speech tagging ,  <scope> noun phrase chunking </scope>  (TREF) ,  morphology disambiguation(REF) . 
<scope> The constraints may be expressed </scope>  by a set of elimination rules applied in a sequence (REF) or  <scope> by a set of restrictions applied in parallel </scope>  (TREF) . 
<scope> The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger </scope>  (GTREF) ,  and parsed with MaltParser (REF) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank4 ,  so that dependency triples could be extracted . 
<scope> The corpus is parsed with the highly accurate CDG parser </scope>  (TREF) and has the same dependency format as TuBa-D/Z (REF) . 
<scope> The corpus (see Section 4) is distributed with annotations for sentence breaks ,  tokenization ,  and part of speech information automatically generated by the GATE toolkit  </scope> (TREF) . 8 For parsing we use the REF parser . 9 For partial parses ,  we employ CASS (REF) . 
<scope> The CRF tagger was implemented in MALLET </scope>  (REF)  <scope> using the original feature templates from </scope>  (TREF) . 
<scope> The current state-of-the-art segmentation software developed by  </scope> (TREF) ,   <scope> which ranks as the best in the SIGHAN bakeo </scope>  (REF) ,   <scope> attains word precision and recall of 96 . 9% and 96 . 8% ,  respectively ,  on the PKU track .  </scope>
<scope> The data consist of sections of the Wall Street Journal (WSJ) part of the Penn TreeBank </scope>  (TREF) ,  with information on predicate-argument structures extracted from the PropBank corpus (REF) . 
<scope> The data set consisting of 249 , 994 TFSs was generated by parsing the Figure 3 :  The size of Dpi ;  for the size of the data set 800 bracketed sentences in the Wall Street Journal corpus (the first 800 sentences in Wall Street Journal 00) in the Penn Treebank </scope>  (TREF) with the XHPSG grammar (REF) . 
<scope> The datasets are </scope>  the CoNLL-X shared task data for Czech and German (REF) , 5  <scope> the Penn Treebank for English </scope>  (TREF) ,  and the REF shared task data for Italian (REF) . 
<scope> The data structure used to represent the disentangled discourse </scope>  varies from a simple connected sub-graph (REF) ,  to  <scope> a stack/tree </scope>  (GTREF) ,  to a full directed acyclic graph (DAG :  Rose et al . 
<scope> The data structure used to represent the disentangled discourse varies from a simple connected sub-graph </scope>  (TREF) ,  to a stack/tree (GREF) ,  to a full directed acyclic graph (DAG :  Rose et al . 
<scope> The detection process detects possible occurrences of unknown words </scope>  (REF) ,   <scope> so that deeper morphological analysis is carried out only at the places where unknown word morphemes were detected </scope>  (TREF) . 
<scope> The disambiguation rules are similar to phonological rewrite rules </scope>  (TREF) ,  and the parsing algorithm is similar to the algorithm for combining the morphological rules with the lexicon (REF) . 
<scope> The discourse structure hierarchy has been shown to be useful for other tasks </scope>  :  understanding specific lexical and prosodic phenomena (GREF) ,  natural language generation (REF) ,   <scope> predictive/generative models of postural shifts  </scope> (TREF) ,  and essay scoring (REF) . 
<scope> The distributional hypothesis addresses the problem of modeling word similarity </scope>  (GTREF) ,  and can be extended to selectional preference (REF) and lexical substitution (REF) as well . 
<scope> The efficacy of this approach has been well-established in many areas ,  including </scope>  automated evaluation of machine translation systems (REF) ,  text summarization (REF) ,  question answering (REF) ,   <scope> document retrieval  </scope> (TREF) ,  and many others . 
<scope> The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker </scope>  (TREF) ,  and the German data was labelled with the decision-tree-based TreeTagger (REF) . 
<scope> The English (physician) side recogniser </scope>  is compiled from the large English resource grammar described in Chapter 9 of (REF) ,  and  <scope> was constructed in the same way as the one described in </scope>  (TREF) <scope>  ,  which was used for a headache examination task .  </scope>
<scope> The English side of the parallel data is parsed by our implementation of the Berkeley parser  </scope> (TREF) trained on the combination of Broadcast News treebank from Ontonotes (REF) and a speechified version of the WSJ treebank (REF) to achieve higher parsing accuracy (REF) . 
<scope> The estimation of translation model parameters usually relies heavily on word-aligned corpora </scope>  ,  not only for phrase-based and hierarchical phrase-based models (GREF) ,   <scope> but also for syntax-based models </scope>  (GTREF) . 
<scope> The experiments are based on the Rondane corpus ,  a Redwoods </scope>  (TREF)  <scope> style corpus which is distributed with the English Resource Grammar </scope>  (REF) . 
<scope> The experiments conducted on MUC and ACE data indicate state-of-the-art results when compared with the methods reported in </scope>  (REF) and (TREF) . 
<scope> The extraction procedure utilizes a head percolation table as introduced by  </scope> TREF  <scope> in combination with a variation of </scope>  REF <scope> approach to the differentiation between complement and adjunct .  </scope>
<scope> The extraction procedure utilizes a head percolation table </scope>  as introduced by REF  <scope> in combination with a variation of  </scope> TREF  <scope> approach to the differentiation between complement and adjunct .  </scope>
<scope> The factorization problem we address is more closely related to work on factorizing synchronous context-free grammars (CFGs) </scope>  (GTREF) and on factorizing synchronous TAGs (REF) . 
<scope> The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks ;  recent examples include </scope>  dependency parsing (REF) ,   <scope> parse re-ranking </scope>  (TREF) ,  pronoun resolution (REF) ,  and semantic role labeling (REF) . 
<scope> The features used are :  the length of t ;  a single-parameter distortion penalty on phrase reordering in a ,  as described in </scope>  (TREF) ;  phrase translation model probabilities ;  and 4-gram language model probabilities logp(t) ,  using Kneser-Ney smoothing as implemented in the SRILM toolkit (REF) . 
<scope> The final alignments ,  in both the baseline and the feature-enhanced models ,  are computed by training the generative models in both directions </scope>  ,  combining the result with hard union competitive thresholding (REF) ,  and  <scope> using agreement training for the HMM </scope>  (TREF) . 
<scope> The first five lines of Table 2 report such measures for the five best semantic role labelling systems </scope>  (GTREF) according to (REF) . 
<scope> The first is usually focus on exploiting automatic generated labeled data from the unlabeled data  </scope> (GTREF) ,  the second is on combining supervised and unsupervised methods ,  and only unlabeled data are considered (GREF) . 
<scope> The first one (Phr) is a phrasal system (REF) based on Bracketing Transduction Grammar </scope>  (TREF) <scope> with a lexicalized reordering component based on maximum entropy model .  </scope>
<scope> The first three components are responsible for generating SCF cues from the training corpora and the last component ,  consisting of the Binomial Hypothesis Test </scope>  (TREF) and a back-off algorithm (REF) ,   <scope> is used to filter SCF cues on the basis of their reliability and likelihood .  </scope>
<scope> The following section reviews stochastic uni cation grammars </scope>  (TREF) and the statistical quantities required for ef ciently estimating such grammars from parsed training data (REF) . 
<scope> The formalism we use is </scope>  Combinatory Categorial Grammar (REF) ,  together  <scope> with a parsing model described in </scope>  TREF  <scope> which we adapt for use with partial data .  </scope>
<scope> The general chart mining technique can easily be adapted to learn other challenging linguistic phenomena ,  such as the countability of nouns </scope>  (TREF) ,  subcategorization properties of verbs or nouns (REF) ,  and general multiword expression (MWE) extraction (REF) . 
<scope> The head rules created by REF have been used in almost all recent work on statistical dependency parsing of English  </scope> (GTREF) . 
<scope> The help messages are based on the TargetedHelp approach successfully used in spoken dialogue  </scope> (TREF) ,  together with the error classification we developed for tutorial dialogue (REF) . 
<scope> The heuristic method is inconsistent in the limi </scope> t (REF)  <scope> while EM is degenerate ,  placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible </scope>  (TREF) . 
<scope> The Hindi dependency treebank </scope>  (TREF) used for the experiment was released as part of the ICON09 dependency parsing tools contest (REF) . 
<scope> The human language technology (HLT) society in Europe has been particularly zealous for the standardization ,  making a series of attempts </scope>  such as EAGLES 3  ,  PROLE/SIMPLE (REF) ,  ISLE/MILE (GREF)  <scope> and more recently multilingual lexical database generation from parallel texts in 20 European languages </scope>  (TREF) . 
<scope> The idea of a linguistic hierarchy is not novel ,  having roots in  </scope> both linguistics [GREF] and Artificial Intelligence [TREF] . 
<scope> The idea of topic signature terms  </scope> was introduced by Lin and Hovy (REF) in the context of single document summarization ,  and  <scope> was later used in several multi-document summarization systems </scope>  (GTREF) . 
<scope> The idea of topic signature terms was introduced by Lin and Hovy </scope>  (TREF)  <scope> in the context of single document summarization </scope>  ,  and was later used in several multi-document summarization systems (GREF) . 
<scope> The idea of using synchronous TAG in machine translation has been pursued by several researchers </scope>  (GTREF) ,  but only recently in its probabilistic form (GREF) . 
<scope> The identification of metonymy becomes important for NLP tasks such as question answering </scope>  (TREF) or geographic information retrieval (REF) . 
<scope> The implementation has been inspired by </scope>  experience in extracting information from very large corpora (REF) and  <scope> performing experiments on maximum entropy sequence tagging </scope>  (GTREF) . 
<scope> The implementation includes path-length </scope>  (GTREF) ,  information-content (GREF) and text-overlap (GREF) measures ,  as described in REF . 
<scope> The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e . g .  parallel corpora or untagged monolingual corpora in two languages) </scope>  (GTREF) ,  or sense-tagged seed examples (REF) . 
<scope> The interdigitation is handled using a compile-replace process using the replace operator </scope>  (REF) (TREF) . 
<scope> The issue is that the most compact finitestate representations of these </scope>  (TREF) <scope>  ,  which exploit backoff structure ,  are not purely m-gram for any m .  They yield more compact hypergraphs </scope>  (REF) <scope>  ,  but unfortunately those hypergraphs might not be treatable by Fig .  </scope>
<scope> The Joshua 1 . 0 release also included re-implementations of </scope>  suffix array grammar extraction (GREF) and  <scope> minimum error rate training </scope>  (GTREF) . 
<scope> The key component here is the error model ,  which </scope>  should not only  <scope> capture </scope>  orthographic similarities (REF) ,   <scope> but also phonetic similarities  </scope> (TREF) . 
<scope> The labeled precision/recall results of our model are much worse than those reported in  </scope> (TREF) and (REF) . 
<scope> The last two approaches ,  hereafter referred to as ZHN and BBN ,  replace the BLEU objective function ,  with the usual BLEU score on expected n-gram counts </scope>  (TREF) and with an expected BLEU score for normal n-gram counts (REF) ,  respectively . 
<scope> The latter is currently dominating in NER amongst which the most popular methods are decision tree </scope>  (GREF) ,   <scope> Hidden Markov Model </scope>  (GREF) ,   <scope> maximum entropy </scope>  (GTREF) ,  and support vector machines (GREF) . 
<scope> The latter is currently dominating in NER amongst which the most popular methods are </scope>  decision tree (GREF) ,  Hidden Markov Model (GREF) ,  maximum entropy (GREF) ,  and  <scope> support vector machines </scope>  (GTREF) . 
<scope> The latter takes advantage of mostly handcrafted information ,  such as dictionaries </scope>  (GTREF) or thesauri (REF) . 
<scope> The lexical acquisition phase uses the GIZA +  +  word-alignment tool ,  an implementation </scope>  (REF)  <scope> of IBM Model 5 </scope>  (TREF)  <scope> to construct an alignment of MRs with NL strings .  </scope>
<scope> The lexical distribution of grammatical knowledge one finds in many lexiealized grammar formalisms (e . g .   ,  LTAGS </scope>  (TREF) or HPSG (REF))  <scope> is still constrained to declarative notions .  </scope>
<scope> The Lexicalized Tree-Adjoining Grammar (LTAG) formalism </scope>  (TREF) ,  (REF) ,  although not context-free ,  <scope> is the most well-known instance in this category </scope>  . 
<scope> The literature on hypernym extraction offers a higher variability of methods ,  from simple lexical patterns </scope>  (GTREF) to statistical and machine learning techniques (GREF) . 
<scope> The main contribution of this paper is a systematic comparison between phrase-based and syntax-based paraphrase generation using an off-the-shelf statistical machine translation (SMT) decoder </scope>  ,  namely Moses (REF)  <scope> and the word-alignment tool GIZA +  +  </scope>  (TREF) . 
<scope> The majority of graph-based parsers in the shared task were based on what </scope>  TREF  <scope> call the first-order model ,  where the score of each arc is independent of every other arc </scope>  ,  but there were also attempts at exploring higher-order models ,  either with exact inference limited to projective dependency graphs (REF) ,  or with approximate inference (REF) . 
<scope> The majority of previous semi-supervised approaches to IE have been evaluated over preliminary tasks such as the identification of event participants  </scope> (TREF) or sentence filtering (REF) . 
<scope> The manually compiled grammars in our experiment are also intrinsically different to grammars automatically induced from treebanks </scope>  (e . g .  that used in the Charniak parser (REF) or  <scope> the various CCG parsers </scope>  (TREF)) . 
<scope> The mapping rules proposed in </scope>  (TREF) ,  as well as those in (REF) ,   <scope> are similar to the attitude model construction rules ,  while deeper reasoning may underlie some of AclT . s t)E COL1NG-92 ,  NA/VIq ! S ,  23-28 AOt' ; r 1992 9 l 4 I'ROC .  </scope>
<scope> The marked difference in the availability of monolingual vs parallel corpora has led several researchers to develop methods for automatically learning bilingual lexicons </scope>  ,  either by using monolingual corpora (GREF) or  <scope> by exploiting the cross-language evidence of closely related bridge languages that have more resources </scope>  (TREF) . 
<scope> The merits of applying a type discipline even to untyped feature structures is considered in </scope>  TREF  <scope> from a general perspective </scope>  and in REF with special reference to phonological lexica . 
<scope> The model consists of an ensemble of four highly accurate classifiers combined by majority vote :  </scope>  a naive Bayes classifier ,  a maximum entropy model (REF) ,  a boosting model (REF) ,  and  <scope> a Kernel PCA-based model </scope>  (TREF) ,   <scope> which has the advantage of having a signficantly different bias .  </scope>
<scope> The model is a probabilistic head automaton grammar  </scope> (TREF) with a split form that renders it parseable in cubic time (REF) . 
<scope> The model parameters are trained using a discriminative learning algorithm ,  e . g .  averaged perceptron </scope>  (TREF) or MIRA (REF) . 
<scope> The monolingual features include </scope>  the firstand secondorder features presented in REF and  <scope> the parent-child-grandchild features used in </scope>  TREF . 
<scope> The more recent set of techniques includes  </scope> mult iplicative weightupdate algorithms (REF) ,  latent semantic analysis (REF) ,  transformation-based learning (REF) ,   <scope> differential grammars </scope>  (TREF) ,  decision lists (REF) ,  and a variety of Bayesian classifiers (GREF) . 
<scope> The more similar conditions reported in previous work are those experiments performed on the WSJ corpus :  </scope>  (REF) reports 3-4% error rate ,  and (TREF)  <scope> report 96 . 7% accuracy .  </scope>
<scope> The most closely related works were </scope>  (REF) and (TREF) ,   <scope> which proposed opinion frames as a representation ofdiscourse-levelassociationsondialogueandmodeled the scheme to improve opinion polarity classification .  </scope>
<scope> The most common linguistic application for default inheritance is to encode lexical generalizations </scope>  (e . g .   ,  GTREF) ,  but defaults have also been used for specification in syntactic theory (e . g .   ,  GREF) ,  and for the analysis of gapping constructions (REF) and ellipsis (REF) . 
<scope> The most notable methods are based on </scope>  Hidden Markov Models(HMM)(GREF) ,   <scope> transformation rules </scope> (GTREF) ,  and multi-layer neural networks(REF) . 
<scope> The most notable of these include </scope>  the trigram HMM tagger (REF) ,  maximum entropy tagger (REF) ,   <scope> transformation-based tagger </scope>  (TREF) ,  and cyclic dependency networks (REF) . 
<scope> The most popular strategy for capturing nonprojective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser ,  as in pseudo-projective parsing </scope>  (TREF) ,  corrective modeling (REF) ,  or approximate non-projective parsing (REF) . 
<scope> The most recent modification to this approach was the use of </scope>  distance-based ordering (REF) and  <scope> lexicalized ordering </scope>  (TREF) to allow for multiple language models ,  including non-word models such as part-of-speech improved search algorithm ,  in order to improve its speed and efficiency . 
<scope> The MT community has developed not only an extensive literature on alignment </scope>  (GTREF) ,  but also standard ,  proven alignment tools such as GIZA +  +  (REF) . 
<scope> The multi-aspect sentiment (MAS) model </scope>  (TREF) <scope>  ,  which is extended from the multi-grain latent Dirichlet allocation (MG-LDA) model </scope>  (TREF) <scope>  ,  allows sentiment text aggregation for sentiment summary of each rating aspect extracted from MG-LDA .  </scope>
<scope> The natural language applications we consider in this paper are :  (1) unsupervised part-of-speech (POS) tagging </scope>  (GTREF) ,  (2) letter substitution decipherment (GREF) ,  (3) segmentation of space-free English (REF) ,  and (4) Japanese/English phoneme alignment (GREF) . 
<scope> The new tool combines Hobbs algorithm </scope>  (REF)  <scope> and the Resolution of Anaphora Procedure (RAP) algorithm </scope>  (TREF) . 
<scope> The notation is similar to </scope>  the Lexical Functional Grammar (LFG) formalism (REF) and  <scope> PATR-II  </scope> (TREF) . 
<scope> The obtained SCFs comprise the total 163 SCF types which are originally based on the SCFs in the ANLT </scope>  (TREF) and COMLEX (REF)  <scope> dictionaries </scope>  . 
<scope> The original Grammar Matrix consisted of types defining the basic feature geometry ,  types associated with Minimal Recursion Semantics  </scope> (e . g .   ,  (TREF)) ,  types for lex203 ical and syntactic rules ,  and configuration files for the LKB grammar development environment (REF) and the PET system (REF) . 
<scope> The original publications on DATR sought to provide the language with (1) a formal theory of inference </scope>  (TREF)  <scope> and (2) a model-theoretic semantics ( </scope> TREF) . 
<scope> The parameters ,  j ,  were trained using minimum error rate training </scope>  (TREF)  <scope> to maximise the BLEU score </scope>  (REF)  <scope> on a 150 sentence development set </scope>  . 
<scope> The parser is derived from the prosodic phrase system presented in </scope>  TREF and has been implemented as the front end of a version of the Bell Laboratories text-to-speech synthesizer (REF) . 
<scope> The parse trees on the English side of the bitexts were generated using a parser </scope>  (REF)  <scope> implementing the Collins parsing models  </scope> (TREF) . 
<scope> The parsing algorithms used are the arc-eager baseline algorithm ,  the arcstandard variant of the baseline algorithm ,  and the incremental ,  non-projective parsing algorithm </scope>  first described by REF and recently used for deterministic classifier-based parsing by TREF ,  all of which are available in MaltParser . 
<scope> The patterns are those used by </scope>  REF ,   <scope> and four more introduced in our previous work </scope>  (TREF) and (REF) . 
<scope> The pipeline </scope>  extracts a Hiero-style synchronous context-free grammar (REF) ,   <scope> employs suffix-array based rule extraction </scope>  (TREF) ,  and tunes model parameters with minimum error rate training (REF) . 
<scope> The pipeline uses GIZA +  +  model 4 </scope>  (GTREF)  <scope> for pseudo-word alignment </scope>  ,  uses Moses (REF) as phrase-based decoder ,  uses the SRI Language Modeling Toolkit to train language model with modified Kneser-Ney smoothing (GREF) . 
<scope> The POS tagger uses the same contextual predicates as </scope>  REF ;  the supertagger adds contextual predicates corresponding to POS tags and bigram combinations of POS tags (TREF) . 
<scope> The Prague Dependency Treebank also contains annotation for </scope>  light verb constructions (REF) and  <scope> NomBank </scope>  (TREF)  <scope> provides the argument structure of common nouns ,  paying attention to those occurring in support verb constructionsaswell .  </scope>
<scope> The problems of  </scope> non-nominal terms (REF) ,   <scope> term variation  </scope> (TREF) ,  and relevant contexts (REF) ,   <scope> can be considered for improving the performance .  </scope>
<scope> The QG formalism has been previously applied to </scope>  parser adaptation and projection (REF) ,  paraphrase identification (REF) ,  and  <scope> question answering </scope>  (TREF) ;  however the use of QG in summarization is novel to our knowledge . 
<scope> The queries in this corpus are more complex than those in the ATIS database-query corpus used in the speech community </scope>  (REF)  <scope> which makes the GEOQUERY problem harder ,  as also shown by the results in </scope>  (TREF) . 
<scope> The range of research includes </scope>  the interpretation of combined linguistic and diagrammatic input (e . g .  REF) ,   <scope> the generation of multimedia explanations (e . g .  </scope>  TREF) ,  the integration of NLP with hypertext (REF) ,  and the combination of natural language input with pointing (e . g .  REF) ,  menus (Tennant ctal . 
<scope> The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by </scope>  both REF ,  and TREF . 
<scope> Thereafter ,  we employ the standard CTK </scope>  (REF)  <scope> to compute the similarity between two UPSTs ,  since this CTK and its variations are successfully applied in syntactic parsing ,  semantic role labeling </scope>  (TREF) and relation extraction (GREF) as well . 
<scope> There are a number of corpora annotated with discourse structure ,  including </scope>  the well-known RST Treebank (REF) ;   <scope> the Discourse GraphBank </scope>  (TREF) ;  and the Penn Discourse Treebank (REF) . 
<scope> There are currently two supertagging approaches available :  </scope>  LTAG-based (REF) and  <scope> CCG-based </scope>  (TREF) . 
<scope> There are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words </scope>  (TREF) ;  the distance between words (REF) ;  and the number of combined words and frequency of appearance (REF) . 
<scope> There are many POS taggers developed using different techniques for many major languages such as </scope>  transformation-based error-driven learning (REF) ,  decision trees (REF) ,   <scope> Markov model </scope>  (TREF) ,  maximum entropy methods (REF) etc for English . 
<scope> There are some morphological description systems showing some features in common with Humor 99 like paradigmatic morphology </scope>  (TREF) ,  or the Paradigm Description Language (REF) but they don.
<scope> There are theoretical approaches that propose formalisms to represent the structure of code-switched utterances and describe a framework for parsing and generating mixed sentences ,  for example for Marathi and English </scope>  (TREF) ,  or Hindi and English (REF) . 
<scope> The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections </scope>  (TREF)) or by developing more surface patterns (REF) . 
<scope> The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model  </scope> (TREF) or edit distance alignments allowing shifts (REF) . 
<scope> The recent research from the RTE community focused on acquiring large quantities of textual entailment pairs from news headlines  </scope> (TREF) and negative examples from sequential sentences with transitional discourse connectives (REF) . 
<scope> The recent research has been mainly promoted in the web people search task </scope>  (TREF) such as (REF) ,  ACE2008 such as (REF) and NIST TAC KBP (REF) evaluations . 
<scope> Therefore ,  a variety of techniques have been developed to both enrich and generalize the naive grammar </scope>  ,  ranging from simple tree annotation  <scope> and symbol splitting (GTREF) </scope>  to full lexicalization and intricate smoothing (GREF) . 
<scope> Therefore ,  domain adaptation methods have recently been proposed in several NLP areas </scope>  ,  e . g .  ,  word sense disambiguation (REF) ,   <scope> statistical parsing </scope>  (GTREF) ,  and lexicalized-grammar parsing (GREF) . 
<scope> Therefore gradient-free optimization algorithms ,  such as Powells method or downhill simplex </scope>  (REF) ,   <scope> are often employed in weight tuning </scope>  (TREF) . 
<scope> Therefore ,  many approaches have concentrated on different ways of estimating lexical coherence of text segments ,  such as semantic similarity between words </scope>  (TREF) ,  similarity between blocks of text (REF) ,  and adaptive language models (REF) . 
<scope> Therefore ,  research has adopted techniques previously developed 1016 to segment topics in text </scope>  (GTREF) and in read speech (e . g .   ,  broadcast news) (GREF) . 
<scope> The regulating aspects of semantic orientation of a text are natural language context information </scope>  (TREF) language properties (REF) ,  domain pragmatic knowledge (REF) and lastly most challenging is the time dimension (REF) . 
<scope> There has also been a lot of work involving bilingual corpora ,  including the IBM Candide project </scope>  (TREF) ,   <scope> which used statistical data to align words in sentence pairs from parallel corpora in an unsupervised fashion through the EM algorithm ;  </scope>  REF used character frequencies to align words in a parallel corpus ;  Smadja et al . 
<scope> There has also been extensive work on finding synonymous terms and word associations ,  as well as automatic acquisition of IS-A (or genus-head) relations from </scope>  dictionary definitions and glosses (REF) and from  <scope> free text </scope>  (GTREF) . 
<scope> There has been considerable research on methods to acquire translation lexicons </scope>  from either MRDs (GREF) or  <scope> from parallel text </scope>  (GTREF) ,  but this has generally been limited to a small number of languages . 
<scope> There has been no comparison between corpusbased approaches for anaphora resolution </scope>  and more traditional algorithms based on focusing (REF) or  <scope> centering </scope>  (TREF) except for Azzam et al . 
<scope> There has been plenty of theoretical work such as </scope>  (REF) ,  (TREF)  <scope> which shows that just as sentences can be decomposed into smaller constituents ,  a discourse can be decomposed into smaller units called discourse segments .  </scope>
<scope> There has been plenty of theoretical work such as  </scope> (TREF) ,  (REF)  <scope> which shows that just as sentences can be decomposed into smaller constituents ,  a discourse can be decomposed into smaller units called discourse segments .  </scope>
<scope> There has been various research on the extraction of taxonomic information |'io111 a corpus ,  including extraction of hyponyms by using linguistic patterns </scope>  (TREF) and extraction of synonyms based on the similarity of sets of co-occurring words (GREF) . 
<scope> There have been a number of other approaches for learning semantic parsers ,  including ones based on  </scope> machine translation techniques (GREF) ,  parsing models (GREF) ,   <scope> in1513 ductive logic programming algorithms  </scope> (GTREF) ,  probabilistic automata (REF) ,  and ideas from string kernels and support vector machines (GREF) . 
<scope> There have been proposals to detect a particular relation ,  e . g .  </scope>  ,  CAUSE (REF) ,   <scope> INTENT </scope>  (TREF) ,  PART-WHOLE (REF) and IS-A (REF) . 
<scope> There have been proposals to detect a particular relation </scope>  ,  e . g .  ,  CAUSE (REF) ,  INTENT (REF) ,  PART-WHOLE (REF) and IS-A (TREF) . 
<scope> There is also a distinction between Centering as a theory for resolving anaphoric pronouns  </scope> (GREF) ,   <scope> and the attempts to use a centering approach to resolving pronouns in an implementation </scope>  (TREF) . 
<scope> There is a number of both unsupervised morphology learning systems that use raw wordforms as training data </scope>  (GTREF) and supervised morphology learning systems using segmented wordforms into stems and affixes as training data (REF) . 
<scope> There is a substantial amount of related studies which deal with the discovery of various relationship types represented in useful resources such as WordNet ,  including </scope>  hypernymy (GREF) ,  synonymy (GREF) and  <scope> meronymy </scope>  (GTREF) . 
<scope> There is a variety of methods that have been used in NE recognition ,  such as </scope>  HMM ,  Maximum Entropy Models ,  Decision Trees ,   <scope> Boosting and Voted Perceptron </scope>  (TREF) ,  Syntactic Structure based approaches and WordNet-based approaches (GREF) . 
<scope> There is less consensus on the preference order :  (sentence-wise) left-to-right </scope>  (GTREF) or right-to-left (recency) (REF) . 
<scope> There is much disagreement about the units and elementary relations of discourse structure ,  but they agree that the structures are hierarchical </scope>  ,  most commonly trees (REF) ,   <scope> while others have argued for directed acyclic graphs </scope>  (TREF) ,  or general graphs (REF) . 
<scope> There is work on lexicon induction using string distance or other phonetic/orthographic comparison techniques ,  such as  </scope> TREF or semantic comparison using resources such as WordNet (REF) . 
<scope> The representation simplifies many phenomena usually discussed in the formal semantic literature (see the next section) ,  but is tailored for use in Question Answering </scope>  (TREF)  <scope> or Textual Entailment </scope>  (TREF)  <scope> applications .  </scope>
<scope> The re-ranking algorithms include </scope>  rescoring (REF) and  <scope> Minimum Bayes-Risk (MBR) decoding </scope>  (GTREF) . 
<scope> The reranking kernel in equation 4 ,  consisting in summing four different kernels ,  has been proposed in  </scope> (TREF) for syntactic parsing reranking ,  where the basic kernel was a Tree Kernel ,  and the idea was taken in turn from (REF) ,  where pairs where used to learn preference ranking . 
<scope> The rest of this paper is concerned with the extension of a Shake-andBake like transfer approach </scope>  (GTREF) or the kind of semantic-based transfer approach as described for example in REF to cope with local ambiguities . 
<scope> The results are compared against two state of the art approaches :  a su568 pervised machine learning model ,  Semantic Scattering </scope>  (TREF) ,  and a webbased probabilistic model (REF) . 
<scope> The results are reported in terms of standard BLEU </scope>  (TREF) (and its case sensitive version ,  BLEU-c) and tested for statistical significance using an approximate randomization test (REF) with 100 iterations . 
<scope> The results have demonstrated the existence of priming effects in corpus data :  they  </scope> occur for specific syntactic constructions (GREF) ,  consistent with the experimental literature ,  but also <scope> generalize to syntactic rules across the board ,  which repeated more often than expected by chance  </scope> (GTREF) . 
<scope> The same idea has been successfully applied to </scope>  the parse-tree reranking task (GREF) and  <scope> predicate argument classification </scope>  (TREF) . 
<scope> The same idea has been successfully applied to the parse-tree reranking task </scope>  (GTREF) and predicate argument classification (REF) . 
<scope> The same problem was again highlighted by the results obtained with and without the frame information in the Senseval-3 competition  </scope> (TREF) of FrameNet (REF) role labeling task . 
<scope> The </scope>  Lin (REF) and  <scope> Leacock-Chodorow </scope>  (TREF) <scope> similarity measures </scope>  and the BanerjeePedersen (REF) relatedness measures  <scope> were used .  </scope>
<scope> The score can be the number of constraints satisfied by the interpretation  </scope> (GTREF) ,  where these constraints might be assigned relative weights by the linguist (GREF) or calculated by the computer (REF) . 
<scope> The score of a sentence is the sum of words scores from that sentence  </scope> (TREF) Indicator phrase method :  REF noticed that in scientific papers it is possible to identify phrases such as in this paper ,  we present ,  in conclusion ,  which are usually meta-discourse markers . 
<scope> These approaches can be further categorized into </scope>  ones that use conditional entropy between letters to detect segment boundaries (GREF) ,  approaches that use minimal description length and thereby minimize the size of the lexicon as measured in entries and  <scope> links between the entries to constitute a word form </scope>  (GTREF) . 
<scope> These approaches rely on presence and scores of sentiment-bearing words that have been acquired from </scope>  dictionaries (REF) or  <scope> corpora  </scope> (TREF) . 
<scope> These approaches use different learning mechanisms to combine features ,  including decision trees </scope>  (GTREF) exponential models (REF) or other probabilistic models (GREF) . 
<scope> These are passed into a fast algorithm for </scope>  maximum spanning tree (REF) or  <scope> maximum projective spanning tree  </scope> (TREF) . 
<scope> These belong to two main categories based on machine learning </scope>  (GTREF) and language or domain specific rules (GREF) . 
<scope> These categories were automatically generated using the labeled parses in Penn Treebank </scope>  (TREF) and the labeled semantic roles of PropBank (REF) . 
<scope> The second constraint ,  known as the cohesion constraint </scope>  (TREF) ,   <scope> uses the dependency tree (REF) of the English sentence to restrict possible link combinations .  </scope>
<scope> The second constraint ,  known as the cohesion constraint </scope>  (TREF) ,   <scope> uses the dependency tree </scope>  (REF)  <scope> of the English sentence to restrict possible link combinations </scope>  . 
<scope> The second feature is the count of discontinuous phrases that are in configurations  </scope> (cross-serial DTU (REF) and  <scope> bonbon  </scope> (TREF)) <scope> that cant be handled by 2-SCFG systems .  </scope>
<scope> The second order algorithm of REF uses in addition to </scope>  TREF  <scope> the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild .  </scope>
<scope> The second parser uses generalised top-down strategies  </scope> (TREF) and a restricted bidirectional algorithm (REF) for error detection and correction . 
<scope> The second system participating in the task  </scope> (TREF)  <scope> was an adaptation of an existing LFG-based system  </scope> for deep semantic analysis (REF) ,  whose output was mapped to FrameNet-style annotation . 
<scope> These constraints can be </scope>  lexicalized (GREF) ,   <scope> unlexicalized </scope>  (GTREF) or automatically learned (GREF) . 
<scope> These constraints can be </scope>  lexicalized (GREF) ,   <scope> unlexicalized  </scope> (GTREF) or automatically learned (GREF) . 
<scope> These features are calculated by mining the parse trees ,  and then could be used for resolution by using manually designed rules </scope>  (GTREF) ,  or using machine-learning methods (GREF) . 
<scope> The segmentation into nuclei relies on a manually built chunker </scope>  ,  similar to the one described in (Ait-REF) ,  and  <scope> resembles the one proposed in </scope>  (TREF) . 
<scope> These include CCM </scope>  (TREF) ,  the DMV and DMV + CCM models (REF) ,  (U)DOP based models (GREF) ,  an exemplar based approach (REF) ,  guiding EM using contrastive estimation (REF) ,  and the incremental parser of REF which we use here . 
<scope> These include exploring </scope>  query logs (REF) ,   <scope> unrelated corpus </scope>  (TREF) ,  and parallel or comparable corpus (GREF) . 
<scope> These include harvesting the web for </scope>  translations or comparable corpora (GREF) ,  improving SMT models so that they are better suited to the low resource setting (Al-GREF) ,  or  <scope> designing models that are capable of learning translations from monolingual corpora </scope>  (GTREF) . 
<scope> These include ,  just to mention </scope>  the most popular 1337 ones ,  DIRT (REF) ,   <scope> VerbOcean </scope>  (TREF) ,  FrameNet (REF) ,  and Wikipedia (GREF) . 
<scope> These include </scope>  Malt Parser (REF) ,  MSTParser (REF) ,   <scope> Stanford Parser </scope>  (TREF) and C&C Parser (REF) . 
<scope> These include </scope>  not only domain driven disambiguation algorithms (REF) but also  <scope> graph theoretic ones </scope>  (TREF) as well as algorithms that quantify the degree of association between senses and their co-occurring contexts (REF) . 
<scope> These include several off-the-shelf statisical NLP tools such as  </scope> the Stanford POS tagger (REF) ,   <scope> the Stanford named-entity recognizer (NER)  </scope> (TREF) and the Stanford Parser (REF) . 
<scope> These include several off-the-shelf statisical NLP tools such as the Stanford POS tagger </scope>  (TREF) ,  the Stanford named-entity recognizer (NER) (REF) and the Stanford Parser (REF) . 
<scope> These large constructions are excluded completely by models that only allow elementary trees up to a certain depth (typically 4 or 5) into the symbolic grammar </scope>  (GTREF) ,  or only elementary trees with exactly one lexical anchor (REF) . 
<scope> The selection criterioncanbeacombinationoftranslationmodel and language model scores with  </scope> multiple comparison tests (REF) ,  or <scope> statistical confidence models </scope>  (TREF) . 
<scope> These methods have been used in machine translation (GTREF) </scope>  ,  terminology research and translation aids (GREF) ,  bilingual lexicography (REF) ,  collocation studies (REF) ,  word-sense disambiguation (GREF) and information retrieval in a multilingual environment (REF) . 
<scope> These methods look at the context words and discourse surrounding the source word and use methods ranging from </scope>  boostrapping (REF) ,   <scope> EM iterations </scope>  (GTREF) ,  and the cohesive relation between the source sentence and translation candidates (GREF) . 
<scope> These methods often involve using a statistic such as  </scope> 2 (REF) or  <scope> the log likelihood ratio </scope>  (TREF)  <scope> to create a score to measure the strength of correlation between source and target words .  </scope>
<scope> These models ,  for example ,  include </scope>  Restricted Boltzmann Machines (GREF) and Sigmoid Belief Networks (SBNs) (REF) for classification and regression tasks ,  Factorial HMMs (REF) for sequence labeling problems ,   <scope> Incremental SBNs for parsing problems  </scope> (TREF) ,  1Among the versions which do not exploit labeled data from the target domain . 
<scope> Thesemodelsforreferenceresolutiontakeintoaccountlinguisticfactors ,  suchasrelativesalienceof candidate antecedents ,   </scope> which have been modeled in Centering Theory (REF) <scope> by ranking candidate antecedents appearing in the preceding discourse  </scope> (GTREF) . 
<scope> The sentences are parsed by a syntactic parser </scope>  (TREF)  <scope> that we trained on syntactic dependency annotations for French </scope>  (REF) . 
<scope> These phrases are called syntactic phrases </scope>  which are consistent with syntactic constituents (REF) ,   <scope> and have been shown to be helpful in tree-based systems </scope>  (GTREF) . 
<scope> These problems formulations are similar to those studied in  </scope> (REF) and (GTREF) ,  respectively . 
<scope> The sequential classi cation approach can handle </scope>  many correlated features ,  as demonstrated in work on maximum-entropy (GREF) and  <scope> a variety of other linear classi ers ,  including </scope>  winnow (REF) ,  AdaBoost (REF) ,  and  <scope> support-vector machines </scope>  (TREF) . 
<scope> These relations are mainly prefixal or suffixal with two exceptions ,  </scope>  (TREF) and (REF) ,   <scope> who use string edit distances to estimate formal similarity </scope>  . 
<scope> These results are in line with the rates reported in other recent corpus studies of NSUs </scope>  :  11 . 15% in (REF) ,  10 . 2% in (REF) ,   <scope> 8 . 2% in </scope>  (TREF) . 
<scope> These results are in line with the rates reported in other recent corpus studies of NSUs :  </scope>  11 . 15% in (REF) ,   <scope> 10 . 2% in </scope>  (TREF) ,  8 . 2% in (REF) . 
<scope> These scores are higher than those of several other parsers  </scope> (e . g .  TREF ,  99 ;  REF) ,  but remain behind tim scores of REF who obtains 90 . 1% LP and 90 . 1% LR for sentences _< 40 words . 
<scope> These systems include </scope>  an ensemble method (REF) and  <scope> an approach of tree revision learning with a selection method of only using short training sentences (shorter than 30 words) </scope>  (TREF) . 
<scope> These techniques can be broadly categorized in two genres ,  one follows classical manual annotation </scope>  (REF) ; (REF) ;  (TREF)  <scope> techniques </scope>  and the others proposed various automatic techniques (REF) . 
<scope> These techniques were applied and examined in different domains ,  such as </scope>  customer reviews (GREF) and  <scope> news articles </scope>  (GTREF) . 
<scope> The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus ,  which would be relatively much harder to incorporate in approaches like </scope>  (REF) and (TREF) . 
<scope> The skip-chain CRFs  </scope> (GTREF)  <scope> model the long distance dependency between context and answer sentences </scope>  and the 2D CRFs (REF) model the dependency between contiguous questions . 
<scope> The standard approach to word alignment employs directional Markov models that align the words of a sentence f to those of its translation e ,  such as </scope>  IBM Model 4 (REF) or  <scope> the HMM-based alignment model  </scope> (TREF) . 
<scope> The string-to-tree </scope>  (TREF) and tree-to-tree (REF)  <scope> methods have also been the subject of experimentation </scope>  ,  as well as other formalisms such as Dependency Trees (REF) . 
<scope> The structure of citation and collaboration networks has been studied in </scope>  (GTREF) ,  and summarization of scientific documents is discussed in (REF) . 
<scope> The study is conducted on both a simple Air Travel Information System (ATIS) corpus </scope>  (TREF) and the more complex Wall Street Journal (WSJ) corpus (REF) . 
<scope> The subtree kernel method </scope>  (TREF)  <scope> for shortest path enclosed subtrees (REF) was adopted in our model .  </scope>
<scope> The success of statistical methods in particular has been quite evident in the area of syntactic parsing ,  most recently with the outstanding results of </scope>  (TREF) and (REF)  <scope> on the now-standard English test set of the Penn Treebank </scope>  (REF) . 
<scope> The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise </scope>  REF (TREF) . 
<scope> The system developed by  </scope> (TREF) ,  where use is made of an iterative reestimation algorithm derived from the well-known inside-outside algorithm (REF) ,   <scope> obtains 90 . 4% bracketing accuracy .  </scope>
<scope> The system was trained in a standard manner ,  using a minimum error-rate training (MERT) procedure  </scope> (TREF) with respect to the BLEU score (REF) on held-out development data to optimize the loglinear model weights . 
<scope> The tagger is based on a full scale two-level morphological specification of Turkish </scope>  (TREF) ,  implemented on the PC-KIMMO environment (REF) . 
<scope> The taggers used were the Brill Tagger </scope>  (TREF)  <scope> for English </scope>  ,  the Treetagger for French (REF) ,  and the LoPar Tagger (Schmidt and Schulte im REF) for German . 
<scope> The tagger we used ,  </scope>  implemented by REF for Swedish ,  <scope> is the TnT-tagger  </scope> (TREF) ,  trained on the SUC Corpus (REF) . 
<scope> The tagging task can be solved by using general machine learning techniques such as </scope>  maximum entropy (ME) models (REF) and  <scope> support vector machines </scope>  (GTREF) . 
<scope> The texts were processed using the GENIA tagger  </scope> (TREF) . We used constituency parse trees automatically produced by two different constituency parsers reported in (REF) and (REF) . 
<scope> The third voting model ,  a boosting model </scope>  (REF) ,   <scope> was built as boosting has consistently turned in very competitive scores on related tasks such as named entity classification </scope>  (REF)(TREF) . 
<scope> The toolkit also implements suffix-array grammar extraction  </scope> (TREF) and minimum error rate training (REF) . 
<scope> The top portion corresponds to the MEMD2B maximum entropy model described in </scope>  (TREF) ;  the bottom portion corresponds to the linear combination of a trigram and IBM 2 used in the TransType experiments (REF) . 
<scope> The trade-off between speed and end-toend translation quality is investigated and compared to </scope>  Inversion Transduction Grammars (REF) and  <scope> the standard tool for word alignment ,  GIZA +  +  </scope>  (GTREF) . 
<scope> The training data may contain a few hundred randomly selected transliteration pairs from a transliteration dictionary </scope>  (GTREF) or just a few carefully selected transliteration pairs (GREF) . 
<scope> The translation models and language models were learned by using GIZA +  +  </scope>  (TREF) and the CMU-Cambridge Toolkit (REF) ,  respectively . 
<scope> The translation quality is evaluated by </scope>  case-sensitive NIST (REF) and  <scope> BLEU </scope>  (TREF)2 . 
<scope> The translation system is a factored phrasebased translation system that uses </scope>  the Moses toolkit (REF) for decoding and training ,   <scope> GIZA +  +  for word alignment </scope>  (TREF) ,  and SRILM (REF) for language models . 
<scope> The treatment is similar to the discourse copying analysis of </scope>  (TREF) ,  and to the substitutional treatment suggested by Kamp within Discourse Representation Theory ,  described in (REF) . 
<scope> The tree-based reranker includes </scope>  the features described in (REF) as well as  <scope> features based on non-projective edge attributes explored in  </scope> (GTREF) . 
<scope> The tree is produced by a state-of-the-art dependency parser </scope>  (TREF) trained on the Wall Street Journal Penn Treebank (REF) . 
<scope> The trees may be learned directly from parallel corpora </scope>  (TREF) ,  or provided by a parser trained on hand-annotated treebanks (REF) . 
<scope> The trees may be </scope>  learned directly from parallel corpora (REF) ,  or  <scope> provided by a parser trained on hand-annotated treebanks </scope>  (TREF) . 
<scope> The tuning metric was BLEU ,  but we reported results in BLEU </scope>  (TREF) and TER (REF) . 
<scope> The UG is implemented as a UB-GCG </scope>  ,  erabedded in a default inheritance network of lexical types (REF) ,   <scope> implemented in the YADU framework </scope>  (TREF) . 
<scope> The UMLS has been widely used in many natural language processing tasks ,  including  </scope> information retrieval (REF) ,  extraction (REF) ,  and <scope> text summarization </scope>  (GTREF) . 
<scope> The underlying formalisms used has been quite broad and include </scope>  simple formalisms such as ITGs (REF) ,  hierarchicalsynchronousrules(REF) ,  string to tree models by (REF) and (REF) ,   <scope> synchronous CFG models such </scope>  (TREF) (REF) ,  synchronous Lexical Functional Grammar inspired approaches (REF) and others . 
<scope> The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it ,  which is inspired by </scope>  PageRank (REF) and  <scope> LexRank </scope>  (TREF) . 
<scope> The understanding can be induced using dependencies between  </scope> words (REF) ,  rhetorical relations (REF) ,   <scope> events  </scope> (TREF) . 
<scope> The understanding can be induced using dependencies between words </scope>  (TREF) ,  rhetorical relations (REF) ,  events (REF) . 
<scope> The University of Sheffield has pioneered in the Gate and Gate 2 projects the development of an architecture for text engineering </scope>  (TREF) ,  (REF) . 
<scope> The usage of special knowledge bases to determine projections of categories </scope>  (TREF)  <scope> would have presupposed language-dependent knowledge </scope>  ,  so we investigated two other options :  Flat rules (REF) and binary rules . 
<scope> The use of chart pruning techniques ,  typically some form of beam search ,  is essential for </scope>  practical parsing using Penn Treebank parsers (GREF) ,  as well as  <scope> practical parsers based on linguistic formalisms ,  such as HPSG </scope>  (TREF) and LFG (REF) . 
<scope> The use of chart pruning techniques ,  typically some form of beam search ,  is essential for  </scope> practical parsing using Penn Treebank parsers (GREF) ,  as well as  <scope> practical parsers based on linguistic formalisms ,  such as </scope>  HPSG (REF) and  <scope> LFG </scope>  (TREF) . 
<scope> The use of patterns is a widely accepted as an effective approach in the field of Natural Language Processing (NLP) ,  in tasks like Question-Answering (QA) </scope>  (GTREF) or Question Generation (QG) (GREF) . 
<scope> The uses of this procedure include </scope>  information retrieval (GREF) ,  summarization (REF) ,  text understanding ,  <scope> anaphora resolution </scope>  (TREF) ,  language modelling (GREF) and improving document navigation for the visually disabled (REF) . 
<scope> The variance semiring is essential for many interesting training paradigms such as  </scope> deterministic 40 annealing (REF) ,  minimum risk (REF) ,   <scope> active and semi-supervised learning  </scope> (GTREF) . 
<scope> The variance semiring is essential for many interesting training paradigms such as  </scope> deterministic 40 annealing (REF) ,  <scope> minimum risk  </scope> (TREF) ,  active and semi-supervised learning (GREF) . 
<scope> The various extraction measures </scope>  have been discussed in great detail in the literature (GREF) ,   <scope> their performance has been compared </scope>  (GTREF) ,  and the methods have been combined to improve overall performance (REF) . 
<scope> The various extraction </scope>  measures have been discussed in great detail in the literature (GREF) ,  their performance has been compared (GREF) ,  and the  <scope> methods have been combined to improve overall performance </scope>  (TREF) . 
<scope> The version of SDTW employed by </scope>  (TREF) and REF  <scope> employed an algorithm of complexity O(Klog(L)) from </scope>  (REF)  <scope> to find subpaths .  </scope>
<scope> The weather domain has proven to be a fruitful domain for further research as witnessed e . g .  by the system for generating marine forecasts presented by </scope>  TREF ,  by the work by REF ,  by the system generating public weather reports in Bulgarian reported on by REF and the system translating Finnish marine forecasts into Swedish by Bl~tberg (1988) . 
<scope> The web as a corpus has been successfully used for many areas in NLP (REF) such as </scope>  WSD (REF) ,   <scope> obtaining frequencies for bigrams </scope>  (TREF) and noun compound bracketing (REF) . 
<scope> The web as a corpus has been successfully used for many areas in </scope>  NLP (REF) such as  <scope> WSD  </scope> (TREF) ,  obtaining frequencies for bigrams (REF) and noun compound bracketing (REF) . 
<scope> The Web is being explored </scope>  not only as a super corpus for NLP and linguistic research (REF) but also ,  more importantly to MT research ,   <scope> as a treasure for mining bitexts of various language pairs </scope>  (GTREF) . 
<scope> The weights of the distortion ,  translation and language models were optimized  </scope> with respect to BLEU scores (REF)  <scope> on a given held-out set of sentences with Minimum Error Rate Training  </scope> (MERT ;  (TREF)) <scope> in 15 iterations .  </scope>
<scope> The words are searched in three different sentiment lexicons ,  the Subjectivity Word lists  </scope> (TREF) ,  SentiWordNet (REF) and WordNet Affect (REF) . 
<scope> The work described here makes use only of </scope>  lexical distribution information ,  in lieu of prosodic cues such as intonational pitch ,  pause ,  and duration (REF) ,  discourse markers such as oh ,  well ,  ok ,  however (GREF) ,  pronoun reference resolution (GREF) and  <scope> tense and aspect </scope>  (GTREF) . 
<scope> The work done by </scope>  (TREF)  <scope> focuses directly on coordination of noun phrases </scope>  in the context of the Collins parser (REF) by building a right conjunct using features from the already built left conjunct . 
<scope> They are all based on co-occurrence statistics ,  albeit using different context representations such as co-occurrence of words within phrases </scope>  (GTREF) ,  bigrams (GREF) ,  small windows around a word (REF) ,  or larger contexts such as sentences (GREF) or large windows of up to 20 words (REF) . 
<scope> They are based on dominance constraints  </scope> (GTREF) and extend them with parallelism (REF) and binding constraints . 
<scope> They are most commonly used for parsing and linguistic analysis </scope>  (GTREF) ,  but are now commonly seen in applications like machine translation (REF) and question answering (REF) . 
<scope> They can be categorized into </scope>  those based on Chinese phonemes (GREF) and  <scope> those that dont rely on Chinese phonemes </scope>  (GTREF) . 
<scope> They can be categorized into those based on Chinese phonemes </scope>  (GTREF) and those that dont rely on Chinese phonemes (GREF) . 
<scope> They can be roughly divided into three categories :  </scope>  string-to-tree models (e . g .  ,  (GREF)) ,   <scope> tree-to-string models </scope>  (e . g .  ,  (GTREF)) ,  and tree-totree models (e . g .  ,  (GREF)) . 
<scope> They can be roughly divided into three categories </scope>  :  string-to-tree models (e . g .  ,  (GREF)) ,  tree-to-string models (e . g .  ,  (GREF)) ,  and  <scope> tree-totree models </scope>  (e . g .  ,  (GTREF)) . 
<scope> They can be </scope>  facts in a database (REF) ,  propositions (REF) ,  discourse trees (REF) ,  or  <scope> sentences </scope>  (GTREF) . 
<scope> They filter out unlikely lexical entries just to help parsing </scope>  (TREF) ,  or the probabilistic models for phrase structures were trained independently of the supertaggers probabilistic models (GREF) . 
<scope> They have been applied to incremental parsing </scope>  (TREF) ,  grammar formalisms (GREF) ,  discourse (REF) ,  and scope underspeci cation (GREF) . 
<scope> They have been applied to part-of-speech (POS) tagging in supervised  </scope> (TREF) ,  semi-supervised (GREF) and unsupervised (REF) <scope> training scenarios .  </scope>
<scope> They have been used in three ways :  as a postprocess for parse selection </scope>  (GTREF) ,  a preprocess to find more probable bracketing structures (REF) ,  and online to rank each constituent produced ,  as in Tsuruoka et al . 
<scope> They have  </scope>  <scope> discussed simple sentenses </scope>  (GTREF) ,  dialogues (REF) ,  stereotypical lead sentences of newspaper articles (REF) ,  intrasentential resolution (GREF) or organization names in newspaper articles (REF) . 
<scope> They include </scope>  word sense disambiguation ,  e . g .  ,  (REF) ,  POS tagging (REF) ,  named entity recognition (REF) ,   <scope> word segmentation ,  e . g .  ,   </scope> (TREF) ,  and parsing ,  e . g .  ,  (GREF) . 
<scope> They include those using </scope>  naive Bayes (REF) ,   <scope> decision lists </scope>  (TREF) ,  nearest neighbor (REF) ,  transformation-based learning (REF) ,  neural networks (REF) ,  Winnow (REF) ,  boosting (REF) ,  and naive Bayesian ensemble (REF) . 
<scope> They include those using  </scope> Nave Bayes (REF) ,  Decision List (REF) ,   <scope> Nearest Neighbor  </scope> (TREF) ,  Transformation Based Learning (REF) ,  Neural Network (Towell and 1 In this paper ,  we take English-Chinese translation as example ;  it is a relatively easy process ,  however ,  to extend the discussions to translations between other language pairs . 
<scope> They obtain information about noun countability by merging lexical entries from COMLEX </scope>  (TREF) and the ALTJ/E Japanese-to-English semantic transfer dictionary (REF) . 
<scope> They </scope>  provide indispensable training data for statistical machine translation (GREF) and  <scope> have been found useful in research on automatic lexical acquisition </scope>  (GTREF) ,  crosslanguage information retrieval (GREF) ,  and annotation projection (GREF) . 
<scope> They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser ,  WASP </scope>  (TREF) or KRISP (REF) ,   <scope> or an existing supervised natural-language generator ,  WASP1 </scope>  (TREF) . 
<scope> This algorithm  </scope> is referred to as GHKM (REF) and <scope> is widely used in SSMT systems </scope>  (GTREF) . 
<scope> This ,  along with the difficulty and labor-intensiveness of generating exhaustive lists of rules ,  has led researchers to focus on automatic methods for building inference resources such as </scope>  inference rule collections (GREF) and  <scope> paraphrase collections </scope>  (TREF) . 
<scope> This approach allows to combine strengths of generality of context attributes as in n-gram models  </scope> (GTREF)  <scope> with their specificity as for binary features in MaxEnt taggers </scope>  (GREF) . 
<scope> This approach has been shown to be highly effective in various tasks ranging from  </scope> collaborative filtering (REF) to  <scope> parsing  </scope> (TREF) . 
<scope> This approach has been successfully applied to </scope>  parsing (REF) ,   <scope> tagging </scope>  (TREF) ,  and language modeling for speech recognition (REF) . 
<scope> This approach is adopted from text generation where plan-operators are responsible for choosing linguistic means in order to create coherent stretches of text </scope>  (see ,  for instance ,  (REF) and (TREF)) . 
<scope> This approach is adopted </scope>  in (REF) and  <scope> in </scope>  (TREF)  <scope> where only a dozen categories are used .  </scope>
<scope> This approach is also used in base-NP chunking </scope>  (TREF) and named entity recognition (REF) as well as word segmentation . 
<scope> This approach to minimally supervised classifier construction has been widely studied </scope>  (TREF) ,  especially in cases in which the features of interest are orthogonal in some sense (e . g .   ,  GREF) . 
<scope> This approach was successfully used in large vocabulary continuous speech recognition </scope>  (TREF) and in a phrase-based SMT systems (GREF) . 
<scope> This can be done automatically with unparsed corpora </scope>  (GTREF) ,   <scope> from parsed corpora such as Marcus et al . 's (1993) Treebank </scope>  (GREF) or manually as was done for COMLEX (REF) . 
<scope> This can be done by </scope>  smoothing the observed frequencies 7 (REF) or by  <scope> class-based methods </scope>  (GTREF) . 
<scope> This can be formalized as cooccurrence statistics collected from unstructured documents </scope>  (GTREF) ,  or distributional concept or word vectors with features extracted from either unstructured documents (GREF) or (semi-)structured knowledge resources (GREF) . 
<scope> This characteristic of our corpus  </scope> is similar to problems with noisy and comparable corpora (REF) ,  and it  <scope> prevents us from using methods developed in the MT community based on clean parallel corpora ,  such as  </scope> (TREF) . 
<scope> This conclusion is the same as </scope>  (REF) and (TREF)  <scope> opinion ,  i . e .  ,  because typically occurs in the second span </scope>  . 
<scope> This contrasted with the much higher performance obtained using a constituent-todependency conversion approach with accurate ,  but much slower ,  constituency parsers </scope>  such as the REF and  <scope> Berkeley  </scope> (GTREF)  <scope> parsers ,  which achieved 89 . 1 and 87 . 9 labeled F1 scores ,  respectively .  </scope>
<scope> This contrasts with the techniques proposed by REF and by TREF ,  which are extensions of parsing algorithms for probabilistic context-free grammars ,  and require considerably more involved proofs of correctness .  </scope>
<scope> This could allow </scope>  extremely large feature sets (REF) ,  or  <scope> the look-up of expensive corpus-based features such as word-pair mutual information  </scope> (TREF) . 
<scope> This distribution can be compared to error distributions reported in  </scope> (REF) and in (TREF) . 
<scope> This formalism is related directly to the Core Language Engine ,  but more conceptually it is closely related to that of other unification-based grammar formalisms with a context-free skeleton ,  such as </scope>  PATR-II (REF) ,   <scope> Categorial Unification Grammar  </scope> (TREF) ,  Generalized Phrase-Structure Grammar (REF) ,  and Lexical Functional Grammar (REF) . 
<scope> This formalism </scope>  ,  used in the implementation of TACITUS (REF-2) ,  accommodates a large variety of discourse inferences and ,  moreover ,  <scope> provides an elegant manner of localizing ambiguities ,  as was shown in  </scope> (TREF) . 
<scope> This grammar is used to provide all the language modeling capabilities of the system ,  including </scope>  the language model used in the speech recognizer ,  the syntactic and semantic interpretation of user utterances (REF) ,  and  <scope> the generation of system responses </scope>  (TREF) . 
<scope> This has been shown both in supervised settings </scope>  (GTREF) and unsupervised settings (GREF) in which constraints are used to bootstrap the model . 
<scope> This includes both the parsers that attach probabilities to parser moves </scope>  (GTREF) ,  but also those of the lexicalized PCFG variety (GREF) . 
<scope> This includes extracting semantic relations expressed in the encyclopedic definitions by means of Hearst patterns  </scope> (TREF) ,  detection of semantic variations (REF) between category labels ,  as well as using the categorized pages as bag-of-words to compute scores of idf-based semantic overlap (Monz & de REF) between categories . 
<scope> This includes the automatic generation of sense-tagged data using monosemous relatives </scope>  (GTREF) ,  automatically bootstrapped disambiguation patterns (GREF) ,  parallel texts as a way to point out word senses bearing different translations in a second language (GREF) ,  and the use of volunteer contributions over the Web (Chklovski and REF) . 
<scope> This includes the automatic generation of sense-tagged data using  </scope> monosemous relatives (GREF) ,  automatically bootstrapped disambiguation patterns (GREF) ,  <scope> parallel texts as a way to point out word senses bearing different translations in a second language </scope>  (GTREF) ,  and the use of volunteer contributions over the Web (Chklovski and REF) . 
<scope> Thisincludestheusualcombinationof word clustering  </scope> usingmkcls3 (REF) ,   <scope> twoway word alignment using GIZA +  +  4  </scope> (TREF) ,  and alignment symmetrization using the grow-diag-final-and heuristic (REF) . 
<scope> This includes work on question answering </scope>  (TREF) ,  sentiment analysis (REF) ,  MT reordering (REF) ,  and many other tasks . 
<scope> This includes work on </scope>  phrasestructure parsing (GREF) ,  dependency parsing (GREF) as well as  <scope> a number of other formalisms </scope>  (GTREF) . 
<scope> This is adopted from </scope>  (TREF) <scope>  ,  based on the fact that WER correlates with concept accuracy </scope>  (CA ,  (REF)) . 
<scope> This is the approach taken by IBM Models 4 +  </scope>  (GTREF) ,  and more recently by the LEAF model (REF) . 
<scope> This is the case of the MWE grammar </scope>  (TREF) and of the wide-coverage LinGO ERG (REF) ,   <scope> both implemented on the framework of HPSG and successfully integrated with this database .  </scope>
<scope> This is the principle behind the parser </scope>  defined by REF ,   <scope> which is still in wide use today </scope>  (Corston-GTREF) . 
<scope> This is the reason why more recent works have focussed on ATMS techniques </scope>  (REF)  <scope> and their relations to chart parsing </scope>  (TREF) . 
<scope> This is well illustrated by the Collins parser </scope>  (GTREF) ,  scrutinized by REF ,  where several transformations are applied in order to improve the analysis of noun phrases ,  coordination and punctuation . 
<scope> This iterative procedure has been successfully applied to a variety of NLP tasks ,  such as </scope>  hypernym/hyponym extraction (REF) ,   <scope> word sense disambiguation </scope>  (TREF) ,  question answering (REF) ,  and information extraction (GREF) . 
<scope> This kind of features have been widely used in both newswire NER system ,  such as  </scope> (TREF) ,  and biomedical NER system ,  such as (GREF) . 
<scope> This latter type of annotation has been attempted both on </scope>  abstracts ,  (e . g .  ,  GREF) and  <scope> full papers ,  (e . g .  </scope>  GTREF) ,  <scope> with the number of distinct annotation categories varying between 4 and 14 .  </scope>
<scope> This learning approach has also been applied to a number of other tasks ,  including prepositional phrase attachment disambiguation </scope>  (TREF) ,  bracketing text (REF) and labeling nonterminal nodes (REF) . 
<scope> This method has been successfully applied to tasks such as </scope>  word detection (REF) and  <scope> topic boundary detection </scope>  (TREF) . 
<scope> This methodology is very similar to the way </scope>  [TREF]  <scope> evaluate their probabilistic TS model in comparison to the approach of [REF] .  </scope>
<scope> This metric correlates significantly with human judgments  </scope> and is better than Simple String Accuracy (REF)  <scope> for judging compression quality  </scope> (TREF) . 
<scope> This paper focuses on dependency parsing ,  which has become widely used in </scope>  relation extraction (REF) ,   <scope> machine translation </scope>  (TREF) ,  question answering (REF) ,  and many other NLP applications . 
<scope> This paper focuses on the ACE RDC subtask ,  where many machine learning methods have been proposed ,  including  </scope> supervised methods (GREF) ,  semi-supervised methods (GREF) ,  and  <scope> unsupervised methods </scope>  (GTREF) . 
<scope> This paper focuses on the ACE RDC subtask ,  where many machine learning methods have been proposed ,  including supervised methods </scope>  (GTREF) ,  semi-supervised methods (GREF) ,  and unsupervised methods (GREF) . 
<scope> This proposal has a partial similarity with the Conceptual Density </scope>  (TREF) and DRelevant (REF)  <scope> to get the concepts from a hierarchy that they associate with the sentence </scope>  . 
<scope> This research is close in spirit to the work of  </scope> TREF  <scope> on classifying the semantics of derivational affixes </scope>  ,  and REF on learning verb aspect . 
<scope> This result should be compared with other uniform scheme's such as SLD-resolution </scope>  or some implementations of type inference (REF ,  this volume)  <scope> which clearly are also uniform but facessevere problems in the case of lexicalist grammars ,  as such scheme's do not take into account the specific nature of lexicalist grammars </scope>  (TREF) . 
<scope> This results in important savings ,  both in space and time ,  over simply running a single-source shortest-path algorithm for directed acyclic graphs </scope>  (REF)  <scope> over the full acceptor a57a133a31 </scope>  (TREF) . 
<scope> This technique of using subsequences as independent examples is used  </scope> both in the corpora used in REF and (REF) ,  and  <scope> to an even larger extent in the corpus used in </scope>  TREF ,  who would also have in their corpus the artificially constructed sequence using crocidolites in 1956 . 
<scope> This tendency has manifested itself with respect to linguistic tools ,  perhaps seen most clearly in the evolution from ATNs with their strongly procedural grammars to PATR-II in its various incarnations </scope>  (GTREF) ,  and to logic-based formalisms such as DCG (REF) . 
<scope> This type of domain adaptation is reminiscent of self-training </scope>  (GTREF) and co-training (GREF) ,   <scope> except that the goal here is not to further improve the performance of the very best model .  </scope>
<scope> This type of pragmatic information has been noted to be very important for  </scope> automatically synthesizing utterance with appropriate intonation (REF) ,  and for  <scope> generating sentences with appropriate word order in free word order languages such as Turkish </scope>  (TREF) . 
<scope> This was mainly because of their attested strength at earlier Senseval evaluations </scope>  (GTREF) and mutual complementarity discovered by us (REF) . 
<scope> This was used ,  for example ,  by  </scope> (GREF) in information extraction ,  and by (TREF) <scope> in POS tagging .  </scope>
<scope> This was used ,  for example ,  by </scope>  (GTREF)  <scope> in information extraction </scope>  ,  and by (REF) in POS tagging . 
<scope> This was what </scope>  (TREF)  <scope> did </scope>  and we did this in training with the REUTERS corpus (REF) in which syntactic roles are annotated . 
<scope> This yielded a WER of 0 . 43 ,  which is a typical WER for lectures and conference presentations </scope>  (GTREF) ,  though a lower WER is possible in a more ideal condition (REF) ,  e . g .  ,  when the same course from the previous semester by the same instructor is available . 
<scope> Those approaches include </scope>  a naive Bayes classifier combined with the EM algorithm (GREF) ,   <scope> Co-training </scope>  (GTREF) ,  and Transductive Support Vector Machines (REF) . 
<scope> Those for the 4-tag set ,  adopted from  </scope> (REF) and (TREF) ,  i <scope> nclude C2 ,  C1 ,  C0 ,  C1 ,  C2 ,  C2C1 ,  C1C0 ,  C1C1 ,  C0C1and C1C2 .  </scope>
<scope> Though we use salience factors based on the </scope>  TREF ,   <scope> we have substantially deviated from the basic algorithm and have also used factors from REF ,  where named entity and ontology are considered for resolution </scope>  . 
<scope> Three approaches are dominating </scope>  ,  i . e .  knowledge-based approach (REF) ,  information retrieval-based approach (REF) and  <scope> machine learning approach </scope>  (TREF) ,   <scope> in which the last approach is found very popular .  </scope>
<scope> Three examples of such systems are </scope>  (1) GETA/ARIANE (GREF) ;  (2)  <scope> LMT  </scope> (TREF) ;  and (3) METAL (GREF) . 
<scope> Three runs have been submitted for the SemEval task 1 on Coreference Resolution  </scope> (REF) ,   <scope> optimizing Corrys performance for </scope>  BLANC (Recasens and Hovy ,  in prep) ,  MUC (REF) and  <scope> CEAF </scope>  (TREF) . 
<scope> Thus ,  an important number of </scope>  unsupervised (GREF) ,  supervised (GREF) ,  and  <scope> combined </scope>  (TREF) <scope> methods have been developed to this end .  </scope>
<scope> Thus ,  some research has been focused on deriving different word-sense groupings to overcome the finegrained distinctions of </scope>  WN (REF) ,  (REF) ,  (REF) ,  (REF) ,  (TREF) and (REF) . 
<scope> Thus ,  the Penn Treebank of American English  </scope> (TREF)  <scope> has been used to train and evaluate the best available parsers of unrestricted English text  </scope> (GREF) . 
<scope> Thus ,  the second class of SBD systems employs machine learning techniques such as </scope>  decision tree classifiers (REF) ,   <scope> neural networks </scope>  (TREF) ,  and maximum-entropy modeling (REF) . 
<scope> Thus ,  the task of Joyce's text planner is  </scope> similar in definition to TEXT's (REF) ,  but  <scope> different from that of Penman </scope>  (TREF) ,  which expects the content selection task to have already been performed . 
<scope> Thus ,  we used the five taggers ,  MBL </scope>  (TREF) ,  MXPOST (REF) ,  fnTBL (REF) ,  TnT ,  and IceTagger3 ,  in the same manner as described in (REF) ,  but with the following minor changes . 
<scope> Thus ,  we used the five taggers </scope>  ,  MBL (REF) ,   <scope> MXPOST </scope>  (TREF) ,  fnTBL (REF) ,  TnT ,  and IceTagger3 ,   <scope> in the same manner as described in (REF) ,  but with the following minor changes </scope>  . 
<scope> Till now ,  co-training has been successfully applied to </scope>  statistical parsing (REF) ,  reference resolution (REF) ,  part of speech tagging (REF) ,   <scope> word sense disambiguation </scope>  (TREF) and email classification (REF) . 
<scope> TMs have been used for </scope>  statistical machine translation (REF) ,  word alignment of a translation corpus (REF) ,  multilingual document retrieval (REF) ,  automatic dictionary construction (REF) ,  and  <scope> data preparation for word sense disambiguation programs </scope>  (TREF) . 
<scope> To accommodate multiple overlapping features on observations ,  some other approaches view the sequence labeling problem as a sequence of classification problems ,  including </scope>  support vector machines (SVMs) (REF) and  <scope> a variety of other classifiers </scope>  (GTREF) . 
<scope> To break the restriction of the treebank scale ,  lots of works have been devoted to the unsupervised methods </scope>  (GTREF) and the semi-supervised methods (GREF)  <scope> to utilize the unannotated text .  </scope>
<scope> To compute the features which we extract in the next section ,  all instances in our data sets were </scope>  part-of-speech tagged by the MXPOST tagger (REF) ,  parsed with the MaltParser2 ,  and  <scope> named entity tagged with the Stanford NE tagger </scope>  (TREF) . 
<scope> To cope with speech recognition errors ,  several confirmation strategies have been proposed :  confirmation management methods based on confidence measures of speech recognition results </scope>  (GTREF) and implicit confirmation that includes previous recognition results into systems prompts (REF) . 
<scope> To date ,  accurate parsers have been developed for LTAG </scope>  (TREF) ,  CCG (GREF) ,  and LFG (GREF) . 
<scope> To date ,  accurate parsers have been developed for  </scope> LTAG (REF) ,   <scope> CCG  </scope> (GTREF) ,  and LFG (GREF) . 
<scope> To derive the joint counts c(s , t) from which p(s|t) and p(t|s) are estimated ,  we use the phrase induction algorithm described in  </scope> (REF) <scope>  ,  with symmetrized word alignments generated using IBM model 2 </scope>  (TREF) . 
<scope> To derive the joint counts c( ? s ,  ? t) from which p( ? s| ? t) and p( ? t| ? s) are estimated ,  we use the phrase induction algorithm described in </scope>  (TREF) ,  with symmetrized word alignments generated using IBM model 2 (REF) . 
<scope> To derive the joint counts c(s , t) from which p(s|t) and p(t|s) are estimated ,  we use the phrase induction algorithm described in </scope>  (TREF) ,  with symmetrized word alignments generated using IBM model 2 (REF) . 
<scope> To do this we parse every sentence twice ,  once with a dependency parser </scope>  (TREF) and once with a phrase-structure parser (REF) . 
<scope> To enable the surface realization of the English parses via RealPro ,  we automatically converted the phrase structures of the English Tree Bank into deep-syntactic dependency structures (DSyntSs) of the Meaning-Text Theory (MTT) </scope>  (REF)  <scope> using Xias converter </scope>  (TREF)  <scope> and our own conversion grammars </scope>  . 
<scope> To evaluate ,  we automatically converted its labeled constituents into unlabeled dependencies ,  </scope>  using deterministic head-percolation rules (REF) ,   <scope> discarding punctuation ,  any empty nodes ,  etc .  ,  as is standard practice </scope>  (GTREF) . 
<scope> To extract author and title information ,  systems have used  </scope> both the Support Vector Machine (SVM) (REF) and  <scope> CRFs  </scope> (GTREF) . 
<scope> To facilitate comparisons with the performance of previous methods ,  we adopted the experimental settings used to examine high-performance semi-supervised NLP systems ;  i . e .  ,  NER </scope>  (GTREF) and dependency parsing (GREF) . 
<scope> To make sense tagging more precise ,  it is advisable to place constraint on the translation counterpart c of w .  SWAT considers only those translations c that has been linked with  </scope> w based the Competitive Linking Algorithm (REF) and <scope> logarithmic likelihood ratio  </scope> (TREF) . 
<scope> To obtain packed forests ,  we used the Chinese parser </scope>  (TREF) modified by Haitao Mi and the English parser (REF) modified by Liang Huang to produce entire parse forests . 
<scope> To our knowledge no systems directly address Problem 1 ,  instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list  </scope> (GTREF) ,  or else making local independence assumptions which side-step the issue (GREF) . 
<scope> To overcome its limitation to one-place predicates ,   </scope> TREF  <scope> introduced a constraint-based procedure that could generate referring expressions involving relations </scope>  ;  and as a response to the computational complexity of greedy algorithms like FB ,  Reiter and Dale (GREF) introduced the psycholinguistically motivated Incremental Algorithm (IA) . 
<scope> To overcome these difficulties ,  some systems make use of domain-independent knowledge about lexical cohesion :   </scope> a lexical network built from a dictionary in (REF) ;  a thesaurus in (REF) ;   <scope> a large set of lexical cooccurrences collected from a corpus in  </scope> (TREF) . 
<scope> To produce this ,  we segment sentences with MXTerminator  </scope> (TREF) and parse the corpus with the selftrained Charniak parser (REF) . 
<scope> To quantify their findings ,  these studies use notions of agreement </scope>  (GTREF) and/or reliability (REF ;  Passonneau and Litman ,  to appear ;  REF) . 
<scope> To score the output of the coreference models ,  we employ </scope>  the commonly-used MUC scoring program (REF) and  <scope> the recently-developed CEAF scoring program </scope>  (TREF) . 
<scope> To set the weights ,  m ,  we performed minimum error rate training </scope>  (REF)  <scope> on the development set using Bleu </scope>  (TREF)  <scope> as the objective function .  </scope>
<scope> To the best of our knowledge ,  only (REF) and </scope>  (TREF)  <scope> mention having tried this kind of annotation ,  as a side job for their temporal expressions mark-up systems </scope>  . 
<scope> To the best of our knowledge ,  three 118 types of joint approaches have been proposed :  N-best based approach </scope>  (TREF) ,  synchronous joint approach (REF) ,  and a joint approach where parsing and SRL are performed simultaneously (Llus and M`arquez ,  2008) . 
<scope> Training the transliteration model is typically done under  </scope> supervised settings (GREF) ,  or  <scope> weakly supervised settings with additional temporal information </scope>  (GTREF) . 
<scope> Transformation-based learning </scope>  (TREF) ,  Support Vector Machines (REF) and Conditional Random Fields (REF)  <scope> were applied by one system each .  </scope>
<scope> Transliteration methods can be categorized into grapheme-based </scope>  (GTREF) ,  phoneme-based (GREF) ,  and combined (REF)  <scope> approaches </scope>  . 
<scope> TREF find that all but 0 . 02% of productions in the REF training data </scope>  ,  which includes various languages ,   <scope> can be binarized by their algorithm ,  but they do not give the fan-out or parsing complexity of the resulting rules .  </scope>
<scope> Twitter can provide suitable material for many aplications such as named entity recognition (NER) </scope>  (TREF) and sentiment analysis (REF) . 
<scope> Two main approaches have generally been considered </scope>  :  rule-based (GREF)  <scope> probabilistic  </scope> (GTREF) . 
<scope> Two main approaches have generally been considered :  </scope>  rule-based (GREF)  <scope> probabilistic </scope>  (GTREF) . 
<scope> Two measures are used to evaluate the parses :  lexical accuracy ,  which is the percentage of correctly tagged words compared to the extracted gold standard corpus </scope>  (TREF) and average crossing bracket rate (CBR) (REF) . 
<scope> Two notable exceptions are </scope>  TREF <scope> for POS tagging </scope>  ,  and REF for coreference resolution . 
<scope> Two other entries used MIRA </scope>  (TREF) or online passive-aggressive learning (REF)  <scope> to train a globally normalized model .  </scope>
<scope> Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT)  </scope> (REF)  <scope> and Minimum BayesRisk (MBR) decoding </scope>  (TREF) . 
<scope> Two types of investigations that would undoubtedly be enhanced are explorations of the interrelation of lexical cohesion and global discourse structure </scope>  (GTREF) ,  and identification of lexicaliza- :  tion patterns for domain-specific concepts (REF) . 
<scope> Two unsupervised discourse segmentation algorithms are investigated :   </scope> TextTiling (REF) and <scope> Hidden Markov Modeling </scope>  (TREF) . 
<scope> Typical existing summarization methods include  </scope> centroid-based methods (e . g .  ,  MEAD (REF)) ,   <scope> graph-ranking based methods (e . g .  ,  LexPageRank </scope>  (TREF)) ,  non-negative matrix factorization (NMF) based methods (e . g .  ,  (REF)) ,  Conditional random field (CRF) based summarization (REF) ,  and LSA based methods (REF) . 
<scope> Typically ,  the local context around the 215 word to be sense-tagged is used to disambiguate the sense </scope>  (TREF) ,  and it is common for linguistic resources such as WordNet (GREF) ,  or bilingual data (REF) to be employed as well as more longrange context . 
<scope> Under this approach ,  topic representations like those introduced in </scope>  (TREF) and (REF)  <scope> are used to identify a set of text passages that are relevant to a users domain of interest .  </scope>
<scope> Unfortunately ,  most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed ,  as reviewed respectively in </scope>  (TREF) and (REF) . 
<scope> Unlike previous work ,  our solution neither </scope>  requires larger applicability contexts (REF) ,   <scope> nor depends on pseudo nodes </scope>  (REF) or  <scope> auxiliary rules </scope>  (TREF) . 
<scope> Unlike  </scope> (TREF) ,   <scope> in the shared task  </scope> we used only the simplest feed-forward approximation ,  which replicates the computation of a neural network of the type proposed in (REF) . 
<scope> Unlike </scope>  (TREF) ,   <scope> in the shared task we used only the simplest feed-forward approximation </scope>  ,  which replicates the computation of a neural network of the type proposed in (REF) . 
<scope> Unsupervised dependency parsing has seen rapid progress recently ,  with error reductions on English </scope>  (REF)  <scope> of about 15% in six years </scope>  (GREF) ,   <scope> and better and better results for other languages </scope>  (GTREF) ,   <scope> but results are still far from what can be achieved with small seeds ,  language-specific rules </scope>  (REF)  <scope> or using cross-language adaptation </scope>  (GREF) . 
<scope> Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling </scope>  (GTREF) and dependency parsing (REF)  <scope> with a great deal of success .  </scope>
<scope> Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling </scope>  (GTREF) and dependency parsing (REF) with a great deal of success . 
<scope> Uses for k-best lists include </scope>  minimum Bayes risk decoding (GREF) ,  discriminative reranking (GREF) ,  and  <scope> discriminative training </scope>  (GTREF) . 
<scope> Uses for k-best lists include </scope>  minimum Bayes risk decoding (GREF) ,   <scope> discriminative reranking </scope>  (GTREF) ,  and discriminative training (GREF) . 
<scope> Using coreference information has been shown to be beneficial in a number of NLP applications including  </scope> Information Extraction (REF) ,  Text Summarization (REF) ,   <scope> Question Answering  </scope> (TREF) ,  and Machine Translation . 
<scope> Using MRDs for WSD was suggested </scope>  in (REF) ;   <scope> several researchers subsequently continued and improved this line of work </scope>  (GTREF) . 
<scope> Using </scope>  the IBM translation models IBM-1 to IBM-5 (REF) ,  as well as  <scope> the Hidden-Markov alignment model </scope>  (TREF) ,   <scope> we can produce alignments of good quality .  </scope>
<scope> Using techniques similar to those described in </scope>  (TREF) in combination with our work on GLARF (GREF) ,   <scope> we expect to build a hand-coded PROPBANKER a program designed to produce a PropBank/NomBank style analysis from Penn Treebank style input .  </scope>
<scope> Using web text for various NLP-tasks has been proven to be useful </scope>  (REF) ,   <scope> also for NC interpretation </scope>  (TREF) . 
<scope> Usually ,  these semantic informations are automatically acquired from corpora by means of various techniques as latent semantic analysis </scope>  (TREF) ,  mutual information (REF) or co-occurrence in ann-word window (GREF) . 
<scope> Various approaches have been proposed following the patterns of </scope>  (REF) and  <scope> clustering </scope>  (GTREF) . 
<scope> Various approaches have proposed to overcome this limitation ,  notably the use of word-classes  </scope> (GTREF) ,  of generalized back-off strategies (REF) or the explicit integration of morphological information in the random-forest model (GREF) . 
<scope> Various lexical resource-based </scope>  (TREF) and distributional measures (REF) have been proposed to measure semantic relatedness and distance between terms . 
<scope> Various machine learning strategies have been proposed to address this problem ,  including </scope>  semi-supervised learning (REF) ,   <scope> domain adaptation </scope>  (GTREF) ,  multi-task learning (GREF) ,  self-taught learning (REF) ,  etc .  A commonality among these methods is that they all require the training data and test data to be in the same feature space . 
<scope> Various machine learning strategies have been proposed to address this problem ,  including </scope>  semi-supervised learning (REF) ,   <scope> domain adaptation  </scope> (GTREF) ,  multi-task learning (GREF) ,  self-taught learning (REF) ,  etc .  <scope> A commonality among these methods is that they all require the training data and test data to be in the same feature space .  </scope>
<scope> Various statistical approaches ,  e . g .  ,  a maximum entropy model </scope>  (TREF) ,  HMMs and SVMs (REF) ,   <scope> have been used with various feature sets including surface and syntactic features ,  word formation patterns ,  morphological patterns ,  part-of-speech tags ,  head noun triggers ,  and coreferences .  </scope>
<scope> Various systems have focused on the recovery of ill-formed text at the </scope>  morphosyntactic level (REF) ,  the syntactic level (GREF) ,  and the  <scope> semantic level </scope>  (GTREF) . 
<scope> Various types of DGs are used in existing systems according to these classifications ,  such as non-label word DG </scope> (GTREF)4 ,  syntactic-label word DG (REF) ,  semantic-label word DG(REF) ,  non-label WPP DG(GREF) ,  syntactic-label WPP DG(REF) ,  semantic-label concept DG(REF) . 
<scope> Very briefly ,  the TextRank system </scope>  (TREF)  similar in spirit with the concurrently proposed LexRank method (REF)   <scope> works by building a graph representation of the text ,  where sentences are represented as nodes ,  and weighted edges are drawn using inter-sentential word overlap .  </scope>
<scope> VN was also substantially extended </scope>  (REF)  <scope> using the Levin classes extension proposed in </scope>  (TREF) . 
<scope> We accordingly introduce approaches which attempt to include semantic information into the coreference models from a variety of knowledge sources ,  e . g </scope>  .  WordNet (REF) ,  Wikipedia (REF) and  <scope> automatically harvested patterns </scope>  (GTREF) . 
<scope> We achieve good performance using this approach ,  with results competitive with earlier work </scope>  (GTREF) and higher recall and F-measure than that reported in REF when tested on the same corpus . 
<scope> Weakly supervised learning methods for semantic lexicon generation have utilized </scope>  co-occurrence statistics (GREF) ,  syntactic information (GREF) ,   <scope> lexico-syntactic contextual patterns (e . g .  ,  resides in  </scope> <location> <scope> or moved to  </scope> <location> <scope> ) </scope>  (GTREF) ,  and local and global contexts (REF) . </location></location>.
<scope> We also compared our results with other summarization methods such as LexPageRank </scope>  (TREF) and REF LSA-based method . 
<scope> We also compare our performance against </scope>  (REF) and (TREF)  <scope> and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF .  </scope>
<scope> We also showed in  </scope> TREF  <scope> that ,  in contrast with </scope>  REF <scope>  ,  adding distance to the dependency features in the log-linear model does improve parsing accuracy </scope>  . 
<scope> We also trained on the following much larger unlabeled datasets utilized in </scope>  TREF :    <scope> BLLIP :  5k articles of newswire parsed with the REF parser .  </scope>
<scope> We also train two SCFG-based MT systems :   </scope> a hierarchical phrase-based SMT (REF) system and  <scope> a syntax augmented machine translation (SAMT) system using the approach described in </scope>  TREF . 
<scope> We also tried to use features in verb ontology such as VERBOCEAN (REF) which is used in </scope>  (TREF) . 
<scope> We are considering both the Hidden Markov Model tagger TnT  </scope> (TREF) and the Decision Tree Tagger (REF) ,  with parameter files from Sharoff et al . 
<scope> Webber </scope>  (TREF)  <scope> improved upon the above work by specifying rules for how events are related to one another in a discourse </scope>  and Sing and Sing defined semantic constraints through which events can be related (REF) . 
<scope> We believe that an automatic means of finding a predominant sense would be useful </scope>  for systems that use it as a means of backing-off (GREF) and  <scope> for systems that use it in lexical acquisition </scope>  (GTREF) because of the limited size of hand-tagged resources . 
<scope> We believe that this is an aggressive number of senses for a discrimination system to attempt ,  considering that </scope>  (TREF)  <scope> experimented with 2 and 3 senses </scope>  ,  and (REF) made binary distinctions . 
<scope> We build on previous work that applies semantics to morphology </scope>  (GTREF) ,  and also on work that exploits web-scale data for semantic analysis (GREF) . 
<scope> We build two translation systems :  One using tree-based models without additional linguistic annotation ,  which are known as hierarchical phrasebased models </scope>  (TREF) ,  and another system that uses linguistic annotation on the target side ,  which are known under many names such as string-to-tree models or syntactified target models (REF) . 
<scope> We calculated the corpus frequencies based on the tag and dependency output of </scope>  RASP (REF) for English ,  and  <scope> CaboCha </scope>  (TREF)  <scope> for Japanese .  </scope>
<scope> We collect statistics with respect to the following two resources :  </scope>  General Inquirer (REF) and  <scope> Opinion Finder  </scope> (TREF) . 
<scope> We compare against several competing systems ,  </scope>  the first of which is based on the original IBM Model 4 for machine translation (REF) and  <scope> the HMM machine translation alignment model </scope>  (REF)  <scope> as implemented in the GIZA +  +  package </scope>  (TREF) . 
<scope> We compared the word alignment performance of our model with that of GIZA +  +  1 . 03 1 </scope>  (GTREF) ,  and HMBiTAM (REF) implemented by us . 
<scope> We compare ,  for French ,  a supervised lexicalized parsing algorithm with a semi-supervised unlexicalized algorithm </scope>  (TREF) along the lines of (REF) . 
<scope> We compare our approach with a dictionary based approach ,  such as word-by-word translation ,  and supervised approaches ,  such as </scope>  CCA (GREF) and  <scope> OPCA </scope>  (TREF) . 
<scope> We compare the algorithm with state-of-the-art algorithms ,  including Locality Sensitive Hashing </scope>  (GTREF) and DivideSkip (REF) . 
<scope> We compare those algorithms to  </scope> generalized iterative scaling (GIS) (REF) ,  non-preconditioned CG ,  and <scope> voted perceptron training </scope>  (TREF) . 
<scope> We compare to using an integrated lexicalized reordering model  </scope> (TREF) ,  a forestto-string translation model (REF) and finally the syntactic pre-ordering technique of REF applied to the phrase-based baseline . 
<scope> We considered a variety of tools like ROUGE </scope>  (TREF) and METEOR (REF)  <scope> but decided they were unsuitable for this task .  </scope>
<scope> We do 10-fold cross validation and use what is referred to in </scope>  (TREF)  <scope> as the KGC kernel </scope>  with SVMlight (REF) in our experiments . 
<scope> We do not believe that the quality of our tags matches that of the better methods of  </scope> REF ,   <scope> much less the recent results of </scope>  TREF . 
<scope> We establish a common ground for interpretation across modalities by relating each layer to a set of ontologies that model categories on which the events ,  states ,  and entities at that layer can be interpreted ,  following recent work in </scope>  information fusion [REF] and  <scope> dialogue systems </scope>  [TREF] . 
<scope> We expect the use of this system to be beneficial :  it employs </scope>  a robust statistical parser (REF) which yields complete though shallow parses ,  and  <scope> a comprehensive SCF classifier ,  which incorporates 163 SCF distinctions ,  a superset of those found in the ANLT </scope>  (TREF) and COMLEX (REF) dictionaries . 
<scope> We experimented with a conditional Markov model tagger that performed well on language-independent NER </scope>  (TREF) and the identification of gene and protein names (REF) . 
<scope> We exploit the focusing mechanism proposed by Sidner </scope>  (REF) (TREF) (REF)  <scope> extending and refining her algorithms .  </scope>
<scope> We find that for virtually all measures and datasets ,  older systems using relatively simple models and algorithms  </scope> (GTREF)  <scope> work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods </scope>  (GREFBerg-REF) . 
<scope> We first pick up off-the-shelf technology ,  in our case </scope>  the rule-based Brill tagger (REF) and  <scope> the statistically-based TNT tagger </scope>  (TREF) ,  both  <scope> trained on newspaper data ,  and run it on medical text data .  </scope>
<scope> We focus specifically on POS induction systems ,  where no prior knowledge is available ,  in contrast to </scope>  POS disambiguation systems (GREF) ,  which use a dictionary to provide possible tags for some or all of the words in the corpus ,  or  <scope> prototype-driven systems </scope>  (TREF) ,   <scope> which use a small set of prototypes for each tag class ,  but no dictionary .  </scope>
<scope> We follow a sign-based approach for the description of linguistic entities based on Headdriven Phrase Structure Grammar  </scope> (REF)  <scope> and the variant described in  </scope> TREF . 
<scope> We follow the work of </scope>  (GTREF) <scope> and choose the hypothesis that best agrees with other hypotheses on average as the backbone  </scope> by applying Minimum Bayes Risk (MBR) decoding (REF) . 
<scope> We formulated this task after the well-studied task of semantic role labeling in English (e . g .  </scope>   ,  GTREF) ,  <scope> whose goal is to assign one of 20 semantic role labels to each phrase in a sentence with respect to a given predicate ,  </scope>  based on the annotations provided by PropBank (REF) . 
<scope> We further  </scope> remove all stopwords using the Snowball stopword list (REF) ,  and <scope> stem all words in the sentence and the norm word list using NLTK </scope>  (TREF) . 
<scope> We generate initializations using OpinionFinder </scope>  (TREF) ,  which were shown to be a reasonable substitute for human annotations in the Movie Reviews dataset (REF) . 6 We consider two additional (baseline) methods for initialization :  using a random set of sentences ,  and using the last 30% of sentence in the document . 
<scope> We have adopted the evaluation method of </scope>  TREF :   <scope> compare the generated hypernyms with hypernyms present in a lexical resource </scope>  ,  in our case the Dutch part of REF . 
<scope> We have constructed a linguistically enriched treebank named Hinoki  </scope> (TREF) ,  which is based on the same framework as the Redwoods treebank (REF) and uses the Japanese grammar JACY (REF) to construct the treebank . 1 In the construction process ,  we have also encountered the problem just mentioned . 
<scope> We have found that if we first tag every word in the corpus with a part of speech using a method such as </scope>  TREF or REF ,   <scope> and then measure associations between tagged words ,  we can identify interesting contrasts between verbs associated with a following preposition to~in and verbs associated with a following infinitive marker to~to .  </scope>
<scope> We have improved the evaluation framework for Russian by using the Russian WordNet </scope>  (REF)  <scope> instead of back-translations as done in </scope>  (TREF) . 
<scope> We have recently </scope>  (TREF)  <scope> evaluated our method on the coarse-grained English all words task at SemEval (REF) .  </scope>
<scope> We have word alignments from three sources </scope>  :  A small set of hand aligned sentences ,  HMM alignments (REF) and  <scope> alignments obtained using a supervised Maximum Entropy aligner </scope>  (TREF)  <scope> trained on the hand alignments .  </scope>
<scope> We here apply the co-training technique </scope>  ,  first proposed by (REF) and then  <scope> successfully leveraged and analyzed in different settings </scope>  (<span class="trefstyle ref40587" id="s40587 . r2">TREF<div class="tooltip fixed" style="left :  521 . 5px ;  top :  26891px ;  display :  none ;  "><span style="color : black"><tref%.
<scope> We integrated keyphrase extraction methods such as TextRank  </scope> (TREF) and KEA (REF) . 
<scope> We model these degrees explicitly using a latent mixture of authors model ,  which takes its inspiration from </scope>  the learning machinery of LDA (REF) and <scope> its supervised variant Labeled LDA </scope>  (TREF) . 
<scope> We obtained word alignments of training data by first running GIZA +  +  </scope>  (TREF) and then applying the refinement rule grow-diag-final-and (REF) . 
<scope> We obtained word alignments of training data </scope>  by first running GIZA +  +  (REF)  <scope> and then applying the refinement rule grow-diag-final-and </scope>  (TREF) . 
<scope> We parsed the Chinese text using the Stanford parser </scope>  (TREF) and the English text using TurboParser (REF) . 
<scope> We performed a comparison between the existing CFG filtering techniques for LTAG (REF) and HPSG (REF) ,  using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank </scope>  (TREF)  <scope> into HPSG-style .  </scope>
<scope> We performed GIZA +  +   </scope> (TREF) and the grow-diag-final-and symmetrizing strategy (REF) on the training set to obtain alignments . 
<scope> We prepared six state-of-the-art abbreviation recognizers as baselines :  </scope>  Schwartz and Hearsts method (SH) (2003) ,  SaRAD (REF) ,  ALICE (REF) ,  Chang and Schutzes method (CS) (REF) ,  Nadeau and Turneys method (NT) (REF) ,  and  <scope> Okazaki et al . s method (OZ)  </scope> (TREF) . 
<scope> We present a comparison of three statistical parsing architectures that output typed dependencies for French :  one constituency-based architecture featuring the Berkeley parser </scope>  (TREF) ,  and two dependency-based systems using radically different parsing methods ,  MSTParser (REF) and MaltParser (REF) . 
<scope> We present fully automated algorithms for constructing intensional summaries using </scope>  knowledge discovery techniques (GREF) ,  and  <scope> decisiontheoretic user models </scope>  (TREF) . 
<scope> We present our work under the construction in </scope>  (TREF) ,  following notation from (REF) ,  extending the formal description to reflect grammars with an arbitrary number of nonterminals in each rule . 
<scope> We present performance results on the REF version of PropBank on gold-standard parse trees as well as results on automatic parses generated by Charniaks parser </scope>  (TREF) . 
<scope> We present results in the form of search error analysis and translation quality as measured by the BLEU score </scope>  (TREF) on the IWSLT 06 text translation task (REF)1 ,  comparing Cube Pruning with our two-pass approach . 
<scope> We propose no new solutions to handling irregular verb forms ,  but suggest using non-string-based techniques ,  such as those presented in </scope>  (REF) ,  (TREF) and (REF) . 
<scope> We replicated the parse reranking experimental setup used for the evaluation of the Tree Kernel in </scope>  (TREF) ,  where the candidate list was provided by the generative probabilistic model (REF) (model 2) . 
<scope> We report case-insensitive scores for  </scope> version 0 . 6 of METEOR (REF) with all modules enabled ,   <scope> version 1 . 04 of IBM-style BLEU </scope>  (TREF) ,  and version 5 of TER (REF) . 
<scope> We report case-insensitive scores for version 0 . 6 of METEOR  </scope> (TREF) with all modules enabled ,  version 1 . 04 of IBM-style BLEU (REF) ,  and version 5 of TER (REF) . 
<scope> We report case-insensitive scores on  </scope> version 0 . 6 of METEOR (REF) with all modules enabled ,   <scope> version 1 . 04 of IBM-style BLEU  </scope> (TREF) ,  and version 5 of TER (REF) . 
<scope> We report results on three metrics ,  Bleu </scope>  (TREF) ,  NIST (REF) ,  and Meteor optimized on fluency/adequacy (REF) . 
<scope> We report the results of topological field parsing of German using the unlexicalized ,  latent variable-based Berkeley parser </scope>  (TREF) Without any languageor model-dependent adaptation ,  we achieve state-of-the-art results on the TuBa-D/Z corpus ,  and a modified NEGRA corpus that has been automatically annotated with topological fields (REF) . 
<scope> We rescore the ASR N-best lists with the standard HMM </scope>  (TREF) and IBM (REF) <scope> MT models .  </scope>
<scope> We review the adaptor grammar generative process below ;  for an informal introduction see </scope>  TREF and for details of the adaptor grammar inference procedure see REF . 
<scope> We </scope>  parse the data using the Collins Parser (REF) ,  and then  <scope> tag person ,  location and organization names using the Stanford Named Entity Recognizer </scope>  (TREF) . 
<scope> We </scope>  use the SVM-Light Toolkit (REF) for the implementation of SVM ,  and <scope> use the Stanford Parser </scope>  (TREF)  <scope> as the parser and the constituent-to-dependency converter .  </scope>
<scope> We </scope>  use the SVM-Light Toolkit version 6 . 02 (REF) for the implementation of SVM ,   <scope> and use the Stanford Parser version 1 . 6  </scope> (TREF) as the constituent parser and the constituent-to-dependency converter . 
<scope> We selected for comparative evaluation three approaches extensively cited in the literature :  Kennedy and Boguraevs parserfree version of Lappin and Leass RAP </scope>  (TREF) ,  Baldwins pronoun resolution method (REF) and Mitkovs knowledge-poor pronoun resolution approach (REF) . 
<scope> We selected for comparative evaluation three approaches extensively cited in the literature :   </scope> Kennedy and Boguraevs parserfree version of Lappin and Leass RAP (REF) ,   <scope> Baldwins pronoun resolution method </scope>  (TREF) and Mitkovs knowledge-poor pronoun resolution approach (REF) . 
<scope> We set all weights by optimizing Bleu </scope>  (REF)  <scope> using minimum error rate training (MERT) </scope>  (TREF)  <scope> on a separate development set of 2 , 000 sentences (Indonesian or Spanish) ,  </scope>  and we used them in a beam search decoder (REF) to translate 2 , 000 test sentences (Indonesian or Spanish) into English . 
<scope> We split the returned documents into classes encompassing </scope>  n-grams (terms of word length n) ,  adjectives (using a part-of-speech tagger (REF)) and  <scope> noun phrases (using a lexical chunker </scope>  (TREF)) . 
<scope> We substitute our language model and use MERT </scope>  (REF)  <scope> to optimize the BLEU score </scope>  (TREF) . 
<scope> We then apply Brills rule-based tagger </scope>  (TREF) and BaseNP noun phrase chunker (REF)  <scope> to extract noun phrases from these sentences .  </scope>
<scope> We then built separate English-to-Spanish and Spanish-to-English directed word alignments </scope>  using IBM model 4 (REF) ,   <scope> combined them using the intersect + grow heuristic </scope>  (TREF) ,  and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (REF) . 
<scope> We then present the algorithms behind three kernels that we are particularly interested in :  </scope>  subsequence kernel (REF) ,   <scope> dependency tree kernel </scope>  (TREF) and shortest path dependency kernel (REF) . 
<scope> We then review the functional grammar formalism  </scope> (REF) that has been <scope> used in other generation systems </scope>  (e . g .   ,  GTREF) . 
<scope> We trained an IBM style translation model  </scope> (REF) <scope> using GIZA +  +  </scope>  (TREF)  <scope> on the 500 test lines used in our experiments paired with corresponding English lines from an online Bible .  </scope>
<scope> We trained our models on the 90-million word written component of the British National Corpus (REF) ,  parsed with the RASP toolkit </scope>  (TREF) . 
<scope> We tune all feature weights automatically </scope>  (TREF) to maximize the BLEU (REF) score on the dev set . 
<scope> We use a program to label syntactic arguments with the roles they are playing </scope>  (TREF) ,  and the rules for complement/adjunct distinction given by (REF)  <scope> to never allow deletion of the complement .  </scope>
<scope> We use a standard maximum entropy classifier </scope>  (TREF)  <scope> implemented as part of MALLET </scope>  (REF) . 
<scope> We used a support vector machine  </scope> (REF)  <scope> with an implementation of the original tree kernel </scope>  (TREF) . 
<scope> We used CRFsbased Japanese dependency parser </scope>  (TREF) and named entity recognizer (REF) for sentiment extraction and constructing feature vectors for readability score ,  respectively . 
<scope> We used minimum error rate training (MERT)  </scope> (TREF)  <scope> to tune the feature weights  </scope> for maximum BLEU (REF) on the development set . 
<scope> We used OntoNotes 3 . 0  </scope> (TREF) ,  and made the same data modifications as (REF) <scope> to ensure consistency between the parsing and named entity annotations .  </scope>
<scope> We used </scope>  GIZA +  +  (REF) to perform word alignment in both directions ,  and  <scope> grow-diag-final-and </scope>  (TREF)  <scope> to generate symmetric word alignment </scope>  . 
<scope> We used the MosesChart decoder  </scope> (TREF) and the Moses toolkit (REF)  <scope> for tuning and decoding .  </scope>
<scope> We use </scope>  Charniaks parser (REF) and  <scope> Berkeleys parser </scope>  (TREF)  <scope> as the </scope>  two  <scope> individual parsers ,  where </scope>  Charniaks parser represents the best performance of the lexicalized model and  <scope> the Berkeleys parser represents the best performance of the un-lexicalized model .  </scope>
<scope> We use </scope>  existing generation resources for English  SURGE [REF] for syntactic realization and  <scope> the lexical chooser described in </scope>  [TREF] and the HUGG grammar for syntactic realization in Hebrew [REF] . 
<scope> We use  </scope> MXPOST tagger (REF) for POS tagging ,   <scope> Charniak parser </scope>  (TREF)  <scope> for extracting syntactic relations </scope>  ,  and David Blei ? s version of LDA1 for LDA training and inference . 
<scope> We use the IBM Model 1 </scope>  (TREF)  <scope> (uniform distribution) </scope>  and the Hidden Markov Model (HMM ,  first-order dependency ,  (REF))  <scope> to estimate the alignment model </scope>  . 
<scope> We use the LTAG-spinal treebank described in (REF) ,  which was extracted from the Penn Treebank (PTB) (REF) with Propbank </scope>  (TREF)  <scope> annotations .  </scope>
<scope> We use the same preprocessing steps as </scope>  TREF <scope>  :  during both training and testing ,  the parser is given text POS-tagged by the tagger of REF ,  with capitalization stripped and outermost punctuation removed .  </scope>
<scope> We use the uncontrolled keyphrases for evaluation as proposed in (REF) and followed by </scope>  (TREF) . 
<scope> We use the Waterloo MultiText System </scope>  (REF)  <scope> to search in a corpus of about 10105  English words </scope>  (TREF) . 
<scope> We use TinySVM2 along with YamCha3 </scope>  (REF) (TREF) as the SVM training and classification software . 
<scope> We will also study the effect of other window sizes and the combination of this unsupervised approach with minimally-supervised approaches such as </scope>  (REF) (TREF) . 
<scope> We will investigate the utility of more complex error detection algorithms such as the ones described in </scope>  (TREF) and (REF) . 
<scope> We wish to apply this direct ,  Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT) ,  by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees </scope>  (GTREF) ,  as opposed to formally syntactic systems such as Hiero (REF) . 
<scope> We word-aligned the Chinese-English parallel text using GIZA +  +  followed by link deletion </scope>  (TREF) ,  and the Arabic-English parallel text using a combination of GIZA +  +  and LEAF (REF) . 
<scope> We word-align the corpus with Giza +  +  </scope>  (TREF) ,  and then apply the (completely unsupervised) 841 algorithm of REF ,  which extracts MWE candidates from the aligned corpus and re-ranks them using statistics computed from a large monolingual corpus . 
<scope> When compared with other supertag sets of automatically extracted lexicalized grammars ,  the (effective) size of our supertag set ,  1 , 361 lexical entries ,  is between the CCG supertag set (398 categories) used by  </scope> TREF and the LTAG supertag set (2920 elementary trees) used by REF . 
<scope> When deciding on pruning an argument ,  previous approaches either used a set of hand-crafted rules </scope>  (e . g .  TREF) ,  or utilized a subcategorization lexicon (e . g .  REF) . 
<scope> When examples are labeled automatically ,  </scope>  through user feedback (REF) or  <scope> from textual pseudo-examples </scope>  (GTREF) ,  <scope> faster learning can reduce the lag before a new system is useful .  </scope>
<scope> When </scope>  Gaussian smoothing (REF) ,  labeled as  + Gau ,  and  <scope> postprocessing </scope>  (TREF) ,   <scope> labeled as  + post </scope>  ,   <scope> are added ,  we observe 17 . 66% relative improvement (or 3 . 85% absolute) over the previous best f-score of 78 . 2 from Kahn et al .  </scope>
<scope> While a similar direction has been previously explored in  </scope> (GTREF) ,  the recent work of (REF) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions . 
<scope> While chart parsing can famously be cast as deduction </scope>  (REF) ,   <scope> what chart parsing really is is an algebraic closure over the rules of a phrase structure grammar ,  which is most naturally expressed inside a constraint solver such as CHR </scope>  (TREF) . 
<scope> While EM has worked quite well for a few tasks ,  notably machine translations </scope>  (starting with the IBM models 1-5 (REF) <scope>  ,  it has not had success in most others ,  such as </scope>  part-of-speech tagging (REF) ,   <scope> named-entity recognition  </scope> (TREF) and context-free-grammar induction (numerous attempts ,  too many to mention) . 
<scope> While large factors are desirable for capturing sophisticated linguistic constraints ,  they come at the cost of time complexity :  for the projective case ,  adaptations of Eisners algorithm </scope>  (TREF)  <scope> are O(n3) for 1-edge factors </scope>  (REF)  <scope> or sibling 2-edge factors </scope>  (REF) <scope>  ,  and O(n4) for general 2-edge factors </scope>  (REF) or 3-edge factors (REF) . 
<scope> While many works (GREF) view the properties of positivity and negativity as categorical (i . e .   ,  a term is either positive or it is not) ,  others </scope>  (GTREF)  <scope> view them as graded (i . e .   ,  a term may be positive to a certain degree) ,  with the underlying interpretation varying from fuzzy to probabilistic .  </scope>
<scope> Whileour performance of 89 . 4% f-measure needs improvement before it would be worth-while using this parser for routine work ,  it has moved past the accuracy of the Collins-Bikel  </scope> (GTREF)  <scope> parser  </scope> and is not statistically distinguishable from (REF) . 
<scope> While reranking has benefited many tagging and parsing tasks </scope>  (GTREF) including semantic role labeling (REF) ,   <scope> it has not yet been applied to semantic parsing .  </scope>
<scope> While significant time savings have already been reported on the basis of automatic pre-tagging (e . g .   ,  for POS and parse tree taggings in the Penn TreeBank </scope>  (TREF) ,  or named entity taggings for the Genia corpus (REF)) ,  this kind of pre-processing does not reduce the number of text tokens actually to be considered . 
<scope> While string-based approaches </scope>  (GTREF)  <scope> are too restrictive to cover the wide variation within the correct contexts ,  </scope>  bag-of-words approaches such as REF are too permissive and would miss many of the distinctions between correct and incorrect contexts . 
<scope> While theoretically sound ,  this approach </scope>  is computationally challenging both in practice (REF) and in theory (REF) ,   <scope> may suffer from reference reachability problems  </scope> (TREF) ,  and in the end may lead to inferior translation quality (REF) . 
<scope> While there are also other principles proposed in the literature </scope>  ,  including the Minimal Attachment Principle (REF) ,   <scope> they are generally either not highly functional or are covered by the above three principles in any case </scope>  (GTREF) . 
<scope> While there is no prior work on generating textual descriptions of geo-referenced data ,  there have been studies on describing spatial data in the context of </scope>  route directions ((REF) ,  (REF)) ,  scene descriptions (REF) ,   <scope> geometric descriptions </scope>  (TREF) and spatial descriptions (REF) . 
<scope> While the work by Kleinbauer et al .  is among the earliest research on abstracting multi-party dialogues ,  much attention in recent years has been paid to extractive summarization of such conversations ,  including meetings  </scope> (TREF) ,  emails (GREF) ,  telephone conversations (REF) and internet relay chats (REF</span>) . 
<scope> While traditional LM research focuses on </scope>  how to make the model smarter via how to better estimate the probability of unseen words (REF) ;  and  <scope> how to model the grammatical structure of language </scope>  (e . g .  ,  TREF) ,   <scope> recent studies show that significant improvements can be achieved using stupid n-gram models trained on very large corpora (e . g .  ,  REF) .  </scope>
<scope> While various machine learning approaches </scope>  ,  such as generative modeling (REF) ,  maximum entropy (REF) and  <scope> support vector machines </scope>  (GTREF) ,   <scope> have been applied in the relation extraction task ,  no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations </scope>  . 
<scope> While various machine learning approaches ,  such as generative modeling  </scope> (TREF) ,  maximum entropy (REF) and support vector machines (GREF) ,   <scope> have been applied in the relation extraction task ,  no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations .  </scope>
<scope> While we have demonstrated exhaustive parsing efficiency ,  our model could be used with any of the efficient search best-first approaches documented in the literature ,  from those used in the Charniak parser  </scope> (GREF)  <scope> to A parsing </scope>  (TREF) . 
<scope> Widely used dependency parsers which generate deep dependency representations include </scope>  Minipar (REF) ,  which uses a declarative grammar ,  and  <scope> the Stanford parser </scope>  (TREF) ,   <scope> which performs a conversion from a standard phrase-structure parse .  </scope>
<scope> With a few exceptions (e . g .   ,   </scope> GTREF)  <scope> NLG researchers have interpreted CT as a theory of pronominalisation only (e . g .   ,  REF) .  </scope>
<scope> With different training corpus sizes ,  they focus on translation into English from Arabic </scope>  (GTREF) ,  Czech (GREF) ,  German (REF) or Catalan ,  Spanish and Serbian (REF) . 
<scope> With non-local features ,  we cannot use efcient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training  </scope> CRFs (REF) and <scope> perceptrons  </scope> (TREF) . 
<scope> Without them ,  the model is arc-factored ,  and exact inference in it is well studied :  finding the most probable parse tree takes O(n3) time with the ChuLiu-Edmonds algorithm </scope>  (REF) , 2 and computing posterior marginals for all arcs takes O(n3) time via the matrix-tree theorem (GTREF) . 
<scope> Word-aligned corpora have been found to be an excellent source for translation-related knowledge ,  not only for phrase-based models </scope>  (GTREF) ,  but also for syntax-based models (e . g .  ,  (GREF)) . 
<scope> Word-alignment information can be estimated from alignment models ,  such as </scope>  the IBM alignment models (<span id="s9557 . r1" class="refstyle ref9557">REF</span>) and <font style="BACKGROUND-COLOR :  yellow">HMM-based alignment models </font>(<span id="s9557 . r2" class="trefstyle ref9557">GTREF</span>) . 
<scope> Word-level alignment is a critical component of a wide range of NLP applications ,  such as construction  </scope> of bilingual lexicons (REF) ,   <scope> word sense disambiguation </scope>  (TREF) ,  projection of language resources (REF) ,  and statistical machine translation . 
<scope> WordNet-based measures consider two terms to be close if they occur close to each other in the network (connected by only a few arcs) ,  if their definitions share many terms </scope>  (GTREF) ,  or if they share a lot of information (GREF) . 
<scope> Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a  </scope> list of recurrent light verbs (REF) or a <scope> list of nominalizations  </scope> (GTREF) . 
<scope> Work has been done on detecting relations between </scope>  noun phrases (GREF) ,   <scope> named entities </scope>  (TREF) ,  and clauses (REF) . 
<scope> Work on acquisition of syntactic information from text corpora includes Brent's </scope>  (TREF)  <scope> verb subcategorization frame recognition technique </scope>  and Smadja's (REF) collocation acquisition algorithm . 
<scope> XLE/ParGram </scope>  (TREF ,  see also REF)  <scope> applies a hand-built Lexical Functional Grammar for English and a stochastic parse selection model .  </scope>
Secondly ,   <scope> The GHKM algorithm  </scope> (REF) ,  which is originally developed for extracting tree-to-string rules from 1-best trees ,  <scope> has been successfully extended to packed forests recently </scope>  (TREF) . 
Secondly ,   <scope> The GHKM algorithm </scope>  (TREF) ,   <scope> which is originally developed for extracting tree-to-string rules from 1-best trees ,  </scope>  has been successfully extended to packed forests recently (REF) . 
Secondly ,  we can construct automatic part-ofspeech taggers and process untagged corpora (GREF) ;  this method boasts a high degree of accuracy ,  although often  <scope> the construction of the automatic tagger involves a bootstrapping process based on a core corpus which has been manually tagged </scope>  (TREF) . 
Semi-supervised dependency parsing has attracted a lot of attention recently (GREF) ,  but  <scope> there has ,  to the best of our knowledge ,  been no previous attempts to apply tri-training or related combinations of ensemble-based and semisupervised methods to any of these tasks ,  except for the work of </scope>  TREF  <scope> discussed in Sect .  </scope>
Since part of the chunking errors could be caused by POS errors ,  we also compared the same baseNP chunker on the santo corpus tagged with i) the Brill tagger as used in [REF] ,  ii)  <scope> the Memory-Based Tagger (MBT) as described in </scope>  [TREF] . 
Since the success of phrase-based methods ( <scope> have also achieved state-of-the-art performance .  </scope>
Since we need to annotate new sentences with syntactic parse ,  POS tags and shallow parses ,  we do not use annotations in the CoNLL distribution ;  instead ,   <scope> we re-annotate the data using publicly available part of speech tagger and shallow parser1 ,  </scope>  REF  <scope> parser </scope>  (TREF)  <scope> and Stanford parser </scope>  (REF) . 
So far we are aware of only one English NomBank-based SRL system (REF) ,  which uses the maximum entropy classifier ,  although  <scope> similar efforts are reported on the Chinese NomBank by </scope>  (TREF) 208 and on FrameNet by (REF) using a small set of hand-selected nominalizations . 
Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (GREF) ,   <scope> others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed </scope>  (GTREF) . 
Some of these component models were also evaluated on other Senseval-3 tasks :  the Basque ,  Catalan ,  Italian ,  and Romanian Lexical Sample tasks (REF) ,  as well as  <scope> Semantic Role Labeling </scope>  (TREF) . 
Some of these methods make use of prior knowledge in the form of an existing thesaurus (GREF) ,  while  <scope> others do not rely on any prior knowledge </scope>  (GTREF) . 
Some of these systems use features based on syntactic constituents produced by a Charniak parser (GREF) and others use only a  <scope> flat syntactic representation produced by a syntactic chunker </scope>  (GTREF) . 
Some researchers simply use the first sense (REF) or all possible senses (REF) ,  while  <scope> others overcome this problem with word sense disambiguation </scope>  (TREF) . 
Some studies approach the problem with semantic vector comparisons in the style of REF ,  e . g TREF and Cook et al . 
<SPAN id=s17783 . r1 class="trefstyle ref17783">TREF<DIV style="DISPLAY :  none" class="tooltip fixed"><SPAN style="COLOR :  black">Riley (1989)</TREF></SPAN></DIV></SPAN><INPUT name=ref[17783][1] value=0 type=hidden><INPUT id=cb17783-1 name=ref[17783][1] value=1 CHECKED type=checkbox> <FONT style="BACKGROUND-COLOR :  yellow">determined that in the Tagged Brown corpus (</FONT><SPAN id=s17783 . r2 class="refstyle ref17783"><FONT style="BACKGROUND-COLOR :  yellow">REF</FONT><DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  1551px ;  LEFT :  557px" class="tooltip fixed"><FONT style="BACKGROUND-COLOR :  yellow">Francis and Kucera ,  1982</REF></FONT></DIV></SPAN><INPUT name=ref[17783][2] value=0 type=hidden><FONT style="BACKGROUND-COLOR :  yellow"><INPUT id=cb17783-2 name=ref[17783][2] value=1 type=checkbox>) about 90% of pe78 riods occur at the end of sentences ,  10% at the end of abbreviations ,  and about 0 . 5% as both abbreviations and sentence delimiters . </FONT>.
(<SPAN id=s60276 . r1 class="refstyle ref60276">REF<DIV style="DISPLAY :  none" class="tooltip fixed"><REF>Turney ,  2002</REF></DIV><DIV style="DISPLAY :  none" class="tooltip fixed"><REF>Turney ,  2002</REF></DIV><DIV style="DISPLAY :  none ;  TOP :  642px ;  LEFT :  161px" class="tooltip fixed"><REF>Turney ,  2002</REF></DIV><DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  616px ;  LEFT :  160px" class="tooltip fixed">Turney ,  2002</REF></DIV></SPAN><INPUT name=ref[60276][1] value=0 type=hidden><INPUT id=cb60276-1 name=ref[60276][1] value=1 CHECKED type=checkbox>) used patterns representing part-of-speech sequences ,  (<SPAN id=s60276 . r2 class="trefstyle ref60276">TREF<DIV style="DISPLAY :  none" class="tooltip fixed"><SPAN style="COLOR :  black"><TREF>Hatzivassiloglou and McKeown ,  1997</TREF></SPAN></DIV><DIV style="DISPLAY :  none" class="tooltip fixed"><SPAN style="COLOR :  black"><TREF>Hatzivassiloglou and McKeown ,  1997</TREF></SPAN></DIV><DIV style="DISPLAY :  none" class="tooltip fixed"><SPAN style="COLOR :  black"><TREF>Hatzivassiloglou and McKeown ,  1997</TREF></SPAN></DIV><DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  616px ;  LEFT :  583px" class="tooltip fixed"><SPAN style="COLOR :  black">Hatzivassiloglou and McKeown ,  1997</TREF></SPAN></DIV></SPAN><INPUT name=ref[60276][2] value=0 type=hidden><INPUT id=cb60276-2 name=ref[60276][2] value=1 type=checkbox>) <FONT style="BACKGROUND-COLOR :  yellow">recognized adjectival phrases</FONT> ,  and (<SPAN id=s60276 . r3 class="refstyle ref60276">REF<DIV style="DISPLAY :  none ;  TOP :  663px ;  LEFT :  248px" class="tooltip fixed"><REF>Wiebe et al .   ,  2001</REF></DIV><DIV style="DISPLAY :  none ;  TOP :  663px ;  LEFT :  248px" class="tooltip fixed"><REF>Wiebe et al .   ,  2001</REF></DIV><DIV style="DISPLAY :  none ;  TOP :  663px ;  LEFT :  248px" class="tooltip fixed"><REF>Wiebe et al .   ,  2001</REF></DIV><DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  636px ;  LEFT :  246px" class="tooltip fixed">Wiebe et al .   ,  2001</REF></DIV></SPAN><INPUT name=ref[60276][3] value=0 type=hidden><INPUT id=cb60276-3 name=ref[60276][3] value=1 CHECKED type=checkbox>) learned N-grams . 
<span style="background-color :  yellow ; ">          Thus ,  we employ an overgenerate-andrank approach ,  which has been applie.
SPE rules must be restricted so as not to apply to their own output (REF) and  <scope> there is no guarantee that the transducer encoding an SPE rule can be expressed using the two-level rule notation </scope>  (TREF) . 
Straightforward implementations of the algorithm have proved too timeconsuming (REF) and  <scope> efforts have been made to alter the algorithm to improve efficiency </scope>  (TREF) . 
Sub-events (REF) and  <scope> sub-topics </scope>  (TREF)  <scope> also contribute to the framework used for comparing documents in multidocument summarization .  </scope>
Support Vector Machines (SVMs) (REF) and Maximum Entropy (ME) method (REF) are powerful learning methods that satisfy such requirements ,   <scope> and are applied successfully to other NLP tasks </scope>  (GTREF) . 
Support Vector Machines (SVMs) (REF) and  <scope> Maximum Entropy (ME) method </scope>  (TREF)  <scope> are powerful learning methods that satisfy such requirements ,  and are applied successfully to other NLP tasks  </scope> (GREF) . 
Table 4 also shows several unsupervised systems ,  all of which except Cymfony and (REF) participated in S3LS (check (TREF) for further details on the systems) . 
Taking the advantage of recent release of Chinese PropBank (REF) and Chinese NomBank (TREF) ,   <scope> Xue and his colleagues </scope>  (GTREF)  <scope> pioneered the exploration of Chinese verbal and nominal SRLs ,  given golden predicates </scope>  . 
The approach presented here has some resemblance to the bracketing transduction grammars (BTG) of (REF) ,   <scope> which have been applied to a phrase-based machine translation system in  </scope> (TREF) . 
The availability of annotated corpora like PropBank and FrameNet (REF) have provided rapid development of research into <scope> SRL </scope>  (GTREF) . 
The combined training corpus from which we extracted our grammar consisted of 123 , 609 sentence pairs ,  which was then filtered for length and aligned using the GIZA +  +  implementation of IBM Model 4 (REF) to obtain one-to-many alignments in either direction and  <scope> symmetrized using the grow-diag-final-and method </scope>  (TREF) . 
The context of a word can be described by the sentence in which it occurs (TREF) or a surrounding word-window (GREF) . 
The corpora are :  the Hansard corpus (TREF)  <scope> of English/French Canadian Parliamentary proceedings (En-Fr) </scope>  ,  and the English/Spanish portion of the Europarl corpus (REF) where the annotation is from EPPS (REF) (En-Es) using standard test and development set split . 
The corpus was sentence-aligned statistically (REF) ;   <scope> Chinese words and collocations were extracted </scope>  (Fung and GTREF) ;  then translation pairs were learned via an EM procedure (REF) . 
The creation of the Penn English Treebank (REF) ,  a syntactically interpreted corpus ,  played a crucial role in  <scope> the advances in natural language parsing technology </scope>  (GTREF) <scope> for English .  </scope>
The data consists of sections of the Wall Street Journal part of the Penn TreeBank (REF) ,   <scope> with information on predicate-argument structures extracted from the PropBank corpus </scope>  (TREF) . 
The EDPM metric (REF) improves this line of research by using arc labels derived from a Probabilistic Context-Free Grammar (PCFG) parse to replace the LFG labels ,  showing that  <scope> a PCFG parser is sufficient for preprocessing ,  compared to a dependency parser in </scope>  (REF) and (TREF) . 
The features we used are as follows :   word posterior probability (REF) ;   3 ,  4-gram target language model ;   word length penalty ;   Null word length penalty ;   <scope> We use MERT </scope>  (TREF)  <scope> to tune the weights of the CN .  </scope>
The first dataset is a subset of the ICSI-MR corpus (REF) ,  where the gold standard for thematic segmentations has been provided by taking into account the agreement of at least three <scope> human annotators  </scope> (TREF) . 
The first one is a hypotheses testing approach (GREF) while <scope> the second one is closer to a model estimating approach </scope>  (GTREF) . 
The first one (SYS1) is a hierarchical phrase-based system (REF) based on Synchronous Context Free Grammar (SCFG) ,   <scope> and the second one (SYS2) is a phrasal system </scope>  (TREF) b <scope> ased on Bracketing Transduction Grammar </scope>  (REF)  <scope> with a lexicalized reordering component based on maximum entropy model .  </scope>
The general idea of exploiting frequency correlations to acquire word translations from comparable corpora has been explored in several previous studies (e . g .   ,  (GREF)) . Recently ,  a method based on Pearson correlation was proposed to mine word pairs from comparable corpora (REF) ,   <scope> an idea similar to the method used in </scope>  (TREF)  <scope> for sentence alignment .  </scope>
The generalisation of vibhakti to account for word order essentially introduces a facility analogous to topological fields ( <scope> as found in Topological Dependency Grammar </scope>  (TREF) and DACHS (REF)) to PGF . 
The integrated memory mechanism is motivated by anaphoric resolution mechanism in Categorial Type Logic (GREF) ,  Type Logical Grammar (GREF) ,  and CCG (REF) ,  and <scope> gap resolution in MemoryInductive Categorial Grammar  </scope> (TREF) ,  as it is designed for associating fillers and gaps found in an input sentence . 
The lower section of Table 1 reports the results achieved by our best model on the test data set and compare them both to those obtained by the only  <scope> unlexicalised dependency model we know of </scope>  (TREF) and to those achieved by the stateof-the-art dependency parser in (REF) . 
The marked difference in the availability of monolingual vs parallel corpora has led several researchers to  <scope> develop methods for automatically learning bilingual lexicons </scope>  ,  either  <scope> by using monolingual corpora </scope>  (GTREF) or by exploiting the cross-language evidence of closely related bridge languages that have more resources (REF) . 
The model is trained by gradient ascent using  <scope> the l-BFGS method </scope>  (REF) ,   <scope> which has been successfully used for training log linear models </scope>  (TREF)  <scope> in many natural language tasks ,  including alignment .  </scope>
The original approach (REF) was limited due to computational constraints but  <scope> recent work </scope>  (GTREF)  <scope> has improved the efficiency by using word alignments as constraints on the set of possible phrase pairs .  </scope>
The parser of (REF) received sentence as input and conducted word segmentation and syntactic parsing at the same time ,  but they did not utilize the character information in generating subtree ;  (TREF)s <scope> dependency parsing tree totally abandoned the word concept ,  so the dependency relations are the relations between characters .  </scope>
The parser we use is the incremental parser of (REF) ,   <scope> POS tags are induced using the unsupervised POS tagger of  </scope> ((TREF) ,  neyessenmorph model) . 
The performance of VE on BR87 is on par with other  <scope> state-of-the-art semi-supervised segmentation algorithms such as WordEnds </scope>  (TREF) and HDP (REF) . 
The Prague Dependency Treebank also contains annotation for light verb constructions (REF) and  <scope> NomBank </scope>  (TREF)  <scope> provides the argument structure of common nouns ,  paying attention to those occurring in support verb constructionsaswell .  </scope>
The properties of the formalism are well established (REF) ,  and  <scope> the research has also led to the development of </scope>  a large standard grammar (XTAG REF) ,  and  <scope> a parser XTAG </scope>  (TREF) . 
There are basically two kinds of systems working at these segmentation levels :  <scope> the most widespread rely on statistical models ,  in particular the IBM ones  </scope> (TREF) ;  others combine simpler association measures with different kinds of linguistic information (GREF) . 
There are three major types of models :   <scope> Heuristic models as in  </scope> (TREF) ,  generative models as the IBM models (REF) and discriminative models (GREF) . 
Therefore ,  for both Chinese and English SRL systems ,  we use the 3-best parse trees of Berkeley parser (REF) and  <scope> 1-best parse trees of Bikel parser  </scope> (TREF) and Stanford parser (REF) as inputs . 
Therefore ,   <scope> CFGs used in spoken-dialogue applications often represent regular languages </scope>  (GTREF) ,  either by construction or as a result of a finite-state approximation of  more general CFG (REF) . 
Therefore ,  <scope> various 'quasi-destructive' algorithms </scope>  (GTREF) and algorithms using 'skeletal' DACS with updates (GREF)  <scope> have been proposed ,  which attempt to minimize copying .  </scope>
Therefore ,  various 'quasi-destructive' algorithms (GTREF) and algorithms using 'skeletal' DACS with updates (GREF)  <scope> have been proposed ,  which attempt to minimize copying .  </scope>
There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings ,  ranging from putting priors over grammar probabilities (REF) to putting non-parametric priors over derivations (REF) to  <scope> learning the set of states in a grammar </scope>  (GTREF) . 
There has been work on detecting relations within noun phrases (GREF) ,  clauses (REF) and  <scope> syntax-based comma resolution </scope>  (TREF) . 
There is work on discriminative models for dependency parsing (REF) ;   <scope> since there are efficient decoding algorithms available </scope>  (TREF) ,  complete resources such as the Penn Treebank can used for estimation ,  leading to accurate parsers . 
The relation between bridging descriptions and their anchors may be arbitrarily complex (GREF) ,  and the same description may relate to different anchors in a text :   <scope> this makes it difficult to decide what the intended anchor and the intended link are  </scope> (TREF) . 
The re-ranking algorithms include rescoring (REF) and  <scope> Minimum Bayes-Risk (MBR) decoding </scope>  (GTREF) . 
The resolution of this problem can lead to a complete ,  realistic and coherent analysis of the natural language ,  therefore  <scope> major attention is drawn to the opinion ,  sentiment and emotion analysis ,  and to the identification of beliefs ,  thoughts ,  feelings and judgments </scope>  (REF) ,  (TREF) . 
The rules are then treated as events in a relative frequency estimate . 4  <scope> We used Giza +  +  Model 4 to obtain word alignments  </scope> (TREF) ,  using the grow-diag-final-and heuristic to symmetrise the two directional predictions (REF) . 
These approaches either use examples from the same language ,  e . g .  ,  (GREF) ,  or  <scope> they try to imitate the parse of a given sentence using the parse of the corresponding sentence in some other language  </scope> (GTREF) . 
These frequencies can be used to calculate probability estimates P(c | a1a2 an) for each category c .   <scope> Tries have previously been used in both supervised </scope>  (TREF) and unsupervised (REF) <scope> named entity recognition .  </scope>
These include Variation of Information (VI) (REF) ,   <scope> V-measure </scope>  (TREF) ,  and their respective variants NVI (REF) and V-beta (REF) . 
These models ,  for example ,  include Restricted Boltzmann Machines (GREF) and Sigmoid Belief Networks (SBNs) (REF) for classification and regression tasks ,  Factorial HMMs (REF) for sequence labeling problems ,  <scope> Incremental SBNs for parsing problems  </scope> (TREF) ,  1Among the versions which do not exploit labeled data from the target domain . 
The sibling features we use are similar to those of (REF) ,  and  <scope> the grandchildren features are similar to those of </scope>  (TREF) . 
The simplest of these (REF) make no use of information from syntactic theories or syntactic annotations ,  whereas  <scope> others have successfully incorporated syntactic information on the </scope>  target side (GREF) or  <scope> the source side </scope>  (GTREF) . 
The string-to-tree (REF) and tree-to-tree (REF) methods  <scope> have also been the subject of experimentation ,  as well as other formalisms such as Dependency Trees </scope>  (TREF) . 
The techniques proposed in the literature fall under three categories :  rule-based (GREF) ,  machine learningbased (O .  GREF) and  <scope> hybrid solutions  </scope> (GTREF) . 
The text is parsed using the RASP parser (REF) ,  and  <scope> subcategorizations are extracted using the system of </scope>  TREF . 
The two grammar systems to compare are Link Grammar (REF) and t <scope> he Conexor Functional Dependency Grammar parser </scope>  (TREF) (henceforth referred to as Conexor FDG) . 
The weak equivalence of TAGs ,  a modified version of Head Grammars and Linear Indexed Grammars has been shown in REF ,  while  <scope> the weak equivalence of these three formalisms and Combinatory Categorial Grammars has been shown in </scope>  GTREF . 
They are used to help information retrieval (REF) ,  machine or  <scope> semi-automated translation </scope>  ,  (GTREF) or generation (REF) . 
They either tried to improve semantic relation acquisition by putting the different evidence together into a single classifier (TREF) or to improve the coverage of semantic relations by combining and ranking the semantic relations obtained from two source texts (REF) . 
They have proved successful in the classification of German (Schulte im REF) and  <scope> English verbs  </scope> (TREF) . 
They have  <scope> discussed simple sentenses </scope>  (GTREF) ,  dialogues (REF) ,  stereotypical lead sentences of newspaper articles (REF) ,  intrasentential resolution (GREF) or organization names in newspaper articles (REF) . 
They may rely only on this information (e . g .  ,  (GREF)) ,  or  <scope> they may combine it with additional information as well </scope>  (e . g .  ,  (GTREF)) . 
This accuracy compares very favourably with results reported in (de GREF) for instance ,  to reach the recall of 99 . 3 % ,   <scope> the system by ( </scope> TREF) <scope> has to leave as many as three readings per word in its output .  </scope>
This approach is quite different from those adopted for the translation of single words (GREF) ,  since for single words polysemy cannot be ignored ;  indeed ,  the problem of sense disambiguation has been linked to  <scope> the problem of translating ambiguous words </scope>  (GTREF) . 
This approach is quite different from those adopted for the translation of single words (GREF) ,  since for single words polysemy cannot be ignored ;   <scope> indeed ,  the problem of sense disambiguation has been linked to the problem of translating ambiguous words </scope>  (GTREF) . 
This discussion will be based on an MT prototype under development at ISSCO Geneva (REF) which employs a  <scope> grammar development tool for unification grammars known as UD ,  or Unification Device  </scope> (TREF) . 
This idea has been developed and applied to  <scope> a wide variety tasks ,  including morphological analysis </scope>  (GTREF) ,  part-of-speech induction (GREF) ,  and grammar induction (GREF) . 
This includes work on phrasestructure parsing (GREF) ,  dependency parsing (GREF) as well as  <scope> a number of other formalisms </scope>  (GTREF) . 
This metric is broadly comparable to the predicateargument dependencies of CCGBank (REF) or of the ENJU grammar (REF) ,  and also somewhat similar to the  <scope> grammatical relations (GR) of the </scope>  TREF  <scope> version of DepBank .  </scope>
This paper shows that the OMT technique to apply SVD that we proposed in (REF) compares favorably to  <scope> SMA ,  which has been previously used in </scope>  (TREF) ,  and that k-NN excels SVM on the features from the reduced space . 
This perspective provides a general framework into which many existing GRE approaches fit :  Traditional attribute selection (REF) corresponds to building DL formulas that are conjunctions of atoms ;   <scope> relational REs as in  </scope> TREF  <scope> are formulas of EL </scope>  ;  and so on . 
This  <scope> discriminative property is shared by the methods of </scope>  (GTREF) ,  and also the Conditional Random Field methods of (REF) . 
This was done in works by (REF) and (TREF) ,   <scope> which substituted relatives or similar words in place of the target word within a given context .  </scope>
Though this rule is not cyclic ,  it becomes cyclic upon off-line abstraction :  magic_vp (VForm ,  [CSem I_3 ,  SSem)  :  magic_vp (VForm ,  [CSem2l_] ,  SSem)  .  <scope> Through trimming this magic rule ,  e . g .  ,  </scope>  given a bounded term depth (REF) or  <scope> a restrictor </scope>  (TREF) <scope>  ,  constructing an abstract unfolding tree reveals the fact that a cycle results from the magic rule .  </scope>
To gather training examples from these parallel texts ,  we used the approach we described in (TREF) and (REF) . 
Tokenisation ,  species word identification and chunking were implemented in-house using the LTXML2 tools (REF) ,  whilst abbreviation extraction used the Schwartz and Hearst abbreviation extractor (REF) and lemmatisation used  <scope> morpha </scope>  (TREF) . 
To make sense tagging more precise ,  it is advisable to place constraint on the translation counterpart c of w .  <scope> SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm </scope>  (TREF) and logarithmic likelihood ratio (REF) . 
To provide semantics for nouns ,  we use CoreLex (REF) ,  in turn based on the <scope> generative lexicon </scope> (TREF) . 
To this end ,   <scope> we adopt the principles of default inheritance (REF) ,  as embodied in the DATR language </scope>  (TREF) . 
Translation spotting finds different applications ,  for example in bilingual concordancers ,  such as the TransSearch system (REF) ,  and  <scope> example-based machine translation </scope>  (TREF) . 
TREF and 484 REF  <scope> used Wikipedias category information in the disambiguation process .  </scope>
(TREF) and (REF)  <scope> modify the distortion model of the HMM alignment model </scope>  (REF)  <scope> to reflect tree distance rather than string distance </scope>  ;  (REF) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints . 
(TREF)) ,  and the phrase-based approach to Statistical Machine Translation (REF)  <scope> has led to the development of heuristics for obtaining alignments between phrases of any number of words .  </scope>
(TREF) and ,  to a much more limited extent ,  (REF)  <scope> are the only researchers we are aware of who used function labels for parsing .  </scope>
(TREF) and (TREF) <scope> defined several classes of structured features that were successfully employed with tree kernels for the different stages of an SRL process .  </scope>
TREF as well as REF  <scope> made use of synchronous grammars to transform a variablefree tree-structured meaning representation into sentences </scope>  . 
TREF<input type="hidden" value="0" name="ref[14908][1]"><input type="chec.
(TREF)  <scope> also reports a result of 81 . 9% for a word only version of the system (REF) that we extend </scope>  (difference with our result is statistically significant at the 4% level) . 
(TREF) <scope> and many subsequent works) ,  or uses the position of characters in a word as the basis for segmentation </scope>  (REF) . 
(TREF)  <scope> and (REF) use hard syntactic constraints to perform reorderings according to source-side parse trees .  </scope>
TREF  <scope> back off words in n-grams (and semantic relations) to their respective POS tags (e . g .  ,  great-movie to adjective-noun) </scope>  ;  REF propose a method by only backing off the head word in dependency relation pairs to its POS tag (e . g .  ,  great-movie to great-noun) ,  which are supposed to be more generalized than word pairs . 
TREF  <scope> began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers ,  </scope>  followed by REF ,  among others . 
TREF  <scope> calls this trade-off specificity </scope>  ;  equivalent observations were made by GREFwho refer to the tendency for large windows to wash out ,  smear or defocus those associations exhibited at smaller scales . 
(TREF)  <scope> compare two tagging frameworks for tagging French </scope>  ,  one that is statistical ,  built upon the Xerox tagger (REF) ,  and another based on linguistic constraints only . 
TREF  <scope> compiled a list of speculation keywords from the examples in </scope>  (REF) and extended this list by using WordNet (REF) and UMLS SPECIALIST Lexicon (REF) . 
TREF  <scope> describes extensions to a chart structure for generation originally presented in </scope>  (REF) that is used to generate multiple paraphrases from a semantic input . 
TREF  <scope> describes this technique in detail </scope>  ,  while Pinkham &amp ;  Corston-REF describes its integration with MSR-MT and investigates its effect on translation quality . 
TREF <scope> describe the supertagger ,  which uses log-linear models to define a distribution over the lexical category set for each local five-word context containing the target word </scope>  (REF) . 
TREF  <scope> examined randomized tests for estimating the signi cance of F scores ,  and in particular the bootstrap over the test set </scope>  (GREF) . 
TREF  <scope> extended the work of REF for largescale anaphoricity determination by additionally detecting non-anaphoric instances of it using Minipars pleonastic category Subj </scope>  . 
TREF  <scope> extend </scope>  REF  <scope> to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling .  </scope>
TREF  <scope> extend this by using latent semantic analysis (REF) to require that a proposed stem + affix split is sufficiently semantically similar to the stem before the split is accepted .  </scope>
TREF  <scope> found that this feature improved their English verb clusters ,  </scope>  but in REF ,  we found this feature to not contribute significantly to Arabic verb clustering quality . 
TREF  <scope> give a rough comparison of RASP with the Parc LFG parser </scope>  (REF) on DepBank ,   <scope> obtaining similar results overall ,  but acknowledging that the results are not strictly comparable because of the different annotation schemes used .  </scope>
(TREF)  <scope> had shown the use of dependency in Prepositional Phrase disambiguation </scope>  ,  and the experimental results reported in (REF) demonstrate that a language model which encodes a rich notion of predicate argument structure (e . g .  including long-range relations arising through coordination) can significantly improve the parsing performances . 
TREF  <scope> have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names </scope>  ,  whereas REF have looked for coreference chains within each document ,  take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words . 
TREF  <scope> have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT systems output and yield significant improvements in correlations of BLEU </scope>  (REF)  <scope> scores with human judgments of translation adequacy .  </scope>
TREF <scope> outline the syntax and theory of inference for DATR ,  a language for lexical knowledge representation ,  and (1989b) they provide a semantics for the language </scope>  that is loosely based on the approach taken by REF in his semantics for autoepistemic logic . 
TREF  <scope> presents a well-developed and robust version of this class and has made his system ,  Linguistica ,  freely available </scope>  (REF) . 
TREF  <scope> propose an approach to acquiring predominant senses from corpora which makes use of the category information in the Macquarie Thesaurus (REF) .  </scope>
TREF  <scope> propose a unified framework composed of vocabulary ,  syntactic ,  elements of lexical cohesion ,  entity coherence and discourse relations to measure text quality </scope>  ,  which resembles the composition of rubrics in the area of essay scoring (REF) . 
TREF  <scope> proposed an information-theoretic measure known as the Variation of Information (VI) </scope>  described by REF as an evaluation of an unsupervised tagging . 
TREF  <scope> proposed a parsing model based on Combinatory Categorial Grammar (REF) </scope>  ,  in which referential information was used to resolve syntactic ambiguities . 
TREF  <scope> reported Kappa agreement scores of between 0 . 11 and 0 . 35 across 6 annotators </scope>  ,  REF reported 0 . 38 on telephone conversation and 0 . 37 on lecture speech ,  using 3 annotators ,  and REF reported 0 . 32 on meeting data . 
TREF  <scope> shows that also the estimator </scope>  (REF)  <scope> uses is biased and inconsistent ,  and will ,  even in the limit of in nite data ,  not correctly identify many possible distributions over trees </scope>  . 
(TREF)  <scope> use hand crafted patterns to discover part-of (meronymy) relationships </scope>  ,  and (REF) discover various interesting relations between verbs . 
TREF  <scope> uses alignments of cognate pairs for the historical linguistics task of comparative reconstruction </scope>  and REF use alignments to compute relative distances between words from various Dutch dialects . 
TREF  <scope> uses a rule based technique that scores a compound on possible semantic interpretations </scope>  ,  while REF implements a graph based uni cation procedure over semantic feature structures for the head . 
TREF  <scope> uses the number of page hits as the web-count of the queried ngram (which is problematic  </scope> according to REF) . 
TREF  <scope> uses this term with reference to REF </scope>  ,  whose parser has to find all children of a token before it can attach that token to its head . 
TREF  <scope> went even further and </scope>  ,  as in (REF) ,   <scope> proposed to directly compare the distribution of words in full documents with the distribution of words in automatic summaries to derive a content-based evaluation measure .  </scope>
(TREF) u <scope> se transformationbased error-driven learning </scope>  (REF) t <scope> o derive disambiguation rules based on simple context information (e . g .  right and left adjacent words or POSs) .  </scope>
T <scope> he NYU team switched over </scope>  from a system that performed full parsing (PROTEUS) in MUC-5 (REF)  <scope> to a regular expression matching parser in MUC-6 </scope>  (TREF) . 
T <scope> hus ,  a lot of alignment techniques have been suggested at </scope>  ;  the sentence (REF) ,  phrase (REF) ,  nomt t)hrase (REF) ,   <scope> word  </scope> (GTREF) ,  collocation (REF)  <scope> and terminology level .  </scope>
Two approaches (GREF) use clusters obtained from large amounts of unlabeled data to augment their labeled data by introducing new features ,  and  <scope> two approaches </scope>  (GTREF)  <scope> combine probability distributions obtained from labeled data with probability distributions obtained from unlabeled data .  </scope>
Two other interpretations ,   <scope> the Greedy heuristic interpretation </scope>  (TREF) and the local brevity interpretation (REF)  <scope> lead to algorithms that have polynomial complexity in the same order of magnitude .  </scope>
Uchimoto et al .   ,  for example ,  used as such a feature whether a particular type of bunsetsu is between two bunsetsus in a dependency relation (REF) ,  and  <scope> Sassano used information about what is just before and after the modifying bunsetsu and modifyee bunsetsu </scope>  (TREF) . 
Unlike (REF) ,   <scope> we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing ,  or a specific type of semantic role labeling </scope>  (TREF) . 
Unsupervised Learning :  Results To test the effectiveness of the above unsupervised learning algorithm ,  we ran a number of experiments using two different corpora and part of speech tag sets :  <scope> the Penn Treebank Wall Street Journal Corpus </scope>  [TREF] and the original Brown Corpus [REF] . 
Van der REF present a similar experiment for Dutch ,   <scope> in which they tested most of the best performing measures according to </scope>  TREF . 
Variations of the algorithm presented in REF have been used in keyword extraction and extractive summarization (REF) ,   <scope> word sense disambiguation </scope>  (TREF) ,  and abstractive summarization (REF) . 
Various approaches have been proposed following the patterns of (REF) and  <scope> clustering  </scope> (GTREF) . 
Various filtering techniques ,   <scope> such as </scope>  (REF) and (TREF) ,   <scope> have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation .  </scope>
V-Measure is the harmonic mean of h and c .  VM = 2hch + c (7)  <scope> In the paired F-Score </scope>  (REF)  <scope> evaluation ,  the clustering problem is transformed into a classification problem </scope>  (TREF) . 
We also compare our algorithm with two state-ofthe-art systems ,   <scope> SimFinder  </scope> (TREF) and Decomposition (REF) . 10 Figure 6 shows Precision/Recall curves for the different systems . 
We also experiment with the Jiang and Conraths (REF) measure which relies only on the is-a hierarchy ,  but proved to be the <scope> best WordNet-based measure in the task of ranking words  </scope> (TREF) . 
We also <FONT style="BACKGROUND-COLOR :  yellow">compare our performance against </FONT>(<SPAN id=s44481 . r1 class="trefstyle ref44481">TREF<DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  1449px ;  LEFT :  455px" class="tooltip fixed"><SPAN style="COLOR :  black">Bunescu and Mooney ,  2004</TREF></SPAN></DIV></SPAN><INPUT name=ref[44481][1] value=0 type=hidden><INPUT id=cb44481-1 name=ref[44481][1] value=1 CHECKED type=checkbox>) and (<SPAN id=s44481 . r2 class="refstyle ref44481">REF<DIV style="FILTER :    ;  ZOOM :  1 ;  DISPLAY :  none ;  TOP :  1449px ;  LEFT :  612px" class="tooltip fixed">Finkel et al .   ,  2005</REF></DIV></SPAN><INPUT name=ref[44481][2] value=0 type=hidden><INPUT id=cb44481-2 name=ref[44481][2] value=1 CHECKED type=checkbox>) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF . 
We created 14 feature sets representing some classes from (GREF) ,  some Framenet lemmas with frame element experiencer (REF) ,   <scope> adjectives manually annotated for polarity  </scope> (TREF) ,  and some subjectivity clues listed in (REF) . 
We evaluated our systems using two commonly-used scoring programs :  MUC (TREF) and B3 (REF) . 
We evaluate the translation quality using case-insensitive BLEU metric (REF) without dropping OOV words ,   <scope> and the feature weights are tuned by minimum error rate training </scope>  (TREF) . 
We evaluate translation output using case-insensitive BLEU (REF) ,  as provided by NIST ,  and  <scope> METEOR  </scope> (TREF) ,  version 0 . 6 ,  with Porter stemming and WordNet synonym matching . 
We focus on training using Gibbs sampling (REF) ,  because  <scope> it has been popularly applied in the natural language literature ,  e . g .  ,  </scope>  (GTREF) . 
We follow REF and  <scope> induce the POS tags using the fully unsupervised POS induction algorithm of </scope>  TREF . 
We follow REF in ranking entities according to their grammatical roles ;  subjects are ranked more highly than objects ,  which are in turn ranked higher than other grammatical roles ;  <scope> ties are broken using left-to-right ordering of the grammatical roles in the sentence  </scope> (TREF) . 
We follow the experimental setup in (REF) ,  running the algorithm on English ,  German and Chinese corpora :  the WSJ Penn Treebank (English) ,  the Negra corpus (REF) (German) ,  and  <scope> version 5 . 0 of the Chinese Penn Treebank  </scope> (TREF) . 
We follow the experimental setup in (TREF) ,  r <scope> unning the algorithm on English ,  German and Chinese corpora :  the WSJ Penn Treebank (English) ,  the Negra corpus </scope>  (REF) <scope> (German) ,  and version 5 . 0 of the Chinese Penn Treebank </scope>  (REF) . 
We follow the work of (GREF) and  <scope> choose the hypothesis that best agrees with other hypotheses on average as the backbone by applying Minimum Bayes Risk (MBR) decoding </scope>  (TREF) . 
We have (11) Hypernym Patterns based on patterns proposed by (REF) and (REF) ,  (12) Sibling Patterns which are basically conjunctions ,  and (13)  <scope> Part-of Patterns based on patterns proposed by  </scope> (REF) and (TREF) . 
We have presented here a general-purpose algorithm for k-best parsing and applied it to two  <scope> state-of-the-art ,  large-scale NLP systems </scope>  :  Bikels implementation of Collins lexicalized PCFG model (GREF) and  <scope> Chiangs synchronous CFG based decoder </scope>  (TREF)  <scope> for machine translation </scope>  . 
We have presented here a general-purpose algorithm for k-best parsing and applied it to two state-of-the-art ,  large-scale NLP systems :   <scope> Bikels implementation of Collins lexicalized PCFG model  </scope> (GTREF) and Chiangs synchronous CFG based decoder (REF) for machine translation . 
We have selected a representative set of 22 metric variants corresponding to six different families :  BLEU (REF) ,  NIST (REF) ,  GTM (REF) ,  mPER (REF) ,  mWER (REF) and  <scope> ROUGE </scope>  (TREF) . 
We intend to experiment with different guidelines and instructions ,  and to screen (REF) and  <scope> weight Turkers responses </scope>  (TREF) ,   <scope> in order to lower the number of Turkers required for this task .  </scope>
We obtained four more sets of alignments from the Berkeley aligner (BA) (REF) ,  <scope> the HMM aligner (HA)  </scope> (TREF) ,  the alignment based on partial words (PA) ,  and alignment based on dependency based reordering (DA) (REF) . 
We pre-process all corpora using the following tools :  We use NLTK2 for sentence splitting ,  OpenNLP3 for POS tagging ,  YamCha (REF) for chunking ,  the C&C tools (REF) for CCG parsing and named entity recognition ,  and  <scope> the Stanford parser  </scope> (GTREF)  <scope> for constituency and dependency parsing .  </scope>
We see no good reason ,  however ,  why such text spans should necessarily be sentences ,  since the majority of tagging paradigms (e . g .   ,  Hidden Markov Model [HMM] [REF] ,  Brills [REF] ,  and  <scope> MaxEnt </scope>  [TREF])  <scope> do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .  </scope>
We then parsed each sentence using the  <scope> PCFG parser of REF ,  a modified version of the Berkeley parser </scope>  (GTREF) ,  for the tree structure and part-ofspeech tags . 
We use 2-best parse trees of Berkeley parser (REF) and 1-best parse tree of Bikel parser (REF) and  <scope> Stanford parser </scope>  (TREF) as inputs to the full parsing based system . 
We used the following publicly available tools :  the Charniak Parser (REF) for parsing sentences and  <scope> SVM-light-TK </scope>  (GTREF) ,  in which we coded our new kernels for RTE . 
We used the Kyoto corpus (REF) for evaluation data ,   <scope> and chose </scope>  KNP (REF) and  <scope> CaboCha </scope>  (TREF)  <scope> for comparison </scope>  . 
We used two baseline systems for comparative purposes :  the rule-based dependency parser ,  KNP (REF) ,  and  <scope> the probabilistic model of syntactic and case structure analysis  </scope> (TREF) ,  in which coordination disambiguation is the same as that of KNP . 
We use MXPOST tagger (REF) for POS tagging ,   <scope> Charniak parser </scope>  (TREF)  <scope> for extracting syntactic relations </scope>  ,  SVMlight1 for SVM classifier and David Bleis version of LDA2 for LDA training and inference . 
We use news articles portion of the Wall Street Journal corpus (WSJ) from the Penn Treebank (TREF) in conjunction with the self-trained North American News Text Corpus (NANC ,  REF) . 
We use TnT (TREF)  <scope> for English POS tagging </scope>  and ChaSen (REF) for Japanese morphological analysis ,   <scope> and label each token to either content or functional depending on its partof-speech .  </scope>
We write act(i) to denote the feature representation extracted for action act at location i .   <scope> The model is trained using a variant of the structured perceptron </scope>  (TREF) ,  similar to the algorithm of (GREF) . 
Whereas  <scope> language generation has benefited from syntax </scope>  [GTREF] ,  the performance of statistical phrase-based machine translation when relying solely on syntactic phrases has been reported to be poor [REF] . 
While i <scope> ntegration in  </scope> (TREF)  <scope> was still restricted to morphological and PoS information </scope>  ,  (REF) extended shallow-deep integration at the lexical level to lexico-semantic information ,  and named entity expressions ,  including multiword expressions . 
While most interest in boundary prediction has been focussed on synthesis (GREF) ,  currently  <scope> there is considerable interest in predicting boundaries to aid recognition </scope>  (GTREF) . 
While much of the earlier work in MERT (GREF) relies on standard convex optimization techniques applied to non-convex problems ,   <scope> the Och algorithm </scope>  (TREF)  <scope> represents a significant advance for MERT since it applies a series of special line minimizations that happen to be exhaustive and efficient .  </scope>
While NER over formal text such as news articles and webpages is a well-studied problem (GREF) ,   <scope> there has been recent work on NER over informal text such as emails and blogs </scope>  (GTREF) . 
While performance on identifying explicit discourse relations in the PDTB is as high as 93% (REF) ,   <scope> identifying implicit ones has been shown to be a difficult task with accuracy of 40% at Level-2 types </scope>  (TREF) . 
While previous work has not looked into the reader-comments relationship ,   <scope> there has been related work on identifying political orientations or viewpoints </scope>  (GTREF) ;  whether a piece of text expresses support or opposition in congressional debates (REF) or online debates (REF) ;  as well as identifying contrastive relationship (REF) . 
While psycholinguistic research has shown (GREF) that the strict interpretation of  <scope> the Gricean Quantity Maxim ,  adopted for example by  </scope> TREF ,  is not observed by speakers ,  brevity has remained a central concern in recent approaches (REF) . 
While psycholinguistic research has shown (GREF) that the strict interpretation of the Gricean Quantity Maxim ,  adopted for example by REF ,  is not observed by speakers ,   <scope> brevity has remained a central concern in recent approaches </scope>  (TREF) . 
While (REF) and (TREF)  <scope> generate slides from documents by modeling the task in creative ways ,  </scope>  we aim to learn something deeper regarding how humans actually go about the task . 
While  <scope> studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments </scope>  (GTREF) ,  we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments ;  correlation studies have been made of individual components (REF) ,  but not of systems . 
While the general methods of POS tagging vary from study to study  Maximum Entropy (REF) ,  conditional random fields (REF) ,   <scope> perceptron  </scope> (TREF) ,  Bidirectional Dependency Network (REF)  the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word . 
While WordNet-based approaches have obtained promising results for measuring semantic similarity (GREF) ,  <scope> the results for the more general notion of semantic relatedness have been less promising </scope>  (TREF) . 
Word-level alignment is a critical component of a wide range of NLP applications ,  such as  <scope> construction of bilingual lexicons  </scope> (TREF) ,  word sense disambiguation (REF) ,  projection of language resources (REF) ,  and statistical machine translation . 
Words ,  chunks and their relations in the texts were analyzed by CaboCha (REF) ,  and  <scope> named entities were analyzed by the SVM-based NE tagger </scope>  (TREF) . 
W <scope> e pre-process the dataset with the following tools :  </scope>  the Charniak Parser (REF) for parsing sentences ,   <scope> the WordNet similarity package </scope>  (TREF)  <scope> for computing WBOW and for linking the two tweets in a pair </scope>  ,  and SVMlight (REF) ,  extended with the syntactic first-order rule kernels described in (REF) for creating the SYNT and the FOR feature spaces . 
Yet ,  single word based models (GREF) are still highly relevant :   <scope> many phrase based systems extract phrases from the alignments found by training the single word based models ,  and those that train phrases directly usually underperform these systems </scope>  (TREF<div class="tooltip fixed active" style="left :  197px ;  top :  5531px ;  display :  block ;  "><span style="color : black"><tref>DeNero et al .  ,  2006</tref></div></span>) . 
