# orga

### big TODOs

* k-fold cross validation (somehow)
* generate a complete dataset (instead of just one month)
* pre-selection of candidate recommendations to speed up process
    * IR type method: generate index for comparison targets, retrieve e.g. 1k targets, really compare to those
    * clustering of comparison targets, "
    * [k4nn](https://arxiv.org/abs/1802.08301), "

### recent notes
* compare in context FoS annotations w/ all annotations of cited paper (i.e. look at references to 22k set)
    * also compare to FoS annotations of citing paper to see if it is an "outlier" context
    * (MAG also has FoS for papers, mby also use, but their confidence values are not that telling)
* ensure parallelisation of bibitem matching
    * start
* @preselection: _clustering as offline step_, online compare to centroids

---

* check confidence values of MAG FoS annotation false positives
* analyse MAG FoS annotations *in citation contexts* overall (number per context, from the right field, what level etc.)
* arXiv complete plain text dump (on tuco)
    * change pipeline to do parts and delete stuff inbetween
    * debug mode switch for generation of log files
* mby more conclusive/exact/detailed analysis on PoS pattern comparison between cit. and non cit. sentences 
* mby answer: distinction of integral vs. non-integral citations, relevant for analysis/representation of cit. contexts?
    * given a sentence w/ a placeholder, is retroactive distinction of integral vs. non-integral possible?

---

* marker surroundings more like patterns (find top 10 (-3, 3)) e.g. NN V NN [] <EOS> <EOS> <EOS>
* NEL precision (also recall) manual check (any concepts missing in FoS?)
    * also manually look into position of MAG FoS annotations in relation to citations markers
    * how should the FoS representation look like? (!!!) -> answer: how will MAG FoS annotations be incorporated in approach?
* false positive recheck?
* how frequent are cases like "ImageNet []"?
* also: MAG postgresql DB set up: try to use
* also, for all above, look into using MAG citation contexts

---

* heatmap 引き続き
    * commonness/frequency of pos tags/pos pattern (e.g. is "proper noun [] verb" common)
    * heatmap per discipline
* MAG matching
    * (neural) parcit /sliding window/fuzzy matching test
* NEL
    * mag annotations as normal entities → eval.
    * mag annotations recall (manual check, check contexts and see if anything should've been annotated that is not)
    * mag / spotlight entities, most frequent in 1712 (show e.g. top 15 most frequent entities)
        * do they fit do distribution of fields of research in 1712

---

* test MAG thingy linking (could be used for pre-filtering)
* using NEL *only* for `<dataset/algo/etc.> []` and the claim and argument approaches for all other citation contexts (by first classifying the context) could be an option

---

* refine usage of entities
    * manual evaluation: get some citation context and the contained linked entities; check: are the linkings correct; check: is the presence of the entities relevant for the citation itself (also consider special cases like <NEL>[] like ImageNet[1])
* storage of annotations for now okay
* do use replacement tokens (CIT, FORMULA, etc.) instead of just deleting them from plain text
* maybe have a look at MAG citation contexts (feed into current baseline?)

---

* match to MAG IDs instead of arXiv IDs
* mby treat "co-appearing" citations as valid (e.g. "some stuff super important [1][2][3]" <- all three count)
* general idea: high weight on smaller local context, then also consider (with some lower weight) larger context
* do pre-selection before cosine sim (or similar) -> how?
* (NEL) annotate documents, *then* extract citation contexts (b/c annotators use large context -> better annotation results)

### overall

* thema verstehen → expose (mail mit details kommt noch) schreiben → thesis anemlden
* genaue richtung noch nicht so fest wie vermutet

# links / notes

* arXiv
    * 1.4 M documents → processing each for **0.42 seconds** takes **~1 week**
    * [sources](https://arxiv.org/year/cs/18) (single dl)
    * [IDs over time](https://arxiv.org/help/arxiv_identifier)
    * availability of DOIs (in CS only dump from Dec. 2017)
        * ```
        >>> len(tree.xpath("//dc:identifier[starts-with(text(),'doi')]", namespaces={'dc':'http://purl.org/dc/elements/1.1/'}))
        29739
        ```
        * → 28775 of which are contained in MAG
        * ```
        >>> len(tree.xpath("//ListRecords/record"))
        155308
        ```
        * → 19%
* MAG
    * [schema](https://microsoftdocs.github.io/microsoft-academic/microsoft-academic-graph/reference/data-schema.html)
    * [blog post w/ stats](https://www.microsoft.com/en-us/research/?post_type=msr-blog-post&p=480543&preview=true)
    * [oldish examples on quora](https://www.quora.com/Where-can-I-obtain-a-schema-description-of-Microsoft-Academic-Graph-data/answer/Milind-Gokhale-1)
    * availability of DOIs
        * ```
        saiert@shetland:~$ cut -f3 -d$'\t' /vol1/mag/data/2018-07-19/dumps/Papers.txt | sed '/^\s*$/d' | wc -l
        75266339
        ```
        * ```
        saiert@shetland:~$ cat /vol1/mag/data/2018-07-19/dumps/Papers.txt | wc -l
        206252196
        ```
        * → 36.5%

# questions

* entity based citerec
    * go-to example/paper? (candidates for similar task see below)
* general (still somewhat far away)
    * not a single of the citerec approaches in CireRecSurvey uses the Microsoft Academic Graph (or arXiv for that matter) as dataset, right? (→existence of approaches to directly compare to?)
    * similar to ^ baseline to start from?
    * other approaches often are a large ensemble of techniques/features/prior work reused — should I rather build/try out "from scratch" or  recreate/assemble existing things? in both cases: is trying for improvement just enough or do choices need to be justified further?
    * ^ pick/engineer vs "brute force" (i.e. just try a lot of different stuff and see what contributes most to a good result)

# structured notes

### general

* task
    * mail
        * <small>**keinen komplett neuen Ansatz** zu citation recommendation entwerfen, sondern eher ein gutes lauffähiges System zu citation recommendation bauen. Bereits die Nachimplementierung eines "state-of-the-art"-Systems wäre eine contribution</small>
        * <small>Wahrscheinlich werden wir uns eher **semantisch-strukturierte Repräsentationsformen (entities, claims, arguments) für citation recommendation** widmen.</small>
        * <small>(Ashwath ... Indizierung des großen Microsoft Academic Graphs (MAG) und um die Verwendung eines state-of-the-art approaches für citation recommendation)</small>
        * <small>in Ihrer Arbeit eher mit **semantischen und pragmatischen Ansätzen für citation recommendation** beschäftigen. Diesbezüglich gibt es nämlich **in der Literatur noch recht wenig**</small>
        * <small>An sich ist der Microsoft Academic Graph als Datensatz für citation recommendation geeignet, allerdings enthält er nur einen Satz als citation context -- und ein größerer Kontext wäre oft angebracht, besonders, wenn wir nicht nur Entitäten zitieren, sondern ganze Argumente. Ganze Papers (fulltext) sind an sich nicht im MAG verfügbar. Die Lösung läge darin, stattdessen die Paper von arXiv.org als Subset zu verwenden. Diese sind m.E. auch im MAG. Außerdem können diese von arXiv heruntergeladen werden und liegen im Format TeX (+bibtex) vor, was den Vorteil hat, besser an den Reintext und an die Zitate zu kommen. Allerdings müsste ein TeX parser geschrieben werden, der den Rohtext usw. extrahiert. In Haskell wurde ein solch einfacher Parser bereits entwickelt [1], allerdings ohne Berücksichtigung von section headers etc.</small>
        * <small>Zitate kann man differenzieren dahingehend, ob eine Entität (z.B. ein Konzept oder ein Datensatz oder eine Methode) zitiert wird, oder eine Behauptung (claim), oder eine Behauptung/Argument im Zusammenspiel einer Argumentationskette. Demzufolge kann man für jede dieser Möglichkeiten geeignete Repräsentationsformen (aus der Computerlinguistik) für ein besseres citation recommendation verwenden. **Konkret kann man z.B. mentions im Content des Papers zu Entitäten in Wikipedia oder Wikidata verlinken (entity linking bzw. text annotation), sowie claims oder ganze Argumentationsketten extrahieren und entsprechend formal repräsentieren** (einige Beispiele diesbzgl. sind in [2-5]; die genauen Ansätze, die wir verwenden werden, können wir später festlegen). Insgesamt ginge es also hier um die **Anwendung von NLP-Komponenten auf wissenschaftliche Texte**.</small>
        * <small>Im Rahmen der Masterarbeit würde man **für jedes Unterthema (z.B. entity, claim, argumentation) einen Ansatz für citation recommendation entwickeln**. Für eine Einschätzung der Schwierigkeiten bei der Ansatzentwicklung und dann später für eine Evaluation der Ansätze müssen wohl einige Texte auch manuell annotiert werden. Außerdem würde man die Implementierungen der einzelnen Systeme zur recommendation in das Gesamtsystem von Ashwath integrieren.</small>
    * gist
        * starting point
            * MAG has much going for it, but only one sentence citation context
            * arXiv has paper sources (TeX)
        * use intersection of MAG and arXiv
        * get plain text from arXiv source
        * match MAG citation contexts to arXiv plain texts (? if so, plain text of arXiv would be sufficient (no need to retain citations))
        * → MAG with arbitrary large citation contexts
        * ———
        * in large citation contexts, find and formally represent "semantic NLP components" (using *all* the buzzwords)
        * ML
        * ???
        * profit 
* defs
    * argument = claim + one or more premises justifying the claim
        * [premises] [step(s) of deduction] [claim]
        * x (and y), therefore z

### start out w/ MAG or arXiv

* MAG
    * `+` matching MAG IDs to arXiv IDs should be quite reliable (could even use citation contexts to match to arXiv plain text)
    * `+` citation network is already given
    * `-` (blind?) reliance on MS / less transparent
    * `-` citation markers might not be *that* reliable (need to match arXiv source bibitem strings to MAG IDs)
    * `o` might enable just using latexpand+detex for TeX processing
* argument for arXiv
    * `-` matching arXiv source bibitem strings to arXiv IDs might not be *that* reliable
    * `+` more transparent
    * `+` citation markers are (albeit dependent on initial matching) certain
    * `+` (if we don't add in the MAG:) only rely on one source → cleaner approach (?)

questions to be answered:

* how hard is matching arXiv bibitem strings to arXiv IDs?
* if starting from arXiv, what is gained from adding in the MAG?
* ...

### tex parser

* grabcite[/test-tex](https://github.com/agrafix/grabcite/tree/master/test-tex) mby "edge cases"? (download source by ID, don't just use .tex file from repo)

##### existing software

* ~~[plastex](https://github.com/tiarno/plastex) — largish, last commit 1y ago, python~~
    * in addition to requirements.txt also `$ pip install Pillow Unidecode` for less noisy output
    * TEST: neat output when it works, **but** failed on all tested arXiv papers and it's own documentation
* [TexSoup](https://github.com/alvinwan/texsoup) — smallish, recent commits, python
    * doesn't handle definitions (`\dev`) [source](https://stackoverflow.com/a/50151171)
    * [stackoverflow question](https://stackoverflow.com/questions/49779853/) specifically asking about arXiv TeX (by someone using TeXSoup)
    * TEST: works on most tested; fails with e.g. `1607.00138`
    * find citations
        ```
        soup = TexSoup(tex_str)
        c = soup.find_all(name='cite')
        ```
    * output text
        ```
        soup = TexSoup(tex_str)
        for cntnt in soup.document.contents:
            if type(cntnt) == str:
                print(cntnt)
        ```
    * **TESTED** on larger sample: 32% fails
        * 54 of 169 failed — can be reduced to 50 by replacing preamble w/ a clean one
        * → mby do 70% TexSoup + 30% fallback sth. (e.g. detex + python script to put cites back in?)
* [grabcite](https://github.com/agrafix/grabcite)
    * installation
        * required additional packages `libpcre++-dev`, `libpq-dev`, `libghc-hdbc-odbc-dev` (+500 MB of dependencies)
    * unpack arXiv dump
        * file given for param `--arxiv-meta-xml` has do be in cwd (giving a path results in `grabcite-datagen: InvalidRelFile "..."`)
        * arXiv: *"Note: Many of the formats above are served gzipped (Content-Encoding: x-gzip). Your browser may silently uncompress after downloading so the files you see saved may appear uncompressed."*
        * grabcite:
            * expects arXiv sources with `.gz` extension in input folder
            * has `gzHandler` and `tarGzHandler` (see `src/GrabCite/Arxiv.hs`)
            * → arXiv sources manually downloaded are mere `tar` archives w/o file extension → need to be gzipped and renamed to have `.gz` file extension (*not* `.tar.gz`)
    * generate data set
        * tries to connect to papergrep.com (expired namecheap.com registration that probably once hosted [this](https://github.com/agrafix/papergrep))
* [opendetex](https://github.com/pkubowicz/opendetex) / [detex](https://www.freebsd.org/cgi/man.cgi?query=detex) — ?, recent commits, compiled
    * specifically for getting plain text
    * `-c` flag is supposed to preserve `\cite`s, but breaks output completely for first tested paper `1010.2903`
    * TEST: opendetex seems to leave in more control sequences than on system detex
* [LaTeXML](https://github.com/brucemiller/LaTeXML) — *very* mature, recent commits, perl
    * LaTeX→XML
    * existing (dead?) [project on on arXiv data](https://kwarc.info/projects/arXMLiv/) ([active LaTeXML fork](https://github.com/KWARC/LaTeXML))
    * ~~install from source~~
        * `sudo apt install cpanminus libxml-libxml-perl libxml-libxslt-perl libxml2-dev libimage-size-perl`
        * `sudo cpanm XML::LibXML`
        * `sudo cpanm Parse::RecDescent`
        * `perl Makefile.PL`
        * `make`
        * `make test` → **fails**
        * `make install`
    * install:
        * `sudo apt install libtext-unidecode-perl`
        * `sudo apt install latexml`
    * misc notes:
        * xpath matches in output require namespace. e.g.:
            * `tree.xpath('//LaTeXML:Math', namespaces={'LaTeXML':'http://dlmf.nist.gov/LaTeXML'})`
            * `etree.strip_elements(tree, '{http://dlmf.nist.gov/LaTeXML}Math', with_tail=False)`
* [Tralics](https://www-sop.inria.fr/marelle/tralics/) — ?, 2015, C++
    * LaTeX→XML
    * [apparently](https://jblevins.org/log/xml-tools) *fast*
* [flap](https://github.com/fchauvel/flap)
    * can be run from inside python by copying `flap/ui.py` and replacing bottom (click) part with `Controller(OSFileSystem(), Display(sys.stdout, False)).run('some.tex', 'out_folder')`
    * somewhat slow?
    * → try out [alternatives](https://tex.stackexchange.com/questions/21838/replace-inputfilex-by-the-content-of-filex-automatically)
    * → latexpand looks good
        * has `--expand-usepackage` flag
        * has `--expand-bbl FILE` flag (mby useful)

##### arXiv

* IDs
    * metadata record: `oai:arXiv.org:0704.0046`　　(xPath: `/ListRecords/record/header/identifier[text()='<id>']`)
    * web: `https://arxiv.org/abs/0704.0046`
    * data dump: `0704.0046.gz`
* only *some* metadata records have DOIs
* `\cite` in text, then `\bibitem` lower down or in `.bib` file
* problems
    * sometimes a paper's [v2](https://arxiv.org/format/1306.0555v2) gives empty source while it's [v1](https://arxiv.org/format/1306.0555v1) has proper source
    * [cases](https://arxiv.org/format/1607.00145) where "source" is just a single `\includepdf`
    * `physics0001026`: html instead of latex
    * `physics0001060`: postscript instead of latex
    * `astro-ph0001005`: using file extension .ltx for latex
    * `hep-th0001079`: few sentences of plaintext
    * `hep-th0001205`, `quant-ph0001041`, ...: TeX (not LaTeX)
    * `ph0001041`: weird LaTeX?
    * `1712.10228` is in Japanese
    * `hep-lat0001017` is creating new commands for marking math environments, *with a newly created command for newcommand*, ***outside the preamble***  
        ```
        [...]
        \begin{document}                                                                                                        
        \newcommand{\nc}{\newcommand}                                                                                           
        \nc{\be}{\begin{equation}}                                                                                              
        \nc{\ee}{\end{equation}}                                                                                                
        \nc{\bea}{\begin{eqnarray}}                                                                                             
        \nc{\eea}{\end{eqnarray}}                                                                                               
        \nc{\bib}{\bibitem}                                                                                                     
        \nc{\1}{16^3\times{32}}                                                                                                 
        \nc{\2}{24^3\times{36}}
        [...]
        ```

##### arXiv source dump

* lots of tar archives like `arXiv_src_0001_001.tar` and a manifest file `arXiv_src_manifest.xml` w/ some metadata
* top level archive content examples:
    * `cs0001012.gz` → `arXiv:cs/0001012`
    * `gr-qc0001036.pdf` → `arXiv:gr-qc/0001036`
    * `1502.00318.gz` → `arXiv:1502.00318`
* 流れ
    * step 1: normalize (separate steps to allow for separate evaluation of results)
        * check for file ending (pdf/gz)
        * gz: unzip and check if tar archive or tex file (`tarfile.is_tarfile()`)
        * tar: extract contents, identify main tex file (`\begin{document}` mby?), flatten with [flap](https://github.com/fchauvel/flap), also check for `\includepdf`
        * save tex[+bib] / pdf in output folder
    * step 2: get plain text
        * pdf: pdf2text (citation markers byebye)
        * tex: TeXSoup
        * save metadata from XML (mby in JSON)
        * tex: replace local citation markers w/ globally unique one (UUID, save UUID→original bibitem text, save UUID→citing paper ID index)
    * step 3: networking
        * try to match citation UUIDs to arXiv IDs (and mby at one point other identifiers)

**test w/ 2364 input docs**  
**step 1**
```
saiert@shetland:~/arxiv_test$ time python3 normalize_arxiv_dump.py /home/saiert/arxiv_test/0001_001/0001/ /home/saiert/arxiv_test/0001_001/normalized

real    3m37.049s
user    3m16.520s
sys     0m20.866s
```
(~0.09 sec per doc)  
→ all of arXiv in 37 hours  
→ 2364m→2250 95% success

**step 2**  
on shetland, 50min for 392 docs (~7 sec per doc); 289 min for 2364 docs (~7.7 sec per doc)  
→ all of arXiv in 128 days (>4 months)  
→ 2218\*→2138 96% success (90% overall)  
\*32 PDFs, therefore not 2250

##### arXiv processing speed up

* math pre removal
    * time: 3m53s (~0.09s/doc) | 122m (3.3s/doc)
    * coverage: unchanged | 2066 **93%**

```
saiert@shetland:~/arxiv_test$ time python3 normalize_arxiv_dump.py /home/saiert/arxiv_test/0001_001/0001/ /home/saiert/arxiv_test/0001_001/normalized

real    3m53.446s
user    3m33.779s
sys     0m19.952s

output: 2218 docs

saiert@shetland:~/arxiv_test$ time python3 parse_latex_latexml.py 0001_001/normalized/ 0001_001/text

real    122m26.729s
user    113m3.402s
sys     8m43.531s

output 2066 docs
```

* ーネ申 **tralics** ネ申ー
    * on shinobu: 2218 docs in 53s → 0.024s/doc
    * on shinobu w/ SQLite DB instead of separate text files for metadata: 2m13s
    * in: 2216, out: 2199 (30 messed up) → 98% (91.8% overall) (latexml only 2138)
    * a bit more preamble noise (already in the XML in `<p>`s, so no straightforward way to deal with it)

**tralics problem docs with latexml**  
(messed up output (but not skip b/c error) for 0001_001; for which latexml outputs 2138, tralics 2199)
```
tralics problem         latexml out                 latexml better
astro-ph0001027.txt     none                        (x)
astro-ph0001100.txt     none                        (x)
astro-ph0001144.txt     also messed up
astro-ph0001152.txt     also messed up
astro-ph0001153.txt     also messed up
(astro-ph0001462.txt)   fine (more content)         x
astro-ph0001540.txt     none                        (x)
astro-ph0001546.txt     none                        (x)
(cond-mat0001187.txt)   same
(cond-mat0001222.txt)   no weird "aaaaaaaaa"
cs0001010.txt           same
gr-qc0001071.txt        fine                        x
hep-lat0001018.txt      fine                        x
hep-ph0001051.txt       also messed up
hep-ph0001203.txt       fine                        x
(hep-th0001048.txt)     same
hep-th0001109.txt       fine                        x
hep-th0001139.txt       same
hep-th0001146.txt       same
hep-th0001181.txt       fine                        x
hep-th0001199.txt       fine                        x
math0001022.txt         same
math0001029.txt         same
math0001070.txt         same
math0001103.txt         same
math0001164.txt         fine                        x
math0001166.txt         same
nucl-th0001065.txt      fine                        x
```

```
python3 parse_latex_tralics.py ~/dl/foo/0001_normalized ~/dl/foo/0001_text_new 70,28s user 20,82s system 68% cpu 2:13,90 total
```

**tralics citation/bibitem parsing** (tested on Dec 2017 complete dump)    
```
Citations: 488725    *(295710 unique)
Unmatched citations: 93315
```

→ Tralics is supposed to be able to understand [commands from a lot of packages](https://www-sop.inria.fr/marelle/tralics/packages.html) (esp. natbib is relevant here) but apparently doesn't  
→ fixing `\citep`s, `\bibitem[bla{problem}bla]{key}`s etc. helps with a lot of citations (see regex and results below)  
→ still a problem are [nmras](https://www-sop.inria.fr/marelle/tralics/packages.html) style bibliographies (predictable format so mby. #TODO)

```
NATBIB_PATT = re.compile((r'\\cite(t|p|alt|alp|author|year|yearpar)\s*?\*?\s*?'                                         
                           '(\[[^\]]*?\]\s*?)*?\s*?\*?\s*?\{([^\}]+?)\}'),                                              
                         re.I)
cntnt = NATBIB_PATT.sub(r'\\cite{\3}', cntnt)

BIBOPT_PATT = re.compile(r'\\bibitem\s*?\[[^]]*?\]', re.I|re.M)
cntnt = BIBOPT_PATT.sub(r'\\bibitem', cntnt)
```

```
Citations: 565613 (not unique)                                                  
Unmatched citations: 28372
```

**→ increase from 70% to 95%** (assuming above 565613 are 100%)

impact on example data set:

```
$ cat items_1712_100w_2mincont_not_stemmed.csv | wc -l
17640
$ cat items_1712_100w_2mincont_not_stemmed_NATBIB_FIX.csv | wc -l
28673

```

##### bibitem matching (arxiv)

* arxiv.org web API test (use unrealistic b/c time)
    * match for e.g. first bibitem in `1711.00002`
        * original: `L. J. Ba and R. Caruana. Do deep nets really need to be deep? In Proceedings of NIPS, 2014.`
        * query: `http://export.arxiv.org/api/query?search_query=all:Caruana%20deep%20nets%20really%20need%20deep%20Proceedings%20NIPS%202014` (words longer 2 letters)
            * (comment added afterwards: in general way better results when concatenating words with %2B (+) instead of %20)
* source given links/arXiv IDs (tested w/ 377 docs from 2017)  
```
sqlite> select count(*) from bibitem;
10270
sqlite> select count(distinct uuid) from bibitemlinkmap;
1615
sqlite> select count(distinct uuid) from bibitemarxividmap;
476
```
* Solr test

```
$ time python3 solrize.py
no metadata in 1008.3138 in arxiv-physics-oai_dc-cursor147000.xml
no metadata in 1008.3138 in arxiv-physics:quant-ph-oai_dc-cursor13000.xml
no metadata in 1005.0836 in arxiv-math-oai_dc-cursor50000.xml
python3 solrize.py  430,84s user 8,54s system 98% cpu 7:27,43 total
```

```
$ time ./bin/post -c arxiv_meta ~/dl/foo/newArxivMetaHarvesting201712/solrized
[...]
2643 files indexed.
COMMITting Solr index changes to http://localhost:8983/solr/arxiv_meta/update...
Time spent: 0:19:19.727
./bin/post -c arxiv_meta ~/dl/foo/newArxivMetaHarvesting201712/solrized  10,20s user 28,05s system 3% cpu 19:20,09 total
```
* bibitem matching w/ Solr

```
- - - - - using title_query_split_heuristic - - - - -
total: 10270
matches: 612
checked: 476
false negatives: 463
false positives: 0
python3 match_bibitems.py path ~/dl/foo/1712_text  35,91s user 1,67s system 74% cpu 50,206 total
```

```
- - - - - using title_author_query_words - - - - -
$ python3 match_bibitems.py path ~/dl/foo/1712_text
total: 10270
matches: 1703
checked: 476
false negatives: 315
false positives: 1
python3 match_bibitems.py path ~/dl/foo/1712_text  38,76s user 1,87s system 43% cpu 1:33,12 total
```

**NOTE**: bibitems that include IDs might be more likely to not include a title. In a sample of 250 of above 476 checked bibitems, only 117 included a proper title.

**optimistic extrapolation**  
1703 + 315 out of 10270 → 19.6%  
assuming 1.45 M papers each 27 citations → 7.6 M citations

~~**(!!!) somewhere along the line duplicates found there way in arXiv ID map(!!!)** (05.11.2017)~~ (fixed)  
→ add constraints to DB, rerun pipleline for Dec. 2017  
```
sqlite> select count(*) from bibitemarxividmap;
70353
sqlite> delete from bibitemarxividmap where id not in (select min(id) from bibitemarxividmap group by uuid, arxiv_id);
sqlite> select count(*) from bibitemarxividmap;
62232
```

##### bibitem matching (MAG)

* e.g. `$ curl 'localhost:8983/solr/mag_papers/select?q=original_title:Buda%2BMaki%2Band%2BMazurowski%2Bsystematic%2Bstudy%2Bthe%2Bclass%2Bimbalance%2B...'`
    * fields of interest: `paper_id`, `normalized_title`, `original_title`, `publication_year`, `citation_count`, `estimated_citation_count`, (`doi`)
    * for authors: `mag_paper_author_affiliations/select?q=paper_id:` → `author_id`, `mag_authors/select?q=author_id:` → `display_name` / `normalized_name`
* strict title matching **50.9%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/all_text/
    - - - - - - - - - - - - - - - - -
    312534/312536
    matches: 159201
    checked: 35876          ← wrong, actually 22092 (or even fewer, check +ParsCit result)
    false negatives: 22141
    false positives: 1102
    
    real    1115m19.231s
    user    42m41.461s
    sys     3m40.510s
    ```
* +filtering Proceedings/Transactions/etc. *segments* **51.3%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/all_text/
    - - - - - - - - - - - - - - - - -
    312535/312536
    matches: 160350
    checked: 35875          ← wrong, actually 22092 (or even fewer, check +ParsCit result)
    false negatives: 22183
    false positives: 1043
    DOI rebounds: 259
    
    real    1374m51.740s
    user    37m37.951s
    sys     3m37.655s
    ```
* +name list filtering, +arXiv ID usage **57.3%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/all_text/
    - - - - - - - - - - - - - - - - -
    312536/312536
    matches: 179107
    checked: 35876          ← wrong, actually 22092 (or even fewer, check +ParsCit result)
    false negatives: 16616
    #Phys. Rev.: 36382
    false positives: 843
    DOI rebounds: 164
    by arXiv ID: 35799
    by arXiv ID fail: 0
    
    real    1494m35.258s
    user    35m8.912s
    sys     4m7.753s
    ```
* +DOI ID usage **61.5%**
    ```
    $ python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/all_text/
    - - - - - - - - - - - - - - - - -
    312536/312536
    matches: 192056
    checked: 22092
    #Phys. Rev.: 26867
    false positives: 1344
    by arXiv ID: 35799
    by arXiv ID fail: 0
    by doi: 26230
    ```
* +ParsCit (cancelled b/c time) **52%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/parscit_test/
    - - - - - - - - - - - - - - - - -
    123166/312536
    matches: 64123
    checked: 7496
    Phys. Rev.: 10467
    no title: 29034
    false positives: 422
    by arXiv ID: 13939
    by arXiv ID fail: 0
    by doi: 10348
    
    real    2985m14.393s
    user    21m10.869s
    sys     2m21.369s
    ```
* +sliding window fuzzy matching (cancelled b/c time) **54.8%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/parscit_and_fuzzy_window_test/
    - - - - - - - - - - - - - - - - -
    113486/312536
    matches: 62246
    checked: 7557
    Phys. Rev.: 9768
    no title: 26604
    false positives: 421
    by arXiv ID: 13103
    by arXiv ID fail: 0
    by doi: 9600
    
    real    2774m1.938s
    user    20m48.099s
    sys     2m15.738s
    ```
* full phrase queries etc. for speedup **53.7%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/solr_phrase_query_test/
    312536/312536
    matches: 168063
    checked: 22434
    Phys. Rev.: 26867
    no title: 73304         <- (!!!) if all resolvable 77%
    false positives: 1158
    by arXiv ID: 35798
    by arXiv ID fail: 0
    by doi: 26230
    >>> avg time aID: 0.44
    >>> avg time DOI: 0.52
    >>> avg time ParsCit: 0.05
    >>> avg time Solr: 0.06
    >>> avg time check: 0.00
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 15899.40         4.5h
    >>> time DOI: 13764.04         3.5h
    >>> time ParsCit: 11630.38     3.2h
    >>> time Solr: 14447.63        4.0h
    >>> time check: 48.72
    >>> time DB r: 500.97
    >>> time DB w: 210.44
    
    real    963m6.286s            16h    (→ full arXiv in ~48d)
    user    95m50.464s
    sys     6m36.414s
    ```
    * ↑ manual check of 5% false positives
        ```
        manually checked: 25
          tot  perc  | of 5%
        O: 19   76%  |  3.8%
        △:  4   16%  |  8.8%
        Σ  23   92%  |
                     |
        X:  2    8%  |  0.4%
        
        notes
        - quite some falsely marked b/c evaluated case sensitive
        - some DOIs linking to an identical URL
        - few identical in different context or close derivative content by same auth.
        - very few actually wrong
        ```
* problems:
    * phys rev's
    * bibitems without title
    * formulas in bibitems:
        * `measurement of longrange nearside twoparticle angular correlations in pp collisions at s 13 tev` not in  
            `vardan khachatryan et al measurement of longrange nearside twoparticle angular correlations in pp collisions at formula 13 tev phys rev lett 11617172302 2016 151003068`  
            b/c of FORMULA replacement
        * *"ARGUS Collaboration, H. Albrecht et al., “Observation of FORMULA semileptonic decay”, Phys. Lett. B303 (1993) 368."*
    * author typos: *"S. Chakraborty M. Hanmandlu, K. R. Murali Mohan et al. Uncon**ss**trained handwritten character recognition based on fuzzy logic. Pattern Recognition, 2003."*
    * special characters
        * bibitem: "s-step iterative methods for symmetric linear systems"
        * MAG: "s -step iterative methods for symmetric linear systems"
        * ---
        * bibitem: "É. Cancès, A. Levitt, G. Panati, and G. Stoltz. Robust determination of maximally localized Wannier functions. Phys. Rev. B 95(7), 075114 (2017)."
        * MAG: "Robust determination of maximally-localized Wannier functions"
    * MAG noise:
        * *Pascual Jordan: Schwerkraft und Weltall, Vieweg and Sohn, Braunschweig(1952), VII+207p. 1,680円*
        * multiple papers with identical normalized title and (similar) authors (→ hard to distinguish, heuristically dealt with by number of citations)
    * non citations
        * *According to the numerous discussions with my colleagues D. Fischer and P. Mokler an experimental verification of our theoretical predictions is feasible.* (in 1305.0417)
    * single process single thread bibitem-matching: extrapolated >40 days for whole arXiv dump → parallelization
* bibitem-matching parallelization challenges:
    * reading in all bibitems via SQLAlchemy ORM: can't be distributed to subprocesses:
        ``` 
        The communication protocol between processes uses pickling, and the pickled data is prefixed with the size of the pickled data. For your method, all arguments together are pickled as one object.
        You produced an object that when pickled is larger than fits in a i struct formatter (a four-byte signed integer), which breaks the assumptions the code has made.
        ```
    * reading in bibitems via SQLAlchemy ORM in the subprocesses (and then selecting distributed chuck start/stops): server out of memory
    * → read in bibitems via plain SQL, save in tuples, split and distribute tuples to subprocesses
    * looking up xref links from bibitemlinkmap while matching takes too long → read into memory beforehand (build lookup map)
    * resumability:
        * 35M bibitems, xM logged done, resume after stop:
        * looking through millions long log of stuff already done while matching not feasible → remove beforehand 
        * removing several million from 35M doesn't work feasibly with loop in loop → sort both lists and create more efficient removal algorithm

##### experiments

1712_001.tar.gz probably way too small:  
```
sqlite> select count(distinct uuid) from bibitemarxividmap;
476
sqlite> select count(distinct arxiv_id) from bibitemarxividmap;
463
```  
→ only one citation context per cited document  
→ huge risk of overfitting

0001_001.tar.gz  
```
sqlite> select count(*) from bibitemarxividmap;
2135
sqlite> select count(distinct arxiv_id) from bibitemarxividmap;
1778
sqlite> select count(distinct arxiv_id) from bibitemarxividmap where arxiv_id in (select arxiv_id from bibitemarxividmap group by arxiv_id having count(arxiv_id) > 1);
227    <- citations w/ more than one context
```

**get all citations that have more than one context**  
`sqlite> select uuid, arxiv_id from bibitemarxividmap where arxiv_id in (select arxiv_id from bibitemarxividmap group by arxiv_id having count(arxiv_id) > 1) order by arxiv_id desc;`  
with `in_doc`:  
`sqlite> select in_doc, bibitem.uuid, arxiv_id from bibitemarxividmap join bibitem on bibitemarxividmap.uuid = bibitem.uuid where arxiv_id in (select arxiv_id from bibitemarxividmap group by arxiv_id having count(arxiv_id) > 1) order by arxiv_id desc;`

##### december 2017 dump complete test

* on shetland
* w/ sqlite DB (probably not the most performant) 
* solr instance on shinobu (→ latency)

```
$ time python3 prepare.py /home/saiert/transit/ /home/saiert/arxiv_test_2017_12_all

total: 295710
matches: 52800
checked: 17329
false negatives: 9141
false positives: 159

real    293m9.947s
user    101m10.978s
sys     12m8.748s
```

(parse_latex_tralics (run individually later) is only 19m4.407s of above)

```
$ du -hs metadata.db 
80M	metadata.db
```

**re-run w/ natbib fixes**  
```
$ python3 match_bibitems.py path /home/saiert/arxiv_test_2017_12_all_tralics_preprocessing/all_text
total: 317010
matches: 59277
checked: 21294
false negatives: 10364
false positives: 197
```

(complete) **re-run w/ throwing out tables and figures**  
```
total: 312536
matches: 58613
checked: 20784
false negatives: 10110
false positives: 188
```

##### 1991-2017 dump sample (per month per field ~1%) (14.262 docs)

* on shetland
* w/ sqlite DB (probably not the most performant) 
* **w/o matching**
    ```
    $ time python3 prepare.py /home/saiert/arxiv_sample/ /home/saiert/arxiv_test_1991-2017_sample
    real    91m13.989s
    user    68m19.553s
    sys     14m26.658s
    ```
* matching (same approach as 1712 *full phrase queries etc. for speedup 53.7%*, but only 34.1%)
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_1991-2017_sample_one_percent/arxiv_sample_one_percent_text/
    - - - - - - - - - - - - - - - - -
    375309/375312
    matches: 128178
    checked: 8709
    Phys. Rev.: 52157
    no title: 151838
    false positives: 422
    by arXiv ID: 30200
    by arXiv ID fail: 0
    by doi: 10124
    >>> avg time aID: 0.45
    >>> avg time DOI: 0.51
    >>> avg time ParsCit: 0.02
    >>> avg time Solr: 0.06
    >>> avg time check: 0.00
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 13483.77
    >>> time DOI: 5151.65
    >>> time ParsCit: 5402.39
    >>> time Solr: 12391.17
    >>> time check: 57.81
    >>> time DB r: 390.43
    >>> time DB w: 110.25
    
    real    632m8.252s
    user    81m40.911s
    sys     5m11.341s
    ```
* querying arXiv title from a DB instead of arXiv's web API
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_1991-2017_sample_one_percent/181130_test/
    - - - - - - - - - - - - - - - - -
    375312/375312
    matches: 122667
    checked: 11117
    Phys. Rev.: 52179
    no title: 151919
    maybe (!) false positives: 404
    by arXiv ID: 30580
    by arXiv ID fail: 5167
    by doi: 10129
    by doi fail: 929
    >>> avg time aID: 0.10
    >>> avg time DOI: 0.54
    >>> avg time ParsCit: 0.02
    >>> avg time Solr: 0.06
    >>> avg time check: 0.00
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 2980.74
    >>> time DOI: 5506.91
    >>> time ParsCit: 5271.82
    >>> time Solr: 13172.52
    >>> time check: 57.20
    >>> time DB r: 362.16
    >>> time DB w: 98.81
    
    real    475m3.166s
    user    114m13.424s
    sys     22m4.537s
    ```
* MAG DB notes: existence of index = *55.000 times faster*
    ```
    MAG=> explain analyze select paperid from papers where originaltitle = 'Fun with Fonts: Algorithmic Typography';
                                                           QUERY PLAN                                                       
    ------------------------------------------------------------------------------------------------------------------------
     Index Scan using idx_paperstitle on papers  (cost=0.82..8.84 rows=1 width=8) (actual time=0.098..0.100 rows=1 loops=1)
       Index Cond: (originaltitle = 'Fun with Fonts: Algorithmic Typography'::text)
     Planning time: 0.166 ms
     Execution time: 0.135 ms
    (4 rows)
    
    MAG=> explain analyze select paperid from papers where papertitle = 'fun with fonts algorithmic typography';
                                                             QUERY PLAN                                                          
    -----------------------------------------------------------------------------------------------------------------------------
     Gather  (cost=1000.00..9391587.60 rows=1 width=8) (actual time=16615.889..16619.206 rows=1 loops=1)
       Workers Planned: 2
       Workers Launched: 2
       ->  Parallel Seq Scan on papers  (cost=0.00..9390587.50 rows=1 width=8) (actual time=14324.340..16610.937 rows=0 loops=3)
             Filter: (papertitle = 'fun with fonts algorithmic typography'::text)
             Rows Removed by Filter: 69930913
     Planning time: 0.232 ms
     Execution time: 16619.256 ms
    (8 rows)
    ```
* switching from Solr to PostgreSQL (querying normalized paper titles)
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_1991-2017_sample_one_percent/181202_test/
    - - - - - - - - - - - - - - - - -
    375311/375312
    matches: 121147
    checked: 11077
    Phys. Rev.: 52178
    no title: 151918
    maybe (!) false positives: 383
    by arXiv ID: 30580
    by arXiv ID fail: 5661
    by doi: 10129
    by doi fail: 1147
    >>> avg time aID: 0.10
    >>> avg time DOI: 0.50
    >>> avg time ParsCit: 0.01
    >>> avg time MAGDB: 0.01
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 2948.98
    >>> time DOI: 5109.72
    >>> time ParsCit: 4883.21
    >>> time MAGDB: 1237.15
    >>> time DB r: 322.05
    >>> time DB w: 84.23
    
    real    258m50.437s
    user    81m58.758s
    sys     16m20.550s
    ```
* speculatively cutting off ParsCit identified titles
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_1991-2017_sample_one_percent/181202_test/
    - - - - - - - - - - - - - - - - -
    375311/375312
    matches: 130895
    checked: 11078
    Phys. Rev.: 52179
    no title: 151918
    maybe (!) false positives: 384
    by arXiv ID: 30580
    by arXiv ID fail: 5661
    by doi: 10129
    by doi fail: 1147
    >>> avg time aID: 0.09
    >>> avg time DOI: 0.51
    >>> avg time ParsCit: 0.01
    >>> avg time MAGDB: 0.01
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 2690.60
    >>> time DOI: 5160.01
    >>> time ParsCit: 4401.49
    >>> time MAGDB: 1979.58
    >>> time DB r: 292.79
    >>> time DB w: 80.83
    
    real    257m43.921s
    user    87m58.216s
    sys     13m30.044s
    ```
* guess APS journal paper DOIs (41%)
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_1991-2017_sample_one_percent/181202_test/
    - - - - - - - - - - - - - - - - -
    375312/375312
    matches: 154379
    checked: 11079
    Phys. Rev.: 52179
    no title: 123439
    maybe (!) false positives: 385
    by arXiv ID: 30580
    by arXiv ID fail: 5661
    by doi: 10129
    by doi fail: 1147
    APS doi rebound: 28480
    >>> avg time aID: 0.09
    >>> avg time DOI: 0.53
    >>> avg time ParsCit: 0.01
    >>> avg time MAGDB: 0.01
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 2800.11
    >>> time DOI: 5423.30
    >>> time ParsCit: 4630.38
    >>> time MAGDB: 2169.40
    >>> time DB r: 309.20
    >>> time DB w: 103.15
    
    real    595m7.421s
    user    102m17.161s
    sys     17m5.209s
    ```

##### complete dump

* on tuco
* w/ sqlite DB 
* **w/o matching**
    ```
    $ time python3 prepare.py /vol2/data/arxiv-src-data/ /home/saiert/arxiv-txt-data
    397/1758
    real    793m38.471s
    user    580m4.249s
    sys     101m8.986s (uncaught exception)
    
    527/1758
    real    255m53.911s
    user    187m32.075s
    sys     30m34.096s (uncaught exception)
    
    1758/1758
    947829 files
    70863 PDFs
    real    2496m46.268s
    user    1766m47.943s
    sys     358m11.095s
    
    -> 3546,28m / 59h / 2.5d
    ```  
    ```
    $ ls arxiv-txt-data | grep .txt | wc -l
    1151707
    $ du -h arxiv-txt-data
    50G     arxiv-txt-data
    $ du -h arxiv-txt-data/metadata.db 
    8,4G    arxiv-txt-data/metadata.db
    ```  
    ```
    sqlite> select count(*) from bibitem;
    35053329
    ```
    * 100% → 1.340.770
    * extrapolated #PDFs: 100.240 → 1.240.530
    * pipleline output: 1.151.707 (92.8%)
    * with citations: 1.018.976 (82.1%) (76.0% incl. PDFs)
    * 56.077.906 citation markers
* **matching**
    * twice resumed with 5M and again 5M done in previous sessions; 10 parallel processes
        ```
        $ python3 match_bibitems_mag.py path /home/saiert/arxiv-txt-data 10
        [batch #6]: - - - - - - - - - - - - - - - - -                 
        [batch #6]: 2462301/2462338                                   
        [batch #6]: matches: 876586                                   
        [batch #6]: checked: 70311                                    
        [batch #6]: Phys. Rev.: 355204                                
        [batch #6]: no title: 723153                                  
        [batch #6]: maybe (!) false positives: 2239                   
        [batch #6]: by arXiv ID: 190948                               
        [batch #6]: by arXiv ID fail: 51041                           
        [batch #6]: by doi: 68736                                     
        [batch #6]: by doi fail: 11965                                
        [batch #6]: APS doi rebound: 188593                           
        [batch #6]: >>> avg time aID: 0.10                            
        [batch #6]: >>> avg time DOI: 0.52                            
        [batch #6]: >>> avg time ParsCit: 0.03                        
        [batch #6]: >>> avg time MAGDB: 0.02                          
        [batch #6]: >>> avg time DB r: 0.00                           
        [batch #6]: >>> avg time DB w: 0.02
        [batch #6]: >>> time aID: 18683.87                            
        [batch #6]: >>> time DOI: 36193.84                            
        [batch #6]: >>> time ParsCit: 55165.55                        
        [batch #6]: >>> time MAGDB: 26188.81                          
        [batch #6]: >>> time DB r: 30.61                              
        [batch #6]: >>> time DB w: 13187.03
        ```  
        ```
        [batch #8]: - - - - - - - - - - - - - - - - -
        [batch #8]: 2462301/2462338
        [batch #8]: matches: 877929
        [batch #8]: checked: 70544
        [batch #8]: Phys. Rev.: 355071
        [batch #8]: no title: 724875
        [batch #8]: maybe (!) false positives: 2264
        [batch #8]: by arXiv ID: 191175
        [batch #8]: by arXiv ID fail: 50541
        [batch #8]: by doi: 69380
        [batch #8]: by doi fail: 12158
        [batch #8]: APS doi rebound: 188111
        [batch #8]: >>> avg time aID: 0.10
        [batch #8]: >>> avg time DOI: 0.52
        [batch #8]: >>> avg time ParsCit: 0.03
        [batch #8]: >>> avg time MAGDB: 0.02
        [batch #8]: >>> avg time DB r: 0.00
        [batch #8]: >>> avg time DB w: 0.02
        [batch #8]: >>> time aID: 18689.88
        [batch #8]: >>> time DOI: 36617.27
        [batch #8]: >>> time ParsCit: 55179.79
        [batch #8]: >>> time MAGDB: 26167.74
        [batch #8]: >>> time DB r: 30.43
        [batch #8]: >>> time DB w: 13238.70
        ['done', 'done', 'done', 'done', 'done', 'done', 'done', 'done', 'done', 'done']
        ```
        *number of matched bibitems*  
        ```
        sqlite> select count(*) from bibitemmagidmap;
        14169900        -> 40.4%
        ```
        *number of unique cited docs*  
        ```
        sqlite> select count(distinct mag_id) from bibitemmagidmap;
        2315884
        ```
        *number of unique cited docs usable for evaluation (!!! but all 2.3M usable for live system)*  
        ```
        sqlite> select count(*) from (select count(distinct in_doc) from bibitemmagidmap join bibitem on bibitemmagidmap.uuid = bibitem.uuid group by mag_id having count(distinct in_doc) > 1);
        1320050 
        ```
        *number of bibitems usable for evaluation*  
        ```
        sqlite> select count(uuid) from bibitemmagidmap where mag_id in (select mid from (select count(distinct in_doc), mag_id as mid from bibitemmagidmap join bibitem on bibitemmagidmap.uuid = bibitem.uuid group by mag_id having count(distinct in_doc) > 1));
        13162556
        ```
        *↑ simplified (by assuming a paper doesn't have 2 distinct bibitems referencing the same cited doc) (0.1% change likely due to matching errors)*
        ```
        sqlite> select count(uuid) from bibitemmagidmap where mag_id in (select mag_id from bibitemmagidmap group by mag_id having count(uuid) > 1);
        13183546
        ```
        *way to query DB for evaluation (limit for smaller tests)*
        ```
        sqlite> select bibitem.uuid, mag_id, in_doc, bibitem_string from bibitemmagidmap join bibitem on bibitemmagidmap.uuid = bibitem.uuid where mag_id in (select mag_id from bibitemmagidmap group by mag_id having count(uuid) > 1 limit X) order by mag_id;
        ```
    * further notes on results:
        * top 50 most matched cited docs (31/50 correct):
            * 6662 [2595998925](https://academic.microsoft.com/#/detail/2595998925) -> should actually have been [2166248051](https://academic.microsoft.com/#/detail/2166248051) (same title & author)
            * 5817 [201838852](https://academic.microsoft.com/#/detail/201838852) -> containing work
            * 5424 [2547311611](https://academic.microsoft.com/#/detail/2547311611) -> containing work
            * 5296 [1975165548](https://academic.microsoft.com/#/detail/1975165548) -> should actually have been [1631356911](https://academic.microsoft.com/#/detail/1631356911) (same title)
            * 5133 [1981368803](https://academic.microsoft.com/#/detail/1981368803) ✔
            * 4309 [250990466](https://academic.microsoft.com/#/detail/250990466) -> containing work
            * 4283 [2044442377](https://academic.microsoft.com/#/detail/2044442377) -> should actually have been [2099111195](https://academic.microsoft.com/#/detail/2099111195)
            * 4029 [1526931241](https://academic.microsoft.com/#/detail/1526931241) -> should actually have been [2120062331](https://academic.microsoft.com/#/detail/2120062331)
            * 3979 [1889142700](https://academic.microsoft.com/#/detail/1889142700) ✔
            * 3880 [2030690537](https://academic.microsoft.com/#/detail/2030690537) ✔
            * 3394[1858542512](https://academic.microsoft.com/#/detail/1858542512) ✔
            * 3262 [2139937287](https://academic.microsoft.com/#/detail/2139937287) ✔
            * 3117 [2139937287](https://academic.microsoft.com/#/detail/2139937287) ✔
            * 3113 [1978553093](https://academic.microsoft.com/#/detail/1978553093) ✔
            * 3068 [1972485395](https://academic.microsoft.com/#/detail/1972485395) -> should actually have been [2157005274](https://academic.microsoft.com/#/detail/2157005274) (same title & author)
            * 2937 [2794162265](https://academic.microsoft.com/#/detail/2794162265) -> should actually have been [2029403139](https://academic.microsoft.com/#/detail/2029403139)
            * 2918 [2763160969](https://academic.microsoft.com/#/detail/2763160969) -> should actually have been [2167727518](https://academic.microsoft.com/#/detail/2167727518)
            * 2752 [2011208902](https://academic.microsoft.com/#/detail/2011208902) ✔
            * 2734 [2334487375](https://academic.microsoft.com/#/detail/2334487375) -> should actually have been  *C. Itzykson and J.-B. Zuber, Quantum Field Theory* (yet to be found in MAG) and/or [1577720992](https://academic.microsoft.com/#/detail/1577720992)
            * 2734 [1909359706](https://academic.microsoft.com/#/detail/1909359706) ✔
            * 2731 [1686810756](https://academic.microsoft.com/#/detail/1686810756) ✔
            * 2723 [2800235016](https://academic.microsoft.com/#/detail/2800235016) -> should actually have been [2094438648](https://academic.microsoft.com/#/detail/2094438648)
            * 2680 [2798540069](https://academic.microsoft.com/#/detail/2798540069) -> should actually have been [1486187347](https://academic.microsoft.com/#/detail/1486187347)
            * 2656 [2039609754](https://academic.microsoft.com/#/detail/2039609754) ✔
            * 2607 [1522301498](https://academic.microsoft.com/#/detail/1522301498) ✔
            * 2586 [1992204683](https://academic.microsoft.com/#/detail/1992204683) ✔
            * 2559 [2095721433](https://academic.microsoft.com/#/detail/2095721433) ✔
            * 2497 [2258584306](https://academic.microsoft.com/#/detail/2258584306) ✔
            * 2448 [2135046866](https://academic.microsoft.com/#/detail/2135046866) ✔
            * 2430 [254615967](https://academic.microsoft.com/#/detail/254615967) -> should actually have been [1488001281](https://academic.microsoft.com/#/detail/1488001281)
            * 2407 [2037768897](https://academic.microsoft.com/#/detail/2037768897) ✔
            * 2354 [2147875900](https://academic.microsoft.com/#/detail/2147875900) ✔
            * 2250 [2073595060](https://academic.microsoft.com/#/detail/2073595060) ->  Phys. Rev. Lett. DOI heuristic took *Phys. Rev. Lett. 73 (1994) XXXX* for article #1994 from Phys. Rev. Lett. 73
            * 2214 [1970127494](https://academic.microsoft.com/#/detail/1970127494) ✔
            * 2211 [2046747256](https://academic.microsoft.com/#/detail/2046747256) -> should actually have been [2028815089](https://academic.microsoft.com/#/detail/2028815089)
            * 2196 [2051051926](https://academic.microsoft.com/#/detail/2051051926) ✔
            * 2125 [2194775991](https://academic.microsoft.com/#/detail/2194775991) ✔
            * 2080 [2222357604](https://academic.microsoft.com/#/detail/2222357604) -> should actually have been [1575147392](https://academic.microsoft.com/#/detail/1575147392)
            * 2028 [2064777467](https://academic.microsoft.com/#/detail/2064777467) ✔
            * 1950 [2800027815](https://academic.microsoft.com/#/detail/2800027815) -> should actually have been [2751862591](https://academic.microsoft.com/#/detail/2751862591)
            * 1941 [1525575583](https://academic.microsoft.com/#/detail/1525575583) ✔
            * 1938 [1530042113](https://academic.microsoft.com/#/detail/1530042113) ✔
            * 1926 [2044336104](https://academic.microsoft.com/#/detail/) ->  Phys. Rev. Lett. DOI heuristic took *Phys. Rev. Lett. 64 (1990) XXXX* for article #1990 from Phys. Rev. Lett. 64
            * 1923 [1979544533](https://academic.microsoft.com/#/detail/1979544533) ✔
            * 1868 [2035413341](https://academic.microsoft.com/#/detail/2035413341) ✔
            * 1800 [2134251287](https://academic.microsoft.com/#/detail/2134251287) ✔
            * 1800 [2030164271](https://academic.microsoft.com/#/detail/2030164271) ✔
            * 1797 [2107240173](https://academic.microsoft.com/#/detail/2107240173) ✔
            * 1790 [1986510200](https://academic.microsoft.com/#/detail/1986510200) ✔
            * 1786 [2034920898](https://academic.microsoft.com/#/detail/2034920898) ✔
        * → main problems:
            * works with identical normalized title → FIX: re-check all in MAG w/ same title for author and #cit
                * snapshot: `8329000/14169900 (521923 updated)`
                    * TODO: "re-run" for counting cases of no author match
                * → FIX integrated in actual matching process
            * wrong title identified
                * via e.g. APS DOI heuristic (`Phys. Rev. Lett. 73 (1994) 3070` → 1994 as article number) → FIX: pre-filter dates in brackets
                * in general: for all bibitems matched to a certain MAG ID, mby check overall text similarity of bibitem texts
            * wrong "extra matches" under certain conditions because a match variable was not reset for every turn of a loop → FIX: fix code, re-run matching

### citation recommendation baseline

* idea: only syntax (mby +metadata) based
* misc notes:
    * [python code example using gensim](https://github.com/charles-vdulac/japanese-content-engine) (from [this presentation](https://www.youtube.com/watch?v=-xVeMQXMEcg))
    * [*simple* python code example using only pandas and scikit-learn](http://blog.untrod.com/2016/06/simple-similar-products-recommendation-engine-in-python.html)
    * [graph database](https://www.kernix.com/blog/an-efficient-recommender-system-based-on-graph-database_p9) [based thingy](https://www.kernix.com/blog/recommender-system-based-on-natural-language-processing_p10)

**train/test data generation stuff**  
```
$ cat items_1712_100w_4mincont_not_stemmed_oneperdoc.csv | wc -l    ← only one context per uuid per doc
11613
$ cat items_1712_100w_4mincont_not_stemmed.csv | wc -l              ← notice there's multiple contexts per doc
77121
$ cat items_1712_100w_4mincont_2mindoc_not_stemmed.csv | wc -l      ← notice you should do per doc splitting (require >1 docs)
41359
```

**TFIDF baseline test w/ gensim**  
(many fails, below a lucky nice example)  
```
$ python3 recommend.py items_1712_100w_4mincont.csv
similar to: math/9904022 (6768)
correct: [6186, 9728, 9771]
- - - - - - - -
✔ 0.309774786233902: 9770
✔ 0.179429829120636: 6186
  0.1443936824798584: 10067
  0.13785989582538605: 7983
  0.13637538254261017: 4082
  0.1355295479297638: 3858
  0.13414070010185242: 9969
  0.1162891760468483: 1831
  0.11208947747945786: 4716
  0.11141400039196014: 3620
  0.10494373738765717: 6270
```

**TFIDF baseline test w/ gensim** (80/20 split, then combine training vectors for same cited doc)  
```
$ time python3 recommend.py items_1712_100w_4mincont_stemmed.csv
- - - - - 1404/1404 - - - - -
#1: 199
in top 5: 663
in top 10: 954
avg: 14.554843304843304
ndcg: 0.4466429864293447
map: 0.2992083662933575
ndcg@5: 0.3093782646226242
map@5: 0.25600664767331355

real    15m13.260s
user    15m14.722s
sys     0m14.319s
```

not stemmed  
```
$ time python3 recommend.py items_1712_100w_4mincont_not_stemmed.csv
- - - - - 1404/1404 - - - - -
#1: 254
in top 5: 753
in top 10: 980
avg: 18.957264957264957
ndcg: 0.48227821110408237
map: 0.3448019509227324
ndcg@5: 0.3660963359894245
map@5: 0.3098765432098757

real    15m10.916s
user    15m12.664s
sys     0m13.642s
```

minimum of 2 citation contexts  
```
$ time python3 recommend.py items_1712_100w_2mincont_not_stemmed.csv
- - - - - 6843/6843 - - - - -
#1: 596
in top 5: 1895
in top 10: 2690
avg: 333.87315504895514
ndcg: 0.3212211934067741
map: 0.1825363717938699
ndcg@5: 0.18340135284347936
map@5: 0.15270836377807132

real    234m12.826s
user    233m30.086s
sys     0m54.815s
```

more data (natbib fix)  
```
$ python3 recommend.py items_1712_100w_2mincont_not_stemmed_NATBIB_FIX.csv
- - - - - 10497/10497 - - - - -
#1: 699
in top 5: 2382
in top 10: 3600
avg: 481.9152138706297
ndcg: 0.2925079165753687
map: 0.15257636645208303
ndcg@5: 0.14781602404672087
map@5: 0.12187450382649058
```

currently running evals: test difference between one per doc (2.7k test) and many per doc (18k)

one per doc:  
```
$ time python3 recommend.py ~/items_1712_100w_4mincont_not_stemmed_oneperdoc.csv
- - - - - 2790/2790 - - - - -
#1: 366
in top 5: 1305
in top 10: 1769
avg: 30.940501792114695
ndcg: 0.43196490690778483
map: 0.28601776529361
ndcg@5: 0.3027048501650121
map@5: 0.24845280764635663

real    61m20.370s
user    61m13.365s
sys     0m21.400s
```

many per doc *without* per doc splitting:  
```
$ python3 recommend.py ~/items_1712_100w_4mincont_not_stemmed.csv
- - - - - 18438/18438 - - - - -
#1: 6201
in top 5: 12608
in top 10: 14513
avg: 37.909643128321946
ndcg: 0.597700200736707
map: 0.48999624579362355
ndcg@5: 0.5218584144074571
map@5: 0.46789872365042323
```

many per doc *with* per doc splitting:  
```
$ python3 recommend.py items_1712_100w_4mincont_2mindoc_not_stemmed.csv
- - - - - 7787/7787 - - - - -
#1: 909
in top 5: 2995
in top 10: 4081
avg: 97.53460896365738
ndcg: 0.38836365919123905
map: 0.24484656370685862
ndcg@5: 0.25365219494863167
map@5: 0.21056675656007817
```

counting adjacent citations as half relevant:  
```
$ python3 recommend.py items_1712_100w_4mincont_2mindoc_not_stemmed.csv
- - - - - 7787/7787 - - - - -
#1: 909
in top 5: 2995
in top 10: 4081
ndcg@5: 0.23297871616321733   <-- slightly worse, because IDCG in nDCG Formula > 1 and apparently not many co citations in top 5
map@5: 0.21056675656007817
```

using MAG ID mapped larger dataset (↑41359 vs. ↓74074):  
```
$ python3 recommend.py items_1712_100w_4mincont_2mindoc_not_stemmed_mag.csv
- - - - - 14301/14301 - - - - -
#1: 1488
in top 5: 4778
in top 10: 6480
ndcg@5: 0.1968435487531205
map@5: 0.18446961750926533
```

##### notes on evaluation

* thoughts on exact re-recommendation
    * exact re-recommendation would imply the assumption, that for a given context only this very citation is acceptable
    * including direct co-citations (`\cite{a,b}`) acknowledges, that either of the cited docs is relevant
        * a performance drop of nDCG is due to a more realistic shift in expectation: ideal is *not* to have a relevant doc somewhere in the top 5 but to have 5 relevant docs to choose from
        * in the same vein, "indirect co-citation" (in the same context) could very well be considered somewhat relevant
* considerations when train/test splitting
    * always ensure a "stratified" split (i.e. don't just split 80/20 along all items in some order, split each *grouped per cited doc*-mini-package 80/20)
    * because an author's word choice and a paper's content can be a strong signal, it would be ideal to not have citations from one paper split (some in test, some in training)
        * just like typhoon/sequence wise splits in pyphoon
        * → require at least 2 *containing docs* for evaluation, then do a document wise train/test split
* misc
    * minimum threshold for citation contexts (2 and 4 tested) has large impact on performance
 


### citation recommendation based on entities

* no clue of argumentative structure yet → chose fixed citation context size
* mby equivalent [1](https://link.springer.com/content/pdf/10.1007%2F978-3-319-30671-1_3.pdf), [2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.8098&rep=rep1&type=pdf), [3](http://l3s.de/~fetahu/publications/cikm2016.pdf), [4](http://www.l3s.de/~fetahu/publications/fetahu_cikm2015.pdf) (recommend news articles for wikipedia articles referencing past events)

##### named entity linking

* find tool for linking stuff to wiki-/dbpedia
* `curl -d '<item><text>Convolutional neural networks have been used for artificial intelligence.</text></item>' -X POST http://132.230.150.127:8080/text-annotation-with-offset-Nov14/`
* citations + POS tags (only in part involving named entities):
    * `[] (+ sub clause) + verb` (e.g. *[] show that* or *[], cited a gazillion times, found that*)
    * `proper Noun (+ et al.) + []` (e.g. *the work of ____ []*)
    * `proper Noun (+ "dataset") + []` (e.g. *DBpedia []*) (mentioned in Abu-Jbara2013 (there: noun phrase + []))
    * `preposition (+ Authors) + []` (most general case; e.g. *in []*, *by []*, *from []*)
    * `[].` (sentence end; could be anything)
* spotlight docker test
    * `$ docker pull dbpedia/spotlight-english`
    * `$ docker run -i -p 2222:80 dbpedia/spotlight-english:latest spotlight.sh`
    * `$ time python3 spotlight_annotate.py /home/saiert/arxiv_test_2017_12_all_181110/all_text/`  
        ```
        real    40m35.011s
        user    3m25.997s
        sys     0m5.937s
        ```  
        ```
        $ ls arxiv_test_2017_12_all_181110/all_text/ | grep txt | wc -l
        8996
        $ ls arxiv_test_2017_12_all_181110/all_text/ | grep json | wc -l
        8821
        ```
* MAG entity test
    * 8808 citation contexts (100w)
        ```
        $ ls citcontxs | wc -l
        8808
        $ du -hs citcontxs
        35M     citcontxs
        $ time java -jar linking_file_tarek.jar citcontxs citcontxs_out/
        [...]
        [22:00:08] [INFO ] [MentionDetectorLSH  ]       Collisions: 1366140
        
        real    143m10.872s
        user    367m38.134s
        sys     5m17.375s
        ```
    * 22k plain text *papers*
        ```
        $ ls /home/saiert/arxiv_test_1991-2017_sample/arxiv_sample_text/ | wc -l
        22840
        $ time java -jar linking_file_tarek.jar /home/saiert/arxiv_test_1991-2017_sample/arxiv_sample_text/ /home/saiert/arxiv_test_1991-2017_sample/arxiv_sample_annot/
        [...]
        
        real    10879m20.052s
        user    55615m56.373s
        sys     393m0.262s
        ```
        * manual evaluation of annotations on 3 example papers (`cs0110014`, `1404.1775`, `1712.08091`)
            * precision: 75; 70; 61
            * recall: 49 (/60); 51; 56
        * annotations in 39.300 annotation context
            * counting all:
                ```
                annot./context:
                    avg.: 7.1
                    min.: 0
                    max.: 41
                 0: 940 (2.4%)
                 1: 1500 (3.8%)
                 2: 2319 (5.9%)
                 3: 3071 (7.9%)
                 4: 3796 (9.7%)
                >4: 27374 (70.2%)

                total #ann. @level
                    0: 2640
                    1: 11517
                    2: 197266
                    3: 37692
                    4: 14907
                    5: 12721
                ```
            * counting only annot. w/ conf. > 5.0:
                ```
                annot./context:
                    avg.: 4.3
                    min.: 0
                    max.: 37
                 0: 3161 (8.1%)
                 1: 4591 (11.8%)
                 2: 5423 (13.9%)
                 3: 5401 (13.8%)
                 4: 4893 (12.5%)
                >4: 15531 (39.8%)
                
                total #ann. @level
                    0: 2640
                    1: 11515
                    2: 131902
                    3: 17344
                    4: 4580
                    5: 0
                ```
        * testing annotations as pre-filter (snapshot) (candidates from *all* MAG papers (not *so* representative b/c arXiv papers mostly phys/math/cs))
            ```
            - - - filter canditates - - -
            min: 813
            max: 67438444
            avg: 1248.1812758145938 (5.949592296973678e-06%)
            still in: 296/2179
            ```

**simply feeding (dbpedia) annotations into TFIDF baseline** (building on above *counting adjacent citations as half relevant*)  
```
$ python3 recommend.py items.csv    (1712 100w 4mincont 2mindoc spotlight_annot)
- - - - - 7787/7787 - - - - - simply feed | artificially let each DBpedia IDs appear unique in corpus (boost TFIDF)
#1: 900 | 890
in top 5: 2993 | 2916
in top 10: 4080 | 4016
ndcg@5: 0.232 | 0.227
map@5: 0.209 | 0.205
```

**only using (dbpedia) annotations**  
```
$ python3 recommend.py items.csv    (1712 100w 4mincont 2mindoc spotlight_annot)
- - - - - 7787/7787 - - - - -
#1: 559
in top 5: 1760
in top 10: 2571
ndcg@5: 0.13512704458191738
map@5: 0.12488763323487953

real    67m51.744s
user    67m43.101s
sys     0m22.163s
```

##### ~~named entity recognition~~ (probably not so important)

* [NLTK](https://www.nltk.org/)
    * simple example:  
        ```
        import nltk
        s = 'Some sentence with named entities like Bill Gates.'
        sent = nltk.word_tokenize(s)
        sent = nltk.pos_tag(s)
        print(nltk.ne_chunk(sent, binary=True))
        ```
* spaCy
    * apparently faster than NLTK but opinionated (offers one implementation per task, not multiple to choose from)
* gensim
    * "most commonly used for topic modeling and similarity detection"

### citation recommendation based on claims

* some clue of argumentative structure (?) → mby variable citation context size (i.e. sentences)
* [ClausIE](https://gate.d5.mpi-inf.mpg.de/ClausIEGate/ClausIEGate/)
* also consider *fact extraction*

### citation recommendation based on arguments

* *as intro, read* `Stab2016` (+mby `Habernal2017`)

* "detection of context should be related to the identification of the argument around the citation" (HERNANDEZ-ALVAREZ2016)
* [MARGOT](http://margot.disi.unibo.it/) (!!! **local version on tuco** [+old page of same authors](http://argumentationmining.disi.unibo.it/publications.html)

##### michael notes

* definition in paper Argumentation Mining: The Detection, Classification and Structure of Arguments in Text
    * challenges/goals of argumentation mining:
        * extract arguments ("Component identification" sometimes)
        * extract and represent the internal structure of arguments, i.e., the propositions and their relations (an argument consists of at least two propositions, so the assumption here; "Component classification" sometimes)
        * extract the argumentation structure, i.e., the interaction between arguments ("structure identification" sometimes).
* good survey: [Parsing Argumentation Structures in Persuasive Essays](https://arxiv.org/pdf/1604.07370.pdf)
* good survey: [Argumentation Mining in User-Generated Web Discourse](https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00276)
    * dort u.a. Erwähnung/Verlinkung von confirmation bias: Link zu *Recognizing the Absence of Opposing Arguments in Persuasive Essays* (darin: "tendency to ignore opposing arguments is known as *myside bias* or *confirmation bias* (Stanovich et al., 2013). It has been shown that guiding students to include opposing arguments in their writings significantly improves the argumentation quality, the precision of claims and the elaboration of reasons (Wolfe and Britt, 2009). Therefore, it is likely that a system which automatically recognizes the absence of opposing arguments effectively guides students to improve thier argumentation".
* Weitere aktuelle Arbeiten (z.B. gute und schlechte Ansätze) im ["Argument Mining"-Workshop](http://aclweb.org/anthology/W17-51) (z.B. "What works and what does not: Classifier and feature analysis for argument mining")
* Auch noch leicht verwandt: [SemEval-2018 Task12: Argument Reasoning Comprehension](https://competitions.codalab.org/competitions/17327)
* further resources
    * Sehr nützlich: ["Transition words"](https://msu.edu/~jdowell/135/transw.html) benutzt in Recognizing the Absence of Opposing Arguments in Persuasive Essays.
* also
    * Discourse Theory, e.g. Discourse Representation Theory, bzw. Discourse Analysis:
        * "Similar  to  the  identification  of  argumentation structures,  discourse  analysis  aims  at identifying elementary discourse units and discourse relations between them. Existing approaches on discourse analysis mainly differ in the employed discourse theory." [https://arxiv.org/pdf/1604.07370.pdf](https://arxiv.org/pdf/1604.07370.pdf)
    * Argumentation theory

# misc notes

* [Daniel Hershcovich](http://www.cs.huji.ac.il/~danielh/)
* python
    * [scikit-learn](http://scikit-learn.org/)
        * `fit-predict` pattern
        * classification: `train_test_split` function has useful param `stratify` to ensure balanced distribution over classes
    * [pandas](https://pandas.pydata.org/)
        * classification: `pd.plotting.scatter_matrix` in case of not so many features
* scipy random forest works column wise and therefore is more efficient on csc (not csr) sparse matrices
* probably not useful but:
    * [pybtex.org](https://pybtex.org/)
    * [pycld2 (language detection)](https://github.com/aboSamoor/pycld2)