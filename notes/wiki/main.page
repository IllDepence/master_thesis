# orga

### big TODOs

* clean DB of bibitems where in_doc has been removed due to not containing any `{{cite:...}}`s
    * *also* clean DB of bibitems where UUID is not in in_doc
    * *also* clean linked bibitemmagidmap entries in ↑ cases 
* generate a complete dataset ✔
    * generate derivative datase that allows for 5-fold CV ✔
        * fix memory problems in recommendation ✔
            * implement k-fold cross validation 〜
            * last minute changes to BOW baseline (stopwords, case insensitivity, etc.); ✔ then do baseline eval (currently running: 97.2/2.8 split all and 80/20 split 10% (\* adjacent still counted as .5 relevant—**fix** after runs over and numbers compared))
    * pre-selection of candidate recommendations to speed up process
        * IR type method: generate index for comparison targets, retrieve e.g. 1k targets, really compare to those
        * clustering of comparison targets
        * [k4nn](https://arxiv.org/abs/1802.08301)
* test conditional probability sentence end citation on arXiv CS dataset
    * focus on precision b/c few patterns will probably not cover most of the citations at sentence ends

### recent notes

* when restricting test set to last X months of citing papers, how much of all possible cited papers are then covered in our dataset?
    * side thought: mby integrate time of publication of citing papers into learning  
        (=recent citations are more valid. rly true? possibly not necessarily)  
        (better idea? citations in papers with many citations are more valid? page rank like)
* test michael noun phrase tool

---

* evaluate FoS (only) recommendation
    * with limitations found to be "effective" for combined recomm.
    * only in cases where FoS annotations is directly preceding/very close before citation marker (`foo bar <annot> [1] bam baz`)
* evaluation on MAG citation contexts (best just CS↔CS), also, for MAG, filter by language (only English)
    * select, export, run FoS annotation

---

* predpatt annotations
    * distinguish by pattern (e.g. ?a does ?b vs. using ?b)
        * also evaluate useful patterns/throw out those where sub. or obj. is too long etc. (goal: focus on a specific type)
    * predpatt manual evaluation of relevance
        * mby do something with distance
* re-run seeland fos annot. 〜
* manually eval matching accuracy ✔
* test clustering on larger set
* wait for feedback on paper

---

* dataset
    * fill contents into Overleaf project (guidance: LREC2018 paper)
    * more stats on dataset (citing docs per cited doc (expect long tail distribution))
        * → screen *dataset_stats1* (re-calc of cited doc foss) ✔
        * → screen *dataset_stats2* (citing foss + citation fos pairs) 〜 (mby ~25.01.)
    * create visualization, explaining ~~3~~ 4 types of quantities (#contexts, #citing docs, #cited docs) ✔
* FoS
    * rather from context then just from MAG (how a publication is referred to might differ from its content; e.g. standard citation for lod is about dbpedia (or so ...))
* claims
    * for claim extraction think of replacing Citation tokens w/ a name or so
    * extracted claims how to model? how to use?
* for [non-]integral citations look again at dependency parsers (mby use online demos)
* FoS annotation tool probably overwrites; calc speed, restart etc. ✔

---

* dataset generation for the big DB (with sentence size contexts)
    * OpenIE test
    * pre clustering for pre filtering
        * baseline recommendation w/ the big dataset
    * generation of a "sub"/more processed dataset (contexts of only usable citations)
        * FoS/claim/argument annotations
            * semantic approach recommendation w/ the big dataset
* read up on argument mining etc.

---

* compare in context FoS annotations w/ all annotations of cited paper (i.e. look at references to 22k set)
    * also compare to FoS annotations of citing paper to see if it is an "outlier" context
    * (MAG also has FoS for papers, mby also use, but their confidence values are not that telling)
* ensure parallelisation of bibitem matching
    * start
* @preselection: _clustering as offline step_, online compare to centroids

---

* check confidence values of MAG FoS annotation false positives
* analyse MAG FoS annotations *in citation contexts* overall (number per context, from the right field, what level etc.)
* arXiv complete plain text dump (on tuco)
    * change pipeline to do parts and delete stuff inbetween
    * debug mode switch for generation of log files
* mby more conclusive/exact/detailed analysis on PoS pattern comparison between cit. and non cit. sentences 
* mby answer: distinction of integral vs. non-integral citations, relevant for analysis/representation of cit. contexts?
    * given a sentence w/ a placeholder, is retroactive distinction of integral vs. non-integral possible?

---

* marker surroundings more like patterns (find top 10 (-3, 3)) e.g. NN V NN [] <EOS> <EOS> <EOS>
* NEL precision (also recall) manual check (any concepts missing in FoS?)
    * also manually look into position of MAG FoS annotations in relation to citations markers
    * how should the FoS representation look like? (!!!) -> answer: how will MAG FoS annotations be incorporated in approach?
* false positive recheck?
* how frequent are cases like "ImageNet []"?
* also: MAG postgresql DB set up: try to use
* also, for all above, look into using MAG citation contexts

---

* heatmap 引き続き
    * commonness/frequency of pos tags/pos pattern (e.g. is "proper noun [] verb" common)
    * heatmap per discipline
* MAG matching
    * (neural) parcit /sliding window/fuzzy matching test
* NEL
    * mag annotations as normal entities → eval.
    * mag annotations recall (manual check, check contexts and see if anything should've been annotated that is not)
    * mag / spotlight entities, most frequent in 1712 (show e.g. top 15 most frequent entities)
        * do they fit do distribution of fields of research in 1712

---

* test MAG thingy linking (could be used for pre-filtering)
* using NEL *only* for `<dataset/algo/etc.> []` and the claim and argument approaches for all other citation contexts (by first classifying the context) could be an option

---

* refine usage of entities
    * manual evaluation: get some citation context and the contained linked entities; check: are the linkings correct; check: is the presence of the entities relevant for the citation itself (also consider special cases like <NEL>[] like ImageNet[1])
* storage of annotations for now okay
* do use replacement tokens (CIT, FORMULA, etc.) instead of just deleting them from plain text
* maybe have a look at MAG citation contexts (feed into current baseline?)

---

* match to MAG IDs instead of arXiv IDs
* mby treat "co-appearing" citations as valid (e.g. "some stuff super important [1][2][3]" <- all three count)
* general idea: high weight on smaller local context, then also consider (with some lower weight) larger context
* do pre-selection before cosine sim (or similar) -> how?
* (NEL) annotate documents, *then* extract citation contexts (b/c annotators use large context -> better annotation results)

### overall

* thema verstehen → expose (mail mit details kommt noch) schreiben → thesis anemlden
* genaue richtung noch nicht so fest wie vermutet

# links / notes

* arXiv
    * 1.4 M documents → processing each for **0.42 seconds** takes **~1 week**
    * [sources](https://arxiv.org/year/cs/18) (single dl)
    * [IDs over time](https://arxiv.org/help/arxiv_identifier)
    * availability of DOIs (in CS only dump from Dec. 2017)
        * ```
        >>> len(tree.xpath("//dc:identifier[starts-with(text(),'doi')]", namespaces={'dc':'http://purl.org/dc/elements/1.1/'}))
        29739
        ```
        * → 28775 of which are contained in MAG
        * ```
        >>> len(tree.xpath("//ListRecords/record"))
        155308
        ```
        * → 19%
* MAG
    * [schema](https://microsoftdocs.github.io/microsoft-academic/microsoft-academic-graph/reference/data-schema.html)
    * [blog post w/ stats](https://www.microsoft.com/en-us/research/?post_type=msr-blog-post&p=480543&preview=true)
    * [oldish examples on quora](https://www.quora.com/Where-can-I-obtain-a-schema-description-of-Microsoft-Academic-Graph-data/answer/Milind-Gokhale-1)
    * availability of DOIs
        * ```
        saiert@shetland:~$ cut -f3 -d$'\t' /vol1/mag/data/2018-07-19/dumps/Papers.txt | sed '/^\s*$/d' | wc -l
        75266339
        ```
        * ```
        saiert@shetland:~$ cat /vol1/mag/data/2018-07-19/dumps/Papers.txt | wc -l
        206252196
        ```
        * → 36.5%
    * level 0 FoS overlap
        * phys         8,682,931
        * math         6,701,038
        * cs          14,225,297
        * cs+math     2,254
        * math+phys   1,737
        * phys+cs       287

# questions

* entity based citerec
    * go-to example/paper? (candidates for similar task see below)
* general (still somewhat far away)
    * not a single of the citerec approaches in CireRecSurvey uses the Microsoft Academic Graph (or arXiv for that matter) as dataset, right? (→existence of approaches to directly compare to?)
    * similar to ^ baseline to start from?
    * other approaches often are a large ensemble of techniques/features/prior work reused — should I rather build/try out "from scratch" or  recreate/assemble existing things? in both cases: is trying for improvement just enough or do choices need to be justified further?
    * ^ pick/engineer vs "brute force" (i.e. just try a lot of different stuff and see what contributes most to a good result)

# structured notes

### general

* task
    * mail
        * <small>**keinen komplett neuen Ansatz** zu citation recommendation entwerfen, sondern eher ein gutes lauffähiges System zu citation recommendation bauen. Bereits die Nachimplementierung eines "state-of-the-art"-Systems wäre eine contribution</small>
        * <small>Wahrscheinlich werden wir uns eher **semantisch-strukturierte Repräsentationsformen (entities, claims, arguments) für citation recommendation** widmen.</small>
        * <small>(Ashwath ... Indizierung des großen Microsoft Academic Graphs (MAG) und um die Verwendung eines state-of-the-art approaches für citation recommendation)</small>
        * <small>in Ihrer Arbeit eher mit **semantischen und pragmatischen Ansätzen für citation recommendation** beschäftigen. Diesbezüglich gibt es nämlich **in der Literatur noch recht wenig**</small>
        * <small>An sich ist der Microsoft Academic Graph als Datensatz für citation recommendation geeignet, allerdings enthält er nur einen Satz als citation context -- und ein größerer Kontext wäre oft angebracht, besonders, wenn wir nicht nur Entitäten zitieren, sondern ganze Argumente. Ganze Papers (fulltext) sind an sich nicht im MAG verfügbar. Die Lösung läge darin, stattdessen die Paper von arXiv.org als Subset zu verwenden. Diese sind m.E. auch im MAG. Außerdem können diese von arXiv heruntergeladen werden und liegen im Format TeX (+bibtex) vor, was den Vorteil hat, besser an den Reintext und an die Zitate zu kommen. Allerdings müsste ein TeX parser geschrieben werden, der den Rohtext usw. extrahiert. In Haskell wurde ein solch einfacher Parser bereits entwickelt [1], allerdings ohne Berücksichtigung von section headers etc.</small>
        * <small>Zitate kann man differenzieren dahingehend, ob eine Entität (z.B. ein Konzept oder ein Datensatz oder eine Methode) zitiert wird, oder eine Behauptung (claim), oder eine Behauptung/Argument im Zusammenspiel einer Argumentationskette. Demzufolge kann man für jede dieser Möglichkeiten geeignete Repräsentationsformen (aus der Computerlinguistik) für ein besseres citation recommendation verwenden. **Konkret kann man z.B. mentions im Content des Papers zu Entitäten in Wikipedia oder Wikidata verlinken (entity linking bzw. text annotation), sowie claims oder ganze Argumentationsketten extrahieren und entsprechend formal repräsentieren** (einige Beispiele diesbzgl. sind in [2-5]; die genauen Ansätze, die wir verwenden werden, können wir später festlegen). Insgesamt ginge es also hier um die **Anwendung von NLP-Komponenten auf wissenschaftliche Texte**.</small>
        * <small>Im Rahmen der Masterarbeit würde man **für jedes Unterthema (z.B. entity, claim, argumentation) einen Ansatz für citation recommendation entwickeln**. Für eine Einschätzung der Schwierigkeiten bei der Ansatzentwicklung und dann später für eine Evaluation der Ansätze müssen wohl einige Texte auch manuell annotiert werden. Außerdem würde man die Implementierungen der einzelnen Systeme zur recommendation in das Gesamtsystem von Ashwath integrieren.</small>
    * gist
        * starting point
            * MAG has much going for it, but only one sentence citation context
            * arXiv has paper sources (TeX)
        * use intersection of MAG and arXiv
        * get plain text from arXiv source
        * match MAG citation contexts to arXiv plain texts (? if so, plain text of arXiv would be sufficient (no need to retain citations))
        * → MAG with arbitrary large citation contexts
        * ———
        * in large citation contexts, find and formally represent "semantic NLP components" (using *all* the buzzwords)
        * ML
        * ???
        * profit 
* defs
    * argument = claim + one or more premises justifying the claim
        * [premises] [step(s) of deduction] [claim]
        * x (and y), therefore z

### start out w/ MAG or arXiv

* MAG
    * `+` matching MAG IDs to arXiv IDs should be quite reliable (could even use citation contexts to match to arXiv plain text)
    * `+` citation network is already given
    * `-` (blind?) reliance on MS / less transparent
    * `-` citation markers might not be *that* reliable (need to match arXiv source bibitem strings to MAG IDs)
    * `o` might enable just using latexpand+detex for TeX processing
* argument for arXiv
    * `-` matching arXiv source bibitem strings to arXiv IDs might not be *that* reliable
    * `+` more transparent
    * `+` citation markers are (albeit dependent on initial matching) certain
    * `+` (if we don't add in the MAG:) only rely on one source → cleaner approach (?)

questions to be answered:

* how hard is matching arXiv bibitem strings to arXiv IDs?
* if starting from arXiv, what is gained from adding in the MAG?
* ...

### tex parser

* grabcite[/test-tex](https://github.com/agrafix/grabcite/tree/master/test-tex) mby "edge cases"? (download source by ID, don't just use .tex file from repo)

##### existing software

* ~~[plastex](https://github.com/tiarno/plastex) — largish, last commit 1y ago, python~~
    * in addition to requirements.txt also `$ pip install Pillow Unidecode` for less noisy output
    * TEST: neat output when it works, **but** failed on all tested arXiv papers and it's own documentation
* [TexSoup](https://github.com/alvinwan/texsoup) — smallish, recent commits, python
    * doesn't handle definitions (`\dev`) [source](https://stackoverflow.com/a/50151171)
    * [stackoverflow question](https://stackoverflow.com/questions/49779853/) specifically asking about arXiv TeX (by someone using TeXSoup)
    * TEST: works on most tested; fails with e.g. `1607.00138`
    * find citations
        ```
        soup = TexSoup(tex_str)
        c = soup.find_all(name='cite')
        ```
    * output text
        ```
        soup = TexSoup(tex_str)
        for cntnt in soup.document.contents:
            if type(cntnt) == str:
                print(cntnt)
        ```
    * **TESTED** on larger sample: 32% fails
        * 54 of 169 failed — can be reduced to 50 by replacing preamble w/ a clean one
        * → mby do 70% TexSoup + 30% fallback sth. (e.g. detex + python script to put cites back in?)
* [grabcite](https://github.com/agrafix/grabcite)
    * installation
        * required additional packages `libpcre++-dev`, `libpq-dev`, `libghc-hdbc-odbc-dev` (+500 MB of dependencies)
    * unpack arXiv dump
        * file given for param `--arxiv-meta-xml` has do be in cwd (giving a path results in `grabcite-datagen: InvalidRelFile "..."`)
        * arXiv: *"Note: Many of the formats above are served gzipped (Content-Encoding: x-gzip). Your browser may silently uncompress after downloading so the files you see saved may appear uncompressed."*
        * grabcite:
            * expects arXiv sources with `.gz` extension in input folder
            * has `gzHandler` and `tarGzHandler` (see `src/GrabCite/Arxiv.hs`)
            * → arXiv sources manually downloaded are mere `tar` archives w/o file extension → need to be gzipped and renamed to have `.gz` file extension (*not* `.tar.gz`)
    * generate data set
        * tries to connect to papergrep.com (expired namecheap.com registration that probably once hosted [this](https://github.com/agrafix/papergrep))
* [opendetex](https://github.com/pkubowicz/opendetex) / [detex](https://www.freebsd.org/cgi/man.cgi?query=detex) — ?, recent commits, compiled
    * specifically for getting plain text
    * `-c` flag is supposed to preserve `\cite`s, but breaks output completely for first tested paper `1010.2903`
    * TEST: opendetex seems to leave in more control sequences than on system detex
* [LaTeXML](https://github.com/brucemiller/LaTeXML) — *very* mature, recent commits, perl
    * LaTeX→XML
    * existing (dead?) [project on on arXiv data](https://kwarc.info/projects/arXMLiv/) ([active LaTeXML fork](https://github.com/KWARC/LaTeXML))
    * ~~install from source~~
        * `sudo apt install cpanminus libxml-libxml-perl libxml-libxslt-perl libxml2-dev libimage-size-perl`
        * `sudo cpanm XML::LibXML`
        * `sudo cpanm Parse::RecDescent`
        * `perl Makefile.PL`
        * `make`
        * `make test` → **fails**
        * `make install`
    * install:
        * `sudo apt install libtext-unidecode-perl`
        * `sudo apt install latexml`
    * misc notes:
        * xpath matches in output require namespace. e.g.:
            * `tree.xpath('//LaTeXML:Math', namespaces={'LaTeXML':'http://dlmf.nist.gov/LaTeXML'})`
            * `etree.strip_elements(tree, '{http://dlmf.nist.gov/LaTeXML}Math', with_tail=False)`
* [Tralics](https://www-sop.inria.fr/marelle/tralics/) — ?, 2015, C++
    * LaTeX→XML
    * [apparently](https://jblevins.org/log/xml-tools) *fast*
* [flap](https://github.com/fchauvel/flap)
    * can be run from inside python by copying `flap/ui.py` and replacing bottom (click) part with `Controller(OSFileSystem(), Display(sys.stdout, False)).run('some.tex', 'out_folder')`
    * somewhat slow?
    * → try out [alternatives](https://tex.stackexchange.com/questions/21838/replace-inputfilex-by-the-content-of-filex-automatically)
    * → latexpand looks good
        * has `--expand-usepackage` flag
        * has `--expand-bbl FILE` flag (mby useful)

##### arXiv

* IDs
    * metadata record: `oai:arXiv.org:0704.0046`　　(xPath: `/ListRecords/record/header/identifier[text()='<id>']`)
    * web: `https://arxiv.org/abs/0704.0046`
    * data dump: `0704.0046.gz`
* only *some* metadata records have DOIs
* `\cite` in text, then `\bibitem` lower down or in `.bib` file
* problems
    * sometimes a paper's [v2](https://arxiv.org/format/1306.0555v2) gives empty source while it's [v1](https://arxiv.org/format/1306.0555v1) has proper source
    * [cases](https://arxiv.org/format/1607.00145) where "source" is just a single `\includepdf`
    * `physics0001026`: html instead of latex
    * `physics0001060`: postscript instead of latex
    * `astro-ph0001005`: using file extension .ltx for latex
    * `hep-th0001079`: few sentences of plaintext
    * `hep-th0001205`, `quant-ph0001041`, ...: TeX (not LaTeX)
    * `ph0001041`: weird LaTeX?
    * `1712.10228` is in Japanese
    * `hep-lat0001017` is creating new commands for marking math environments, *with a newly created command for newcommand*, ***outside the preamble***  
        ```
        [...]
        \begin{document}
        \newcommand{\nc}{\newcommand}
        \nc{\be}{\begin{equation}}
        \nc{\ee}{\end{equation}}
        \nc{\bea}{\begin{eqnarray}}
        \nc{\eea}{\end{eqnarray}}
        \nc{\bib}{\bibitem}
        \nc{\1}{16^3\times{32}}
        \nc{\2}{24^3\times{36}}
        [...]
        ```

##### arXiv source dump

* lots of tar archives like `arXiv_src_0001_001.tar` and a manifest file `arXiv_src_manifest.xml` w/ some metadata
* top level archive content examples:
    * `cs0001012.gz` → `arXiv:cs/0001012`
    * `gr-qc0001036.pdf` → `arXiv:gr-qc/0001036`
    * `1502.00318.gz` → `arXiv:1502.00318`
* 流れ
    * step 1: normalize (separate steps to allow for separate evaluation of results)
        * check for file ending (pdf/gz)
        * gz: unzip and check if tar archive or tex file (`tarfile.is_tarfile()`)
        * tar: extract contents, identify main tex file (`\begin{document}` mby?), flatten with [flap](https://github.com/fchauvel/flap), also check for `\includepdf`
        * save tex[+bib] / pdf in output folder
    * step 2: get plain text
        * pdf: pdf2text (citation markers byebye)
        * tex: TeXSoup
        * save metadata from XML (mby in JSON)
        * tex: replace local citation markers w/ globally unique one (UUID, save UUID→original bibitem text, save UUID→citing paper ID index)
    * step 3: networking
        * try to match citation UUIDs to arXiv IDs (and mby at one point other identifiers)

**test w/ 2364 input docs**  
**step 1**
```
saiert@shetland:~/arxiv_test$ time python3 normalize_arxiv_dump.py /home/saiert/arxiv_test/0001_001/0001/ /home/saiert/arxiv_test/0001_001/normalized

real    3m37.049s
user    3m16.520s
sys     0m20.866s
```
(~0.09 sec per doc)  
→ all of arXiv in 37 hours  
→ 2364m→2250 95% success

**step 2**  
on shetland, 50min for 392 docs (~7 sec per doc); 289 min for 2364 docs (~7.7 sec per doc)  
→ all of arXiv in 128 days (>4 months)  
→ 2218\*→2138 96% success (90% overall)  
\*32 PDFs, therefore not 2250

##### arXiv processing speed up

* math pre removal
    * time: 3m53s (~0.09s/doc) | 122m (3.3s/doc)
    * coverage: unchanged | 2066 **93%**

```
saiert@shetland:~/arxiv_test$ time python3 normalize_arxiv_dump.py /home/saiert/arxiv_test/0001_001/0001/ /home/saiert/arxiv_test/0001_001/normalized

real    3m53.446s
user    3m33.779s
sys     0m19.952s

output: 2218 docs

saiert@shetland:~/arxiv_test$ time python3 parse_latex_latexml.py 0001_001/normalized/ 0001_001/text

real    122m26.729s
user    113m3.402s
sys     8m43.531s

output 2066 docs
```

* ーネ申 **tralics** ネ申ー
    * on shinobu: 2218 docs in 53s → 0.024s/doc
    * on shinobu w/ SQLite DB instead of separate text files for metadata: 2m13s
    * in: 2216, out: 2199 (30 messed up) → 98% (91.8% overall) (latexml only 2138)
    * a bit more preamble noise (already in the XML in `<p>`s, so no straightforward way to deal with it)

**tralics problem docs with latexml**  
(messed up output (but not skip b/c error) for 0001_001; for which latexml outputs 2138, tralics 2199)
```
tralics problem         latexml out                 latexml better
astro-ph0001027.txt     none                        (x)
astro-ph0001100.txt     none                        (x)
astro-ph0001144.txt     also messed up
astro-ph0001152.txt     also messed up
astro-ph0001153.txt     also messed up
(astro-ph0001462.txt)   fine (more content)         x
astro-ph0001540.txt     none                        (x)
astro-ph0001546.txt     none                        (x)
(cond-mat0001187.txt)   same
(cond-mat0001222.txt)   no weird "aaaaaaaaa"
cs0001010.txt           same
gr-qc0001071.txt        fine                        x
hep-lat0001018.txt      fine                        x
hep-ph0001051.txt       also messed up
hep-ph0001203.txt       fine                        x
(hep-th0001048.txt)     same
hep-th0001109.txt       fine                        x
hep-th0001139.txt       same
hep-th0001146.txt       same
hep-th0001181.txt       fine                        x
hep-th0001199.txt       fine                        x
math0001022.txt         same
math0001029.txt         same
math0001070.txt         same
math0001103.txt         same
math0001164.txt         fine                        x
math0001166.txt         same
nucl-th0001065.txt      fine                        x
```

```
python3 parse_latex_tralics.py ~/dl/foo/0001_normalized ~/dl/foo/0001_text_new 70,28s user 20,82s system 68% cpu 2:13,90 total
```

**tralics citation/bibitem parsing** (tested on Dec 2017 complete dump)    
```
Citations: 488725    *(295710 unique)
Unmatched citations: 93315
```

→ Tralics is supposed to be able to understand [commands from a lot of packages](https://www-sop.inria.fr/marelle/tralics/packages.html) (esp. natbib is relevant here) but apparently doesn't  
→ fixing `\citep`s, `\bibitem[bla{problem}bla]{key}`s etc. helps with a lot of citations (see regex and results below)  
→ still a problem are [nmras](https://www-sop.inria.fr/marelle/tralics/packages.html) style bibliographies (predictable format so mby. #TODO)

```
NATBIB_PATT = re.compile((r'\\cite(t|p|alt|alp|author|year|yearpar)\s*?\*?\s*?'
                           '(\[[^\]]*?\]\s*?)*?\s*?\*?\s*?\{([^\}]+?)\}'),
                         re.I)
cntnt = NATBIB_PATT.sub(r'\\cite{\3}', cntnt)

BIBOPT_PATT = re.compile(r'\\bibitem\s*?\[[^]]*?\]', re.I|re.M)
cntnt = BIBOPT_PATT.sub(r'\\bibitem', cntnt)
```

```
Citations: 565613 (not unique)
Unmatched citations: 28372
```

**→ increase from 70% to 95%** (assuming above 565613 are 100%)

impact on example data set:

```
$ cat items_1712_100w_2mincont_not_stemmed.csv | wc -l
17640
$ cat items_1712_100w_2mincont_not_stemmed_NATBIB_FIX.csv | wc -l
28673

```

##### bibitem matching (arxiv)

* arxiv.org web API test (use unrealistic b/c time)
    * match for e.g. first bibitem in `1711.00002`
        * original: `L. J. Ba and R. Caruana. Do deep nets really need to be deep? In Proceedings of NIPS, 2014.`
        * query: `http://export.arxiv.org/api/query?search_query=all:Caruana%20deep%20nets%20really%20need%20deep%20Proceedings%20NIPS%202014` (words longer 2 letters)
            * (comment added afterwards: in general way better results when concatenating words with %2B (+) instead of %20)
* source given links/arXiv IDs (tested w/ 377 docs from 2017)  
```
sqlite> select count(*) from bibitem;
10270
sqlite> select count(distinct uuid) from bibitemlinkmap;
1615
sqlite> select count(distinct uuid) from bibitemarxividmap;
476
```
* Solr test

```
$ time python3 solrize.py
no metadata in 1008.3138 in arxiv-physics-oai_dc-cursor147000.xml
no metadata in 1008.3138 in arxiv-physics:quant-ph-oai_dc-cursor13000.xml
no metadata in 1005.0836 in arxiv-math-oai_dc-cursor50000.xml
python3 solrize.py  430,84s user 8,54s system 98% cpu 7:27,43 total
```

```
$ time ./bin/post -c arxiv_meta ~/dl/foo/newArxivMetaHarvesting201712/solrized
[...]
2643 files indexed.
COMMITting Solr index changes to http://localhost:8983/solr/arxiv_meta/update...
Time spent: 0:19:19.727
./bin/post -c arxiv_meta ~/dl/foo/newArxivMetaHarvesting201712/solrized  10,20s user 28,05s system 3% cpu 19:20,09 total
```
* bibitem matching w/ Solr

```
- - - - - using title_query_split_heuristic - - - - -
total: 10270
matches: 612
checked: 476
false negatives: 463
false positives: 0
python3 match_bibitems.py path ~/dl/foo/1712_text  35,91s user 1,67s system 74% cpu 50,206 total
```

```
- - - - - using title_author_query_words - - - - -
$ python3 match_bibitems.py path ~/dl/foo/1712_text
total: 10270
matches: 1703
checked: 476
false negatives: 315
false positives: 1
python3 match_bibitems.py path ~/dl/foo/1712_text  38,76s user 1,87s system 43% cpu 1:33,12 total
```

**NOTE**: bibitems that include IDs might be more likely to not include a title. In a sample of 250 of above 476 checked bibitems, only 117 included a proper title.

**optimistic extrapolation**  
1703 + 315 out of 10270 → 19.6%  
assuming 1.45 M papers each 27 citations → 7.6 M citations

~~**(!!!) somewhere along the line duplicates found there way in arXiv ID map(!!!)** (05.11.2017)~~ (fixed)  
→ add constraints to DB, rerun pipleline for Dec. 2017  
```
sqlite> select count(*) from bibitemarxividmap;
70353
sqlite> delete from bibitemarxividmap where id not in (select min(id) from bibitemarxividmap group by uuid, arxiv_id);
sqlite> select count(*) from bibitemarxividmap;
62232
```

##### bibitem matching (MAG)

* e.g. `$ curl 'localhost:8983/solr/mag_papers/select?q=original_title:Buda%2BMaki%2Band%2BMazurowski%2Bsystematic%2Bstudy%2Bthe%2Bclass%2Bimbalance%2B...'`
    * fields of interest: `paper_id`, `normalized_title`, `original_title`, `publication_year`, `citation_count`, `estimated_citation_count`, (`doi`)
    * for authors: `mag_paper_author_affiliations/select?q=paper_id:` → `author_id`, `mag_authors/select?q=author_id:` → `display_name` / `normalized_name`
* strict title matching **50.9%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/all_text/
    - - - - - - - - - - - - - - - - -
    312534/312536
    matches: 159201
    checked: 35876          ← wrong, actually 22092 (or even fewer, check +ParsCit result)
    false negatives: 22141
    false positives: 1102
    
    real    1115m19.231s
    user    42m41.461s
    sys     3m40.510s
    ```
* +filtering Proceedings/Transactions/etc. *segments* **51.3%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/all_text/
    - - - - - - - - - - - - - - - - -
    312535/312536
    matches: 160350
    checked: 35875          ← wrong, actually 22092 (or even fewer, check +ParsCit result)
    false negatives: 22183
    false positives: 1043
    DOI rebounds: 259
    
    real    1374m51.740s
    user    37m37.951s
    sys     3m37.655s
    ```
* +name list filtering, +arXiv ID usage **57.3%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/all_text/
    - - - - - - - - - - - - - - - - -
    312536/312536
    matches: 179107
    checked: 35876          ← wrong, actually 22092 (or even fewer, check +ParsCit result)
    false negatives: 16616
    #Phys. Rev.: 36382
    false positives: 843
    DOI rebounds: 164
    by arXiv ID: 35799
    by arXiv ID fail: 0
    
    real    1494m35.258s
    user    35m8.912s
    sys     4m7.753s
    ```
* +DOI ID usage **61.5%**
    ```
    $ python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/all_text/
    - - - - - - - - - - - - - - - - -
    312536/312536
    matches: 192056
    checked: 22092
    #Phys. Rev.: 26867
    false positives: 1344
    by arXiv ID: 35799
    by arXiv ID fail: 0
    by doi: 26230
    ```
* +ParsCit (cancelled b/c time) **52%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/parscit_test/
    - - - - - - - - - - - - - - - - -
    123166/312536
    matches: 64123
    checked: 7496
    Phys. Rev.: 10467
    no title: 29034
    false positives: 422
    by arXiv ID: 13939
    by arXiv ID fail: 0
    by doi: 10348
    
    real    2985m14.393s
    user    21m10.869s
    sys     2m21.369s
    ```
* +sliding window fuzzy matching (cancelled b/c time) **54.8%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/parscit_and_fuzzy_window_test/
    - - - - - - - - - - - - - - - - -
    113486/312536
    matches: 62246
    checked: 7557
    Phys. Rev.: 9768
    no title: 26604
    false positives: 421
    by arXiv ID: 13103
    by arXiv ID fail: 0
    by doi: 9600
    
    real    2774m1.938s
    user    20m48.099s
    sys     2m15.738s
    ```
* full phrase queries etc. for speedup **53.7%**
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_2017_12_all_181116/solr_phrase_query_test/
    312536/312536
    matches: 168063
    checked: 22434
    Phys. Rev.: 26867
    no title: 73304         <- (!!!) if all resolvable 77%
    false positives: 1158
    by arXiv ID: 35798
    by arXiv ID fail: 0
    by doi: 26230
    >>> avg time aID: 0.44
    >>> avg time DOI: 0.52
    >>> avg time ParsCit: 0.05
    >>> avg time Solr: 0.06
    >>> avg time check: 0.00
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 15899.40         4.5h
    >>> time DOI: 13764.04         3.5h
    >>> time ParsCit: 11630.38     3.2h
    >>> time Solr: 14447.63        4.0h
    >>> time check: 48.72
    >>> time DB r: 500.97
    >>> time DB w: 210.44
    
    real    963m6.286s            16h    (→ full arXiv in ~48d)
    user    95m50.464s
    sys     6m36.414s
    ```
    * ↑ manual check of 5% false positives
        ```
        manually checked: 25
          tot  perc  | of 5%
        O: 19   76%  |  3.8%
        △:  4   16%  |  8.8%
        Σ  23   92%  |
                     |
        X:  2    8%  |  0.4%
        
        notes
        - quite some falsely marked b/c evaluated case sensitive
        - some DOIs linking to an identical URL
        - few identical in different context or close derivative content by same auth.
        - very few actually wrong
        ```
* problems:
    * phys rev's
    * bibitems without title
    * formulas in bibitems:
        * `measurement of longrange nearside twoparticle angular correlations in pp collisions at s 13 tev` not in  
            `vardan khachatryan et al measurement of longrange nearside twoparticle angular correlations in pp collisions at formula 13 tev phys rev lett 11617172302 2016 151003068`  
            b/c of FORMULA replacement
        * *"ARGUS Collaboration, H. Albrecht et al., “Observation of FORMULA semileptonic decay”, Phys. Lett. B303 (1993) 368."*
    * author typos: *"S. Chakraborty M. Hanmandlu, K. R. Murali Mohan et al. Uncon**ss**trained handwritten character recognition based on fuzzy logic. Pattern Recognition, 2003."*
    * special characters
        * bibitem: "s-step iterative methods for symmetric linear systems"
        * MAG: "s -step iterative methods for symmetric linear systems"
        * ---
        * bibitem: "É. Cancès, A. Levitt, G. Panati, and G. Stoltz. Robust determination of maximally localized Wannier functions. Phys. Rev. B 95(7), 075114 (2017)."
        * MAG: "Robust determination of maximally-localized Wannier functions"
    * MAG noise:
        * *Pascual Jordan: Schwerkraft und Weltall, Vieweg and Sohn, Braunschweig(1952), VII+207p. 1,680円*
        * multiple papers with identical normalized title and (similar) authors (→ hard to distinguish, heuristically dealt with by number of citations)
    * non citations
        * *According to the numerous discussions with my colleagues D. Fischer and P. Mokler an experimental verification of our theoretical predictions is feasible.* (in 1305.0417)
    * single process single thread bibitem-matching: extrapolated >40 days for whole arXiv dump → parallelization
* bibitem-matching parallelization challenges:
    * reading in all bibitems via SQLAlchemy ORM: can't be distributed to subprocesses:
        ``` 
        The communication protocol between processes uses pickling, and the pickled data is prefixed with the size of the pickled data. For your method, all arguments together are pickled as one object.
        You produced an object that when pickled is larger than fits in a i struct formatter (a four-byte signed integer), which breaks the assumptions the code has made.
        ```
    * reading in bibitems via SQLAlchemy ORM in the subprocesses (and then selecting distributed chuck start/stops): server out of memory
    * → read in bibitems via plain SQL, save in tuples, split and distribute tuples to subprocesses
    * looking up xref links from bibitemlinkmap while matching takes too long → read into memory beforehand (build lookup map)
    * resumability:
        * 35M bibitems, xM logged done, resume after stop:
        * looking through millions long log of stuff already done while matching not feasible → remove beforehand 
        * removing several million from 35M doesn't work feasibly with loop in loop → sort both lists and create more efficient removal algorithm

##### experiments

1712_001.tar.gz probably way too small:  
```
sqlite> select count(distinct uuid) from bibitemarxividmap;
476
sqlite> select count(distinct arxiv_id) from bibitemarxividmap;
463
```  
→ only one citation context per cited document  
→ huge risk of overfitting

0001_001.tar.gz  
```
sqlite> select count(*) from bibitemarxividmap;
2135
sqlite> select count(distinct arxiv_id) from bibitemarxividmap;
1778
sqlite> select count(distinct arxiv_id) from bibitemarxividmap where arxiv_id in (select arxiv_id from bibitemarxividmap group by arxiv_id having count(arxiv_id) > 1);
227    <- citations w/ more than one context
```

**get all citations that have more than one context**  
`sqlite> select uuid, arxiv_id from bibitemarxividmap where arxiv_id in (select arxiv_id from bibitemarxividmap group by arxiv_id having count(arxiv_id) > 1) order by arxiv_id desc;`  
with `in_doc`:  
`sqlite> select in_doc, bibitem.uuid, arxiv_id from bibitemarxividmap join bibitem on bibitemarxividmap.uuid = bibitem.uuid where arxiv_id in (select arxiv_id from bibitemarxividmap group by arxiv_id having count(arxiv_id) > 1) order by arxiv_id desc;`

##### december 2017 dump complete test

* on shetland
* w/ sqlite DB (probably not the most performant) 
* solr instance on shinobu (→ latency)

```
$ time python3 prepare.py /home/saiert/transit/ /home/saiert/arxiv_test_2017_12_all

total: 295710
matches: 52800
checked: 17329
false negatives: 9141
false positives: 159

real    293m9.947s
user    101m10.978s
sys     12m8.748s
```

(parse_latex_tralics (run individually later) is only 19m4.407s of above)

```
$ du -hs metadata.db 
80M	metadata.db
```

**re-run w/ natbib fixes**  
```
$ python3 match_bibitems.py path /home/saiert/arxiv_test_2017_12_all_tralics_preprocessing/all_text
total: 317010
matches: 59277
checked: 21294
false negatives: 10364
false positives: 197
```

(complete) **re-run w/ throwing out tables and figures**  
```
total: 312536
matches: 58613
checked: 20784
false negatives: 10110
false positives: 188
```

##### 1991-2017 dump sample (per month per field ~1%) (14.262 docs)

* on shetland
* w/ sqlite DB (probably not the most performant) 
* **w/o matching**
    ```
    $ time python3 prepare.py /home/saiert/arxiv_sample/ /home/saiert/arxiv_test_1991-2017_sample
    real    91m13.989s
    user    68m19.553s
    sys     14m26.658s
    ```
* matching (same approach as 1712 *full phrase queries etc. for speedup 53.7%*, but only 34.1%)
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_1991-2017_sample_one_percent/arxiv_sample_one_percent_text/
    - - - - - - - - - - - - - - - - -
    375309/375312
    matches: 128178
    checked: 8709
    Phys. Rev.: 52157
    no title: 151838
    false positives: 422
    by arXiv ID: 30200
    by arXiv ID fail: 0
    by doi: 10124
    >>> avg time aID: 0.45
    >>> avg time DOI: 0.51
    >>> avg time ParsCit: 0.02
    >>> avg time Solr: 0.06
    >>> avg time check: 0.00
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 13483.77
    >>> time DOI: 5151.65
    >>> time ParsCit: 5402.39
    >>> time Solr: 12391.17
    >>> time check: 57.81
    >>> time DB r: 390.43
    >>> time DB w: 110.25
    
    real    632m8.252s
    user    81m40.911s
    sys     5m11.341s
    ```
* querying arXiv title from a DB instead of arXiv's web API
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_1991-2017_sample_one_percent/181130_test/
    - - - - - - - - - - - - - - - - -
    375312/375312
    matches: 122667
    checked: 11117
    Phys. Rev.: 52179
    no title: 151919
    maybe (!) false positives: 404
    by arXiv ID: 30580
    by arXiv ID fail: 5167
    by doi: 10129
    by doi fail: 929
    >>> avg time aID: 0.10
    >>> avg time DOI: 0.54
    >>> avg time ParsCit: 0.02
    >>> avg time Solr: 0.06
    >>> avg time check: 0.00
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 2980.74
    >>> time DOI: 5506.91
    >>> time ParsCit: 5271.82
    >>> time Solr: 13172.52
    >>> time check: 57.20
    >>> time DB r: 362.16
    >>> time DB w: 98.81
    
    real    475m3.166s
    user    114m13.424s
    sys     22m4.537s
    ```
* MAG DB notes: existence of index = *55.000 times faster*
    ```
    MAG=> explain analyze select paperid from papers where originaltitle = 'Fun with Fonts: Algorithmic Typography';
                                                           QUERY PLAN
    ------------------------------------------------------------------------------------------------------------------------
     Index Scan using idx_paperstitle on papers  (cost=0.82..8.84 rows=1 width=8) (actual time=0.098..0.100 rows=1 loops=1)
       Index Cond: (originaltitle = 'Fun with Fonts: Algorithmic Typography'::text)
     Planning time: 0.166 ms
     Execution time: 0.135 ms
    (4 rows)
    
    MAG=> explain analyze select paperid from papers where papertitle = 'fun with fonts algorithmic typography';
                                                             QUERY PLAN
    -----------------------------------------------------------------------------------------------------------------------------
     Gather  (cost=1000.00..9391587.60 rows=1 width=8) (actual time=16615.889..16619.206 rows=1 loops=1)
       Workers Planned: 2
       Workers Launched: 2
       ->  Parallel Seq Scan on papers  (cost=0.00..9390587.50 rows=1 width=8) (actual time=14324.340..16610.937 rows=0 loops=3)
             Filter: (papertitle = 'fun with fonts algorithmic typography'::text)
             Rows Removed by Filter: 69930913
     Planning time: 0.232 ms
     Execution time: 16619.256 ms
    (8 rows)
    ```
* switching from Solr to PostgreSQL (querying normalized paper titles)
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_1991-2017_sample_one_percent/181202_test/
    - - - - - - - - - - - - - - - - -
    375311/375312
    matches: 121147
    checked: 11077
    Phys. Rev.: 52178
    no title: 151918
    maybe (!) false positives: 383
    by arXiv ID: 30580
    by arXiv ID fail: 5661
    by doi: 10129
    by doi fail: 1147
    >>> avg time aID: 0.10
    >>> avg time DOI: 0.50
    >>> avg time ParsCit: 0.01
    >>> avg time MAGDB: 0.01
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 2948.98
    >>> time DOI: 5109.72
    >>> time ParsCit: 4883.21
    >>> time MAGDB: 1237.15
    >>> time DB r: 322.05
    >>> time DB w: 84.23
    
    real    258m50.437s
    user    81m58.758s
    sys     16m20.550s
    ```
* speculatively cutting off ParsCit identified titles
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_1991-2017_sample_one_percent/181202_test/
    - - - - - - - - - - - - - - - - -
    375311/375312
    matches: 130895
    checked: 11078
    Phys. Rev.: 52179
    no title: 151918
    maybe (!) false positives: 384
    by arXiv ID: 30580
    by arXiv ID fail: 5661
    by doi: 10129
    by doi fail: 1147
    >>> avg time aID: 0.09
    >>> avg time DOI: 0.51
    >>> avg time ParsCit: 0.01
    >>> avg time MAGDB: 0.01
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 2690.60
    >>> time DOI: 5160.01
    >>> time ParsCit: 4401.49
    >>> time MAGDB: 1979.58
    >>> time DB r: 292.79
    >>> time DB w: 80.83
    
    real    257m43.921s
    user    87m58.216s
    sys     13m30.044s
    ```
* guess APS journal paper DOIs (41%)
    ```
    $ time python3 match_bibitems_mag.py path /home/saiert/arxiv_test_1991-2017_sample_one_percent/181202_test/
    - - - - - - - - - - - - - - - - -
    375312/375312
    matches: 154379
    checked: 11079
    Phys. Rev.: 52179
    no title: 123439
    maybe (!) false positives: 385
    by arXiv ID: 30580
    by arXiv ID fail: 5661
    by doi: 10129
    by doi fail: 1147
    APS doi rebound: 28480
    >>> avg time aID: 0.09
    >>> avg time DOI: 0.53
    >>> avg time ParsCit: 0.01
    >>> avg time MAGDB: 0.01
    >>> avg time DB r: 0.00
    >>> avg time DB w: 0.00
    >>> time aID: 2800.11
    >>> time DOI: 5423.30
    >>> time ParsCit: 4630.38
    >>> time MAGDB: 2169.40
    >>> time DB r: 309.20
    >>> time DB w: 103.15
    
    real    595m7.421s
    user    102m17.161s
    sys     17m5.209s
    ```

##### complete dump

* on tuco
* w/ sqlite DB 
* **w/o matching**
    ```
    $ time python3 prepare.py /vol2/data/arxiv-src-data/ /home/saiert/arxiv-txt-data
    397/1758
    real    793m38.471s
    user    580m4.249s
    sys     101m8.986s (uncaught exception)
    
    527/1758
    real    255m53.911s
    user    187m32.075s
    sys     30m34.096s (uncaught exception)
    
    1758/1758
    947829 files
    70863 PDFs
    real    2496m46.268s
    user    1766m47.943s
    sys     358m11.095s
    
    -> 3546,28m / 59h / 2.5d
    ```  
    ```
    $ ls arxiv-txt-data | grep .txt | wc -l
    1151707
    $ du -h arxiv-txt-data
    50G     arxiv-txt-data
    $ du -h arxiv-txt-data/metadata.db 
    8,4G    arxiv-txt-data/metadata.db
    ```  
    ```
    sqlite> select count(*) from bibitem;
    35053329
    ```
    * 100% → 1.340.770
    * extrapolated #PDFs: 100.240 → 1.240.530
    * pipleline output: 1.151.707 (92.8%)
    * with citations: 1.018.976 (82.1%) (76.0% incl. PDFs)
    * 56.077.906 citation markers
* **matching**
    * twice resumed with 5M and again 5M done in previous sessions; 10 parallel processes
        ```
        $ python3 match_bibitems_mag.py path /home/saiert/arxiv-txt-data 10
        [batch #6]: - - - - - - - - - - - - - - - - -
        [batch #6]: 2462301/2462338
        [batch #6]: matches: 876586
        [batch #6]: checked: 70311
        [batch #6]: Phys. Rev.: 355204
        [batch #6]: no title: 723153
        [batch #6]: maybe (!) false positives: 2239
        [batch #6]: by arXiv ID: 190948
        [batch #6]: by arXiv ID fail: 51041
        [batch #6]: by doi: 68736
        [batch #6]: by doi fail: 11965
        [batch #6]: APS doi rebound: 188593
        [batch #6]: >>> avg time aID: 0.10
        [batch #6]: >>> avg time DOI: 0.52
        [batch #6]: >>> avg time ParsCit: 0.03
        [batch #6]: >>> avg time MAGDB: 0.02
        [batch #6]: >>> avg time DB r: 0.00
        [batch #6]: >>> avg time DB w: 0.02
        [batch #6]: >>> time aID: 18683.87
        [batch #6]: >>> time DOI: 36193.84
        [batch #6]: >>> time ParsCit: 55165.55
        [batch #6]: >>> time MAGDB: 26188.81
        [batch #6]: >>> time DB r: 30.61
        [batch #6]: >>> time DB w: 13187.03
        ```  
        ```
        [batch #8]: - - - - - - - - - - - - - - - - -
        [batch #8]: 2462301/2462338
        [batch #8]: matches: 877929
        [batch #8]: checked: 70544
        [batch #8]: Phys. Rev.: 355071
        [batch #8]: no title: 724875
        [batch #8]: maybe (!) false positives: 2264
        [batch #8]: by arXiv ID: 191175
        [batch #8]: by arXiv ID fail: 50541
        [batch #8]: by doi: 69380
        [batch #8]: by doi fail: 12158
        [batch #8]: APS doi rebound: 188111
        [batch #8]: >>> avg time aID: 0.10
        [batch #8]: >>> avg time DOI: 0.52
        [batch #8]: >>> avg time ParsCit: 0.03
        [batch #8]: >>> avg time MAGDB: 0.02
        [batch #8]: >>> avg time DB r: 0.00
        [batch #8]: >>> avg time DB w: 0.02
        [batch #8]: >>> time aID: 18689.88
        [batch #8]: >>> time DOI: 36617.27
        [batch #8]: >>> time ParsCit: 55179.79
        [batch #8]: >>> time MAGDB: 26167.74
        [batch #8]: >>> time DB r: 30.43
        [batch #8]: >>> time DB w: 13238.70
        ['done', 'done', 'done', 'done', 'done', 'done', 'done', 'done', 'done', 'done']
        ```
        *number of matched bibitems*  
        ```
        sqlite> select count(*) from bibitemmagidmap;
        14169900        -> 40.4%
        ```
        *number of unique cited docs*  
        ```
        sqlite> select count(distinct mag_id) from bibitemmagidmap;
        2315884
        ```
        *number of unique cited docs usable for evaluation (!!! but all 2.3M usable for live system)*  
        ```
        sqlite> select count(*) from (select count(distinct in_doc) from bibitemmagidmap join bibitem on bibitemmagidmap.uuid = bibitem.uuid group by mag_id having count(distinct in_doc) > 1);
        1320050 
        ```
        *number of bibitems usable for evaluation*  
        ```
        sqlite> select count(uuid) from bibitemmagidmap where mag_id in (select mid from (select count(distinct in_doc), mag_id as mid from bibitemmagidmap join bibitem on bibitemmagidmap.uuid = bibitem.uuid group by mag_id having count(distinct in_doc) > 1));
        13162556
        ```
        *↑ simplified (by assuming a paper doesn't have 2 distinct bibitems referencing the same cited doc) (0.1% change likely due to matching errors)*
        ```
        sqlite> select count(uuid) from bibitemmagidmap where mag_id in (select mag_id from bibitemmagidmap group by mag_id having count(uuid) > 1);
        13183546
        ```
        *way to query DB for evaluation (limit for smaller tests)*
        ```
        sqlite> select bibitem.uuid, mag_id, in_doc, bibitem_string from bibitemmagidmap join bibitem on bibitemmagidmap.uuid = bibitem.uuid where mag_id in (select mag_id from bibitemmagidmap group by mag_id having count(uuid) > 1 limit X) order by mag_id;
        ```
    * further notes on results:
        * top 50 most matched cited docs (31/50 correct):
            * 6662 [2595998925](https://academic.microsoft.com/#/detail/2595998925) -> should actually have been [2166248051](https://academic.microsoft.com/#/detail/2166248051) (same title & author)
            * 5817 [201838852](https://academic.microsoft.com/#/detail/201838852) -> containing work
            * 5424 [2547311611](https://academic.microsoft.com/#/detail/2547311611) -> containing work
            * 5296 [1975165548](https://academic.microsoft.com/#/detail/1975165548) -> should actually have been [1631356911](https://academic.microsoft.com/#/detail/1631356911) (same title)
            * 5133 [1981368803](https://academic.microsoft.com/#/detail/1981368803) ✔
            * 4309 [250990466](https://academic.microsoft.com/#/detail/250990466) -> containing work
            * 4283 [2044442377](https://academic.microsoft.com/#/detail/2044442377) -> should actually have been [2099111195](https://academic.microsoft.com/#/detail/2099111195)
            * 4029 [1526931241](https://academic.microsoft.com/#/detail/1526931241) -> should actually have been [2120062331](https://academic.microsoft.com/#/detail/2120062331)
            * 3979 [1889142700](https://academic.microsoft.com/#/detail/1889142700) ✔
            * 3880 [2030690537](https://academic.microsoft.com/#/detail/2030690537) ✔
            * 3394[1858542512](https://academic.microsoft.com/#/detail/1858542512) ✔
            * 3262 [2139937287](https://academic.microsoft.com/#/detail/2139937287) ✔
            * 3117 [2139937287](https://academic.microsoft.com/#/detail/2139937287) ✔
            * 3113 [1978553093](https://academic.microsoft.com/#/detail/1978553093) ✔
            * 3068 [1972485395](https://academic.microsoft.com/#/detail/1972485395) -> should actually have been [2157005274](https://academic.microsoft.com/#/detail/2157005274) (same title & author)
            * 2937 [2794162265](https://academic.microsoft.com/#/detail/2794162265) -> should actually have been [2029403139](https://academic.microsoft.com/#/detail/2029403139)
            * 2918 [2763160969](https://academic.microsoft.com/#/detail/2763160969) -> should actually have been [2167727518](https://academic.microsoft.com/#/detail/2167727518)
            * 2752 [2011208902](https://academic.microsoft.com/#/detail/2011208902) ✔
            * 2734 [2334487375](https://academic.microsoft.com/#/detail/2334487375) -> should actually have been  *C. Itzykson and J.-B. Zuber, Quantum Field Theory* (yet to be found in MAG) and/or [1577720992](https://academic.microsoft.com/#/detail/1577720992)
            * 2734 [1909359706](https://academic.microsoft.com/#/detail/1909359706) ✔
            * 2731 [1686810756](https://academic.microsoft.com/#/detail/1686810756) ✔
            * 2723 [2800235016](https://academic.microsoft.com/#/detail/2800235016) -> should actually have been [2094438648](https://academic.microsoft.com/#/detail/2094438648)
            * 2680 [2798540069](https://academic.microsoft.com/#/detail/2798540069) -> should actually have been [1486187347](https://academic.microsoft.com/#/detail/1486187347)
            * 2656 [2039609754](https://academic.microsoft.com/#/detail/2039609754) ✔
            * 2607 [1522301498](https://academic.microsoft.com/#/detail/1522301498) ✔
            * 2586 [1992204683](https://academic.microsoft.com/#/detail/1992204683) ✔
            * 2559 [2095721433](https://academic.microsoft.com/#/detail/2095721433) ✔
            * 2497 [2258584306](https://academic.microsoft.com/#/detail/2258584306) ✔
            * 2448 [2135046866](https://academic.microsoft.com/#/detail/2135046866) ✔
            * 2430 [254615967](https://academic.microsoft.com/#/detail/254615967) -> should actually have been [1488001281](https://academic.microsoft.com/#/detail/1488001281)
            * 2407 [2037768897](https://academic.microsoft.com/#/detail/2037768897) ✔
            * 2354 [2147875900](https://academic.microsoft.com/#/detail/2147875900) ✔
            * 2250 [2073595060](https://academic.microsoft.com/#/detail/2073595060) ->  Phys. Rev. Lett. DOI heuristic took *Phys. Rev. Lett. 73 (1994) XXXX* for article #1994 from Phys. Rev. Lett. 73
            * 2214 [1970127494](https://academic.microsoft.com/#/detail/1970127494) ✔
            * 2211 [2046747256](https://academic.microsoft.com/#/detail/2046747256) -> should actually have been [2028815089](https://academic.microsoft.com/#/detail/2028815089)
            * 2196 [2051051926](https://academic.microsoft.com/#/detail/2051051926) ✔
            * 2125 [2194775991](https://academic.microsoft.com/#/detail/2194775991) ✔
            * 2080 [2222357604](https://academic.microsoft.com/#/detail/2222357604) -> should actually have been [1575147392](https://academic.microsoft.com/#/detail/1575147392)
            * 2028 [2064777467](https://academic.microsoft.com/#/detail/2064777467) ✔
            * 1950 [2800027815](https://academic.microsoft.com/#/detail/2800027815) -> should actually have been [2751862591](https://academic.microsoft.com/#/detail/2751862591)
            * 1941 [1525575583](https://academic.microsoft.com/#/detail/1525575583) ✔
            * 1938 [1530042113](https://academic.microsoft.com/#/detail/1530042113) ✔
            * 1926 [2044336104](https://academic.microsoft.com/#/detail/) ->  Phys. Rev. Lett. DOI heuristic took *Phys. Rev. Lett. 64 (1990) XXXX* for article #1990 from Phys. Rev. Lett. 64
            * 1923 [1979544533](https://academic.microsoft.com/#/detail/1979544533) ✔
            * 1868 [2035413341](https://academic.microsoft.com/#/detail/2035413341) ✔
            * 1800 [2134251287](https://academic.microsoft.com/#/detail/2134251287) ✔
            * 1800 [2030164271](https://academic.microsoft.com/#/detail/2030164271) ✔
            * 1797 [2107240173](https://academic.microsoft.com/#/detail/2107240173) ✔
            * 1790 [1986510200](https://academic.microsoft.com/#/detail/1986510200) ✔
            * 1786 [2034920898](https://academic.microsoft.com/#/detail/2034920898) ✔
        * → main problems:
            * works with identical normalized title → FIX: re-check all in MAG w/ same title for author and #cit
                * snapshot: `8329000/14169900 (521923 updated)`
                    * TODO: "re-run" for counting cases of no author match
                * → FIX integrated in actual matching process
            * wrong title identified
                * via e.g. APS DOI heuristic (`Phys. Rev. Lett. 73 (1994) 3070` → 1994 as article number) → FIX: pre-filter dates in brackets
                * in general: for all bibitems matched to a certain MAG ID, mby check overall text similarity of bibitem texts
            * wrong "extra matches" under certain conditions because a match variable was not reset for every turn of a loop → FIX: fix code, re-run matching
    * intermediate solution attempt *refining*
        ```
        $ python3 refine_matches.py path /home/saiert/arxiv-txt-data
        ...
        14162000/14169900 (882771 updated)
        ```
        * → means 882.771 in the old matching had same title mismatches
    * rematch; 10 parallel processes
        ```
        $ python3 match_bibitems_mag.py path /home/saiert/arxiv-txt-data 10
        [batch #8]: - - - - - - - - - - - - - - - - -
        [batch #8]: 3505166/3505332
        [batch #8]: matches: 1380100
        [batch #8]: Phys. Rev.: 497904
        [batch #8]: no title: 1192683
        [batch #8]: by arXiv ID: 268239
        [batch #8]: by arXiv ID fail: 0
        [batch #8]: by doi: 113952
        [batch #8]: by doi fail: 0
        [batch #8]: APS doi rebound: 306470
        [batch #8]: >>> avg time aID: 0.10
        [batch #8]: >>> avg time DOI: 0.52
        [batch #8]: >>> avg time ParsCit: 0.03
        [batch #8]: >>> avg time MAGDB: 0.00
        [batch #8]: >>> avg time DB r: 0.00
        [batch #8]: >>> avg time DB w: 0.02
        [batch #2]: - - - - - - - - - - - - - - - - -
        [batch #2]: 3505270/3505332
        [batch #2]: matches: 1430200
        [batch #2]: Phys. Rev.: 501365
        [batch #2]: no title: 1130708
        [batch #2]: by arXiv ID: 284149
        [batch #2]: by arXiv ID fail: 0
        [batch #2]: by doi: 110068
        [batch #2]: by doi fail: 0
        [batch #2]: APS doi rebound: 303153
        [batch #2]: >>> avg time aID: 0.10
        [batch #2]: >>> avg time DOI: 0.52
        [batch #2]: >>> avg time ParsCit: 0.03
        [batch #2]: >>> avg time MAGDB: 0.00
        [batch #2]: >>> avg time DB r: 0.00
        [batch #2]: >>> avg time DB w: 0.02
        ------[ 0 ]------
        >>> time aID: 28085.69
        >>> time DOI: 53523.67
        >>> time ParsCit: 85319.80
        >>> time MAGDB: 9694.57
        >>> time DB r: 57.34
        >>> time DB w: 22954.64
        ------[ 1 ]------
        >>> time aID: 29888.80
        >>> time DOI: 58357.63
        >>> time ParsCit: 85008.60
        >>> time MAGDB: 9732.65
        >>> time DB r: 58.90
        >>> time DB w: 23587.11
        ------[ 2 ]------
        >>> time aID: 28524.82
        >>> time DOI: 57609.68
        >>> time ParsCit: 85138.43
        >>> time MAGDB: 9713.64
        >>> time DB r: 57.63
        >>> time DB w: 23197.84
        ------[ 3 ]------
        >>> time aID: 26444.12
        >>> time DOI: 49158.39
        >>> time ParsCit: 85985.90
        >>> time MAGDB: 9607.78
        >>> time DB r: 58.90
        >>> time DB w: 22072.63
        ------[ 4 ]------
        >>> time aID: 26563.91
        >>> time DOI: 49747.60
        >>> time ParsCit: 85581.47
        >>> time MAGDB: 9628.71
        >>> time DB r: 58.08
        >>> time DB w: 21998.38
        ------[ 5 ]------
        >>> time aID: 29316.37
        >>> time DOI: 57610.68
        >>> time ParsCit: 85173.40
        >>> time MAGDB: 9724.38
        >>> time DB r: 59.43
        >>> time DB w: 23310.95
        ------[ 6 ]------
        >>> time aID: 27495.83
        >>> time DOI: 53550.03
        >>> time ParsCit: 85491.60
        >>> time MAGDB: 9590.67
        >>> time DB r: 59.59
        >>> time DB w: 22540.93
        ------[ 7 ]------
        >>> time aID: 29110.62
        >>> time DOI: 56043.10
        >>> time ParsCit: 84992.81
        >>> time MAGDB: 9632.21
        >>> time DB r: 59.78
        >>> time DB w: 22834.40
        ------[ 8 ]------
        >>> time aID: 26730.46
        >>> time DOI: 59624.85
        >>> time ParsCit: 85106.58
        >>> time MAGDB: 9552.98
        >>> time DB r: 61.35
        >>> time DB w: 22360.04
        ------[ 9 ]------
        >>> time aID: 27939.93
        >>> time DOI: 54095.05
        >>> time ParsCit: 85509.75
        >>> time MAGDB: 9715.96
        >>> time DB r: 59.61
        >>> time DB w: 22807.38
        
        real    6215m51.339s    103.58h 4.3d
        user    7991m28.609s
        sys     2152m58.152s
        ```
        *number of matched bibitems*  
        ```
        sqlite> select count(*) from bibitemmagidmap;
        14046239        -> 40.07% 
        ```
        *number of unique cited docs*  
        ```
        sqlite> select count(distinct mag_id) from bibitemmagidmap;
        2412432
        ```
        *number of unique cited docs usable for evaluation (!!! but all 2.3M usable for live system)*  
        ```
        sqlite> select count(*) from (select count(distinct in_doc) from bibitemmagidmap join bibitem on bibitemmagidmap.uuid = bibitem.uuid group by mag_id having count(distinct in_doc) > 1);
        1310849
        ```
        *number of bibitems usable for evaluation*  
        ```
        sqlite> select count(uuid) from bibitemmagidmap where mag_id in (select mid from (select count(distinct in_doc), mag_id as mid from bibitemmagidmap join bibitem on bibitemmagidmap.uuid = bibitem.uuid group by mag_id having count(distinct in_doc) > 1));
        12939708
        ```
        *↑ simplified (by assuming a paper doesn't have 2 distinct bibitems referencing the same cited doc) (0.1% change likely due to matching errors)*
        ```
        sqlite> select count(uuid) from bibitemmagidmap where mag_id in (select mag_id from bibitemmagidmap group by mag_id having count(uuid) > 1);
        12948919
        ```
    * further notes on results:
        * top 50 most matched cited docs (46.5/50 correct):
            * 13041 [2811252340](https://academic.microsoft.com/#/detail/2811252340) → mismatch of arXiv IDs noted like *T. Heinzl, hep-th/9812190.* w/ MAG noise (**note:** 2811252340 has one author listed with name *He*, which matches *hep-th*)
            * 9671 [1631356911](https://academic.microsoft.com/#/detail/1631356911) ✔
            * 6852 [2120062331](https://academic.microsoft.com/#/detail/2120062331) ✔
            * 6264 [2166248051](https://academic.microsoft.com/#/detail/2166248051) ✔
            * 5145 [2099111195](https://academic.microsoft.com/#/detail/2099111195) ✔
            * 4918 [1981368803](https://academic.microsoft.com/#/detail/1981368803) ✔
            * 4222 [2157005274](https://academic.microsoft.com/#/detail/2157005274) ✔
            * 4219 [2296319761](https://academic.microsoft.com/#/detail/2296319761) ✔
            * 3940 [2030690537](https://academic.microsoft.com/#/detail/2030690537) ✔
            * 3351 [1992204683](https://academic.microsoft.com/#/detail/1992204683) ✔
            * 3328 [2094438648](https://academic.microsoft.com/#/detail/2094438648) ✔
            * 3258 [1858542512](https://academic.microsoft.com/#/detail/1858542512) ✔
            * 3236 [2341283081](https://academic.microsoft.com/#/detail/2341283081) ✔
            * 3200 [2610857016](https://academic.microsoft.com/#/detail/2610857016) ✔
            * 3174 [1488001281](https://academic.microsoft.com/#/detail/1488001281) ✔
            * 3000 [119088996](https://academic.microsoft.com/#/detail/119088996) → should have matched [2064575216](https://academic.microsoft.com/#/detail/2064575216) (Classical theory of fields → The Classical Theory of Fields; same author)
            * 2996 [2618530766](https://academic.microsoft.com/#/detail/2618530766) ✔
            * 2994 [1978553093](https://academic.microsoft.com/#/detail/1978553093) ✔
            * 2913 [322617345](https://academic.microsoft.com/#/detail/322617345) → should have matched [1625053673](https://academic.microsoft.com/#/detail/1625053673) (Quantum Theory of Fields → The Quantum Theory of Fields; same author)
            * 2891 [2029403139](https://academic.microsoft.com/#/detail/2029403139) ✔
            * 2814 [2167727518](https://academic.microsoft.com/#/detail/2167727518) ✔
            * 2770 [2095721433](https://academic.microsoft.com/#/detail/2095721433) ✔
            * 2735 [2595419339](https://academic.microsoft.com/#/detail/2595419339) (✔) → [2078140326](https://academic.microsoft.com/#/detail/2078140326) same title/author/year (more bibitems for the latter)
            * 2712 [1578322733](https://academic.microsoft.com/#/detail/1578322733) ✔
            * 2702 [1686810756](https://academic.microsoft.com/#/detail/1686810756) ✔
            * 2630 [2751862591](https://academic.microsoft.com/#/detail/2751862591) ✔
            * 2567 [2139937287](https://academic.microsoft.com/#/detail/2139937287) ✔
            * 2561 [2798909945](https://academic.microsoft.com/#/detail/2798909945) ✔
            * 2536 [2102787760](https://academic.microsoft.com/#/detail/2102787760) ✔
            * 2512 [2258584306](https://academic.microsoft.com/#/detail/2258584306) ✔
            * 2505 [2039609754](https://academic.microsoft.com/#/detail/2039609754) ✔
            * 2490 [2011208902](https://academic.microsoft.com/#/detail/2011208902) ✔
            * 2476 [1522301498](https://academic.microsoft.com/#/detail/1522301498) ✔
            * 2462 [2134251287](https://academic.microsoft.com/#/detail/2134251287) ✔
            * 2378 [2037768897](https://academic.microsoft.com/#/detail/2037768897) ✔
            * 2319 [1986510200](https://academic.microsoft.com/#/detail/1986510200) ✔
            * 2274 [2194775991](https://academic.microsoft.com/#/detail/2194775991) ✔
            * 2271 [1575147392](https://academic.microsoft.com/#/detail/1575147392) ✔
            * 2243 [2611234537](https://academic.microsoft.com/#/detail/2611234537) ✔
            * 2189 [2071246696](https://academic.microsoft.com/#/detail/2071246696) ✔
            * 2186 [1909359706](https://academic.microsoft.com/#/detail/1909359706) ✔
            * 2179 [2147875900](https://academic.microsoft.com/#/detail/2147875900) ✔
            * 2154 [2028815089](https://academic.microsoft.com/#/detail/2028815089) ✔
            * 2139 [2327894213](https://academic.microsoft.com/#/detail/2327894213) → should have matched [2052959638](https://academic.microsoft.com/#/detail/2052959638) (Introduction To Quantum Field Theory → An Introduction To Quantum Field Theory)
            * 2137 [2051051926](https://academic.microsoft.com/#/detail/2051051926) ✔
            * 2112 [2134984950](https://academic.microsoft.com/#/detail/2134984950) ✔
            * 2108 [1866311589](https://academic.microsoft.com/#/detail/1866311589) ✔
            * 2096 [2064675550](https://academic.microsoft.com/#/detail/2064675550) ✔
            * 2081 [1970127494](https://academic.microsoft.com/#/detail/1970127494) ✔
            * 2066 [2003378896](https://academic.microsoft.com/#/detail/2003378896) ✔
        * **combining output of process #4 and #8**
            * last print at: (3505270+3505166)/2 = 3505218
            * percentage of total: 3505218/35053329 = 0,09999672213728972789
            * check with result size:
                * real 14046239
                * interpolated ((1380100+1430200)/2)*(35053329/3505218) = 14051960,6
                * → 0.0407% error
            * inerpolations:
                * Phys. Rev.: ((497904+501365)/2)*(35053329/3505218) = 4996508,8 → 35.57% (of matches)
                * no title: ((1192683+1130708)/2)*(35053329/3505218) = 11617335,8 → 33.14% (of all bibitems)
                * by arXiv ID: ((268239+284149)/2)*(35053329/3505218) = 2762030,5 → 19.66% (of matches)
                * by doi: ((113952+110068)/2)*(35053329/3505218) = 1120136,7 → 7.97% (of matches)
                * APS doi rebound: ((306470+303153)/2)*(35053329/3505218) = 3048214,9 → 21.70% (of matches)
            * of all bibitems:
                * 33.14% no arXiv ID, DOI, DOI deducable pattern & Neural ParsCit can't identify a title
                * 40.07% matched
                * 26.79% some (/wrong) title identified by Neural ParsCit; unable to match with MAG
            * of all matches:
                * 50,67 via Neural ParsCit
                * 29.67% by DOI
                    * 7.97% directly
                    * 21.70% heuristically
                * 19.66% by arXiv ID
        * **more stats**
            ```
            14046239/14046239
            num_reference_items_total: 13303373
            num_contexts_total: 24558560
            cited_docs_per_fos (slightly more than 2.343.585 b/c few papers have more than 1 level 0 FoS in mag):
              computer science: 321605
              mathematics: 806993
              physics: 824175
              materials science: 35929
              biology: 85023
              business: 5330
              engineering: 41256
              psychology: 24936
              medicine: 19585
              geography: 5302
              history: 1130
              economics: 31729
              chemistry: 100091
              political science: 3306
              sociology: 4071
              environmental science: 2037
              other: 3005
              philosophy: 2735
              geology: 24583
              art: 1144
            citing_docs_per_fos:
              cs: 84824
              math: 210602
              phys: 610159
              other: 21059
            ```
        * **4 numbers** (in brackets w/o considering that bibitems might have no markers associated in the plain text docs)
            * 2.343.585 (2.412.432) cited papers
            * 926.644 (1.018.976) citing papers
            * 13.303.373 (14.046.239) references
            * 24.558.560 (-) citation contexts
        * estimate of citation resolution hardness
            ```
            in arXiv metadata dump:  from citation pair stats:
            
            physics 922504  64.3%   |   51.9%  1146940
            math    357903  24.9%   |   26.1%   575016
            cs      155308  10.8%   |   22.0%   485127
            
                # documents       <--->      # citations
                       \__________________________/
                                    v
                   comparison therefore only works, assuming
                   an equal amout of citations per paper for
                   all fields compared
            ```

##### 2018〜 extension

* plain text extraction w/o matching (~7/8h)
    ```
    $ python3 prepare.py /vol3/data/arxiv-src-data-addition/ /home/saiert/arxiv-txt-data_2018/
    [...]
    430/430
    152153 files
    14587 PDFs
    ```  
    ```
    $ ls arxiv-txt-data_2018/ | grep .txt | wc -l
    131877
    $ du -h arxiv-txt-data_2018/
    6,5G    arxiv-txt-data_2018/
    $ du -h arxiv-txt-data_2018/metadata.db
    1,3G    arxiv-txt-data_2018/metadata.db
    ```
    ```
    sqlite> select count(*) from bibitem;
    4640754
    ```
    * 100% → 152,153
    * minus 14,587 PDFs → 137,566
    * pipleline output: 131,877 (95.9%)
    * with citations: 120,814 (-11,063) (87.8%) (79.4% incl. PDFs)
* matching
    * ...

### citation recommendation baseline

* idea: only syntax (mby +metadata) based
* misc notes:
    * [python code example using gensim](https://github.com/charles-vdulac/japanese-content-engine) (from [this presentation](https://www.youtube.com/watch?v=-xVeMQXMEcg))
    * [*simple* python code example using only pandas and scikit-learn](http://blog.untrod.com/2016/06/simple-similar-products-recommendation-engine-in-python.html)
    * [graph database](https://www.kernix.com/blog/an-efficient-recommender-system-based-on-graph-database_p9) [based thingy](https://www.kernix.com/blog/recommender-system-based-on-natural-language-processing_p10)

**train/test data generation stuff**  
```
$ cat items_1712_100w_4mincont_not_stemmed_oneperdoc.csv | wc -l    ← only one context per uuid per doc
11613
$ cat items_1712_100w_4mincont_not_stemmed.csv | wc -l              ← notice there's multiple contexts per doc
77121
$ cat items_1712_100w_4mincont_2mindoc_not_stemmed.csv | wc -l      ← notice you should do per doc splitting (require >1 docs)
41359
```

**TFIDF baseline test w/ gensim**  
(many fails, below a lucky nice example)  
```
$ python3 recommend.py items_1712_100w_4mincont.csv
similar to: math/9904022 (6768)
correct: [6186, 9728, 9771]
- - - - - - - -
✔ 0.309774786233902: 9770
✔ 0.179429829120636: 6186
  0.1443936824798584: 10067
  0.13785989582538605: 7983
  0.13637538254261017: 4082
  0.1355295479297638: 3858
  0.13414070010185242: 9969
  0.1162891760468483: 1831
  0.11208947747945786: 4716
  0.11141400039196014: 3620
  0.10494373738765717: 6270
```

**TFIDF baseline test w/ gensim** (80/20 split, then combine training vectors for same cited doc)  
```
$ time python3 recommend.py items_1712_100w_4mincont_stemmed.csv
- - - - - 1404/1404 - - - - -
#1: 199
in top 5: 663
in top 10: 954
avg: 14.554843304843304
ndcg: 0.4466429864293447
map: 0.2992083662933575
ndcg@5: 0.3093782646226242
map@5: 0.25600664767331355

real    15m13.260s
user    15m14.722s
sys     0m14.319s
```

not stemmed  
```
$ time python3 recommend.py items_1712_100w_4mincont_not_stemmed.csv
- - - - - 1404/1404 - - - - -
#1: 254
in top 5: 753
in top 10: 980
avg: 18.957264957264957
ndcg: 0.48227821110408237
map: 0.3448019509227324
ndcg@5: 0.3660963359894245
map@5: 0.3098765432098757

real    15m10.916s
user    15m12.664s
sys     0m13.642s
```

minimum of 2 citation contexts  
```
$ time python3 recommend.py items_1712_100w_2mincont_not_stemmed.csv
- - - - - 6843/6843 - - - - -
#1: 596
in top 5: 1895
in top 10: 2690
avg: 333.87315504895514
ndcg: 0.3212211934067741
map: 0.1825363717938699
ndcg@5: 0.18340135284347936
map@5: 0.15270836377807132

real    234m12.826s
user    233m30.086s
sys     0m54.815s
```

more data (natbib fix)  
```
$ python3 recommend.py items_1712_100w_2mincont_not_stemmed_NATBIB_FIX.csv
- - - - - 10497/10497 - - - - -
#1: 699
in top 5: 2382
in top 10: 3600
avg: 481.9152138706297
ndcg: 0.2925079165753687
map: 0.15257636645208303
ndcg@5: 0.14781602404672087
map@5: 0.12187450382649058
```

currently running evals: test difference between one per doc (2.7k test) and many per doc (18k)

one per doc:  
```
$ time python3 recommend.py ~/items_1712_100w_4mincont_not_stemmed_oneperdoc.csv
- - - - - 2790/2790 - - - - -
#1: 366
in top 5: 1305
in top 10: 1769
avg: 30.940501792114695
ndcg: 0.43196490690778483
map: 0.28601776529361
ndcg@5: 0.3027048501650121
map@5: 0.24845280764635663

real    61m20.370s
user    61m13.365s
sys     0m21.400s
```

many per doc *without* per doc splitting:  
```
$ python3 recommend.py ~/items_1712_100w_4mincont_not_stemmed.csv
- - - - - 18438/18438 - - - - -
#1: 6201
in top 5: 12608
in top 10: 14513
avg: 37.909643128321946
ndcg: 0.597700200736707
map: 0.48999624579362355
ndcg@5: 0.5218584144074571
map@5: 0.46789872365042323
```

many per doc *with* per doc splitting:  
```
$ python3 recommend.py items_1712_100w_4mincont_2mindoc_not_stemmed.csv
- - - - - 7787/7787 - - - - -
#1: 909
in top 5: 2995
in top 10: 4081
avg: 97.53460896365738
ndcg: 0.38836365919123905
map: 0.24484656370685862
ndcg@5: 0.25365219494863167
map@5: 0.21056675656007817
```

counting adjacent citations as half relevant:  
```
$ python3 recommend.py items_1712_100w_4mincont_2mindoc_not_stemmed.csv
- - - - - 7787/7787 - - - - -
#1: 909
in top 5: 2995
in top 10: 4081
ndcg@5: 0.23297871616321733   <-- slightly worse, because IDCG in nDCG Formula > 1 and apparently not many co citations in top 5
map@5: 0.21056675656007817
```

using MAG ID mapped larger dataset (↑41359 vs. ↓74074):  
```
$ python3 recommend.py items_1712_100w_4mincont_2mindoc_not_stemmed_mag.csv
- - - - - 14301/14301 - - - - -
#1: 1488
in top 5: 4778
in top 10: 6480
ndcg@5: 0.1968435487531205
map@5: 0.18446961750926533
```

just to show that difficulty increases with dataset size
```
$ python3 recommend_baseline.py items_1ksample_3s_5mindoc_5mincont.csv items_1ksample_3s_5mindoc_5mincont.dict
- - - - - 6389/6389 - - - - -
#1: 4527
in top 5: 5739
in top 10: 5987
ndcg@5: 0.7886603135592953
map@5: 0.7841289716700552
```

combining training set contexts vs not combining (all other evaluations in this document are done *combining* training set contexts)
```
>>> not combining <<<
$ time python3 recommend_baseline_nocluster.py items_1ksample_3s_5mindoc_5mincont.csv items_1ksample_3s_5mindoc_5mincont.dict
- - - - - 6389/6389 - - - - -
#1: 4159
in top 5: 5490
in top 10: 5829
ndcg@5: 0.7410420928469783
map@5: 0.7336020243126191

real    2m30.310s
user    2m31.928s
sys     0m6.052s

>> combining <<<
- - - - - 6389/6389 - - - - - 10 times faster + better results
#1: 4397
in top 5: 5669
in top 10: 5952
ndcg@5: 0.7747797443150142
map@5: 0.7685109824176948

real    0m15.758s
user    0m17.406s
sys     0m5.807s
```

early (~1%) snapshot of first eval with full arXiv dataset (cancelled)
```
$ python3 recommend_baseline.py items_all_3s_5mindoc_5mincont.csv
19400000/19401145 lines
loading dictionary
building corpus
generating TFIDF model
prepring similarities
test set size: 3507348
- - - - - - - -
- - - - - 1/3507348 - - - - -
[...]
- - - - - 35213/3507348 - - - - -
#1: 2782
in top 5: 7123
in top 10: 9512
ndcg@5: 0.12204397533455158
map@5: 0.12257262942663173
```
snapshot of 97.2/2.8 split (cancelled) (estimated full runtime: 8.5 days) (~1.36s/doc)
```
- - - - - 284414/551728 - - - - -
#1: 21055
in top 5: 55903
in top 10: 75398
ndcg@5: 0.12220673380877238
map@5: 0.11773207366725469
```
snapshot of 10% sample (cancelled) (estimated full runtime: 8.5 days) (~0.5s/doc)
```
- - - - - 716990/1428766 - - - - - 6066m
#1: 83205
in top 5: 200705
in top 10: 261160
ndcg@5: 0.1697610086377023
map@5: 0.17504377094972065
```

testing TFIDF all CS (1835797/324466 (82.3/17.7) split)
```
$ python3 recommend_baseline_nocluster.py items_CSall_3s_5mindoc_5mincont.csv items_CSall_3s_5mindoc_5mincont.dict
[...]
- - - - - 324466/324466 - - - - -
#1: 32558
in top 5: 89570
in top 10: 121106
ndcg@5: 0.1712868034589398
map@5: 0.16274411905511044
```
testing TFIDF all CS **one sentence** contexts → only minor difference for BOW approach
```
$ python3 recommend_baseline_nocluster.py items_CSall_1s_5mindoc_5mincont.csv items_CSall_1s_5mindoc_5mincont.dict
- - - - - 324466/324466 - - - - -
#1: 37037
in top 5: 83754
in top 10: 107471
ndcg@5: 0.1578428957340939
map@5: 0.1664695735968292
```
testing TFIDF all CS *one sentence* contexts **with 2k character limit** → slight improvement for BOW approach
```
- - - - - 324460/324460 - - - - -
#1: 41507
in top 5: 92856
in top 10: 117132
ndcg@5: 0.177910862350392
map@5: 0.18562668228236676
```
testing TFIDF "all" MATH
```
$ python3 recommend_baseline_nocluster.py items_MATHCSsize_3s_5mindoc_5mincont.csv items_MATHCSsize_3s_5mindoc_5mincont.dict
[...]
- - - - - 325830/325830 - - - - -
#1: 50483
in top 5: 113511
in top 10: 142262
ndcg@5: 0.1964657159233343
map@5: 0.22582317977675412
```

testing TFIDF CS **in MAG**
```
$ python3 recommend_baseline_nocluster.py items_MAG_CS_EN_arXivCSsize_5mindoc.csv items_MAG_CS_EN_arXivCSsize_5mindoc.dict
- - - - - 344092/344092 - - - - -
#1: 106184
in top 5: 160444
in top 10: 180082
ndcg@5: 0.39330421057547543
map@5: 0.3689516660271934

after heuristically removing authors' names from citations

$ python3 recommend_baseline_nocluster.py items_MAG_CS_EN_arXivCSsize_5mindoc_stripped.csv items_MAG_CS_EN_arXivCSsize_5mindoc_stripped.dict
- - - - - 344092/344092 - - - - -
#1: 84803
in top 5: 134606
in top 10: 154697
ndcg@5: 0.32350134348045967
map@5: 0.3009809876427639

controlling CS EN for citing docs instead of cited

$ python3 recommend_baseline_nocluster.py items_MAG_citing_CS_EN_arXivCSsize_5mindoc_stripped.csv items_MAG_citing_CS_EN_arXivCSsize_5mindoc_stripped.dict
- - - - - 339266/339266 - - - - -
#1: 95167
in top 5: 150830
in top 10: 171915
ndcg@5: 0.36833062588337273
map@5: 0.34291873240062887
```

##### MAG specific notes
query DB for citation contexts for English CS cited papers with at least 5 citing papers ()
```
select paperreferenceid, paperid, citationcontext from papercitationcontexts where paperreferenceid in (select cont.paperreferenceid from papercitationcontexts as cont join (select fos.paperid from paperfieldsofstudy as fos join paperlanguages as lang on fos.paperid = lang.paperid where fieldofstudyid = 41008148 and languagecode = 'en') as foslang on cont.paperreferenceid = foslang.paperid group by cont.paperreferenceid having count(cont.paperid) > 4) order by paperreferenceid;

alternative, EN CS citing papers:
select paperreferenceid, paperid, citationcontext from papercitationcontexts where paperreferenceid in (select cont.paperreferenceid from papercitationcontexts as cont join (select fos.paperid from paperfieldsofstudy as fos join paperlanguages as lang on fos.paperid = lang.paperid where fieldofstudyid = 41008148 and languagecode = 'en') as foslang on cont.paperid = foslang.paperid group by cont.paperreferenceid having count(cont.paperid) > 4) order by paperreferenceid

select paperreferenceid, paperid, citationcontext from
    papercitationcontexts
    where paperreferenceid in
        (select cont.paperreferenceid from
            papercitationcontexts as cont
            join
            (select fos.paperid from
                paperfieldsofstudy as fos
                join
                paperlanguages as lang
                on fos.paperid = lang.paperid
                where fieldofstudyid = 41008148 and languagecode = 'en') as foslang
            on cont.paperreferenceid = foslang.paperid              / on cont.paperid = foslang.paperid
            group by cont.paperreferenceid having count(cont.paperid) > 4)
    order by paperreferenceid;
```

count of contexts as described above
```
MAG=> select count(*) from papercitationcontexts where paperreferenceid in (select cont.paperreferenceid from papercitationcontexts as cont join (select fos.paperid from paperfieldsofstudy as fos join paperlanguages as lang on fos.paperid = lang.paperid where fieldofstudyid = 41008148 and languagecode = 'en') as foslang on cont.paperreferenceid = foslang.paperid group by cont.paperreferenceid having count(cont.paperid) > 4);
  count   
----------
 21955013       / 47410586 when filtering for citing papers
```

### notes on evaluation

* thoughts on exact re-recommendation
    * exact re-recommendation would imply the assumption, that for a given context only this very citation is acceptable
    * including direct co-citations (`\cite{a,b}`) acknowledges, that either of the cited docs is relevant
        * a performance drop of nDCG is due to a more realistic shift in expectation: ideal is *not* to have a relevant doc somewhere in the top 5 but to have 5 relevant docs to choose from
        * in the same vein, "indirect co-citation" (in the same context) could very well be considered somewhat relevant
* considerations when train/test splitting
    * always ensure a "stratified" split (i.e. don't just split 80/20 along all items in some order, split each *grouped per cited doc*-mini-package 80/20)
    * because an author's word choice and a paper's content can be a strong signal, it would be ideal to not have citations from one paper split (some in test, some in training)
        * just like typhoon/sequence wise splits in pyphoon
        * → require at least 2 *containing docs* for evaluation, then do a document wise train/test split
    * algo
        ```
        min_num_train = math.floor(num_contexts * 0.8)
        train_texts_comb = []
        test_texts = []
        for jdx, sub_bag_key in enumerate(order):
            sb_texts = sub_bags_dict[sub_bag_key]
            if len(train_texts_comb) > min_num_train or jdx == len(order)-1:
                test_texts.extend(sb_texts)
            else:
                train_texts_comb.extend(sb_texts)
        ```
        ```
        go through 'per citing doc packages':
          if combined number of train contexts did not reach given percentage yet and not very last citing doc package:
            add to train contexts
          else:
            add to test contexts
        ```
        * results in 3,507,348 / 15,893,797 (18/82) split for full arxiv

* notes on citation styles
    * depending on source of contexts (e.g. LaTeX vs PDF), author/journal preference, writing style, etc. names and dates can end up in citation contexts
        * `As Bartels (2005) points out ...`
        * `Havinga [22], and Lettieri [31] have applied ...`
        * `Mc Laughlin et al. show in [7] a way how ...`
        * `... affected perceived usefulness (Wu and Lu, 2013).`
    * attempts at removing those could, however, remove valid information
        * `Xax [15], Native Client [40], and Drawbridge [29] provide tenants with ...`
        * → question of where to draw the border when a method named after an author becomes widely known in a field 
* misc
    * minimum threshold for citation contexts (2 and 4 tested) has large impact on performance

##### Temporal cut

* coverage of certain cuts in unarXive case
    * all: citing 966,203 — cited 2,412,432
    * Dec '17: citing 8,040 — cited 135,698
    * Nov~Dec '17: citing 17,029 — cited 250,352
    * Oct~Dec '17: citing 26,015 — cited 348,184
    * Sep~Dec '17: citing 34,107 — cited 430,786
    * Aug~Dec '17: citing 41,737 — cited 501,614
    * Jan~Dec '17: citing 95,850 — cited 882,215
    * Jan '16~Dec '17: citing 183,272 — cited 1,289,252
    * Jan '15~Dec '17: citing 263,978 — cited 1,558,873
    * Jan '14~Dec '17: citing 337,697 — cited 1,750,657
    ```
    select count(distinct t.in_doc) from ((select uuid, in_doc from bibitem where in_doc like '1712%' or in_doc like '1711%') as t join (select uuid, mag_id from bibitemmagidmap) as b on t.uuid = b.uuid);
    select count(distinct mag_id) from ((select uuid from bibitem where in_doc like '1712%') as t join (select uuid, mag_id from bibitemmagidmap) as b on t.uuid = b.uuid);
    ```

### citation recommendation with pre-clustering

unstructured notes
```
# create tfidf "view" on corpus (computed on the fly when used)
corpus_tfidf = tfidf[corpus]

# lsi stuff (really useful for speedup?)
from gensim.models import LsiModel
lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)
for t in lsi.print_topics(10):
    print(t)
corpus_lsi = lsi[corpus_tfidf]
for doc in corpus_lsi[:10]:
    print(doc)

# clustering
import gensim
from sklearn.cluster import KMeans
scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)
kmeans = KMeans(n_clusters=2, random_state=0, n_jobs=20).fit(scipy_csc_matrix.transpose())  # 2 min on shetland

>>> len(kmeans.labels_)
961
>>> len(corpus)
961

# faster clustering
from sklearn.cluster import MiniBatchKMeans
kmeans = MiniBatchKMeans(n_clusters=2, random_state=0).fit(scipy_csc_matrix.transpose())
```

first test with bad clusters and bad results
```
cluster sizes:
  50
  909
  1
  1
- - - - - 6389/6389 - - - - -
#1: 3891
in top 5: 4476
in top 10: 4587
ndcg@5: 0.5735516233092415
map@5: 0.6458992017530135

no cluster:
- - - - - 6389/6389 - - - - -
#1: 4397
in top 5: 5669
in top 10: 5952
ndcg@5: 0.7747797443150142
map@5: 0.7685109824176948
```
csc_corpus = corpus2csc(**tfidf[**corpus**]**)
```
cluster sizes:
  280
  71
  252
  230
  128
- - - - - 6389/6389 - - - - -
#1: 3847
in top 5: 4798
in top 10: 4985
ndcg@5: 0.6653964455166995
map@5: 0.6632754212970214
↓ fix test bow generation w/ TFIDF vals ↓
- - - - - 6389/6389 - - - - -
#1: 3982
in top 5: 4962
in top 10: 5159                   no cluster:
ndcg@5: 0.6886718460042281        0.7747797443150142
map@5: 0.6863932801168672         0.7685109824176948
↓ using LDA for clustering ↓
- - - - - 6389/6389 - - - - -
#1: 3393
in top 5: 3906
in top 10: 3961
ndcg@5: 0.5590948786636911
map@5: 0.5649632180309913
```
on TFIDF vals with CSall
```
Iteration 159, inertia 59258.047
Converged at iteration 159: center shift 0.000000e+00 within tolerance 4.317925e-10
cluster sizes:
  522
  4532
  1114
  2853
  3356
  2828
  4880
  2221
  1838
  3894
  1187
  2924
  1147
  280
  1822
  375
  2566
  936
  500
  1082
  2156
  486
  1092
  17464
  1185

(enter to continue)   50 cluster (limited to 1 init, 41 itr.)  500 cluster
324466/324466                  v                               v                                 non cluster numbers
#1: 26078                     #1: 26430                       #1: 23516                          #1: 32558
in top 5: 67882               in top 5: 68204                 in top 5: 59867                    in top 5: 89570
in top 10: 89490              in top 10: 89142                in top 10: 77709                   in top 10: 121106
ndcg@5: 0.1330362841406317    ndcg@5: 0.1350676714519149      ndcg@5: 0.12293981186824066        ndcg@5: 0.1712868034589398
map@5: 0.12652054966211118    map@5: 0.12774147881955733      map@5: 0.11270677564572494         map@5: 0.16274411905511044
```

full data set clustering
```
kmeans = KMeans(
    n_clusters=n_clusters, random_state=27, n_jobs=-1, verbose=1
    ).fit(csc_corpus.transpose())
```
```
[...]
Iteration 122, inertia 9255002310.536
Converged at iteration 122: center shift 1.451601e-06 within tolerance 3.178566e-06
Initialization complete
Iteration  0, inertia 11636376082.000
[...]
Iteration 63, inertia 9173144224.734
Converged at iteration 63: center shift 5.914844e-07 within tolerance 3.178566e-06
cluster sizes:
  92448
  2
  12
  1786
  426494
  1
  310
  11
  1
  1071
  2
  56
  103
  1
  5
  153
  4620
  7
  1
  6
  1
  23
  63
  20816
  13

(enter to continue)
```
10 clusters
```
Iteration 49, inertia 10662151434.133
Converged at iteration 49: center shift 0.000000e+00 within tolerance 3.178566e-06
cluster sizes:
  516830
  1
  1
  304
  1954
  148
  28735
  2
  22
  9
```
↓ 0.22 because of bug in code (took rank in selected cluster which could be size 1)
```
- - - - - 35213/3507348 - - - - -   no cluster numbers
#1: 5822                            2782
in top 5: 10681                     7123
in top 10: 12236                    9512
ndcg@5: 0.06899620276169097         0.12204397533455158
map@5: 0.2270965268508764           0.12257262942663173
```

full dataset 100 clusters
```
Iteration 296, inertia 496332.217
Converged at iteration 296: center shift 0.000000e+00 within tolerance 1.047159e-10
cluster sizes:
  6066
  5830
  2661
  4279
  7293
  21218
  3093
  11019
  3449
  6139
  1212
  3105
  58956
  5523
  5711
  1411
  4673
  6357
  10191
  7497
  4261
  2926
  7707
  5902
  6186
  565
  2326
  9799
  1003
  3039
  2142
  2085
  1125
  6587
  3482
  4487
  1364
  8906
  3591
  7656
  4325
  5654
  4097
  4373
  6724
  2992
  6941
  2701
  3851
  2843
  3554
  2961
  4072
  3213
  2291
  873
  1855
  2577
  3243
  5794
  2369
  4980
  2362
  17982
  5903
  4520
  8001
  2675
  2315
  7475
  1875
  1105
  1810
  4437
  2857
  6195
  2586
  6969
  1295
  1993
  6281
  6003
  882
  7451
  4558
  2563
  13947
  2997
  7656
  2468
  1495
  7907
  3532
  33014
  3650
  2967
  1191
  4872
  3013
  6099

(clustering took ~80 hours (default values; i.e. 10 times, max. 300 iterations))
(continued 19/02/09 22:44)

(snapshot 19/03/11 16:15)
- - - - - 2209402/3507348 - - - - - (~0.07s / recomm) → possible to do 8h clustering + 68h eval ⇨ ~3d
#1: 116986
in top 5: 308590
in top 10: 412979
ndcg@5: 0.08642255712607769
map@5: 0.08381511528163213

- - - - - 3507348/3507348 - - - - -
#1: 184847
in top 5: 490061
in top 10: 656413
ndcg@5: 0.08671017439772115
map@5: 0.08365948669300437
```
full dataset 50 clusters
```
- - - - - 3507305/3507305 - - - - - 0.07s / recomm    (~82h including n_init=2, max_iter=100 clustering)
#1: 196502
in top 5: 525506
in top 10: 707348
ndcg@5: 0.09263015209062501
map@5: 0.08933528735035522
```
full dataset 25 clusters
```
- - - - - 1555002/3507305 - - - - - (snapshot) 0.13s / recomm
#1: 90540
in top 5: 243102
in top 10: 328528
ndcg@5: 0.09575525362643722
map@5: 0.09305566809556776
```

### citation recommendation pre-filtering with LDA/LSI

gist
```
lda = LdaMulticore(tfidf[corpus], id2word=dictionary, num_topics=100)    # LDA model
lda_index = similarities.SparseMatrixSimilarity(
    lda[tfidf[corpus]],
    num_features=num_unique_tokens)
lda_sims = lda_index[tfidf[test_bow]]                                    # rank in LDA space
lda_sims_list = list(enumerate(lda_sims))
lda_sims_list.sort(key=lambda tup: tup[1], reverse=True)
lda_ranking = [s[0] for s in lda_sims_list]
lda_picks = lda_ranking[:1000]                                           # get top 1000
index.index = orig_index[lda_picks]                                      # restrict BOW model to above 1000
sims = index[tfidf[test_bow]]
[...]
bow_ranking = [lda_picks[r] for r in bow_ranking]                        # translate back ranking
```

LDA test with 1000 topics and top 5000 cut:
```
$ python3 recommend_baseline_nocluster.py items_CSall_3s_5mindoc_5mincont.csv items_CSall_3s_5mindoc_5mincont.dict
- - - - - 324460/324460 - - - - -
#1: 28075
in top 5: 69084
in top 10: 86166
ndcg@5: 0.13126081735308817
map@5: 0.13253189915552152
```
→ size problems w/ full data set

**LSI** test with 100 topics and top 1000 cut:
```
$ python3 recommend_baseline_nocluster.py items_CSall_3s_5mindoc_5mincont.csv items_CSall_3s_5mindoc_5mincont.dict
- - - - - 324460/324460 - - - - -
#1: 34293
in top 5: 93035
in top 10: 124284
ndcg@5: 0.1805019751033324
map@5: 0.17029505434671832
```

### citation recommendation based on entities

* no clue of argumentative structure yet → chose fixed citation context size
* mby equivalent [1](https://link.springer.com/content/pdf/10.1007%2F978-3-319-30671-1_3.pdf), [2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.8098&rep=rep1&type=pdf), [3](http://l3s.de/~fetahu/publications/cikm2016.pdf), [4](http://www.l3s.de/~fetahu/publications/fetahu_cikm2015.pdf) (recommend news articles for wikipedia articles referencing past events)

##### named entity linking

* find tool for linking stuff to wiki-/dbpedia
* `curl -d '<item><text>Convolutional neural networks have been used for artificial intelligence.</text></item>' -X POST http://132.230.150.127:8080/text-annotation-with-offset-Nov14/`
* citations + POS tags (only in part involving named entities):
    * `[] (+ sub clause) + verb` (e.g. *[] show that* or *[], cited a gazillion times, found that*)
    * `proper Noun (+ et al.) + []` (e.g. *the work of ____ []*)
    * `proper Noun (+ "dataset") + []` (e.g. *DBpedia []*) (mentioned in Abu-Jbara2013 (there: noun phrase + []))
    * `preposition (+ Authors) + []` (most general case; e.g. *in []*, *by []*, *from []*)
    * `[].` (sentence end; could be anything)
* spotlight docker test
    * `$ docker pull dbpedia/spotlight-english`
    * `$ docker run -i -p 2222:80 dbpedia/spotlight-english:latest spotlight.sh`
    * `$ time python3 spotlight_annotate.py /home/saiert/arxiv_test_2017_12_all_181110/all_text/`  
        ```
        real    40m35.011s
        user    3m25.997s
        sys     0m5.937s
        ```  
        ```
        $ ls arxiv_test_2017_12_all_181110/all_text/ | grep txt | wc -l
        8996
        $ ls arxiv_test_2017_12_all_181110/all_text/ | grep json | wc -l
        8821
        ```
* MAG entity test
    * **idea**
        * if annotated FoSs can be shown to be not more informative than FoS for the whole paper in MAG itself, then annotating FoS MAG *for the dataset* itself is not necessary
            * user input→FoS annotate
            * dataset contexts: just "annotate" with MAG paper FoS
            * → evaluation of how informative FoS annotations are for a MAG paper can still be seen as a predictive measure of how informative the annotations on user input will be
    * 8808 citation contexts (100w)
        ```
        $ ls citcontxs | wc -l
        8808
        $ du -hs citcontxs
        35M     citcontxs
        $ time java -jar linking_file_tarek.jar citcontxs citcontxs_out/
        [...]
        [22:00:08] [INFO ] [MentionDetectorLSH  ]       Collisions: 1366140
        
        real    143m10.872s
        user    367m38.134s
        sys     5m17.375s
        ```
    * 22k plain text *papers*
        ```
        $ ls /home/saiert/arxiv_test_1991-2017_sample/arxiv_sample_text/ | wc -l
        22840
        $ time java -jar linking_file_tarek.jar /home/saiert/arxiv_test_1991-2017_sample/arxiv_sample_text/ /home/saiert/arxiv_test_1991-2017_sample/arxiv_sample_annot/
        [...]
        
        real    10879m20.052s
        user    55615m56.373s
        sys     393m0.262s
        ```
        * manual evaluation of annotations on 3 example papers (`cs0110014`, `1404.1775`, `1712.08091`)
            * precision: 75; 70; 61
            * recall: 49 (/60); 51; 56
        * annotations in 39.300 annotation context
            * counting all:
                ```
                annot./context:
                    avg.: 7.1
                    min.: 0
                    max.: 41
                 0: 940 (2.4%)
                 1: 1500 (3.8%)
                 2: 2319 (5.9%)
                 3: 3071 (7.9%)
                 4: 3796 (9.7%)
                >4: 27374 (70.2%)

                total #ann. @level
                    0: 2640
                    1: 11517
                    2: 197266
                    3: 37692
                    4: 14907
                    5: 12721
                ```
            * counting only annot. w/ conf. > 5.0:
                ```
                annot./context:
                    avg.: 4.3
                    min.: 0
                    max.: 37
                 0: 3161 (8.1%)
                 1: 4591 (11.8%)
                 2: 5423 (13.9%)
                 3: 5401 (13.8%)
                 4: 4893 (12.5%)
                >4: 15531 (39.8%)
                
                total #ann. @level
                    0: 2640
                    1: 11515
                    2: 131902
                    3: 17344
                    4: 4580
                    5: 0
                ```
        * testing annotations as pre-filter (snapshot) (candidates from *all* MAG papers (not *so* representative b/c arXiv papers mostly phys/math/cs))
            ```
            - - - filter candidates - - -

            min: 1              ← based on 11810 contexts
            max: 70,404,442         ← based on 11810 contexts
            still in [0 hops]: 7355/39979   ← based on 39979 c.  (11810 contexts → avg #candidates: 846,145 (0.0040%))
            still in [1 hop ]: 18959/39979  ← based on 39979 c.  (11810 contexts → avg #candidates: 10,461,426 (0.0498%))
            still in [2 hops]: 30924/39979  ← based on 39979 c.
            
            ↓ based on 3995 contexts ↓
            real    11048m22.560s    → almost 8 days for ~4k contexts
            user    0m23.986s
            sys     0m3.719s
            ```
    * 86k of the 1M plain text *papers* (snapshot, "paused")
        ```
        $ time java -jar linking_file_tarek.jar /home/saiert/arxiv-txt-data /home/saiert/arxiv-txt-data_fos-annot/
        [pagerankloading] Executed in 1135 ms.
         INFO [main] (NodeBlacklisting.java:181) - Removed 8 blacklisted nodes from succ/pred from 229304 other nodes.
        [Blacklist] Executed in 323 ms.
        ^C
        real    14327m59.712s
        user    949m36.386s
        sys     63m38.535s
        ```

**simply feeding (dbpedia) annotations into TFIDF baseline** (building on above *counting adjacent citations as half relevant*)  
```
$ python3 recommend.py items.csv    (1712 100w 4mincont 2mindoc spotlight_annot)
- - - - - 7787/7787 - - - - - simply feed | artificially let each DBpedia IDs appear unique in corpus (boost TFIDF)
#1: 900 | 890
in top 5: 2993 | 2916
in top 10: 4080 | 4016
ndcg@5: 0.232 | 0.227
map@5: 0.209 | 0.205
```

**only using (dbpedia) annotations**  
```
$ python3 recommend.py items.csv    (1712 100w 4mincont 2mindoc spotlight_annot)
- - - - - 7787/7787 - - - - -
#1: 559
in top 5: 1760
in top 10: 2571
ndcg@5: 0.13512704458191738
map@5: 0.12488763323487953

real    67m51.744s
user    67m43.101s
sys     0m22.163s
```

———ｎｅｗ—ｓｈｉｎｙ—ｅｒａ———

1k sample test
```
$ python3 recommend_baseline_nocluster.py items_1ksample_wFoS_3s_5mindoc_5mincont.csv items_1ksample_wFoS_3s_5mindoc_5mincont.dict
- - - - - 5422/5422 - - - - - BOW only
#1: 3807
in top 5: 4827
in top 10: 5073
ndcg@5: 0.7941903291193916
map@5: 0.7776650682405032
- - - - - 5422/5422 - - - - - FoS dot product only
#1: 1085
in top 5: 2112
in top 10: 2560
ndcg@5: 0.2942255234311445
map@5: 0.26928562646010107
- - - - - 5422/5422 - - - - - if dot product >=1 push top 2 highest ranked by 1 rank
#1: 3852
in top 5: 4851
in top 10: 5078
ndcg@5: 0.8006535381316311
map@5: 0.7850547153571877
```

using only directly preceding FoS annot (... DBPedia [27] ... case)
```
not enough train/test overlap in a 5k citing doc sample
```


##### ~~named entity recognition~~ (probably not so important)

* [NLTK](https://www.nltk.org/)
    * simple example:  
        ```
        import nltk
        s = 'Some sentence with named entities like Bill Gates.'
        sent = nltk.word_tokenize(s)
        sent = nltk.pos_tag(s)
        print(nltk.ne_chunk(sent, binary=True))
        ```
* spaCy
    * apparently faster than NLTK but opinionated (offers one implementation per task, not multiple to choose from)
* gensim
    * "most commonly used for topic modeling and similarity detection"

### citation recommendation based on claims

* some clue of argumentative structure (?) → mby variable citation context size (i.e. sentences)

* manually tested Open IE 5.0 and PredPatt → latter achieved better results
    * open questions
        * more approaches to compare to? ([Graphene](https://github.com/Lambda-3/Graphene), [Reverb](https://github.com/knowitall/reverb), [ClausIE](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/software/clausie/))
            * also tested ClausIE and Ollie, PredPatt seems most sophisticated
        * how to model facts into citation context representation?

##### understanding PredPatt output
[Stanford Lecture: Dependency Parsing](https://www.youtube.com/watch?v=PVShkZgXznc)

PredPatt works with [dependency grammar](https://en.wikipedia.org/wiki/Dependency_grammar) as opposed to [phrase structure (/constituency) grammar](https://en.wikipedia.org/wiki/Phrase_structure_grammar). ([YT: Grammar of Sentences: Phrases vs. Dependencies](https://www.youtube.com/watch?v=BZij1XlN4wc))

* [universaldependencies.org English grammatical relations](http://universaldependencies.org/docs/en/dep/all.html)
* [Universal Dependencies: A cross-linguistic typology](https://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf)
    * [Government and binding theory](https://en.wikipedia.org/wiki/Government_and_binding_theory)
        * [Government (linguistics)](https://en.wikipedia.org/wiki/Government_(linguistics))
            * → PredPatt's DepTriple(rel, gov, dep) consist of [relation](http://universaldependencies.org/docs/en/dep/all.html), governor, dependant: governor--[relation--]--> dependant


### citation recommendation based on arguments

* *as intro, read* `Stab2016` (+mby `Habernal2017`)

* "detection of context should be related to the identification of the argument around the citation" (HERNANDEZ-ALVAREZ2016)
* [MARGOT](http://margot.disi.unibo.it/) (!!! **local version on tuco** [+old page of same authors](http://argumentationmining.disi.unibo.it/publications.html)

##### michael notes

* definition in paper Argumentation Mining: The Detection, Classification and Structure of Arguments in Text
    * challenges/goals of argumentation mining:
        * extract arguments ("Component identification" sometimes)
        * extract and represent the internal structure of arguments, i.e., the propositions and their relations (an argument consists of at least two propositions, so the assumption here; "Component classification" sometimes)
        * extract the argumentation structure, i.e., the interaction between arguments ("structure identification" sometimes).
* good survey: [Parsing Argumentation Structures in Persuasive Essays](https://arxiv.org/pdf/1604.07370.pdf)
* good survey: [Argumentation Mining in User-Generated Web Discourse](https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00276)
    * dort u.a. Erwähnung/Verlinkung von confirmation bias: Link zu *Recognizing the Absence of Opposing Arguments in Persuasive Essays* (darin: "tendency to ignore opposing arguments is known as *myside bias* or *confirmation bias* (Stanovich et al., 2013). It has been shown that guiding students to include opposing arguments in their writings significantly improves the argumentation quality, the precision of claims and the elaboration of reasons (Wolfe and Britt, 2009). Therefore, it is likely that a system which automatically recognizes the absence of opposing arguments effectively guides students to improve thier argumentation".
* Weitere aktuelle Arbeiten (z.B. gute und schlechte Ansätze) im ["Argument Mining"-Workshop](http://aclweb.org/anthology/W17-51) (z.B. "What works and what does not: Classifier and feature analysis for argument mining")
* Auch noch leicht verwandt: [SemEval-2018 Task12: Argument Reasoning Comprehension](https://competitions.codalab.org/competitions/17327)
* further resources
    * Sehr nützlich: ["Transition words"](https://msu.edu/~jdowell/135/transw.html) benutzt in Recognizing the Absence of Opposing Arguments in Persuasive Essays.
* also
    * Discourse Theory, e.g. Discourse Representation Theory, bzw. Discourse Analysis:
        * "Similar  to  the  identification  of  argumentation structures,  discourse  analysis  aims  at identifying elementary discourse units and discourse relations between them. Existing approaches on discourse analysis mainly differ in the employed discourse theory." [https://arxiv.org/pdf/1604.07370.pdf](https://arxiv.org/pdf/1604.07370.pdf)
    * Argumentation theory

# misc notes

* [Daniel Hershcovich](http://www.cs.huji.ac.il/~danielh/)
* python
    * [scikit-learn](http://scikit-learn.org/)
        * `fit-predict` pattern
        * classification: `train_test_split` function has useful param `stratify` to ensure balanced distribution over classes
    * [pandas](https://pandas.pydata.org/)
        * classification: `pd.plotting.scatter_matrix` in case of not so many features
* scipy random forest works column wise and therefore is more efficient on csc (not csr) sparse matrices
* probably not useful but:
    * [pybtex.org](https://pybtex.org/)
    * [pycld2 (language detection)](https://github.com/aboSamoor/pycld2)
* Japanese arXiv submissions:
    * full: 1612.09014, 1706.03497, 1705.05099, 1705.06376, 1705.06377, 1712.10228
    * abstract: 1706.07104, 1704.02620
* [NLP papers with code](https://paperswithcode.com/area/nlp) (by task)