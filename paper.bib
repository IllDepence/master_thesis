% Encoding: UTF-8

@InProceedings{Kitamoto2016,
  author    = {Asanobu Kitamoto and Yoko Nishimura},
  title     = {{Digital Criticism Platform for Evidence-based Digital Humanities with Applications to Historical Studies of Silk Road}},
  booktitle = {Digital Humanities 2016},
  year      = {2016},
  pages     = {596--600},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/dihu/KitamotoN16},
  crossref  = {DBLP:conf/dihu/2016},
  timestamp = {Wed, 11 Jul 2018 17:05:18 +0200},
  url       = {http://dh2016.adho.org/abstracts/92},
}

@InProceedings{Faerber2018,
  author    = {Michael F{\"{a}}rber and Alexander Thiemann and Adam Jatowt},
  title     = {{A High-Quality Gold Standard for Citation-based Tasks}},
  booktitle = {{Proceedings of the 11th International Conference on Language Resources and Evaluation}},
  year      = {2018},
  series    = {{{LREC} 2018}},
  review    = {- created new data set for cit.rec. based on arXiv
- only compsci papers
- other data sets (most commonly used one is CiteSeerX) have two main problems: (1) noisy citation context, (2) no/low quality links of citations to cited publications
- latex parser GrabCite (also parses other formats)
- 90k papers (for 62k DBLP link), 15M sentences, 2.5M citation markers

- parser: https://github.com/agrafix/grabcite
- DBLP serach thingy: https://github.com/agrafix/papergrep},
}

@InProceedings{Levy2014,
  author    = {Levy, Ran and Bilu, Yonatan and Hershcovich, Daniel and Aharoni, Ehud and Slonim, Noam},
  title     = {{Context Dependent Claim Detection}},
  booktitle = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  year      = {2014},
  pages     = {1489--1500},
  address   = {Dublin, Ireland},
  month     = {August},
  publisher = {Dublin City University and Association for Computational Linguistics},
  review    = {- detect claims (relevant for a given topic(=statement open for discussion)) in free text
- claims may represent an opinion (i.e. can't be checked for truth/falsity)
- classifies/ranks in 3 stages (candidate sentences -> claim within sentence -> ranking)
- uses word net
- uses a lot of existing parsers, features, etc.},
  url       = {http://www.aclweb.org/anthology/C14-1141},
}

@Article{Hassan2015,
  author    = {Hassan, Naeemul and Adair, Bill and Hamilton, James and Li, Chengkai and Tremayne, Mark and Yang, Jun and Yu, Cong},
  title     = {{The Quest to Automate Fact-Checking}},
  year      = {2015},
  month     = {10},
  booktitle = {Proceedings of the 2015 Computation + Journalism Symposium},
  review    = {- argue for pursuing the goal of fully automated fact checking
- uses political discourse as goto example (politicians making claims, voters juding claims for their truth)
- developed own tool ClaimBuster
- supervised ML using SVM (also tested other methods, SVM performed best)
- manually created features
- used AlchemyAPI for sentiment analysis
- no stemming or stop word removal
- POS tags
- used random forest classifier},
}

@Article{BuckinghamShum2000,
  author   = {Buckingham Shum, Simon and Motta, Enrico and Domingue, John},
  title    = {{ScholOnto: an ontology-based digital library server for research documents and discourse}},
  journal  = {International Journal on Digital Libraries},
  year     = {2000},
  volume   = {3},
  number   = {3},
  pages    = {237--248},
  month    = {Oct},
  issn     = {1432-5012},
  abstract = {The internet is rapidly becoming the first place for researchers to publish documents, but at present they receive little support in searching, tracking, analysing or debating concepts in a literature from scholarly perspectives. This paper describes the design rationale and implementation of ScholOnto, an ontology-based digital library server to support scholarly interpretation and discourse. It enables researchers to describe and debate via a semantic network the contributions a document makes, and its relationship to the literature. The paper discusses the computational services that an ontology-based server supports, alternative user interfaces to support interaction with a large semantic network, usability issues associated with knowledge formalisation, new work practices that could emerge, and related work.},
  day      = {01},
  doi      = {10.1007/s007990000034},
  review   = {- proposal to (manually) add semantic annotation to scientific texts (to enable network of contestable claims)
- introduce annotation scheme for above
- "discourse-oriented ontology"
- 90s style interface implementation},
  url      = {https://doi.org/10.1007/s007990000034},
}

@InProceedings{Goudas2014,
  author    = {Goudas, Theodosis and Louizos, Christos and Petasis, Georgios and Karkaletsis, Vangelis},
  title     = {{Argument Extraction from News, Blogs, and Social Media}},
  booktitle = {Artificial Intelligence: Methods and Applications},
  year      = {2014},
  editor    = {Likas, Aristidis and Blekas, Konstantinos and Kalles, Dimitris},
  pages     = {287--299},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {Argument extraction is the task of identifying arguments, along with their components in text. Arguments can be usually decomposed into a claim and one or more premises justifying it. Among the novel aspects of this work is the thematic domain itself which relates to Social Media, in contrast to traditional research in the area, which concentrates mainly on law documents and scientific publications. The huge increase of social media communities, along with their user tendency to debate, makes the identification of arguments in these texts a necessity. Argument extraction from Social Media is more challenging because texts may not always contain arguments, as is the case of legal documents or scientific publications usually studied. In addition, being less formal in nature, texts in Social Media may not even have proper syntax or spelling. This paper presents a two-step approach for argument extraction from social media texts. During the first step, the proposed approach tries to classify the sentences into ``sentences that contain arguments'' and ``sentences that don't contain arguments''. In the second step, it tries to identify the exact fragments that contain the premises from the sentences that contain arguments, by utilizing conditional random fields. The results exceed significantly the base line approach, and according to literature, are quite promising.},
  isbn      = {978-3-319-07064-3},
  review    = {- argument extraction from social media text
- corpus of 204 documents from social media in greek
- two step process: candidate sentences -> identify segments that contain premises
- mby usable citations for CRFs, logistic regeression, random forest, SMVs and naive bayes
- for first step logistic regressen worked best

- (!!!) names "number of named entities" as one feature of state of the art approaches to classify sentences into non-/argumentative (!!!)

- "popular search engines such as Bing" lol
- uses apparently long dead NLP platform Ellogon
- "CRFs are a structure prediction algorithm"},
}

@InProceedings{Mishra2016,
  author    = {Mishra, Arunav and Berberich, Klaus},
  title     = {{Leveraging Semantic Annotations to Link Wikipedia and News Archives}},
  booktitle = {Advances in Information Retrieval},
  year      = {2016},
  editor    = {Ferro, Nicola and Crestani, Fabio and Moens, Marie-Francine and Mothe, Josiane and Silvestri, Fabrizio and Di Nunzio, Giorgio Maria and Hauff, Claudia and Silvello, Gianmaria},
  pages     = {30--42},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {The incomprehensible amount of information available online has made it difficult to retrospect on past events. We propose a novel linking problem to connect excerpts from Wikipedia summarizing events to online news articles elaborating on them. To address this linking problem, we cast it into an information retrieval task by treating a given excerpt as a user query with the goal to retrieve a ranked list of relevant news articles. We find that Wikipedia excerpts often come with additional semantics, in their textual descriptions, representing the time, geolocations, and named entities involved in the event. Our retrieval model leverages text and semantic annotations as different dimensions of an event by estimating independent query models to rank documents. In our experiments on two datasets, we compare methods that consider different combinations of dimensions and find that the approach that leverages all dimensions suits our problem best.},
  isbn      = {978-3-319-30671-1},
  review    = {- for a small part of a wikipedia entry describing a historical event, find news articles (ranked list) describing the event in detail
- useses text, time, geolocations, names entities
- linking method unclear},
}

@Masterthesis{Paszcza2016,
  author = {Paszcza, Bartosz},
  title  = {{Comparison of Microsoft Academic Graph with other scholarly citation databases}},
  month  = {11},
  year   = {2016},
  pages  = {69},
  review = {- analysis of the MAG
-> relating its scope, openness, completeness of information and interoperability to WoS, Scopus, GS (three other scholarly citation datasets )
- written by a non(/not so much) tech person?},
}

@Article{Herrmannova2016,
  author    = {Drahomira Herrmannova and Petr Knoth},
  title     = {{An Analysis of the Microsoft Academic Graph}},
  journal   = {D-Lib Magazine},
  year      = {2016},
  volume    = {22},
  number    = {9/10},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/dlib/HerrmannovaK16a},
  doi       = {10.1045/september2016-herrmannova},
  review    = {- analysis of MAG and comparison to other data sets

- MAG is over many disciplines (not just CS, of which it contains ~17 million papers)
- MAG is automatically assembled -> before using it for a task, the MAG should be checked for noise and bias (what this paper does)
- MAG is largest publicly available dataset of open citation data
- only 30 out of 127 million papers in MAG have citation data
- year of publication field is populated for all papers in MAG
- MAG contains 35 million DOIs
- field of study info in MAG is hierarchical (4 levels of granularity)

- look on citation network from several perspectives
- 80 million disconnected nodes (i.e. papers w/o citations (being cited) and references (citing))
- differences concerning journals' and universities' citation counts compared to other citation ranking sources (but no way to dig deeper since latter are non transparent)},
  timestamp = {Sun, 28 May 2017 13:24:39 +0200},
  url       = {https://doi.org/10.1045/september2016-herrmannova},
}

@Article{Hug2017,
  author   = {Hug, Sven E. and Ochsner, Michael and Br{\"a}ndle, Martin P.},
  title    = {{Citation analysis with microsoft academic}},
  journal  = {Scientometrics},
  year     = {2017},
  volume   = {111},
  number   = {1},
  pages    = {371--378},
  month    = {Apr},
  issn     = {1588-2861},
  abstract = {We explore if and how Microsoft Academic (MA) could be used for bibliometric analyses. First, we examine the Academic Knowledge API (AK API), an interface to access MA data, and compare it to Google Scholar (GS). Second, we perform a comparative citation analysis of researchers by normalizing data from MA and Scopus. We find that MA offers structured and rich metadata, which facilitates data retrieval, handling and processing. In addition, the AK API allows retrieving frequency distributions of citations. We consider these features to be a major advantage of MA over GS. However, we identify four main limitations regarding the available metadata. First, MA does not provide the document type of a publication. Second, the ``fields of study'' are dynamic, too specific and field hierarchies are incoherent. Third, some publications are assigned to incorrect years. Fourth, the metadata of some publications did not include all authors. Nevertheless, we show that an average-based indicator (i.e. the journal normalized citation score; JNCS) as well as a distribution-based indicator (i.e. percentile rank classes; PR classes) can be calculated with relative ease using MA. Hence, normalization of citation counts is feasible with MA. The citation analyses in MA and Scopus yield uniform results. The JNCS and the PR classes are similar in both databases, and, as a consequence, the evaluation of the researchers' publication impact is congruent in MA and Scopus. Given the fast development in the last year, we postulate that MA has the potential to be used for full-fledged bibliometric analyses.},
  day      = {01},
  doi      = {10.1007/s11192-017-2247-8},
  review   = {- 3 ways to access the MA(G)
-> a Academic Knowledge API (which allows retrieval of frequency distributions of citations): https://www.microsoft.com/cognitive-services/en-us/academic-knowledge-api
-> MA search engine: https://academic.microsoft.com
-> snapshot downloads: https://academicgraph.blob.core.windows.net/graph/index.html
- MA(G) does not provide document type of a publication (-> makes it difficult to distinguish between citable and not citable documents)
- MAG has more structured and richer metadata than Google Scholar
- MAG seems to miss authors (for one particular researcher, they're not listed as author in 64% of their publications)

- check feasability of performing a comparative citation analysis between researchers (here 3) using AK API},
  url      = {https://doi.org/10.1007/s11192-017-2247-8},
}

@InProceedings{Sinha2015,
  author    = {Sinha, Arnab and Shen, Zhihong and Song, Yang and Ma, Hao and Eide, Darrin and Hsu, Bo-June (Paul) and Wang, Kuansan},
  title     = {{An Overview of Microsoft Academic Service (MAS) and Applications}},
  booktitle = {Proceedings of the 24th International Conference on World Wide Web},
  year      = {2015},
  series    = {WWW '15 Companion},
  pages     = {243--246},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2742839},
  doi       = {10.1145/2740908.2742839},
  isbn      = {978-1-4503-3473-0},
  keywords  = {academic search, entity conflation, recommender systems},
  location  = {Florence, Italy},
  numpages  = {4},
  review    = {- official citation in case of using MAG (see: https://microsoftdocs.github.io/microsoft-academic/microsoft-academic-graph/reference/data-schema.html)
- description of how MAG was created
-> heterogeneous entity graph containing entities of type: field of study, author, institution, paper, venue (i.e. journal/conference), event (specific conference instance)
- description of 2 applications
-> semantic search (e.g. "papers citing <author> before <year> about <field-of-study> appearing in <journal>")
-> recommendation (e.g. "given a field of study, find out the most prominent authors, the most influential papers, ...")},
  url       = {http://doi.acm.org/10.1145/2740908.2742839},
}

@Book{Besnard2008,
  title     = {{Elements of Argumentation}},
  publisher = {The MIT Press},
  year      = {2008},
  author    = {Besnard, Philippe and Hunter, Anthony},
  isbn      = {0262026430, 9780262026437},
}

@Unpublished{Faerber,
  author = {Michael F{\"{a}}rber and Adam Jatowt},
  title  = {{Citation Recommendation for Scientific Publications}},
  review = {-  (!!!) argument for semantic approach (!!!)
    "One can envision that, in the mid-term, citation recommendation approaches can better capture the semantics of the citation context, with the result that actual fact-based citation recommendation becomes reality."
    (chapter 6 POTENTIAL FUTURE WORK)},
}

@Article{Tbahriti2006,
  author   = {Imad Tbahriti and Christine Chichester and Frédérique Lisacek and Patrick Ruch},
  title    = {{Using argumentation to retrieve articles with similar citations: An inquiry into improving related articles search in the MEDLINE digital library}},
  journal  = {International Journal of Medical Informatics},
  year     = {2006},
  volume   = {75},
  number   = {6},
  pages    = {488 - 495},
  issn     = {1386-5056},
  doi      = {https://doi.org/10.1016/j.ijmedinf.2005.06.007},
  keywords = {Argumentation, MEDLINE, Citation, Information storage and retrieval, Related article search},
  review   = {- find documents with similar citations
- classify abstract sentences into purpose, methods, results, conclusion
- "system could also be used as a platform to aid authors by means of automatic assembly or refinement of their bibliographies through the suggestion of citations coming from documents containing similar arguments"},
  url      = {http://www.sciencedirect.com/science/article/pii/S1386505605000894},
}

@Article{Duma2016,
  author  = {Daniel Duma and Ewan Klein and Maria Liakata and James Ravenscroft and Amanda Clare},
  title   = {{Rhetorical Classification of Anchor Text for Citation Recommendation}},
  journal = {D-Lib Magazine},
  year    = {2016},
  volume  = {22},
  review  = {- annotate citation contexts with CoreSC classes (hypothesis, motivation, background, result, conclusion, etc.)
- "Core Scientific Concepts (CoreSC) scheme"

- uses PubMed Open Access Subset (https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/)

- "a citation to a paper is accompanied by text that often summarizes a key point in the cited paper, or its contribution to the field. It has been found experimentally that there is useful information in these ILCs that is not found in the cited paper itself [17]"},
}

@InProceedings{Liakata2010,
  author    = {Maria Liakata and Simone Teufel and Advaith Siddharthan and Colin R. Batchelor},
  title     = {{Corpora for the Conceptualisation and Zoning of Scientific Papers}},
  booktitle = {LREC},
  year      = {2010},
  review    = {- "We only consider resolvable citations, that is, citations to references that point to a paper that is in our collection, which means we have access to its metadata and full machine-readable contents"},
}

@InProceedings{Kobayashi2018,
  author    = {Kobayashi, Yuta and Shimbo, Masashi and Matsumoto, Yuji},
  title     = {{Citation Recommendation Using Distributed Representation of Discourse Facets in Scientific Articles}},
  booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
  year      = {2018},
  series    = {JCDL '18},
  pages     = {243--251},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3197059},
  doi       = {10.1145/3197026.3197059},
  isbn      = {978-1-4503-5178-2},
  keywords  = {co-citation analysis, discourse facet, natural language processing, representation learning, scientific article},
  location  = {Fort Worth, Texas, USA},
  numpages  = {9},
  review    = {- focus on "co-citations": given context *AND ONE CITATION*, recommend more appropriate citations (in eval: only take [1,2,...] cases, show one, repredict rest)
- vector representations per discourse facet objective/method/results
- "the Citation Function Corpus [33]" (Teufel 2006)},
  url       = {http://doi.acm.org/10.1145/3197026.3197059},
}

@InProceedings{Sugiyama2013,
  author    = {Sugiyama, Kazunari and Kan, Min-Yen},
  title     = {{Exploiting Potential Citation Papers in Scholarly Paper Recommendation}},
  booktitle = {Proceedings of the 13th ACM/IEEE-CS Joint Conference on Digital Libraries},
  year      = {2013},
  series    = {JCDL '13},
  pages     = {153--162},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2467701},
  doi       = {10.1145/2467696.2467701},
  isbn      = {978-1-4503-2077-1},
  keywords  = {citation analysis, collaborative filtering, digital library, information retrieval, recommendation},
  location  = {Indianapolis, Indiana, USA},
  numpages  = {10},
  review    = {- cold start problem: "with every new publication, new citation links are added to older work. In studies depending solely on the citation network, cutting- edge work is marginalized as they do not have any citations yet; this is a kind of 'cold-start problem'"
- old citations being old: "references and citations in a paper are static and never change, newer relevant papers to older ones have the 'responsibility' of creating a citation link between them"
-> introduce artificial links into citation network},
  url       = {http://doi.acm.org/10.1145/2467696.2467701},
}

@InProceedings{Livne2014,
  author    = {Livne, Avishay and Gokuladas, Vivek and Teevan, Jaime and Dumais, Susan T. and Adar, Eytan},
  title     = {{CiteSight: Supporting Contextual Citation Recommendation Using Differential Search}},
  booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research \&\#38; Development in Information Retrieval},
  year      = {2014},
  series    = {SIGIR '14},
  pages     = {807--816},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2609585},
  doi       = {10.1145/2600428.2609585},
  isbn      = {978-1-4503-2257-7},
  keywords  = {citation recommendation, differential search., personalization},
  location  = {Gold Coast, Queensland, Australia},
  numpages  = {10},
  review    = {- use MA(G)
- "citations are power law distributed" (long tail) (w/ citation)
    -> use "citation coupling" to enrich the citation graph},
  url       = {http://doi.acm.org/10.1145/2600428.2609585},
}

@Article{HERNANDEZ-ALVAREZ2016,
  author    = {Hernández-Alvarez, Myriam and Gomez, José M.},
  title     = {{Survey about citation context analysis: Tasks, techniques, and resources}},
  journal   = {Natural Language Engineering},
  year      = {2016},
  volume    = {22},
  number    = {3},
  pages     = {327–349},
  doi       = {10.1017/S1351324915000388},
  publisher = {Cambridge University Press},
  review    = {- good overview (many citations) for citation context identification and citation function classification, etc.
- overview table with different schemes for classifying citation functions},
}

@Article{Moravcsik1975,
  author    = {Michael J. Moravcsik and Poovanalingam Murugesan},
  title     = {{Some Results on the Function and Quality of Citations}},
  journal   = {Social Studies of Science},
  year      = {1975},
  volume    = {5},
  number    = {1},
  pages     = {86--92},
  issn      = {03063127},
  publisher = {Sage Publications, Ltd.},
  review    = {- even earlier: (Garfield, 1964;  Weinstock, 1971)
- standard citation for citation function
- Teufel 2006: "A plethora of manual annotation schemes for citation motivation have been invented over the years. One of the best-known of these studies (Moravcsik and Murugesan, 1975) divides citations in running text into four dimensions: 
    - conceptual or operational use (i.e., use of theory vs. use of technical method)
    - evolutionary or juxtapositional (i.e., own work is based on the cited work vs. own work is an alternative to it)
    - organic or perfunctory (i.e., work is crucially needed for understanding of citing article or just a general acknowledgement)
    - conformative or negational (i.e., is the correctness of the findings disputed?)
-> according to Petric2007 above categories are not all applicaple in all fields of research
    "Swales (1986), for instance, found that the first two criteria were irrelevant in the case of texts in applied linguistics."},
  url       = {http://www.jstor.org/stable/284557},
}

@InProceedings{Teufel2006a,
  author    = {Teufel, Simone and Siddharthan, Advaith and Tidhar, Dan},
  title     = {{Automatic Classification of Citation Function}},
  booktitle = {Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing},
  year      = {2006},
  series    = {EMNLP '06},
  pages     = {103--110},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1610091},
  isbn      = {1-932432-73-6},
  location  = {Sydney, Australia},
  numpages  = {8},
  review    = {- mby useful for claim based approach},
  url       = {http://dl.acm.org/citation.cfm?id=1610075.1610091},
}

@InProceedings{Teufel2006b,
  author    = {Simone Teufel and Advaith Siddharthan and Dan Tidhar},
  title     = {{An annotation scheme for citation function}},
  booktitle = {In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue},
  year      = {2006},
  review    = {- mby useful for claim based approach},
}

@InProceedings{Abujbara2013,
  author    = {Abu-Jbara, Amjad and Ezra, Jefferson and Radev, Dragomir},
  title     = {{Purpose and Polarity of Citation: Towards NLP-based Bibliometrics}},
  booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2013},
  pages     = {596--606},
  publisher = {Association for Computational Linguistics},
  location  = {Atlanta, Georgia},
  review    = {- citation function classes: criticizing, comparison, use, substantiating, basis, neutral (other)},
  url       = {http://aclweb.org/anthology/N13-1067},
}

@Article{Petric2007,
  author   = {Bojana Petrić},
  title    = {{Rhetorical functions of citations in high- and low-rated master's theses}},
  journal  = {Journal of English for Academic Purposes},
  year     = {2007},
  volume   = {6},
  number   = {3},
  pages    = {238 - 253},
  issn     = {1475-1585},
  abstract = {This study compares rhetorical citation functions in eight high- and eight low-graded master's theses in the field of gender studies, written in English as a second language. The following rhetorical functions of citations are identified: attribution, exemplification, further reference, statement of use, application, evaluation, establishing links between sources, and comparison of one's own work with that of other authors. It is shown that both sets of theses use citations predominantly for attribution, suggesting that one of the functions of citation in student writing is knowledge display. The use of citation for non-attribution functions is found to be considerably lower in the low-rated theses than in the high-rated theses, both in the whole theses and in individual chapters. The findings show that there is a relationship between citation use and thesis grade, thus pointing to the importance of effective citation strategies for students’ academic success. In conclusion, the paper argues that source use and citation skills should receive more attention in EAP instruction and suggests activities focusing on this area of academic writing.},
  doi      = {https://doi.org/10.1016/j.jeap.2007.09.002},
  keywords = {Citation, Master's thesis, Academic writing, High- and low-rated writing, English for academic purposes},
  review   = {- types of citation typologies: content based vs. based on formal criteria
- classifies Moravcsik1975 as a "content based citation typology"},
  url      = {http://www.sciencedirect.com/science/article/pii/S1475158507000379},
}

@InProceedings{Lamers2018,
  author    = {Lamers, Wout and Eck , Nees Jan van and Waltman, Ludo and Hoos, Holger},
  title     = {{Patterns in citation context: the case of the field of scientometrics}},
  booktitle = {STI 2018 Conference proceedings},
  year      = {2018},
  pages     = {1114--1122},
  publisher = {Centre for Science and Technology Studies (CWTS)},
  review    = {- focus on marker (in the sense that "As shown by (Whatshername, 1984) it is the case that" )

- detailed description for determining integral vs. non-integral citations:
    "For each citation, we determined whether it is integral or non-integral by processing their accompanying  sentence.
     First, the sentence was stripped of all citation labels. Subsequently, all characters between brackets were removed from
     the remaining character strings, leaving only the primary text of the citing sentence. We then searched this text for the
     last name of the first author of the cited paper. Citations were labelled integral if this author name was found, and
     non-integral if it was not."
    -> remove all "citation labels" (what ware citation labels? "[42]"?)
    -> remove all text in brackets (I assume remove brackets aswell)
    -> if remaining string contains last name of first author: integral

- integral vs. non-integral distinction on _presence of author name with grammatical role_ (as opposed to Thompson2001)
    - "[42] argues that ..." -> non-int},
  url       = {http://hdl.handle.net/1887/65235},
}

@Article{RAHUL2017,
  author    = {Jha, Rahul and Jbara, Amjad-Abu and Qazvinian, Vahed and Radev, Dragomir R.},
  title     = {{NLP-driven citation analysis for scientometrics}},
  journal   = {Natural Language Engineering},
  year      = {2017},
  volume    = {23},
  number    = {1},
  pages     = {93–130},
  doi       = {10.1017/S1351324915000443},
  publisher = {Cambridge University Press},
  review    = {- define "Reference Scope" (part(s) of citation context that is relevant for specific (out of many) cited doc)
    "We use the term Reference Scope to refer to the fragments of a sentence that are relevant to a specific target paper."
- look into automatic reference scope identification

--> paper only on reference scope: Abu-Jbara2012},
}

@Article{Swales1986,
  author  = {Swales, John},
  title   = {{Citation Analysis and Discourse Analysis}},
  journal = {Applied Linguistics},
  year    = {1986},
  volume  = {7},
  number  = {1},
  pages   = {39-56},
  doi     = {10.1093/applin/7.1.39},
  eprint  = {/oup/backfile/content_public/journal/applij/7/1/10.1093_applin_7.1.39/1/39.pdf},
  review  = {- "frequently expressed concern about the adequacy and reliability of simple and straightforward citation counting."

    - negative/critical citations != good (-> high count of citations not necessarily good)
    - *important work can be absorbed into the background knowledge of a subject and is then no longer referenced (e.g. Einstein 1905 for the Theory of Relativity)
    - *morphological and syntactic evolutions:
         - discoveries  become  'named'  (Lotka's  Law,  the  Mossbauer Effect)
         - Proper Name adjectivization takes place (Widdowsonian dichotomies, a Sinclo-Coulthardian approach)
    - citations vary in length
    - are self-citations of the same value as citations by others?

    *relevant in the same way as some citations become obsolete because of newer findings},
  url     = {http://dx.doi.org/10.1093/applin/7.1.39},
}

@Book{Swales1990,
  title     = {{Genre analysis: English in academic and research settings}},
  publisher = {Cambridge University Press},
  year      = {1990},
  author    = {Swales, John},
  review    = {- focus on *sentence* (not citation [marker]), distinction by presence of author's name + having grammatical role (being "some sentence-element")

- distinction between integral and non-integral citations
    "An integral citation is one in which the name of the researcher occurs in the actual citing sentence as some sentence-element;
     in a non-integral citation, the researcher occurs either in parenthesis or is referred to elsewehre by a superscript number or via some other device"
    -> examples in following text; not on Google books but in UB
    
    - in Thompson2001's words: "His primary distinction is between citation forms that are non-integral and those that are integral:
      the former are citations that are outside the sentence, usually placed within brackets, and that play no explicit grammatical role in the sentence,
      while the latter are those that play an explicit grammatical role within a sentence."

    - is, in contrast to citation function, about the "surface form" of the citation
    -> relevant to cit. rec. when thinking of end user system where suggestions come in while typing (-> doesn't really work for integral citations)
    -> relevant to cit. rec. in any other aspect? analysis/representation of cit. contexts?},
}

@PhdThesis{Thompson2001,
  author = {Thompson, Paul},
  title  = {{A pedagogically-motivated corpus-based examination of PhD theses: Macrostructure, citation practices and uses of modal verbs}},
  school = {University of Reading},
  year   = {2001},
  review = {- no explicit statement concerning focus on sentence vs. citation [marker], but says to follow Swales1990

- Nice overview of integral vs. non-integral citation (Swales -> Hyland -> ...)

- "Following Swales (1990:141) citations were divided into integral or non-integral."
    - says to follow Swales, but own words are a bit unclear on *what* exactly a citation is (the author's name, the year number, ...)

- v old v- 

- (!!!) "Citations can also take the form of a number (rather than name and year) reference but none of the writers in the corpus use this style, and thus it was not considered in this study."
   -> integral vs. non-integral is more about having a grammatical role or not than about the authors name appearing or not
        "Swales has argued that ... [42]."                      -> which is the citation? "Swales"-> int / "[42]"->non
        "Swales (1990) has argued that ..."                   -> int
        "It has been argued (Swales 1990) that ..."       -> non
        "[42] argues that ..."                                         -> int

- integral vs. non-integral distinction on _having a grammatical role or not_ (as opposed to Lamers2018)
    - "[42] argues that ..." -> int

- wording not to use (wording in Okamura2008)
    integral: syntactically integrated citation
    non-integral: syntactically non-integrated
    -> b/c in both cases the citations are within the array of words that make up the sentence
         (semantically integrated also doesn't really work)
         (-> stick with _having a grammatical role or not_)},
}

@Article{Hyland1999,
  author  = {Hyland, K},
  title   = {{Academic attribution: citation and the construction of disciplinary knowledge}},
  journal = {Applied Linguistics},
  year    = {1999},
  volume  = {20},
  number  = {3},
  pages   = {341-367},
  doi     = {10.1093/applin/20.3.341},
  eprint  = {/oup/backfile/content_public/journal/applij/20/3/10.1093/applin/20.3.341/2/200341.pdf},
  review  = {- exactly like Swales1990: focus on *sentence* (not citation [marker]), distinction by presence of author's name + having grammatical role (being "some sentence-element")

- nice example sentences for integral vs. non-integral citations
    + mentions [42] style citations ("In the physical sciences, of course, journal styles often require numerical-
                                                      endnote forms, which reduces the prominence of cited authors considerably")
- per field ratio of integral vs. non-integral for corpus of 80 research articles
    - e.g.: bio 10/90, phys 17/83, philosophy 65/35},
  url     = {http://dx.doi.org/10.1093/applin/20.3.341},
}

@Article{Ayala-Gomez2018,
  author  = {Ayala-Gómez, Frederick and Daróczy, Bálint and Benczúr, András and Mathioudakis, Michael and Gionis, Aristides},
  title   = {{Global citation recommendation using knowledge graphs}},
  journal = {Journal of Intelligent \& Fuzzy Systems},
  year    = {2018},
  volume  = {34},
  pages   = {3089-3100},
  month   = {05},
  doi     = {10.3233/JIFS-169493},
  review  = {- global citation recommendation based on abstract
- use LambdaMart (gradient boosted tree based list-wise learning to rank algo)

- use similarity of words in abstract for recommendation candidate preselection (sensible?)
- evalute only in settings "random split within one year" and "recommend from the previous year" (sensible? / realistic?)},
}

@Article{White2004,
  author  = {White, Howard D.},
  title   = {{Citation Analysis and Discourse Analysis Revisited}},
  journal = {Applied Linguistics},
  year    = {2004},
  volume  = {25},
  number  = {1},
  pages   = {89-116},
  doi     = {10.1093/applin/25.1.89},
  eprint  = {/oup/backfile/content_public/journal/applij/25/1/10.1093/applin/25.1.89/2/250089.pdf},
  review  = {- nice table showing "separate disciplines with traditions of citation analysis" (applied linguistics, history and sociology of science, information science)
- according to RAHUL2017: "White (2004) provides a good survey of the different research directions that study or use citations."},
  url     = {http://dx.doi.org/10.1093/applin/25.1.89},
}

@InProceedings{Abujbara2012,
  author    = {Abu-Jbara, Amjad and Radev, Dragomir},
  title     = {{Reference Scope Identification in Citing Sentences}},
  booktitle = {Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2012},
  series    = {NAACL HLT '12},
  pages     = {80--90},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {2382041},
  isbn      = {978-1-937284-20-6},
  location  = {Montreal, Canada},
  numpages  = {11},
  review    = {- get best results for sentence segment classification approach

- call integral / non-integral citations syntactic / non-syntactic
- provide algorithm for detection based on pattern matching and POS tags
- when non-integral, remove marker and use head of closest noun phrase as substitute marker},
  url       = {http://dl.acm.org/citation.cfm?id=2382029.2382041},
}

@Article{Stab2016,
  author        = {Christian Stab and Iryna Gurevych},
  title         = {{Parsing Argumentation Structures in Persuasive Essays}},
  journal       = {CoRR},
  year          = {2016},
  volume        = {abs/1604.07370},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/StabG16},
  eprint        = {1604.07370},
  review        = {- Michael: "good survey"
- TODO: read as intro to argumentation mining},
  timestamp     = {Mon, 13 Aug 2018 16:46:30 +0200},
  url           = {http://arxiv.org/abs/1604.07370},
}

@Article{Habernal2017,
  author     = {Habernal, Ivan and Gurevych, Iryna},
  title      = {{Argumentation Mining in User-generated Web Discourse}},
  journal    = {Comput. Linguist.},
  year       = {2017},
  volume     = {43},
  number     = {1},
  pages      = {125--179},
  month      = apr,
  issn       = {0891-2017},
  acmid      = {3097259},
  address    = {Cambridge, MA, USA},
  doi        = {10.1162/COLI_a_00276},
  issue_date = {April 2017},
  numpages   = {55},
  publisher  = {MIT Press},
  review     = {- Michael: "good survey" (see detailed notes in wiki)
- citation for (the start of) the field of argumentation mining
- makes an argument for basing applied NLP in argumentation mining on "substantial research in argumentation itself"
- -> citation (van Eemeren et al. 2014 ) that there's no real consensus on a theory of argumentation yet

- chapter on theoretical background of argumentation, argumentation models, etc.

- -> what citation recommendation is interested in within a citation context works with a "micro-level model" of argumentation
- Original Toulmin’s model of argument with a nice graph and example
- -> modift Toulmin’s model for their purposes (no qualifier, no warrant, calling grounds "premise" etc.)},
  url        = {https://doi.org/10.1162/COLI_a_00276},
}

@Article{Lippi2016,
  author     = {Lippi, Marco and Torroni, Paolo},
  title      = {{Argumentation Mining: State of the Art and Emerging Trends}},
  journal    = {ACM Trans. Internet Technol.},
  year       = {2016},
  volume     = {16},
  number     = {2},
  pages      = {10:1--10:25},
  month      = mar,
  issn       = {1533-5399},
  acmid      = {2850417},
  address    = {New York, NY, USA},
  articleno  = {10},
  doi        = {10.1145/2850417},
  issue_date = {April 2016},
  keywords   = {Argumentation mining, artificial intelligence, computational linguistics, knowledge representation, machine learning, social media},
  numpages   = {25},
  publisher  = {ACM},
  review     = {- nice overview tables for argumentation mining},
  url        = {http://doi.acm.org/10.1145/2850417},
}

@InProceedings{Zhang2017,
  author    = {Zhang, Sheng and Rudinger, Rachel and Durme, Benjamin Van},
  title     = {{An Evaluation of PredPatt and Open IE via Stage 1 Semantic Role Labeling}},
  booktitle = {IWCS 2017 --- 12th International Conference on Computational Semantics --- Short papers},
  year      = {2017},
  review    = {- PredPatt citation 2

- argument for PredPatt over OpenIE 4.0 (for claim extraction), but also shows that among OpenIE implementations, OpenIE 4.0 compares quite good (side note: running on shetland is OpenIE 5.0 https://github.com/dair-iitd/OpenIE-standalone)
- paper quite sparse on details
- PredPatt is language independent and works with patterns (paper is on refining these patterns)},
  url       = {http://aclweb.org/anthology/W17-6944},
}

@InProceedings{Stanovsky2016,
  author    = {Stanovsky, Gabriel and Dagan, Ido},
  title     = {{Creating a Large Benchmark for Open Information Extraction}},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  year      = {2016},
  pages     = {2300--2305},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/D16-1252},
  location  = {Austin, Texas},
  review    = {- citation for OpenIE 4.0 performing comparatively well (side note: running on shetland is OpenIE 5.0 https://github.com/dair-iitd/OpenIE-standalone)},
  url       = {http://aclweb.org/anthology/D16-1252},
}

@Article{Okamura2008,
  author  = {Okamura, Akiko},
  title   = {{Citation forms in scientific texts: Similarities and differences in L1 and L2 professional writing}},
  journal = {Nordic Journal of English Studies},
  year    = {2008},
  volume  = {7},
  number  = {3},
  pages   = {61--81},
  review  = {- integral/syntactical terms bleed into each other: "integral (syntactically integrated citation) and non-integral (syntactically non-integrated)"
- "Integral citation was further categorized into subject position, non-subject position (passive; clause constituent)" suggests focus on citation marker rather than sentence as a whole but might implicitly refer to author's name (if present) as "citation"

- integral vs. non-integral citations (study on 30 papers)

- "... chemistry and physics papers used a sequential numbering system such as (1, 2, and 3) while biology papers employed an author-date system such as (Smith 2008).
   As the sequential number type creates a more noticeable difference between integral and non-integral citation, the interpretation of the findings needs to be treated with some caution."},
}

@InProceedings{Niklaus2018,
  author    = {Niklaus, Christina and Cetto, Matthias and Freitas, Andr{\'e} and Handschuh, Siegfried},
  title     = {{A Survey on Open Information Extraction}},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  year      = {2018},
  pages     = {3866--3878},
  publisher = {Association for Computational Linguistics},
  location  = {Santa Fe, New Mexico, USA},
  review    = {- overview of information extraction approahces (relevant for: fact/claim based approach)},
  url       = {http://aclweb.org/anthology/C18-1326},
}

@InProceedings{Sordo2015,
  author    = {Sordo, Mohamed and Oramas, Sergio and Espinosa-Anke, Luis},
  title     = {{Extracting Relations from Unstructured Text Sources for Music Recommendation}},
  booktitle = {Natural Language Processing and Information Systems},
  year      = {2015},
  editor    = {Biemann, Chris and Handschuh, Siegfried and Freitas, Andr{\'e} and Meziane, Farid and M{\'e}tais, Elisabeth},
  pages     = {369--382},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {This paper presents a method for the generation of structured data sources for music recommendation using information extracted from unstructured text sources. The proposed method identifies entities in text that are relevant to the music domain, and then extracts semantically meaningful relations between them. The extracted entities and relations are represented as a graph, from which the recommendations are computed. A major advantage of this approach is that the recommendations can be conveyed to the user using natural language, thus providing an enhanced user experience. We test our method on texts from songfacts.com, a website that provides facts and stories about songs. The extracted relations are evaluated intrinsically by assessing their linguistic quality, as well as extrinsically by assessing the extent to which they map an existing music knowledge base. Finally, an experiment with real users is performed to assess the suitability of the extracted knowledge for music recommendation. Our method is able to extract relations between pair of musical entities with high precision, and the explanation of those relations to the user improves user satisfaction considerably.},
  isbn      = {978-3-319-19581-0},
  review    = {- use NEL + relation extraction for recommendation
- view depentency tree as undirected, then look for path between NEL entities
    - use words passed along path as "relation" between the entities (but filter heuristically for linguistically sensible relations)
    - paper includes POS rules that represent relevant/useful relations between serveral entitiy types},
}

@Article{Ley2009,
  author     = {Ley, Michael},
  title      = {{DBLP: Some Lessons Learned}},
  journal    = {Proc. VLDB Endow.},
  year       = {2009},
  volume     = {2},
  number     = {2},
  pages      = {1493--1500},
  month      = aug,
  issn       = {2150-8097},
  acmid      = {1687577},
  doi        = {10.14778/1687553.1687577},
  issue_date = {August 2009},
  numpages   = {8},
  publisher  = {VLDB Endowment},
  review     = {- paper on the design of the DBLP},
  url        = {https://doi.org/10.14778/1687553.1687577},
}

@Article{Garcia2012,
  author  = {Garcia, Alexander and Garcia, Leyla Jael and McLaughlin, Casey and Flager, Stephen},
  title   = {{RDFising PubMed Central}},
  journal = {Bio-ontologies, Long Beach},
  year    = {2012},
  review  = {- point out not all citations in PubMec Central (PMC) XMLs are matched to IDs (example: PMC2971765)},
}

@InProceedings{Duma2014,
  author    = {Duma, Daniel and Klein, Ewan},
  title     = {{Citation resolution: A method for evaluating context-based citation recommendation systems}},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  year      = {2014},
  volume    = {2},
  pages     = {358--363},
  review    = {- cite rec by similar sentences: "exploits  the implicit human relevance judgements found in existing scientific articles"
- citation recommendation by re-prediction, but candidates are only the set of cited docs of the paper
- use ~9K papers from ACL as data set, 278 test documents; 5446 resolvable citations
- use VSM TFIDF
- compare, for document representation, doc content, cit. contexts and doc content+cit. contexts
- test citation contexts of 5~30 words
- report "accuracy" (details?)},
}

@Article{Nasar2018,
  author   = {Nasar, Zara and Jaffry, Syed Waqar and Malik, Muhammad Kamran},
  title    = {{Information extraction from scientific articles: a survey}},
  journal  = {Scientometrics},
  year     = {2018},
  volume   = {117},
  number   = {3},
  pages    = {1931--1990},
  month    = {Dec},
  issn     = {1588-2861},
  abstract = {In last few decades, with the advent of World Wide Web (WWW), world is being overloaded with huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and efficient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scientific literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scientific articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scientific articles. The information insights extracted from scientific articles are classified in two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a significant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Na{\"i}ve-Bayes classification and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to specific information needs from scientific articles. Hence, in this study, state-of-the-art regarding information extraction from scientific articles is covered. This study also consolidates evolving datasets as well as various toolkits and code-bases that can be used for information extraction from scientific articles.},
  day      = {01},
  doi      = {10.1007/s11192-018-2921-5},
  review   = {- on IE for scientific text in general
- Reference Field Extraction as a form of Named Entity Recognition and Classification (NERC), "NERC task from references"
- survey tools, datasets, approaches, etc.},
  url      = {https://doi.org/10.1007/s11192-018-2921-5},
}

@InProceedings{Anzaroot2013,
  author    = {Sam Anzaroot and Andrew McCallum},
  title     = {{A New Dataset for Fine-Grained Citation Field Extraction}},
  booktitle = {ICML Workshop on Peer Reviewing and Publishing Models},
  year      = {2013},
}

@InProceedings{Tkaczyk2018,
  author    = {Tkaczyk, Dominika and Collins, Andrew and Sheridan, Paraic and Beel, Joeran},
  title     = {{Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers}},
  booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
  year      = {2018},
  series    = {JCDL '18},
  pages     = {99--108},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3197048},
  doi       = {10.1145/3197026.3197048},
  isbn      = {978-1-4503-5178-2},
  keywords  = {bibliographic reference parsing, citation parsing, machine learning, sequence tagging},
  location  = {Fort Worth, Texas, USA},
  numpages  = {10},
  review    = {- evaluation of reference string parsers
- "We  were  not  able  to  evaluate  three  tools,  due  to installation errors or missing resources: BibPro, Free_cite and Neural ParsCit." :F},
  url       = {http://doi.acm.org/10.1145/3197026.3197048},
}

@InProceedings{Bast2017,
  author    = {H. Bast and C. Korzen},
  title     = {{A Benchmark and Evaluation for Text Extraction from PDF}},
  booktitle = {2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
  year      = {2017},
  pages     = {1-10},
  month     = {June},
  doi       = {10.1109/JCDL.2017.7991564},
  keywords  = {information retrieval;text analysis;text extraction tool;PDF document;parallel TeX;arXiv.org;Icecite;electronic document formats;Portable document format;Semantics;Benchmark testing;Tools;Data mining;Libraries;Google},
  review    = {- use LaTeX to plain text to evaluate pdf2plain text tools},
}

@Article{Brown2001,
  author  = {Lawrence D. {Brown} and T. Tony {Cai} and Anirban {DasGupta}},
  title   = {{Interval Estimation for a Binomial Proportion}},
  journal = {Statistical Science},
  year    = {2001},
  volume  = {16},
  number  = {2},
  pages   = {101--133},
  review  = {- citaiton for confidence invervals for estimation of bibitem matching accuracy},
}

@InProceedings{Gipp2015,
  author    = {Gipp, Bela and Meuschke, Norman and Lipinski, Mario},
  title     = {{CITREC : An Evaluation Framework for Citation-Based Similarity Measures based on TREC Genomics and PubMed Central}},
  booktitle = {iConference 2015 Proceedings},
  year      = {2015},
  publisher = {iSchools},
  review    = {- parsing PubMed Central OA in text citations is hard b/c they're heterogenous:
    "The extraction of in-text citations from the PMC OAS documents posed some problems to parser  development. Among these challenges was the use of heterogeneous XML-markups
     for labeling in-text citations in the source files. For this reason, we incorporated eight differentmarkup variations into the parser."
    - details: http://www.sciplore.org/files/citrec/CITREC_Parser_Documentation.pdf
- furthermore different types of IDs are used:
    "Different  unique document identifiers, such as PubMed and MEDLINE identifiers (PMId, MEDId) or DOIs are commonly stated for references in the PMC OAS. Nevertheless, it cannot be
     assumed that identifiers are used consistently for all possible references. For instance, some authors might state no unique identifiers, some might use a PMId, some others a DOI or
    vice versa."},
  url       = {http://hdl.handle.net/2142/73680},
}

@Article{Berger2016,
  author  = {Berger, Matthew and McDonough, Katherine and M. Seversky, Lee},
  title   = {{Cite2vec: Citation-Driven Document Exploration via Word Embeddings}},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year    = {2016},
  volume  = {23},
  pages   = {1-1},
  month   = {01},
  doi     = {10.1109/TVCG.2016.2598667},
  review  = {- cite2vec (as title suggests) but used for the purpose discovery/visualization},
}

@InProceedings{He2018,
  author    = {Jian He and Chaomei Chen},
  title     = {{Temporal Representations of Citations for Understanding the Changing Roles of Scientific Publications}},
  booktitle = {Front. Res. Metr. Anal.},
  year      = {2018},
  review    = {- early version: https://arxiv.org/pdf/1711.05822.pdf
- "approach to understanding the changing roles of a publication characterized by its citation contexts"},
}

@InProceedings{Galke2018,
  author    = {Galke, Lukas and Mai, Florian and Vagliano, Iacopo and Scherp, Ansgar},
  title     = {{Multi-Modal Adversarial Autoencoders for Recommendations of Citations and Subject Labels}},
  booktitle = {Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization},
  year      = {2018},
  series    = {UMAP '18},
  pages     = {197--205},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3209236},
  doi       = {10.1145/3209219.3209236},
  isbn      = {978-1-4503-5589-6},
  keywords  = {adversarial autoencoders, multi-modal, neural networks, recommender systems, sparsity},
  location  = {Singapore, Singapore},
  numpages  = {9},
  review    = {- use PMC Open Access Subset (same as Gipp2015)},
  url       = {http://doi.acm.org/10.1145/3209219.3209236},
}

@InProceedings{Gipp2010,
  author    = {Gipp, Bela and Beel, J\"{o}ran},
  title     = {{Citation Based Plagiarism Detection: A New Approach to Identify Plagiarized Work Language Independently}},
  booktitle = {Proceedings of the 21st ACM Conference on Hypertext and Hypermedia},
  year      = {2010},
  series    = {HT '10},
  pages     = {273--274},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1810671},
  doi       = {10.1145/1810617.1810671},
  isbn      = {978-1-4503-0041-4},
  keywords  = {citation analysis, citation order analysis, duplicate detection, language independent, plagiarism detection},
  location  = {Toronto, Ontario, Canada},
  numpages  = {2},
  review    = {- language independent plagiarism detection based on citation order},
  url       = {http://doi.acm.org/10.1145/1810617.1810671},
}

@InProceedings{Beel2013,
  author    = {Beel, Joeran and Langer, Stefan and Genzmehr, Marcel and Gipp, Bela and Breitinger, Corinna and N\"{u}rnberger, Andreas},
  title     = {{Research Paper Recommender System Evaluation: A Quantitative Literature Survey}},
  booktitle = {Proceedings of the International Workshop on Reproducibility and Replication in Recommender Systems Evaluation},
  year      = {2013},
  series    = {RepSys '13},
  pages     = {15--22},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2532512},
  doi       = {10.1145/2532508.2532512},
  isbn      = {978-1-4503-2465-6},
  keywords  = {comparative study, evaluation, recommender systems, research paper recommender systems, survey},
  location  = {Hong Kong, China},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/2532508.2532512},
}

@Article{Beel2016,
  author   = {Beel, Joeran and Gipp, Bela and Langer, Stefan and Breitinger, Corinna},
  title    = {{Research-paper recommender systems: a literature survey}},
  journal  = {International Journal on Digital Libraries},
  year     = {2016},
  volume   = {17},
  number   = {4},
  pages    = {305--338},
  month    = {Nov},
  issn     = {1432-1300},
  abstract = {In the last 16 years, more than 200 research articles were published about research-paper recommender systems. We reviewed these articles and present some descriptive statistics in this paper, as well as a discussion about the major advancements and shortcomings and an overview of the most common recommendation concepts and approaches. We found that more than half of the recommendation approaches applied content-based filtering (55 {\%}). Collaborative filtering was applied by only 18 {\%} of the reviewed approaches, and graph-based recommendations by 16 {\%}. Other recommendation concepts included stereotyping, item-centric recommendations, and hybrid recommendations. The content-based filtering approaches mainly utilized papers that the users had authored, tagged, browsed, or downloaded. TF-IDF was the most frequently applied weighting scheme. In addition to simple terms, n-grams, topics, and citations were utilized to model users' information needs. Our review revealed some shortcomings of the current research. First, it remains unclear which recommendation concepts and approaches are the most promising. For instance, researchers reported different results on the performance of content-based and collaborative filtering. Sometimes content-based filtering performed better than collaborative filtering and sometimes it performed worse. We identified three potential reasons for the ambiguity of the results. (A) Several evaluations had limitations. They were based on strongly pruned datasets, few participants in user studies, or did not use appropriate baselines. (B) Some authors provided little information about their algorithms, which makes it difficult to re-implement the approaches. Consequently, researchers use different implementations of the same recommendations approaches, which might lead to variations in the results. (C) We speculated that minor variations in datasets, algorithms, or user populations inevitably lead to strong variations in the performance of the approaches. Hence, finding the most promising approaches is a challenge. As a second limitation, we noted that many authors neglected to take into account factors other than accuracy, for example overall user satisfaction. In addition, most approaches (81 {\%}) neglected the user-modeling process and did not infer information automatically but let users provide keywords, text snippets, or a single paper as input. Information on runtime was provided for 10 {\%} of the approaches. Finally, few research papers had an impact on research-paper recommender systems in practice. We also identified a lack of authority and long-term research interest in the field: 73 {\%} of the authors published no more than one paper on research-paper recommender systems, and there was little cooperation among different co-author groups. We concluded that several actions could improve the research landscape: developing a common evaluation framework, agreement on the information to include in research papers, a stronger focus on non-accuracy aspects and user modeling, a platform for researchers to exchange information, and an open-source framework that bundles the available recommendation approaches.},
  day      = {01},
  doi      = {10.1007/s00799-015-0156-0},
  review   = {- large survey on (probably (if you have to decide on one)) paper recommendation as opposed to citation recommendation},
  url      = {https://doi.org/10.1007/s00799-015-0156-0},
}

@Article{Beel2016a,
  author   = {Beel, Joeran and Breitinger, Corinna and Langer, Stefan and Lommatzsch, Andreas and Gipp, Bela},
  title    = {{Towards reproducibility in recommender-systems research}},
  journal  = {User Modeling and User-Adapted Interaction},
  year     = {2016},
  volume   = {26},
  number   = {1},
  pages    = {69--101},
  month    = {Mar},
  issn     = {1573-1391},
  abstract = {Numerous recommendation approaches are in use today. However, comparing their effectiveness is a challenging task because evaluation results are rarely reproducible. In this article, we examine the challenge of reproducibility in recommender-system research. We conduct experiments using Plista's news recommender system, and Docear's research-paper recommender system. The experiments show that there are large discrepancies in the effectiveness of identical recommendation approaches in only slightly different scenarios, as well as large discrepancies for slightly different approaches in identical scenarios. For example, in one news-recommendation scenario, the performance of a content-based filtering approach was twice as high as the second-best approach, while in another scenario the same content-based filtering approach was the worst performing approach. We found several determinants that may contribute to the large discrepancies observed in recommendation effectiveness. Determinants we examined include user characteristics (gender and age), datasets, weighting schemes, the time at which recommendations were shown, and user-model size. Some of the determinants have interdependencies. For instance, the optimal size of an algorithms' user model depended on users' age. Since minor variations in approaches and scenarios can lead to significant changes in a recommendation approach's performance, ensuring reproducibility of experimental results is difficult. We discuss these findings and conclude that to ensure reproducibility, the recommender-system community needs to (1) survey other research fields and learn from them, (2) find a common understanding of reproducibility, (3) identify and understand the determinants that affect reproducibility, (4) conduct more comprehensive experiments, (5) modernize publication practices, (6) foster the development and use of recommendation frameworks, and (7) establish best-practice guidelines for recommender-systems research.},
  day      = {01},
  doi      = {10.1007/s11257-016-9174-x},
  url      = {https://doi.org/10.1007/s11257-016-9174-x},
}

@Article{Beel2017,
  author        = {J{\"{o}}ran Beel and Siddharth Dinesh},
  title         = {{Real-World Recommender Systems for Academia: The Pain and Gain in Building, Operating, and Researching them}},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1704.00156},
  abstract      = {- relevant citation for implementation
- talks about evaluation by click through rate},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/BeelD17},
  eprint        = {1704.00156},
  timestamp     = {Mon, 13 Aug 2018 16:48:46 +0200},
  url           = {http://arxiv.org/abs/1704.00156},
}

@Article{Beel2017a,
  author        = {J{\"{o}}ran Beel},
  title         = {{It's Time to Consider "Time" when Evaluating Recommender-System Algorithms}},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1708.08447},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1708-08447},
  eprint        = {1708.08447},
  review        = {- probably woth mentioning when talking about evaluation of proposed approaches},
  timestamp     = {Mon, 13 Aug 2018 16:48:19 +0200},
  url           = {http://arxiv.org/abs/1708.08447},
}

@Article{Yokoi2009,
  author    = {Yokoi, Keisuke and Aizawa, Akiko},
  title     = {{An approach to similarity search for mathematical expressions using MathML}},
  journal   = {Towards a Digital Mathematics Library. Grand Bend, Ontario, Canada, July 8-9th, 2009},
  year      = {2009},
  pages     = {27--35},
  publisher = {Masaryk University Press},
  review    = {- relevant with regards to filtering out formula environments
also see
2016: https://link.springer.com/article/10.1007/s11786-016-0274-0
2014: https://link.springer.com/chapter/10.1007/978-3-319-08434-3_29
2012: https://www.jstage.jst.go.jp/article/jsik/22/3/22_22_253/_article/-char/ja/},
}

@InProceedings{Bhagavatula2018,
  author    = {Chandra Bhagavatula and Sergey Feldman and Russell Power and Waleed Ammar},
  title     = {{Content-Based Citation Recommendation}},
  booktitle = {NAACL-HLT},
  year      = {2018},
  review    = {- kNN preselection
- use PMC OA},
}

@InProceedings{White2016,
  author    = {White, Aaron Steven and Reisinger, Drew and Sakaguchi, Keisuke and Vieira, Tim and Zhang, Sheng and Rudinger, Rachel and Rawlins, Kyle and Van Durme, Benjamin},
  title     = {{Universal Decompositional Semantics on Universal Dependencies}},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  year      = {2016},
  pages     = {1713--1723},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/D16-1177},
  location  = {Austin, Texas},
  review    = {- PredPatt citation 1},
  url       = {http://aclweb.org/anthology/D16-1177},
}

@InProceedings{Popat2016,
  author    = {Popat, Kashyap and Mukherjee, Subhabrata and Str\"{o}tgen, Jannik and Weikum, Gerhard},
  title     = {{Credibility Assessment of Textual Claims on the Web}},
  booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
  year      = {2016},
  series    = {CIKM '16},
  pages     = {2173--2178},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {- mby related work for claim based approach},
  acmid     = {2983661},
  doi       = {10.1145/2983323.2983661},
  isbn      = {978-1-4503-4073-1},
  keywords  = {credibility analysis, rumor and hoax detection, text mining},
  location  = {Indianapolis, Indiana, USA},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/2983323.2983661},
}

@Article{Schneider2013,
  author     = {Schneider, Jodi and Groza, Tudor and Passant, Alexandre},
  title      = {{A Review of Argumentation for the Social Semantic Web}},
  journal    = {Semant. web},
  year       = {2013},
  volume     = {4},
  number     = {2},
  pages      = {159--218},
  month      = apr,
  issn       = {1570-0844},
  acmid      = {2590218},
  address    = {Amsterdam, The Netherlands, The Netherlands},
  issue_date = {April 2013},
  keywords   = {Argumentation, Ontologies, Semantic Web, Social Web},
  numpages   = {60},
  publisher  = {IOS Press},
  review     = {- citation for scholarly discourse + semantic web},
  url        = {http://dl.acm.org/citation.cfm?id=2590215.2590218},
}

@InProceedings{Kitamoto2015,
  author    = {Asanobu Kitamoto and Yoko Nishimura},
  title     = {{Digital Criticism Platform for Supporting Evidence-based Interpretation of Sources}},
  booktitle = {IPSJ SIG Computers and the Humanities Symposium 2015},
  year      = {2015},
  pages     = {211-218},
  month     = {12},
  note      = {(in Japanese)},
}

@InProceedings{Bollacker1998,
  author    = {Bollacker, Kurt D. and Lawrence, Steve and Giles, C. Lee},
  title     = {{CiteSeer: An Autonomous Web Agent for Automatic Retrieval and Identification of Interesting Publications}},
  booktitle = {Proceedings of the Second International Conference on Autonomous Agents},
  year      = {1998},
  series    = {AGENTS '98},
  pages     = {116--123},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {280786},
  doi       = {10.1145/280765.280786},
  isbn      = {0-89791-983-1},
  location  = {Minneapolis, Minnesota, USA},
  numpages  = {8},
  review    = {- Beel2016: "In 1998, Giles et al. introduced the first research-paper recommender system as part of the CiteSeer project"},
  url       = {http://doi.acm.org/10.1145/280765.280786},
}

@Article{Berners-Lee2001,
  author  = {Berners-Lee, Tim and Hendler, James and Lassila, Ora and others},
  title   = {{The semantic web}},
  journal = {Scientific american},
  year    = {2001},
  volume  = {284},
  number  = {5},
  pages   = {28--37},
  review  = {- semweb standard citation (used in Schneider2013)},
}

@Article{Jiang2012,
  author  = {Jiang, Yichen and Jia, Aixia and Feng, Yansong and Zhao, Dongyan},
  title   = {{Recommending academic papers via users' reading purposes}},
  journal = {RecSys'12 - Proceedings of the 6th ACM Conference on Recommender Systems},
  year    = {2012},
  month   = {09},
  doi     = {10.1145/2365952.2366004},
  review  = {- related work
- use tags found on citeulike for a topic model "We leverage social tagging resources to define academic terms by extracting users’ tagging information from citeulike"
- tag = "academic concept"
- academic concepts search for in paper abstract
- also split abstract into problem and solution part
- evaluate with user judgement from 0 to 3; test set 30 papers, training 350k},
}

@InProceedings{Middleton2001,
  author    = {Stuart E. Middleton and David De Roure and Nigel Shadbolt},
  title     = {{Capturing knowledge of user preferences: ontologies in recommender systems}},
  booktitle = {K-CAP},
  year      = {2001},
  abstract  = {- related work
- system that tracks and classifies URLs a researcher borwses
- probably limits above step to direct links to PDF or PS documents (HTML content said to be ignored "at the expense of HTML format papers")
- classifies papers into topics based on "dmoz open directory project hierarchy" (discontinued in 2017 and archived at http://dmoztools.net/), more specifically its taxonomy of computer science topics
- does hybrid content-based + collab. fil. recommendation of papers
- evaluate ontology labelling strategy vs. a flat list labelling strategy
- evaluate in 2 user studies and report that ontology based labellig gives better results (103 papers covering 17 topics, 135 papers covering 23 topics)},
  review    = {- related work},
}

@Article{Middleton2004,
  author     = {Middleton, Stuart E. and Shadbolt, Nigel R. and De Roure, David C.},
  title      = {{Ontological User Profiling in Recommender Systems}},
  journal    = {ACM Trans. Inf. Syst.},
  year       = {2004},
  volume     = {22},
  number     = {1},
  pages      = {54--88},
  month      = jan,
  issn       = {1046-8188},
  acmid      = {963773},
  address    = {New York, NY, USA},
  doi        = {10.1145/963770.963773},
  issue_date = {January 2004},
  keywords   = {Agent, machine learning, ontology, personalization, recommender systems, user modelling, user profiling},
  numpages   = {35},
  publisher  = {ACM},
  review     = {- related work
- new/extended Middleton2001 system; eval somewhat convoluted though},
  url        = {http://doi.acm.org/10.1145/963770.963773},
}

@Article{Zarrinkalam2013,
  author  = {Fattane Zarrinkalam and Mohsen Kahani},
  title   = {{SemCiR: A citation recommendation system based on a novel semantic distance measure}},
  journal = {Program: Electronic Library and Information Systems},
  year    = {2013},
  volume  = {47},
  pages   = {92-112},
  review  = {- related work
- define a measure of semantic similarity based on combining
    - relative text similarity between input text and a fixed size set of textually similar papers
    - a similarity measure based on 6 "relational" features (overlapping authors, same venue, citing same works, being cited by same works, citing each other) *between papers* (candidate paper and a fixed size set of papers *textually similar* to input text)
- paper similarity measure is close to modeling (A,cites,B), (z,authorOf,A), type semantic info
-> combined similarity measure somewhat convoluted

input text -> [fixed size set of textually similar papers] -> semantically similar papers     (<- wouldn't this just always be the highest for the fixed size set papers themselves? mby not b/c of weighted averaging over all fixed size set papers)
                         ^
     these now represent the input text (weighted by their relative similarity to the input text)

- represent test set papers by title, abstract and citation context*s*},
}

@InProceedings{Zarrinkalam2012,
  author       = {Zarrinkalam, Fattane and Kahani, Mohsen},
  title        = {{A multi-criteria hybrid citation recommendation system based on linked data}},
  booktitle    = {2012 2nd International eConference on Computer and Knowledge Engineering (ICCKE)},
  year         = {2012},
  pages        = {283--288},
  organization = {IEEE},
  review       = {- related work
- enriching paper metadata with LOD
- input most likely a whole document (with title, authors, etc. (metadata) as opposed to just a citation context)},
}

@InProceedings{DiNoia2012,
  author    = {Di Noia, Tommaso and Mirizzi, Roberto and Ostuni, Vito Claudio and Romito, Davide and Zanker, Markus},
  title     = {{Linked Open Data to Support Content-based Recommender Systems}},
  booktitle = {Proceedings of the 8th International Conference on Semantic Systems},
  year      = {2012},
  series    = {I-SEMANTICS '12},
  pages     = {1--8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2362501},
  doi       = {10.1145/2362499.2362501},
  isbn      = {978-1-4503-1112-0},
  keywords  = {DBpedia, LinkedMDB, content-based recommender systems, freebase, linked data, movielens, precision, recall, semantic web, vector space model},
  location  = {Graz, Austria},
  numpages  = {8},
  review    = {- related work
- content based recomm. (but with user model) using LOD for movie domain},
  url       = {http://doi.acm.org/10.1145/2362499.2362501},
}

@InProceedings{Zhang2008,
  author    = {Zhang, Ming and Wang, Weichun and Li, Xiaoming},
  title     = {{A Paper Recommender for Scientific Literatures Based on Semantic Concept Similarity}},
  booktitle = {Digital Libraries: Universal and Ubiquitous Access to Information},
  year      = {2008},
  editor    = {Buchanan, George and Masoodian, Masood and Cunningham, Sally Jo},
  pages     = {359--362},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Recently, collaborative tagging has become more and more popular in the Web2.0 community, since tags in these Web2.0 systems reflect the specific content features of the resources. This paper presents a recommender for scientific literatures based on semantic concept similarity computed from the collaborative tags. User profiles and item profiles are presented by these semantic concepts, and neighbor users are selected using collaborative filtering. Then, content-based filtering approach is used to generate recommendation list from the papers these neighbor users tagged. The evaluation is carried out on a dataset crawled from CiteULike, with satisfied experiment results.},
  isbn      = {978-3-540-89533-6},
  review    = {- related work
- use citeulike tags to derive semantic concepts (concepts are comprised of tags)
- semantic similarity between two concepts is given by cosine similarity of tags in a vector space
- associate each paper to each *tag* (not concept) with a certain degree
- hybrid approach},
}

@InProceedings{He2010,
  author    = {He, Qi and Pei, Jian and Kifer, Daniel and Mitra, Prasenjit and Giles, Lee},
  title     = {{Context-aware Citation Recommendation}},
  booktitle = {Proceedings of the 19th International Conference on World Wide Web},
  year      = {2010},
  series    = {WWW '10},
  pages     = {421--430},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1772734},
  doi       = {10.1145/1772690.1772734},
  isbn      = {978-1-60558-799-8},
  keywords  = {bibliometrics, context, gleason's theorem, recommender systems},
  location  = {Raleigh, North Carolina, USA},
  numpages  = {10},
  review    = {- "Perhaps the seminal piece of work" in the area of *local* citation recommendation (Duma2016)
- prototype of RefSeer (Huang2014)},
  url       = {http://doi.acm.org/10.1145/1772690.1772734},
}

@InProceedings{Huang2014,
  author    = {W. {Huang} and and P. {Mitra} and C. L. {Giles}},
  title     = {{RefSeer: A citation recommendation system}},
  booktitle = {IEEE/ACM Joint Conference on Digital Libraries},
  year      = {2014},
  pages     = {371-374},
  month     = {Sep.},
  doi       = {10.1109/JCDL.2014.6970192},
  keywords  = {citation analysis;recommender systems;citation recommendation system;RefSeer;topic based global recommendation;citation-context based local recommendation;Context;Computational modeling;Training;Complexity theory;Context modeling;Bibliographies;Search engines;Citation recommendation;RefSeer},
  review    = {- related work
- local citation recommendation
- (in part same people from penn state as He2010 <- prototype of RefSeer)
- do golbal and local citrec
- 3 sentence citation context
- for 3 sentence query, use each as a separate query},
}

@InProceedings{Huang2015,
  author    = {Huang, Wenyi and Wu, Zhaohui and Liang, Chen and Mitra, Prasenjit and Giles, C. Lee},
  title     = {{A Neural Probabilistic Model for Context Based Citation Recommendation}},
  booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year      = {2015},
  series    = {AAAI'15},
  pages     = {2404--2410},
  publisher = {AAAI Press},
  acmid     = {2886655},
  isbn      = {0-262-51129-0},
  location  = {Austin, Texas},
  numpages  = {7},
  review    = {- related work
- local citation recommendation
- builds on Hunag2014
- neural probabilistic model
- use stochastic gradient descent},
  url       = {http://dl.acm.org/citation.cfm?id=2886521.2886655},
}

@InProceedings{He2011,
  author    = {He, Qi and Kifer, Daniel and Pei, Jian and Mitra, Prasenjit and Giles, C. Lee},
  title     = {{Citation Recommendation Without Author Supervision}},
  booktitle = {Proceedings of the Fourth ACM International Conference on Web Search and Data Mining},
  year      = {2011},
  series    = {WSDM '11},
  pages     = {755--764},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1935926},
  doi       = {10.1145/1935826.1935926},
  isbn      = {978-1-4503-0493-1},
  keywords  = {bibliometrics, context, extraction, recommender systems},
  location  = {Hong Kong, China},
  numpages  = {10},
  review    = {- related work
- local citation recommendation},
  url       = {http://doi.acm.org/10.1145/1935826.1935926},
}

@InProceedings{Ebesu2017,
  author    = {Ebesu, Travis and Fang, Yi},
  title     = {{Neural Citation Network for Context-Aware Citation Recommendation}},
  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year      = {2017},
  series    = {SIGIR '17},
  pages     = {1093--1096},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3080730},
  doi       = {10.1145/3077136.3080730},
  isbn      = {978-1-4503-5022-8},
  keywords  = {citation recommendation, deep learning, neural machine translation},
  location  = {Shinjuku, Tokyo, Japan},
  numpages  = {4},
  review    = {- related work
- local citation recommendation

- "Neural Citation Network"
- inspired by neural machine translation
- can generalize to new papers not present in the training set
- use attention
- learn author representation
- use title

- eval on RefSeer
- after filtering have "4,549,267 context pairs" (given RefSeer contains 105 M contexts quite the filtering)
- report NDCG, MAP, MRR, recall},
  url       = {http://doi.acm.org/10.1145/3077136.3080730},
}

@InProceedings{Caragea2014,
  author    = {Caragea, Cornelia and Wu, Jian and Ciobanu, Alina and Williams, Kyle and Fern{\'a}ndez-Ram{\'i}rez, Juan and Chen, Hung-Hsuan and Wu, Zhaohui and Giles, Lee},
  title     = {{CiteSeerx: A Scholarly Big Dataset}},
  booktitle = {Advances in Information Retrieval},
  year      = {2014},
  editor    = {de Rijke, Maarten and Kenter, Tom and de Vries, Arjen P. and Zhai, ChengXiang and de Jong, Franciska and Radinsky, Kira and Hofmann, Katja},
  pages     = {311--322},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {The CiteSeerxdigital library stores and indexes research articles in Computer Science and related fields. Although its main purpose is to make it easier for researchers to search for scientific information, CiteSeerxhas been proven as a powerful resource in many data mining, machine learning and information retrieval applications that use rich metadata, e.g., titles, abstracts, authors, venues, references lists, etc. The metadata extraction in CiteSeerxis done using automated techniques. Although fairly accurate, these techniques still result in noisy metadata. Since the performance of models trained on these data highly depends on the quality of the data, we propose an approach to CiteSeerxmetadata cleaning that incorporates information from an external data source. The result is a subset of CiteSeerx, which is substantially cleaner than the entire set. Our goal is to make the new dataset available to the research community to facilitate future work in Information Retrieval.},
  isbn      = {978-3-319-06028-6},
  review    = {- citeseer x citation},
}

@InProceedings{Gabor2018,
  author    = {G{\'a}bor, Kata and Buscaldi, Davide and Schumann, Anne-Kathrin and QasemiZadeh, Behrang and Zargayouna, Haifa and Charnois, Thierry},
  title     = {{SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers}},
  booktitle = {Proceedings of The 12th International Workshop on Semantic Evaluation},
  year      = {2018},
  pages     = {679--688},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/S18-1111},
  location  = {New Orleans, Louisiana},
  review    = {- related work
- relation extraction},
  url       = {http://aclweb.org/anthology/S18-1111},
}

@InProceedings{Faerber2018b,
  author    = {Michael F{\"{a}}rber and Alexander Thiemann and Adam Jatowt},
  title     = {{To Cite, or Not to Cite? Detecting Citation Contexts in Text}},
  booktitle = {{Proceedings of the 40th European Conference on Information Retrieval}},
  year      = {2018},
  series    = {{{ECIR} 2018}},
  review    = {- used ACL data from here},
}

@Article{Ginsparg1994,
  author     = {Ginsparg, Paul},
  title      = {{First Steps Towards Electronic Research Communication}},
  journal    = {Computers in Physics},
  year       = {1994},
  volume     = {8},
  number     = {4},
  pages      = {390--396},
  month      = jul,
  issn       = {0894-1866},
  acmid      = {187185},
  address    = {Woodbury, NY, USA},
  doi        = {10.1063/1.4823313},
  issue_date = {July/Aug. 1994},
  numpages   = {7},
  publisher  = {American Institute of Physics Inc.},
  review     = {- citation for arxiv.org starting 1991},
  url        = {http://dx.doi.org/10.1063/1.4823313},
}

@Article{Nanba1999,
  author  = {Hidetsugu Nanba and Manabu Okumura},
  title   = {{Towards Multi-paper Summarization Using Reference Information}},
  journal = {Journal of Natural Language Processing},
  year    = {1999},
  volume  = {6},
  number  = {5},
  pages   = {43-62},
  doi     = {10.5715/jnlp.6.5_43},
  review  = {- based on master's thesis of first author (Nanba1998)},
}

@MastersThesis{Nanba1998,
  author = {Hidetsugu Nanba},
  title  = {{Towards Multi-paper Summarization Using Reference Information}},
  school = {Japan Advanced Institute of Science and Technology},
  year   = {1998},
  month  = {2},
  note   = {(in Japanese)},
  review = {- very early example of using arXiv sources for getting citation data
- used LaTeX files from arXiv (xxx.lanl.gov at the time) to analyse citations
- use 450 articles from "Computation and Language" cs.CL  (cmp-lg at the time)
- describe problem of matching bibitem string to a publication
- introduce paper serach system PRESRI (Paper REtrieval System Using Reference Information) that takes citation interlinking into account
- analyze citation contexts and citation function

- note on having used the *small size* data source arXiv and that using more data automatically crawled from the web is future work

- original title: 論文間の参照情報を考慮した 学術論文要約システムの開発
- title template from publication one year later with original title: 論文間の参照情報を考慮した サーベイ論文作成支援 システムの開発},
  url    = {http://www.ls.info.hiroshima-cu.ac.jp/~nanba/pdf/master_thesis.pdf},
}

@Unknown{Jeong2019,
  author = {Jeong, Chanwoo and Sion, Jang and Shin, Hyuna and Park, Eunjeong and Choi, Sungchul},
  title  = {{A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks}},
  month  = {02},
  year   = {2019},
  review = {- related work},
  url    = {https://www.researchgate.net/publication/331346718_A_Context-Aware_Citation_Recommendation_Model_with_BERT_and_Graph_Convolutional_Networks},
}

@Article{Animesh2018,
  author    = {Prasad, Animesh and Kaur, Manpreet and Kan, Min-Yen},
  title     = {{Neural ParsCit: A Deep Learning Based Reference String Parser}},
  journal   = {International Journal on Digital Libraries},
  year      = {2018},
  volume    = {19},
  pages     = {323-337},
  publisher = {Springer},
  review    = {- neural parscit citation},
  url       = {https://link.springer.com/article/10.1007/s00799-018-0242-1},
}

@InProceedings{Roy2016,
  author = {Dwaipayan Roy and Kunal Ray and Mandar Mitra},
  title  = {{From a Scholarly Big Dataset to a Test Collection for Bibliographic Citation Recommendation}},
  year   = {2016},
  series = {{SBD'16}},
  review = {- CiteSeerX problems},
}

@InProceedings{Bertin2018,
  author      = {Bertin, Marc and Atanassova, Iana},
  title       = {{InTeReC: In-text Reference Corpus for Applying Natural Language Processing to Bibliometrics}},
  booktitle   = {{7th International Workshop on Bibliometric-enhanced Information Retrieval (BIR 2018) to be held as part of the 40th European Conference on Information Retrieval (ECIR)}},
  year        = {2018},
  address     = {Grenoble, France},
  month       = Mar,
  hal_id      = {hal-01742178},
  hal_version = {v1},
  review      = {- use PLOS submissions up to 2013 (why not more recent?)
- only extract citation contexts where exactly 1 cited doc is referenced (b/c "most simple form")
- 300k contexts
- *no information on cited doc* (markers are kept as is (e.g. "[1]")

- verbs in citation contexts are important (check references)},
  url         = {https://hal.archives-ouvertes.fr/hal-01742178},
}

@InProceedings{Bird2008,
  author    = {Steven Bird and Robert Dale and Bonnie J. Dorr and Bryan R. Gibson and Mark Thomas Joseph and Min{-}Yen Kan and Dongwon Lee and Brett Powley and Dragomir R. Radev and Yee Fan Tan},
  title     = {{The ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics}},
  booktitle = {{Proceedings of the International Conference on Language Resources and Evaluation}},
  year      = {2008},
  series    = {{{LREC} 2008}},
  location  = {{Marrakech, Morocco}},
  review    = {- citation for ACL-ARC},
}

@Article{Radev2013,
  author   = {Radev, Dragomir R. and Muthukrishnan, Pradeep and Qazvinian, Vahed and Abu-Jbara, Amjad},
  title    = {{The ACL anthology network corpus}},
  journal  = {Language Resources and Evaluation},
  year     = {2013},
  volume   = {47},
  number   = {4},
  pages    = {919--944},
  month    = {Dec},
  issn     = {1574-0218},
  abstract = {We introduce the ACL Anthology Network (AAN), a comprehensive manually curated networked database of citations, collaborations, and summaries in the field of Computational Linguistics. We also present a number of statistics about the network including the most cited authors, the most central collaborators, as well as network statistics about the paper citation, author citation, and author collaboration networks.},
  day      = {01},
  doi      = {10.1007/s10579-012-9211-2},
  review   = {- citation for ACL-AAN},
  url      = {https://doi.org/10.1007/s10579-012-9211-2},
}

@Article{Jaradeh2019,
  author        = {Mohamad Yaser Jaradeh and S{\"{o}}ren Auer and Manuel Prinz and Viktor Kovtun and G{\'{a}}bor Kismih{\'{o}}k and Markus Stocker},
  title         = {{Open Research Knowledge Graph: Towards Machine Actionability in Scholarly Communication}},
  journal       = {CoRR},
  year          = {2019},
  volume        = {abs/1901.10816},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1901-10816},
  eprint        = {1901.10816},
  review        = {- citation for making scholarly discourse more easily machine processable by semantic annotation (cf. ScholOnto)},
  timestamp     = {Sun, 03 Feb 2019 14:23:05 +0100},
  url           = {http://arxiv.org/abs/1901.10816},
}

@TechReport{Whidby2011,
  author = {Whidby, Michael and Zajic, David and Dorr, Bonnie},
  title  = {{Citation handling for improved summarization of scientific documents}},
  year   = {2011},
  review = {- extend a trimmer tool to be able to "handle" syntactical and non-syntactical citations (syntactical: replace marker with token, non-integral: remove marker and place token at end of sentence ("better solution = future work"))
- specifically state that non-syntactical citations cause problems in generating parse trees (the constituency kind, not dependency trees)},
}

@InProceedings{Mausam2016,
  author       = {{Mausam}},
  title        = {{Open information extraction systems and downstream applications}},
  booktitle    = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
  year         = {2016},
  pages        = {4074--4077},
  organization = {AAAI Press},
  review       = {- Open IE 5.0 citation},
}

@InProceedings{DelCorro2013,
  author       = {Del Corro, Luciano and Gemulla, Rainer},
  title        = {{ClausIE: clause-based open information extraction}},
  booktitle    = {Proceedings of the 22nd international conference on World Wide Web},
  year         = {2013},
  pages        = {355--366},
  organization = {ACM},
  review       = {- ClausIE citaiton},
}

@InProceedings{Mausam2012,
  author    = {Mausam and Schmitz, Michael and Bart, Robert and Soderland, Stephen and Etzioni, Oren},
  title     = {{Open Language Learning for Information Extraction}},
  booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  year      = {2012},
  series    = {EMNLP-CoNLL '12},
  pages     = {523--534},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {2391009},
  location  = {Jeju Island, Korea},
  numpages  = {12},
  review    = {- OLLIE citaiton},
  url       = {http://dl.acm.org/citation.cfm?id=2390948.2391009},
}

@InProceedings{Nivre2016,
  author    = {Joakim Nivre and Marie-Catherine de Marneffe and Filip Ginter and Yoav Goldberg and Jan Hajic and Christopher D. Manning and Ryan McDonald and Slav Petrov and Sampo Pyysalo and Natalia Silveira and Reut Tsarfaty and Daniel Zeman},
  title     = {Universal Dependencies v1: A Multilingual Treebank Collection},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
  year      = {2016},
  publisher = {European Language Resources Association (ELRA)},
  date      = {23-28},
  isbn      = {978-2-9517408-9-1},
  location  = {Portorož, Slovenia},
  review    = {- universal dependencies citation},
}

@Article{Bertin2016,
  author   = {Bertin, Marc and Atanassova, Iana and Sugimoto, Cassidy R. and Lariviere, Vincent},
  title    = {The linguistic patterns and rhetorical structure of citation context: an approach using n-grams},
  journal  = {Scientometrics},
  year     = {2016},
  volume   = {109},
  number   = {3},
  pages    = {1417--1434},
  month    = {Dec},
  issn     = {1588-2861},
  abstract = {Using the full-text corpus of more than 75,000 research articles published by seven PLOS journals, this paper proposes a natural language processing approach for identifying the function of citations. Citation contexts are assigned based on the frequency of n-gram co-occurrences located near the citations. Results show that the most frequent linguistic patterns found in the citation contexts of papers vary according to their location in the IMRaD structure of scientific articles. The presence of negative citations is also dependent on this structure. This methodology offers new perspectives to locate these discursive forms according to the rhetorical structure of scientific articles, and will lead to a better understanding of the use of citations in scientific articles.},
  day      = {01},
  doi      = {10.1007/s11192-016-2134-8},
  review   = {- use PLOS submissions},
  url      = {https://doi.org/10.1007/s11192-016-2134-8},
}

@Article{Wu2011,
  author   = {Wu, X. and Collilieux, X. and Altamimi, Z. and Vermeersen, B. L. A. and Gross, R. S. and Fukumori, I.},
  title    = {Accuracy of the International Terrestrial Reference Frame origin and Earth expansion},
  journal  = {Geophysical Research Letters},
  year     = {2011},
  volume   = {38},
  number   = {13},
  doi      = {10.1029/2011GL047450},
  eprint   = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2011GL047450},
  keywords = {geocenter motion, solid Earth expansion, terrestrial reference frames},
  review   = {- example for a hypothesis becoming obsolete},
  url      = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2011GL047450},
}

@Article{Potter1981,
  author    = {Potter, William Gray},
  title     = {{Lotka’s Law Revisited}},
  year      = {1981},
  issn      = {0024-2594},
  publisher = {Graduate School of Library and Information Science. University of Illinois at Urbana-Champaign},
  review    = {- example for discovery becoming named retrospectively},
  url       = {http://hdl.handle.net/2142/7191},
}

@Book{Aggarwal2016,
  title     = {{Recommender Systems: The Textbook}},
  publisher = {Springer Publishing Company, Incorporated},
  year      = {2016},
  author    = {Aggarwal, Charu C.},
  edition   = {1st},
  isbn      = {3319296574, 9783319296579},
  review    = {- for background chapter},
}

@Article{Gao2016,
  author    = {Gao, Chao AND Wang, Zhen AND Li, Xianghua AND Zhang, Zili AND Zeng, Wei},
  title     = {{PR-Index: Using the h-Index and PageRank for Determining True Impact}},
  journal   = {PLOS ONE},
  year      = {2016},
  volume    = {11},
  number    = {9},
  pages     = {1-13},
  month     = {09},
  doi       = {10.1371/journal.pone.0161755},
  publisher = {Public Library of Science},
  review    = {- example for the use of "citation" as the citations an author receives},
  url       = {https://doi.org/10.1371/journal.pone.0161755},
}

@Article{Myers1970,
  author    = {Myers, C Roger},
  title     = {{Journal citations and scientific eminence in contemporary psychology.}},
  journal   = {American Psychologist},
  year      = {1970},
  volume    = {25},
  number    = {11},
  pages     = {1041},
  publisher = {American Psychological Association},
  review    = {- better example for the use of "citation" as the citations an author receives},
}

@Article{Nadeau2007,
  author  = {Nadeau, David and Sekine, Satoshi},
  title   = {A survey of named entity recognition and classification},
  journal = {Linguisticae Investigationes},
  year    = {2007},
  volume  = {30},
  number  = {1},
  pages   = {3--26},
  month   = {January},
  doi     = {10.1075/li.30.1.03nad},
  review  = {- citation for definition/explanation for named entities},
}

@Article{Elkiss2008,
  author        = {Elkiss, Aaron and Shen, Siwei and Fader, Anthony and Erkan, G{\"u}ne{\c{s}} and States, David and Radev, Dragomir},
  title         = {{Blind men and elephants: What do citation summaries tell us about a research article?}},
  journal       = {Journal of the American Society for Information Science and Technology},
  year          = {2008},
  volume        = {59},
  number        = {1},
  pages         = {51--62},
  __markedentry = {[tarek:6]},
  publisher     = {Wiley Online Library},
}

@Comment{jabref-meta: databaseType:bibtex;}
