% Encoding: UTF-8

@InProceedings{Faerber2018,
  author    = {Michael F{\"{a}}rber and Alexander Thiemann and Adam Jatowt},
  title     = {{A High-Quality Gold Standard for Citation-based Tasks}},
  booktitle = {{Proceedings of the 11th International Conference on Language Resources and Evaluation}},
  year      = {2018},
  series    = {{{LREC} 2018}},
  note      = {r},
  review    = {- created new data set for cit.rec. based on arXiv
- only compsci papers
- other data sets (most commonly used one is CiteSeerX) have two main problems: (1) noisy citation context, (2) no/low quality links of citations to cited publications
- latex parser GrabCite (also parses other formats)
- 90k papers (for 62k DBLP link), 15M sentences, 2.5M citation markers

- parser: https://github.com/agrafix/grabcite
- DBLP serach thingy: https://github.com/agrafix/papergrep},
}

@InProceedings{Levy2014,
  author    = {Levy, Ran and Bilu, Yonatan and Hershcovich, Daniel and Aharoni, Ehud and Slonim, Noam},
  title     = {Context Dependent Claim Detection},
  booktitle = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  year      = {2014},
  pages     = {1489--1500},
  address   = {Dublin, Ireland},
  month     = {August},
  publisher = {Dublin City University and Association for Computational Linguistics},
  note      = {r},
  review    = {- detect claims (relevant for a given topic(=statement open for discussion)) in free text
- claims may represent an opinion (i.e. can't be checked for truth/falsity)
- classifies/ranks in 3 stages (candidate sentences -> claim within sentence -> ranking)
- uses word net
- uses a lot of existing parsers, features, etc.},
  url       = {http://www.aclweb.org/anthology/C14-1141},
}

@Article{Hassan2015,
  author    = {Hassan, Naeemul and Adair, Bill and Hamilton, James and Li, Chengkai and Tremayne, Mark and Yang, Jun and Yu, Cong},
  title     = {The Quest to Automate Fact-Checking},
  year      = {2015},
  month     = {10},
  note      = {r},
  booktitle = {Proceedings of the 2015 Computation + Journalism Symposium},
  review    = {- argue for pursuing the goal of fully automated fact checking
- uses political discourse as goto example (politicians making claims, voters juding claims for their truth)
- developed own tool ClaimBuster
- supervised ML using SVM (also tested other methods, SVM performed best)
- manually created features
- used AlchemyAPI for sentiment analysis
- no stemming or stop word removal
- POS tags
- used random forest classifier},
}

@Article{BuckinghamShum2000,
  author   = {Buckingham Shum, Simon and Motta, Enrico and Domingue, John},
  title    = {ScholOnto: an ontology-based digital library server for research documents and discourse},
  journal  = {International Journal on Digital Libraries},
  year     = {2000},
  volume   = {3},
  number   = {3},
  pages    = {237--248},
  month    = {Oct},
  issn     = {1432-5012},
  note     = {r (ch 1-3)},
  abstract = {The internet is rapidly becoming the first place for researchers to publish documents, but at present they receive little support in searching, tracking, analysing or debating concepts in a literature from scholarly perspectives. This paper describes the design rationale and implementation of ScholOnto, an ontology-based digital library server to support scholarly interpretation and discourse. It enables researchers to describe and debate via a semantic network the contributions a document makes, and its relationship to the literature. The paper discusses the computational services that an ontology-based server supports, alternative user interfaces to support interaction with a large semantic network, usability issues associated with knowledge formalisation, new work practices that could emerge, and related work.},
  day      = {01},
  doi      = {10.1007/s007990000034},
  review   = {- poposal to (manually) add semantic annotation to scientific texts (to enable network of contestable claims)
- introduce annotation scheme for above
- "discourse-oriented ontology"
- 90s style interface implementation},
  url      = {https://doi.org/10.1007/s007990000034},
}

@InProceedings{Goudas2014,
  author    = {Goudas, Theodosis and Louizos, Christos and Petasis, Georgios and Karkaletsis, Vangelis},
  title     = {Argument Extraction from News, Blogs, and Social Media},
  booktitle = {Artificial Intelligence: Methods and Applications},
  year      = {2014},
  editor    = {Likas, Aristidis and Blekas, Konstantinos and Kalles, Dimitris},
  pages     = {287--299},
  address   = {Cham},
  publisher = {Springer International Publishing},
  note      = {r},
  abstract  = {Argument extraction is the task of identifying arguments, along with their components in text. Arguments can be usually decomposed into a claim and one or more premises justifying it. Among the novel aspects of this work is the thematic domain itself which relates to Social Media, in contrast to traditional research in the area, which concentrates mainly on law documents and scientific publications. The huge increase of social media communities, along with their user tendency to debate, makes the identification of arguments in these texts a necessity. Argument extraction from Social Media is more challenging because texts may not always contain arguments, as is the case of legal documents or scientific publications usually studied. In addition, being less formal in nature, texts in Social Media may not even have proper syntax or spelling. This paper presents a two-step approach for argument extraction from social media texts. During the first step, the proposed approach tries to classify the sentences into ``sentences that contain arguments'' and ``sentences that don't contain arguments''. In the second step, it tries to identify the exact fragments that contain the premises from the sentences that contain arguments, by utilizing conditional random fields. The results exceed significantly the base line approach, and according to literature, are quite promising.},
  isbn      = {978-3-319-07064-3},
  review    = {- argument extraction from social media text
- corpus of 204 documents from social media in greek
- two step process: candidate sentences -> identify segments that contain premises
- mby usable citations for CRFs, logistic regeression, random forest, SMVs and naive bayes
- for first step logistic regressen worked best

- (!!!) names "number of named entities" as one feature of state of the art approaches to classify sentences into non-/argumentative (!!!)

- "popular search engines such as Bing" lol
- uses apparently long dead NLP platform Ellogon
- "CRFs are a structure prediction algorithm"},
}

@InProceedings{Mishra2016,
  author    = {Mishra, Arunav and Berberich, Klaus},
  title     = {Leveraging Semantic Annotations to Link Wikipedia and News Archives},
  booktitle = {Advances in Information Retrieval},
  year      = {2016},
  editor    = {Ferro, Nicola and Crestani, Fabio and Moens, Marie-Francine and Mothe, Josiane and Silvestri, Fabrizio and Di Nunzio, Giorgio Maria and Hauff, Claudia and Silvello, Gianmaria},
  pages     = {30--42},
  address   = {Cham},
  publisher = {Springer International Publishing},
  note      = {r (ch 1-3)},
  abstract  = {The incomprehensible amount of information available online has made it difficult to retrospect on past events. We propose a novel linking problem to connect excerpts from Wikipedia summarizing events to online news articles elaborating on them. To address this linking problem, we cast it into an information retrieval task by treating a given excerpt as a user query with the goal to retrieve a ranked list of relevant news articles. We find that Wikipedia excerpts often come with additional semantics, in their textual descriptions, representing the time, geolocations, and named entities involved in the event. Our retrieval model leverages text and semantic annotations as different dimensions of an event by estimating independent query models to rank documents. In our experiments on two datasets, we compare methods that consider different combinations of dimensions and find that the approach that leverages all dimensions suits our problem best.},
  isbn      = {978-3-319-30671-1},
  review    = {- for a small part of a wikipedia entry describing a historical event, find news articles (ranked list) describing the event in detail
- useses text, time, geolocations, names entities
- linking method unclear},
}

@Masterthesis{Paszcza2016,
  author = {Paszcza, Bartosz},
  title  = {Comparison of Microsoft Academic Graph with other scholarly citation databases},
  month  = {11},
  year   = {2016},
  note   = {r (ch 1,""3"")},
  pages  = {69},
  review = {- analysis of the MAG
-> relating its scope, openness, completeness of information and interoperability to WoS, Scopus, GS (three other scholarly citation datasets )
- written by a non(/not so much) tech person?},
}

@Article{Herrmannova2016,
  author    = {Drahomira Herrmannova and Petr Knoth},
  title     = {An Analysis of the Microsoft Academic Graph},
  journal   = {D-Lib Magazine},
  year      = {2016},
  volume    = {22},
  number    = {9/10},
  note      = {r},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/dlib/HerrmannovaK16a},
  doi       = {10.1045/september2016-herrmannova},
  review    = {- analysis of MAG and comparison to other data sets

- MAG is over many disciplines (not just CS, of which it contains ~17 million papers)
- MAG is automatically assembled -> before using it for a task, the MAG should be checked for noise and bias (what this paper does)
- MAG is largest publicly available dataset of open citation data
- only 30 out of 127 million papers in MAG have citation data
- year of publication field is populated for all papers in MAG
- MAG contains 35 million DOIs
- field of study info in MAG is hierarchical (4 levels of granularity)

- look on citation network from several perspectives
- 80 million disconnected nodes (i.e. papers w/o citations (being cited) and references (citing))
- differences concerning journals' and universities' citation counts compared to other citation ranking sources (but no way to dig deeper since latter are non transparent)},
  timestamp = {Sun, 28 May 2017 13:24:39 +0200},
  url       = {https://doi.org/10.1045/september2016-herrmannova},
}

@Article{Hug2017,
  author   = {Hug, Sven E. and Ochsner, Michael and Br{\"a}ndle, Martin P.},
  title    = {Citation analysis with microsoft academic},
  journal  = {Scientometrics},
  year     = {2017},
  volume   = {111},
  number   = {1},
  pages    = {371--378},
  month    = {Apr},
  issn     = {1588-2861},
  note     = {r},
  abstract = {We explore if and how Microsoft Academic (MA) could be used for bibliometric analyses. First, we examine the Academic Knowledge API (AK API), an interface to access MA data, and compare it to Google Scholar (GS). Second, we perform a comparative citation analysis of researchers by normalizing data from MA and Scopus. We find that MA offers structured and rich metadata, which facilitates data retrieval, handling and processing. In addition, the AK API allows retrieving frequency distributions of citations. We consider these features to be a major advantage of MA over GS. However, we identify four main limitations regarding the available metadata. First, MA does not provide the document type of a publication. Second, the ``fields of study'' are dynamic, too specific and field hierarchies are incoherent. Third, some publications are assigned to incorrect years. Fourth, the metadata of some publications did not include all authors. Nevertheless, we show that an average-based indicator (i.e. the journal normalized citation score; JNCS) as well as a distribution-based indicator (i.e. percentile rank classes; PR classes) can be calculated with relative ease using MA. Hence, normalization of citation counts is feasible with MA. The citation analyses in MA and Scopus yield uniform results. The JNCS and the PR classes are similar in both databases, and, as a consequence, the evaluation of the researchers' publication impact is congruent in MA and Scopus. Given the fast development in the last year, we postulate that MA has the potential to be used for full-fledged bibliometric analyses.},
  day      = {01},
  doi      = {10.1007/s11192-017-2247-8},
  review   = {- 3 ways to access the MA(G)
-> a Academic Knowledge API (which allows retrieval of frequency distributions of citations): https://www.microsoft.com/cognitive-services/en-us/academic-knowledge-api
-> MA search engine: https://academic.microsoft.com
-> snapshot downloads: https://academicgraph.blob.core.windows.net/graph/index.html
- MA(G) does not provide document type of a publication (-> makes it difficult to distinguish between citable and not citable documents)
- MAG has more structured and richer metadata than Google Scholar
- MAG seems to miss authors (for one particular researcher, they're not listed as author in 64% of their publications)

- check feasability of performing a comparative citation analysis between researchers (here 3) using AK API},
  url      = {https://doi.org/10.1007/s11192-017-2247-8},
}

@InProceedings{Sinha2015,
  author        = {Sinha, Arnab and Shen, Zhihong and Song, Yang and Ma, Hao and Eide, Darrin and Hsu, Bo-June (Paul) and Wang, Kuansan},
  title         = {An Overview of Microsoft Academic Service (MAS) and Applications},
  booktitle     = {Proceedings of the 24th International Conference on World Wide Web},
  year          = {2015},
  series        = {WWW '15 Companion},
  pages         = {243--246},
  address       = {New York, NY, USA},
  publisher     = {ACM},
  note          = {r},
  __markedentry = {[tarek:]},
  acmid         = {2742839},
  doi           = {10.1145/2740908.2742839},
  isbn          = {978-1-4503-3473-0},
  keywords      = {academic search, entity conflation, recommender systems},
  location      = {Florence, Italy},
  numpages      = {4},
  review        = {- official citation in case of using MAG (see: https://microsoftdocs.github.io/microsoft-academic/microsoft-academic-graph/reference/data-schema.html)
- description of how MAG was created
-> heterogeneous entity graph containing entities of type: field of study, author, institution, paper, venue (i.e. journal/conference), event (specific conference instance)
- description of 2 applications
-> semantic search (e.g. "papers citing <author> before <year> about <field-of-study> appearing in <journal>")
-> recommendation (e.g. "given a field of study, find out the most prominent authors, the most influential papers, ...")},
  url           = {http://doi.acm.org/10.1145/2740908.2742839},
}

@Comment{jabref-meta: databaseType:bibtex;}
