% Encoding: UTF-8

@InProceedings{Faerber2018,
  author    = {Michael F{\"{a}}rber and Alexander Thiemann and Adam Jatowt},
  title     = {{A High-Quality Gold Standard for Citation-based Tasks}},
  booktitle = {{Proceedings of the 11th International Conference on Language Resources and Evaluation}},
  year      = {2018},
  series    = {{{LREC} 2018}},
  note      = {r},
  review    = {- created new data set for cit.rec. based on arXiv
- only compsci papers
- other data sets (most commonly used one is CiteSeerX) have two main problems: (1) noisy citation context, (2) no/low quality links of citations to cited publications
- latex parser GrabCite (also parses other formats)
- 90k papers (for 62k DBLP link), 15M sentences, 2.5M citation markers

- parser: https://github.com/agrafix/grabcite
- DBLP serach thingy: https://github.com/agrafix/papergrep},
}

@InProceedings{Levy2014,
  author    = {Levy, Ran and Bilu, Yonatan and Hershcovich, Daniel and Aharoni, Ehud and Slonim, Noam},
  title     = {Context Dependent Claim Detection},
  booktitle = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  year      = {2014},
  pages     = {1489--1500},
  address   = {Dublin, Ireland},
  month     = {August},
  publisher = {Dublin City University and Association for Computational Linguistics},
  note      = {r},
  review    = {- detect claims (relevant for a given topic(=statement open for discussion)) in free text
- claims may represent an opinion (i.e. can't be checked for truth/falsity)
- classifies/ranks in 3 stages (candidate sentences -> claim within sentence -> ranking)
- uses word net
- uses a lot of existing parsers, features, etc.},
  url       = {http://www.aclweb.org/anthology/C14-1141},
}

@Article{Hassan2015,
  author    = {Hassan, Naeemul and Adair, Bill and Hamilton, James and Li, Chengkai and Tremayne, Mark and Yang, Jun and Yu, Cong},
  title     = {The Quest to Automate Fact-Checking},
  year      = {2015},
  month     = {10},
  note      = {r},
  booktitle = {Proceedings of the 2015 Computation + Journalism Symposium},
  review    = {- argue for pursuing the goal of fully automated fact checking
- uses political discourse as goto example (politicians making claims, voters juding claims for their truth)
- developed own tool ClaimBuster
- supervised ML using SVM (also tested other methods, SVM performed best)
- manually created features
- used AlchemyAPI for sentiment analysis
- no stemming or stop word removal
- POS tags
- used random forest classifier},
}

@Article{BuckinghamShum2000,
  author   = {Buckingham Shum, Simon and Motta, Enrico and Domingue, John},
  title    = {ScholOnto: an ontology-based digital library server for research documents and discourse},
  journal  = {International Journal on Digital Libraries},
  year     = {2000},
  volume   = {3},
  number   = {3},
  pages    = {237--248},
  month    = {Oct},
  issn     = {1432-5012},
  note     = {r (ch 1-3)},
  abstract = {The internet is rapidly becoming the first place for researchers to publish documents, but at present they receive little support in searching, tracking, analysing or debating concepts in a literature from scholarly perspectives. This paper describes the design rationale and implementation of ScholOnto, an ontology-based digital library server to support scholarly interpretation and discourse. It enables researchers to describe and debate via a semantic network the contributions a document makes, and its relationship to the literature. The paper discusses the computational services that an ontology-based server supports, alternative user interfaces to support interaction with a large semantic network, usability issues associated with knowledge formalisation, new work practices that could emerge, and related work.},
  day      = {01},
  doi      = {10.1007/s007990000034},
  review   = {- proposal to (manually) add semantic annotation to scientific texts (to enable network of contestable claims)
- introduce annotation scheme for above
- "discourse-oriented ontology"
- 90s style interface implementation},
  url      = {https://doi.org/10.1007/s007990000034},
}

@InProceedings{Goudas2014,
  author    = {Goudas, Theodosis and Louizos, Christos and Petasis, Georgios and Karkaletsis, Vangelis},
  title     = {Argument Extraction from News, Blogs, and Social Media},
  booktitle = {Artificial Intelligence: Methods and Applications},
  year      = {2014},
  editor    = {Likas, Aristidis and Blekas, Konstantinos and Kalles, Dimitris},
  pages     = {287--299},
  address   = {Cham},
  publisher = {Springer International Publishing},
  note      = {r},
  abstract  = {Argument extraction is the task of identifying arguments, along with their components in text. Arguments can be usually decomposed into a claim and one or more premises justifying it. Among the novel aspects of this work is the thematic domain itself which relates to Social Media, in contrast to traditional research in the area, which concentrates mainly on law documents and scientific publications. The huge increase of social media communities, along with their user tendency to debate, makes the identification of arguments in these texts a necessity. Argument extraction from Social Media is more challenging because texts may not always contain arguments, as is the case of legal documents or scientific publications usually studied. In addition, being less formal in nature, texts in Social Media may not even have proper syntax or spelling. This paper presents a two-step approach for argument extraction from social media texts. During the first step, the proposed approach tries to classify the sentences into ``sentences that contain arguments'' and ``sentences that don't contain arguments''. In the second step, it tries to identify the exact fragments that contain the premises from the sentences that contain arguments, by utilizing conditional random fields. The results exceed significantly the base line approach, and according to literature, are quite promising.},
  isbn      = {978-3-319-07064-3},
  review    = {- argument extraction from social media text
- corpus of 204 documents from social media in greek
- two step process: candidate sentences -> identify segments that contain premises
- mby usable citations for CRFs, logistic regeression, random forest, SMVs and naive bayes
- for first step logistic regressen worked best

- (!!!) names "number of named entities" as one feature of state of the art approaches to classify sentences into non-/argumentative (!!!)

- "popular search engines such as Bing" lol
- uses apparently long dead NLP platform Ellogon
- "CRFs are a structure prediction algorithm"},
}

@InProceedings{Mishra2016,
  author    = {Mishra, Arunav and Berberich, Klaus},
  title     = {Leveraging Semantic Annotations to Link Wikipedia and News Archives},
  booktitle = {Advances in Information Retrieval},
  year      = {2016},
  editor    = {Ferro, Nicola and Crestani, Fabio and Moens, Marie-Francine and Mothe, Josiane and Silvestri, Fabrizio and Di Nunzio, Giorgio Maria and Hauff, Claudia and Silvello, Gianmaria},
  pages     = {30--42},
  address   = {Cham},
  publisher = {Springer International Publishing},
  note      = {r (ch 1-3)},
  abstract  = {The incomprehensible amount of information available online has made it difficult to retrospect on past events. We propose a novel linking problem to connect excerpts from Wikipedia summarizing events to online news articles elaborating on them. To address this linking problem, we cast it into an information retrieval task by treating a given excerpt as a user query with the goal to retrieve a ranked list of relevant news articles. We find that Wikipedia excerpts often come with additional semantics, in their textual descriptions, representing the time, geolocations, and named entities involved in the event. Our retrieval model leverages text and semantic annotations as different dimensions of an event by estimating independent query models to rank documents. In our experiments on two datasets, we compare methods that consider different combinations of dimensions and find that the approach that leverages all dimensions suits our problem best.},
  isbn      = {978-3-319-30671-1},
  review    = {- for a small part of a wikipedia entry describing a historical event, find news articles (ranked list) describing the event in detail
- useses text, time, geolocations, names entities
- linking method unclear},
}

@Masterthesis{Paszcza2016,
  author = {Paszcza, Bartosz},
  title  = {Comparison of Microsoft Academic Graph with other scholarly citation databases},
  month  = {11},
  year   = {2016},
  note   = {r (ch 1,""3"")},
  pages  = {69},
  review = {- analysis of the MAG
-> relating its scope, openness, completeness of information and interoperability to WoS, Scopus, GS (three other scholarly citation datasets )
- written by a non(/not so much) tech person?},
}

@Article{Herrmannova2016,
  author    = {Drahomira Herrmannova and Petr Knoth},
  title     = {An Analysis of the Microsoft Academic Graph},
  journal   = {D-Lib Magazine},
  year      = {2016},
  volume    = {22},
  number    = {9/10},
  note      = {r},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/dlib/HerrmannovaK16a},
  doi       = {10.1045/september2016-herrmannova},
  review    = {- analysis of MAG and comparison to other data sets

- MAG is over many disciplines (not just CS, of which it contains ~17 million papers)
- MAG is automatically assembled -> before using it for a task, the MAG should be checked for noise and bias (what this paper does)
- MAG is largest publicly available dataset of open citation data
- only 30 out of 127 million papers in MAG have citation data
- year of publication field is populated for all papers in MAG
- MAG contains 35 million DOIs
- field of study info in MAG is hierarchical (4 levels of granularity)

- look on citation network from several perspectives
- 80 million disconnected nodes (i.e. papers w/o citations (being cited) and references (citing))
- differences concerning journals' and universities' citation counts compared to other citation ranking sources (but no way to dig deeper since latter are non transparent)},
  timestamp = {Sun, 28 May 2017 13:24:39 +0200},
  url       = {https://doi.org/10.1045/september2016-herrmannova},
}

@Article{Hug2017,
  author   = {Hug, Sven E. and Ochsner, Michael and Br{\"a}ndle, Martin P.},
  title    = {Citation analysis with microsoft academic},
  journal  = {Scientometrics},
  year     = {2017},
  volume   = {111},
  number   = {1},
  pages    = {371--378},
  month    = {Apr},
  issn     = {1588-2861},
  note     = {r},
  abstract = {We explore if and how Microsoft Academic (MA) could be used for bibliometric analyses. First, we examine the Academic Knowledge API (AK API), an interface to access MA data, and compare it to Google Scholar (GS). Second, we perform a comparative citation analysis of researchers by normalizing data from MA and Scopus. We find that MA offers structured and rich metadata, which facilitates data retrieval, handling and processing. In addition, the AK API allows retrieving frequency distributions of citations. We consider these features to be a major advantage of MA over GS. However, we identify four main limitations regarding the available metadata. First, MA does not provide the document type of a publication. Second, the ``fields of study'' are dynamic, too specific and field hierarchies are incoherent. Third, some publications are assigned to incorrect years. Fourth, the metadata of some publications did not include all authors. Nevertheless, we show that an average-based indicator (i.e. the journal normalized citation score; JNCS) as well as a distribution-based indicator (i.e. percentile rank classes; PR classes) can be calculated with relative ease using MA. Hence, normalization of citation counts is feasible with MA. The citation analyses in MA and Scopus yield uniform results. The JNCS and the PR classes are similar in both databases, and, as a consequence, the evaluation of the researchers' publication impact is congruent in MA and Scopus. Given the fast development in the last year, we postulate that MA has the potential to be used for full-fledged bibliometric analyses.},
  day      = {01},
  doi      = {10.1007/s11192-017-2247-8},
  review   = {- 3 ways to access the MA(G)
-> a Academic Knowledge API (which allows retrieval of frequency distributions of citations): https://www.microsoft.com/cognitive-services/en-us/academic-knowledge-api
-> MA search engine: https://academic.microsoft.com
-> snapshot downloads: https://academicgraph.blob.core.windows.net/graph/index.html
- MA(G) does not provide document type of a publication (-> makes it difficult to distinguish between citable and not citable documents)
- MAG has more structured and richer metadata than Google Scholar
- MAG seems to miss authors (for one particular researcher, they're not listed as author in 64% of their publications)

- check feasability of performing a comparative citation analysis between researchers (here 3) using AK API},
  url      = {https://doi.org/10.1007/s11192-017-2247-8},
}

@InProceedings{Sinha2015,
  author    = {Sinha, Arnab and Shen, Zhihong and Song, Yang and Ma, Hao and Eide, Darrin and Hsu, Bo-June (Paul) and Wang, Kuansan},
  title     = {An Overview of Microsoft Academic Service (MAS) and Applications},
  booktitle = {Proceedings of the 24th International Conference on World Wide Web},
  year      = {2015},
  series    = {WWW '15 Companion},
  pages     = {243--246},
  address   = {New York, NY, USA},
  publisher = {ACM},
  note      = {r},
  acmid     = {2742839},
  doi       = {10.1145/2740908.2742839},
  isbn      = {978-1-4503-3473-0},
  keywords  = {academic search, entity conflation, recommender systems},
  location  = {Florence, Italy},
  numpages  = {4},
  review    = {- official citation in case of using MAG (see: https://microsoftdocs.github.io/microsoft-academic/microsoft-academic-graph/reference/data-schema.html)
- description of how MAG was created
-> heterogeneous entity graph containing entities of type: field of study, author, institution, paper, venue (i.e. journal/conference), event (specific conference instance)
- description of 2 applications
-> semantic search (e.g. "papers citing <author> before <year> about <field-of-study> appearing in <journal>")
-> recommendation (e.g. "given a field of study, find out the most prominent authors, the most influential papers, ...")},
  url       = {http://doi.acm.org/10.1145/2740908.2742839},
}

@Book{Besnard2008,
  title     = {Elements of Argumentation},
  publisher = {The MIT Press},
  year      = {2008},
  author    = {Besnard, Philippe and Hunter, Anthony},
  isbn      = {0262026430, 9780262026437},
}

@Unpublished{Faerber,
  author = {Michael F{\"{a}}rber and Adam Jatowt},
  title  = {{Citation Recommendation for Scientific Publications}},
  review = {-  (!!!) argument for semantic approach (!!!)
    "One can envision that, in the mid-term, citation recommendation approaches can better capture the semantics of the citation context, with the result that actual fact-based citation recommendation becomes reality."
    (chapter 6 POTENTIAL FUTURE WORK)},
}

@Article{Tbahriti2006,
  author   = {Imad Tbahriti and Christine Chichester and Frédérique Lisacek and Patrick Ruch},
  title    = {Using argumentation to retrieve articles with similar citations: An inquiry into improving related articles search in the MEDLINE digital library},
  journal  = {International Journal of Medical Informatics},
  year     = {2006},
  volume   = {75},
  number   = {6},
  pages    = {488 - 495},
  issn     = {1386-5056},
  doi      = {https://doi.org/10.1016/j.ijmedinf.2005.06.007},
  keywords = {Argumentation, MEDLINE, Citation, Information storage and retrieval, Related article search},
  review   = {- find documents with similar citations
- classify abstract sentences into purpose, methods, results, conclusion
- "system could also be used as a platform to aid authors by means of automatic assembly or refinement of their bibliographies through the suggestion of citations coming from documents containing similar arguments"},
  url      = {http://www.sciencedirect.com/science/article/pii/S1386505605000894},
}

@Article{Duma2016,
  author  = {Daniel Duma and Ewan Klein and Maria Liakata and James Ravenscroft and Amanda Clare},
  title   = {Rhetorical Classification of Anchor Text for Citation Recommendation},
  journal = {D-Lib Magazine},
  year    = {2016},
  volume  = {22},
  review  = {- annotate citation contexts with CoreSC classes (hypothesis, motivation, background, result, conclusion, etc.)

- uses PubMed Open Access Subset (https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/)

- "a citation to a paper is accompanied by text that often summarizes a key point in the cited paper, or its contribution to the field. It has been found experimentally that there is useful information in these ILCs that is not found in the cited paper itself [17]"},
}

@InProceedings{Liakata2010,
  author    = {Maria Liakata and Simone Teufel and Advaith Siddharthan and Colin R. Batchelor},
  title     = {Corpora for the Conceptualisation and Zoning of Scientific Papers},
  booktitle = {LREC},
  year      = {2010},
  note      = {READ!},
  review    = {- "We only consider resolvable citations, that is, citations to references that point to a paper that is in our collection, which means we have access to its metadata and full machine-readable contents"},
}

@InProceedings{Kobayashi2018,
  author    = {Kobayashi, Yuta and Shimbo, Masashi and Matsumoto, Yuji},
  title     = {Citation Recommendation Using Distributed Representation of Discourse Facets in Scientific Articles},
  booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
  year      = {2018},
  series    = {JCDL '18},
  pages     = {243--251},
  address   = {New York, NY, USA},
  publisher = {ACM},
  note      = {r (ch 1-2)},
  acmid     = {3197059},
  doi       = {10.1145/3197026.3197059},
  isbn      = {978-1-4503-5178-2},
  keywords  = {co-citation analysis, discourse facet, natural language processing, representation learning, scientific article},
  location  = {Fort Worth, Texas, USA},
  numpages  = {9},
  review    = {- only recommend "co-citations": given context *AND ONE CITATION*, recommend more appropriate citations
- "the Citation Function Corpus [33]"},
  url       = {http://doi.acm.org/10.1145/3197026.3197059},
}

@InProceedings{Sugiyama2013,
  author    = {Sugiyama, Kazunari and Kan, Min-Yen},
  title     = {Exploiting Potential Citation Papers in Scholarly Paper Recommendation},
  booktitle = {Proceedings of the 13th ACM/IEEE-CS Joint Conference on Digital Libraries},
  year      = {2013},
  series    = {JCDL '13},
  pages     = {153--162},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2467701},
  doi       = {10.1145/2467696.2467701},
  isbn      = {978-1-4503-2077-1},
  keywords  = {citation analysis, collaborative filtering, digital library, information retrieval, recommendation},
  location  = {Indianapolis, Indiana, USA},
  numpages  = {10},
  review    = {- cold start problem: "with every new publication, new citation links are added to older work. In studies depending solely on the citation network, cutting- edge work is marginalized as they do not have any citations yet; this is a kind of 'cold-start problem'"
- old citations being old: "references and citations in a paper are static and never change, newer relevant papers to older ones have the 'responsibility' of creating a citation link between them"
-> introduce artificial links into citation network},
  url       = {http://doi.acm.org/10.1145/2467696.2467701},
}

@InProceedings{Livne2014,
  author    = {Livne, Avishay and Gokuladas, Vivek and Teevan, Jaime and Dumais, Susan T. and Adar, Eytan},
  title     = {CiteSight: Supporting Contextual Citation Recommendation Using Differential Search},
  booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research \&\#38; Development in Information Retrieval},
  year      = {2014},
  series    = {SIGIR '14},
  pages     = {807--816},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2609585},
  doi       = {10.1145/2600428.2609585},
  isbn      = {978-1-4503-2257-7},
  keywords  = {citation recommendation, differential search., personalization},
  location  = {Gold Coast, Queensland, Australia},
  numpages  = {10},
  review    = {- use MA(G)
- "citations are power law distributed" (long tail) (w/ citation)
    -> use "citation coupling" to enrich the citation graph},
  url       = {http://doi.acm.org/10.1145/2600428.2609585},
}

@Article{HERNANDEZ-ALVAREZ2016,
  author    = {HERNÁNDEZ-ALVAREZ, MYRIAM and GOMEZ, JOSÉ M.},
  title     = {Survey about citation context analysis: Tasks, techniques, and resources},
  journal   = {Natural Language Engineering},
  year      = {2016},
  volume    = {22},
  number    = {3},
  pages     = {327–349},
  note      = {"r"},
  doi       = {10.1017/S1351324915000388},
  publisher = {Cambridge University Press},
  review    = {- good overview (many citations) for citation context identification and citation function classification, etc.
- overview table with different schemes for classifying citation functions},
}

@Article{Moravcsik1975,
  author    = {Michael J. Moravcsik and Poovanalingam Murugesan},
  title     = {Some Results on the Function and Quality of Citations},
  journal   = {Social Studies of Science},
  year      = {1975},
  volume    = {5},
  number    = {1},
  pages     = {86--92},
  issn      = {03063127},
  publisher = {Sage Publications, Ltd.},
  review    = {- even earlier: (Garfield, 1964;  Weinstock, 1971)
- standard citation for citation function
- Teufel 2006: "A plethora of manual annotation schemes for citation motivation have been invented over the years. One of the best-known of these studies (Moravcsik and Murugesan, 1975) divides citations in running text into four dimensions: 
    - conceptual or operational use (i.e., use of theory vs. use of technical method)
    - evolutionary or juxtapositional (i.e., own work is based on the cited work vs. own work is analternative to it)
    - organic or perfunctory (i.e., work is crucially needed for understanding of citing article or just a general acknowledgement)
    - conformative vs.negational (i.e., is the correctness of the findings disputed?)
-> according to Petric2007 above categories are not all applicaple in all fields of research
    "Swales (1986), for instance, found that the first two criteria were irrelevant in the case of texts in applied linguistics."},
  url       = {http://www.jstor.org/stable/284557},
}

@InProceedings{Teufel2006a,
  author    = {Teufel, Simone and Siddharthan, Advaith and Tidhar, Dan},
  title     = {Automatic Classification of Citation Function},
  booktitle = {Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing},
  year      = {2006},
  series    = {EMNLP '06},
  pages     = {103--110},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1610091},
  isbn      = {1-932432-73-6},
  location  = {Sydney, Australia},
  numpages  = {8},
  review    = {- mby useful for claim based approach},
  url       = {http://dl.acm.org/citation.cfm?id=1610075.1610091},
}

@InProceedings{Teufel2006b,
  author    = {Simone Teufel and Advaith Siddharthan and Dan Tidhar},
  title     = {An annotation scheme for citation function},
  booktitle = {In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue},
  year      = {2006},
  review    = {- mby useful for claim based approach},
}

@InProceedings{Abujbara2013,
  author    = {Abu-Jbara, Amjad and Ezra, Jefferson and Radev, Dragomir},
  title     = {Purpose and Polarity of Citation: Towards NLP-based Bibliometrics},
  booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2013},
  pages     = {596--606},
  publisher = {Association for Computational Linguistics},
  location  = {Atlanta, Georgia},
  review    = {- citation function classes: criticizing, comparison, use, substantiating, basis, neutral (other)},
  url       = {http://aclweb.org/anthology/N13-1067},
}

@Article{Petric2007,
  author   = {Bojana Petrić},
  title    = {Rhetorical functions of citations in high- and low-rated master's theses},
  journal  = {Journal of English for Academic Purposes},
  year     = {2007},
  volume   = {6},
  number   = {3},
  pages    = {238 - 253},
  issn     = {1475-1585},
  abstract = {This study compares rhetorical citation functions in eight high- and eight low-graded master's theses in the field of gender studies, written in English as a second language. The following rhetorical functions of citations are identified: attribution, exemplification, further reference, statement of use, application, evaluation, establishing links between sources, and comparison of one's own work with that of other authors. It is shown that both sets of theses use citations predominantly for attribution, suggesting that one of the functions of citation in student writing is knowledge display. The use of citation for non-attribution functions is found to be considerably lower in the low-rated theses than in the high-rated theses, both in the whole theses and in individual chapters. The findings show that there is a relationship between citation use and thesis grade, thus pointing to the importance of effective citation strategies for students’ academic success. In conclusion, the paper argues that source use and citation skills should receive more attention in EAP instruction and suggests activities focusing on this area of academic writing.},
  doi      = {https://doi.org/10.1016/j.jeap.2007.09.002},
  keywords = {Citation, Master's thesis, Academic writing, High- and low-rated writing, English for academic purposes},
  review   = {- types of citation typologies: content based vs. based on formal criteria
- classifies Moravcsik1975 as a "content based citation typology"
-},
  url      = {http://www.sciencedirect.com/science/article/pii/S1475158507000379},
}

@InProceedings{Lamers2018,
  author    = {Lamers, Wout and Eck , Nees Jan van and Waltman, Ludo and Hoos, Holger},
  title     = {Patterns in citation context: the case of the field of scientometrics},
  booktitle = {STI 2018 Conference proceedings},
  year      = {2018},
  pages     = {1114--1122},
  publisher = {Centre for Science and Technology Studies (CWTS)},
  review    = {- detailed description for determining integral vs. non-integral citations:
    "For each citation, we determined whether it is integral or non-integral by processing their accompanying  sentence.
     First, the sentence was stripped of all citation labels. Subsequently, all characters between brackets were removed from
     the remaining character strings, leaving only the primary text of the citing sentence. We then searched this text for the
     last name of the first author of the cited paper. Citations were labelled integral if this author name was found, and
     non-integral if it was not."
    -> remove all "citation labels" (what ware citation labels? "[42]"?)
    -> remove all text in brackets (I assume remove brackets aswell)
    -> if remaining string contains last name of first author: integral

- integral vs. non-integral distinction on _presence of author name with grammatical role_ (as opposed to Thompson2001)
    - "[42] argues that ..." -> non-int},
  url       = {http://hdl.handle.net/1887/65235},
}

@Article{RAHUL2017,
  author    = {JHA, RAHUL and JBARA, AMJAD-ABU and QAZVINIAN, VAHED and RADEV, DRAGOMIR R.},
  title     = {NLP-driven citation analysis for scientometrics},
  journal   = {Natural Language Engineering},
  year      = {2017},
  volume    = {23},
  number    = {1},
  pages     = {93–130},
  doi       = {10.1017/S1351324915000443},
  publisher = {Cambridge University Press},
  review    = {- define "Reference Scope" (part(s) of citation context that is relevant for specific (out of many) cited doc)
    "We use the term Reference Scope to refer to the fragments of a sentence that are relevant to a specific target paper."
- look into automatic reference scope identification

--> paper only on reference scope: Abu-Jbara2012},
}

@Article{Swales1986,
  author  = {Swales, John},
  title   = {Citation Analysis and Discourse Analysis},
  journal = {Applied Linguistics},
  year    = {1986},
  volume  = {7},
  number  = {1},
  pages   = {39-56},
  doi     = {10.1093/applin/7.1.39},
  eprint  = {/oup/backfile/content_public/journal/applij/7/1/10.1093_applin_7.1.39/1/39.pdf},
  review  = {- "frequently expressed concern about the adequacy and reliability of simple and straightforward citation counting."

    - negative/critical citations != good (-> high count of citations not necessarily good)
    - *important work can be absorbed into the background knowledge of a subject and is then no longer referenced (e.g. Einstein 1905 for the Theory  of Relativity)
    - *morphological and syntactic evolutions:
         - discoveries  become  'named'  (Lotka's  Law,  the  Mossbauer Effect)
         - Proper Name adjectivization takes place (Widdowsonian dichotomies, a Sinclo-Coulthardian approach)
    - citations vary in length
    - are self-citations of the same value as citations by others?

    *relevant in the same way as some citations become obsolete because of newer findings},
  url     = {http://dx.doi.org/10.1093/applin/7.1.39},
}

@Book{Swales1990,
  title     = {Genre analysis: English in academic and research settings},
  publisher = {Cambridge University Press},
  year      = {1990},
  author    = {Swales, John},
  review    = {- distinction between integral and non-integral citations
    "An integral citation is one in which the name of the researcher occurs in the actual citing sentence as some sentence-element;
     in a non-integral citation, the researcher occurs either in parenthesis or is referred to elsewehre by a superscript number or via some other device"
    -> examples in following text; not on Google books but in UB
    
    - in Thompson2001's words: "His primary distinction is between citation forms that are non-integral and those that are integral:
      the former are citations that are outside the sentence, usually placed within brackets, and that play no explicit grammatical role in the sentence,
      while the latter are those that play an explicit grammatical role within a sentence."

    - is, in contrast to citation function, about the "surface form" of the citation
    -> relevant to cit. rec. when thinking of end user system where suggestions come in while typing (-> doesn't really work for integral citations)
    -> relevant to cit. rec. in any other aspect? analysis/representation of cit. contexts?},
}

@PhdThesis{Thompson2001,
  author = {Thompson, Paul},
  title  = {A pedagogically-motivated corpus-based examination of PhD theses: Macrostructure, citation practices and uses of modal verbs},
  school = {University of Reading},
  year   = {2001},
  review = {- Nice overview of integral vs. non-integral citation (Swales -> Hyland -> ...)

- (!!!) "Citations can also take the form of a number (rather than name and year) reference but none of the writers in the corpus use this style, and thus it was not considered in this study."
   -> integral vs. non-integral is more about having a grammatical role or not than about the authors name appearing or not
        "Swales has argued that ... [42]."                      -> which is the citation? "Swales"-> int / "[42]"->non
        "Swales (1990) has argued that ..."                   -> int
        "It has been argued (Swales 1990) that ..."       -> non
        "[42] argues that ..."                                         -> int

- integral vs. non-integral distinction on _having a grammatical role or not_ (as opposed to Lamers2018)
    - "[42] argues that ..." -> int

- wording not to use (wording in Okamura2008)
    integral: syntactically integrated citation
    non-integral: syntactically non-integrated
    -> b/c in both cases the citations are within the array of words that make up the sentence
         (semantically integrated also doesn't really work)
         (-> stick with _having a grammatical role or not_)},
}

@Article{Hyland1999,
  author  = {Hyland, K},
  title   = {Academic attribution: citation and the construction of disciplinary knowledge},
  journal = {Applied Linguistics},
  year    = {1999},
  volume  = {20},
  number  = {3},
  pages   = {341-367},
  doi     = {10.1093/applin/20.3.341},
  eprint  = {/oup/backfile/content_public/journal/applij/20/3/10.1093/applin/20.3.341/2/200341.pdf},
  review  = {- nice example sentences for integral vs. non-integral citations
    + mentions [42] style citations ("In the physical sciences, of course, journal styles often require numerical-
                                                      endnote forms, which reduces the prominence of cited authors considerably")
- per field ratio of integral vs. non-integral for corpus of 80 research articles
    - e.g.: bio 10/90, phys 17/83, philosophy 65/35},
  url     = {http://dx.doi.org/10.1093/applin/20.3.341},
}

@Article{Ayala-Gomez2018,
  author  = {Ayala-Gómez, Frederick and Daróczy, Bálint and Benczúr, András and Mathioudakis, Michael and Gionis, Aristides},
  title   = {Global citation recommendation using knowledge graphs},
  journal = {Journal of Intelligent \& Fuzzy Systems},
  year    = {2018},
  volume  = {34},
  pages   = {3089-3100},
  month   = {05},
  doi     = {10.3233/JIFS-169493},
  review  = {- global citation recommendation based on abstract
- use LambdaMart (gradient boosted tree based list-wise learning to rank algo)

- use similarity of words in abstract for recommendation candidate preselection (sensible?)
- evalute only in settings "random split within one year" and "recommend from the previous year" (sensible? / realistic?)},
}

@Article{White2004,
  author  = {White, Howard D.},
  title   = {Citation Analysis and Discourse Analysis Revisited},
  journal = {Applied Linguistics},
  year    = {2004},
  volume  = {25},
  number  = {1},
  pages   = {89-116},
  doi     = {10.1093/applin/25.1.89},
  eprint  = {/oup/backfile/content_public/journal/applij/25/1/10.1093/applin/25.1.89/2/250089.pdf},
  review  = {- nice table showing "separate disciplines with traditions of citation analysis" (applied linguistics, history and sociology of science, information science)
- according to RAHUL2017: "White (2004) provides a good survey of the different research directions that study or use citations."},
  url     = {http://dx.doi.org/10.1093/applin/25.1.89},
}

@InProceedings{Abujbara2012,
  author    = {Abu-Jbara, Amjad and Radev, Dragomir},
  title     = {Reference Scope Identification in Citing Sentences},
  booktitle = {Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2012},
  series    = {NAACL HLT '12},
  pages     = {80--90},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {2382041},
  isbn      = {978-1-937284-20-6},
  location  = {Montreal, Canada},
  numpages  = {11},
  review    = {- get best results for sentence segment classification approach},
  url       = {http://dl.acm.org/citation.cfm?id=2382029.2382041},
}

@Article{Stab2016,
  author        = {Christian Stab and Iryna Gurevych},
  title         = {Parsing Argumentation Structures in Persuasive Essays},
  journal       = {CoRR},
  year          = {2016},
  volume        = {abs/1604.07370},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/StabG16},
  eprint        = {1604.07370},
  review        = {- Michael: "good survey"
- TODO: read as intro to argumentation mining},
  timestamp     = {Mon, 13 Aug 2018 16:46:30 +0200},
  url           = {http://arxiv.org/abs/1604.07370},
}

@Article{Habernal2017,
  author     = {Habernal, Ivan and Gurevych, Iryna},
  title      = {Argumentation Mining in User-generated Web Discourse},
  journal    = {Comput. Linguist.},
  year       = {2017},
  volume     = {43},
  number     = {1},
  pages      = {125--179},
  month      = apr,
  issn       = {0891-2017},
  acmid      = {3097259},
  address    = {Cambridge, MA, USA},
  doi        = {10.1162/COLI_a_00276},
  issue_date = {April 2017},
  numpages   = {55},
  publisher  = {MIT Press},
  review     = {- Michael: "good survey" (see detailed notes in wiki)
- citation for (the start of) the field of argumentation mining
- makes an argument for basing applied NLP in argumentation mining on "substantial research in argumentation itself"
- -> citation (van Eemeren et al. 2014 ) that there's no real consensus on a theory of argumentation yet

- chapter on theoretical background of argumentation, argumentation models, etc.

- -> what citation recommendation is interested in within a citation context works with a "micro-level model" of argumentation
- Original Toulmin’s model of argument with a nice graph and example
- -> modift Toulmin’s model for their purposes (no qualifier, no warrant, calling grounds "premise" etc.)},
  url        = {https://doi.org/10.1162/COLI_a_00276},
}

@Article{Lippi2016,
  author     = {Lippi, Marco and Torroni, Paolo},
  title      = {Argumentation Mining: State of the Art and Emerging Trends},
  journal    = {ACM Trans. Internet Technol.},
  year       = {2016},
  volume     = {16},
  number     = {2},
  pages      = {10:1--10:25},
  month      = mar,
  issn       = {1533-5399},
  acmid      = {2850417},
  address    = {New York, NY, USA},
  articleno  = {10},
  doi        = {10.1145/2850417},
  issue_date = {April 2016},
  keywords   = {Argumentation mining, artificial intelligence, computational linguistics, knowledge representation, machine learning, social media},
  numpages   = {25},
  publisher  = {ACM},
  review     = {- nice overview tables for argumentation mining},
  url        = {http://doi.acm.org/10.1145/2850417},
}

@InProceedings{Zhang2017,
  author    = {Zhang, Sheng and Rudinger, Rachel and Durme, Benjamin Van},
  title     = {An Evaluation of PredPatt and Open IE via Stage 1 Semantic Role Labeling},
  booktitle = {IWCS 2017 --- 12th International Conference on Computational Semantics --- Short papers},
  year      = {2017},
  review    = {- argument for PredPatt over OpenIE 4.0 (for claim extraction), but also shows that among OpenIE implementations, OpenIE 4.0 compares quite good (side note: running on shetland is OpenIE 5.0 https://github.com/dair-iitd/OpenIE-standalone)
- paper quite sparse on details
- PredPatt is language independent and works with patterns (paper is on refining these patterns)},
  url       = {http://aclweb.org/anthology/W17-6944},
}

@InProceedings{Stanovsky2016,
  author    = {Stanovsky, Gabriel and Dagan, Ido},
  title     = {Creating a Large Benchmark for Open Information Extraction},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  year      = {2016},
  pages     = {2300--2305},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/D16-1252},
  location  = {Austin, Texas},
  review    = {- citation for OpenIE 4.0 performing comparatively well (side note: running on shetland is OpenIE 5.0 https://github.com/dair-iitd/OpenIE-standalone)},
  url       = {http://aclweb.org/anthology/D16-1252},
}

@Article{Okamura2008,
  author  = {Okamura, Akiko},
  title   = {Citation forms in scientific texts: Similarities and differences in L1 and L2 professional writing},
  journal = {Nordic Journal of English Studies},
  year    = {2008},
  volume  = {7},
  number  = {3},
  pages   = {61--81},
  review  = {- integral vs. non-integral citations (study on 30 papers)},
}

@InProceedings{Niklaus2018,
  author    = {Niklaus, Christina and Cetto, Matthias and Freitas, Andr{\'e} and Handschuh, Siegfried},
  title     = {A Survey on Open Information Extraction},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  year      = {2018},
  pages     = {3866--3878},
  publisher = {Association for Computational Linguistics},
  location  = {Santa Fe, New Mexico, USA},
  review    = {- overview of information extraction approahces (relevant for: fact/claim based approach)},
  url       = {http://aclweb.org/anthology/C18-1326},
}

@InProceedings{Sordo2015,
  author    = {Sordo, Mohamed and Oramas, Sergio and Espinosa-Anke, Luis},
  title     = {Extracting Relations from Unstructured Text Sources for Music Recommendation},
  booktitle = {Natural Language Processing and Information Systems},
  year      = {2015},
  editor    = {Biemann, Chris and Handschuh, Siegfried and Freitas, Andr{\'e} and Meziane, Farid and M{\'e}tais, Elisabeth},
  pages     = {369--382},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {This paper presents a method for the generation of structured data sources for music recommendation using information extracted from unstructured text sources. The proposed method identifies entities in text that are relevant to the music domain, and then extracts semantically meaningful relations between them. The extracted entities and relations are represented as a graph, from which the recommendations are computed. A major advantage of this approach is that the recommendations can be conveyed to the user using natural language, thus providing an enhanced user experience. We test our method on texts from songfacts.com, a website that provides facts and stories about songs. The extracted relations are evaluated intrinsically by assessing their linguistic quality, as well as extrinsically by assessing the extent to which they map an existing music knowledge base. Finally, an experiment with real users is performed to assess the suitability of the extracted knowledge for music recommendation. Our method is able to extract relations between pair of musical entities with high precision, and the explanation of those relations to the user improves user satisfaction considerably.},
  isbn      = {978-3-319-19581-0},
  review    = {- use NEL + relation extraction for recommendation
- view depentency tree as undirected, then look for path between NEL entities
    - use words passed along path as "relation" between the entities (but filter heuristically for linguistically sensible relations)
    - paper includes POS rules that represent relevant/useful relations between serveral entitiy types},
}

@Article{Ley2009,
  author     = {Ley, Michael},
  title      = {DBLP: Some Lessons Learned},
  journal    = {Proc. VLDB Endow.},
  year       = {2009},
  volume     = {2},
  number     = {2},
  pages      = {1493--1500},
  month      = aug,
  issn       = {2150-8097},
  acmid      = {1687577},
  doi        = {10.14778/1687553.1687577},
  issue_date = {August 2009},
  numpages   = {8},
  publisher  = {VLDB Endowment},
  review     = {- paper on the design of the DBLP},
  url        = {https://doi.org/10.14778/1687553.1687577},
}

@Article{Garcia2012,
  author  = {Garcia, Alexander and Garcia, Leyla Jael and McLaughlin, Casey and Flager, Stephen},
  title   = {RDFising PubMed Central},
  journal = {Bio-ontologies, Long Beach},
  year    = {2012},
  review  = {- point out not all citations in PubMec Central (PMC) XMLs are matched to IDs (example: PMC2971765)},
}

@InProceedings{Duma2014,
  author    = {Duma, Daniel and Klein, Ewan},
  title     = {Citation resolution: A method for evaluating context-based citation recommendation systems},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  year      = {2014},
  volume    = {2},
  pages     = {358--363},
  review    = {- cite rec by similar sentences: "exploits  the implicit human relevance judgements found in existing scientific articles"

- citation recommendation by re-prediction, but candidates are only the set of cited docs of the paper},
}

@Article{Nasar2018,
  author   = {Nasar, Zara and Jaffry, Syed Waqar and Malik, Muhammad Kamran},
  title    = {Information extraction from scientific articles: a survey},
  journal  = {Scientometrics},
  year     = {2018},
  volume   = {117},
  number   = {3},
  pages    = {1931--1990},
  month    = {Dec},
  issn     = {1588-2861},
  abstract = {In last few decades, with the advent of World Wide Web (WWW), world is being overloaded with huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and efficient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scientific literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scientific articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scientific articles. The information insights extracted from scientific articles are classified in two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a significant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Na{\"i}ve-Bayes classification and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to specific information needs from scientific articles. Hence, in this study, state-of-the-art regarding information extraction from scientific articles is covered. This study also consolidates evolving datasets as well as various toolkits and code-bases that can be used for information extraction from scientific articles.},
  day      = {01},
  doi      = {10.1007/s11192-018-2921-5},
  review   = {- on IE for scientific text in general
- Reference Field Extraction as a form of Named Entity Recognition and Classification (NERC), "NERC task from references"
- survey tools, datasets, approaches, etc.},
  url      = {https://doi.org/10.1007/s11192-018-2921-5},
}

@InProceedings{Anzaroot2013,
  author    = {Sam Anzaroot and Andrew McCallum},
  title     = {A New Dataset for Fine-Grained Citation Field Extraction},
  booktitle = {ICML Workshop on Peer Reviewing and Publishing Models},
  year      = {2013},
}

@InProceedings{Tkaczyk2018,
  author    = {Tkaczyk, Dominika and Collins, Andrew and Sheridan, Paraic and Beel, Joeran},
  title     = {Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers},
  booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
  year      = {2018},
  series    = {JCDL '18},
  pages     = {99--108},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3197048},
  doi       = {10.1145/3197026.3197048},
  isbn      = {978-1-4503-5178-2},
  keywords  = {bibliographic reference parsing, citation parsing, machine learning, sequence tagging},
  location  = {Fort Worth, Texas, USA},
  numpages  = {10},
  review    = {- evaluation of reference string parsers
- "We  were  not  able  to  evaluate  three  tools,  due  to installation errors or missing resources: BibPro, Free_cite and Neural ParsCit." :F},
  url       = {http://doi.acm.org/10.1145/3197026.3197048},
}

@InProceedings{Bast2017,
  author    = {H. Bast and C. Korzen},
  title     = {A Benchmark and Evaluation for Text Extraction from PDF},
  booktitle = {2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
  year      = {2017},
  pages     = {1-10},
  month     = {June},
  doi       = {10.1109/JCDL.2017.7991564},
  keywords  = {information retrieval;text analysis;text extraction tool;PDF document;parallel TeX;arXiv.org;Icecite;electronic document formats;Portable document format;Semantics;Benchmark testing;Tools;Data mining;Libraries;Google},
  review    = {- use LaTeX to plain text to evaluate pdf2plain text tools},
}

@Article{Brown2001,
  author  = {Lawrence D. {Brown} and T. Tony {Cai} and Anirban {DasGupta}},
  title   = {Interval Estimation for a Binomial Proportion},
  journal = {Statistical Science},
  year    = {2001},
  volume  = {16},
  number  = {2},
  pages   = {101--133},
  review  = {- citaiton for confidence invervals for estimation of bibitem matching accuracy},
}

@InProceedings{Gipp2015,
  author    = {Gipp, Bela and Meuschke, Norman and Lipinski, Mario},
  title     = {CITREC : An Evaluation Framework for Citation-Based Similarity Measures based on TREC Genomics and PubMed Central},
  booktitle = {iConference 2015 Proceedings},
  year      = {2015},
  publisher = {iSchools},
  review    = {- parsing PubMed Central OA in text citations is hard b/c they're heterogenous:
    "The extraction of in-text citations from the PMC OAS documents posed some problems to parser  development. Among these challenges was the use of heterogeneous XML-markups
     for labeling in-text citations in the source files. For this reason, we incorporated eight differentmarkup variations into the parser."
    - details: http://www.sciplore.org/files/citrec/CITREC_Parser_Documentation.pdf
- furthermore different types of IDs are used:
    "Different  unique document identifiers, such as PubMed and MEDLINE identifiers (PMId, MEDId) or DOIs are commonly stated for references in the PMC OAS. Nevertheless, it cannot be
     assumed that identifiers are used consistently for all possible references. For instance, some authors might state no unique identifiers, some might use a PMId, some others a DOI or
    vice versa."},
  url       = {http://hdl.handle.net/2142/73680},
}

@Article{Berger2016,
  author  = {Berger, Matthew and McDonough, Katherine and M. Seversky, Lee},
  title   = {Cite2vec: Citation-Driven Document Exploration via Word Embeddings},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year    = {2016},
  volume  = {23},
  pages   = {1-1},
  month   = {01},
  doi     = {10.1109/TVCG.2016.2598667},
  review  = {- cite2vec (as title suggests) but used for the purpose discovery/visualization},
}

@InProceedings{He2018,
  author    = {Jian He and Chaomei Chen},
  title     = {Temporal Representations of Citations for Understanding the Changing Roles of Scientific Publications},
  booktitle = {Front. Res. Metr. Anal.},
  year      = {2018},
  review    = {- early version: https://arxiv.org/pdf/1711.05822.pdf
- "approach to understanding the changing roles of a publication characterized by its citation contexts"},
}

@InProceedings{Galke2018,
  author    = {Galke, Lukas and Mai, Florian and Vagliano, Iacopo and Scherp, Ansgar},
  title     = {Multi-Modal Adversarial Autoencoders for Recommendations of Citations and Subject Labels},
  booktitle = {Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization},
  year      = {2018},
  series    = {UMAP '18},
  pages     = {197--205},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3209236},
  doi       = {10.1145/3209219.3209236},
  isbn      = {978-1-4503-5589-6},
  keywords  = {adversarial autoencoders, multi-modal, neural networks, recommender systems, sparsity},
  location  = {Singapore, Singapore},
  numpages  = {9},
  review    = {- use PMC Open Access Subset (same as Gipp2015)},
  url       = {http://doi.acm.org/10.1145/3209219.3209236},
}

@InProceedings{Gipp2010,
  author    = {Gipp, Bela and Beel, J\"{o}ran},
  title     = {Citation Based Plagiarism Detection: A New Approach to Identify Plagiarized Work Language Independently},
  booktitle = {Proceedings of the 21st ACM Conference on Hypertext and Hypermedia},
  year      = {2010},
  series    = {HT '10},
  pages     = {273--274},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1810671},
  doi       = {10.1145/1810617.1810671},
  isbn      = {978-1-4503-0041-4},
  keywords  = {citation analysis, citation order analysis, duplicate detection, language independent, plagiarism detection},
  location  = {Toronto, Ontario, Canada},
  numpages  = {2},
  review    = {- language independent plagiarism detection based on citation order},
  url       = {http://doi.acm.org/10.1145/1810617.1810671},
}

@InProceedings{Beel2013,
  author    = {Beel, Joeran and Langer, Stefan and Genzmehr, Marcel and Gipp, Bela and Breitinger, Corinna and N\"{u}rnberger, Andreas},
  title     = {Research Paper Recommender System Evaluation: A Quantitative Literature Survey},
  booktitle = {Proceedings of the International Workshop on Reproducibility and Replication in Recommender Systems Evaluation},
  year      = {2013},
  series    = {RepSys '13},
  pages     = {15--22},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2532512},
  doi       = {10.1145/2532508.2532512},
  isbn      = {978-1-4503-2465-6},
  keywords  = {comparative study, evaluation, recommender systems, research paper recommender systems, survey},
  location  = {Hong Kong, China},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/2532508.2532512},
}

@Article{Beel2016,
  author   = {Beel, Joeran and Gipp, Bela and Langer, Stefan and Breitinger, Corinna},
  title    = {Research-paper recommender systems: a literature survey},
  journal  = {International Journal on Digital Libraries},
  year     = {2016},
  volume   = {17},
  number   = {4},
  pages    = {305--338},
  month    = {Nov},
  issn     = {1432-1300},
  abstract = {In the last 16 years, more than 200 research articles were published about research-paper recommender systems. We reviewed these articles and present some descriptive statistics in this paper, as well as a discussion about the major advancements and shortcomings and an overview of the most common recommendation concepts and approaches. We found that more than half of the recommendation approaches applied content-based filtering (55 {\%}). Collaborative filtering was applied by only 18 {\%} of the reviewed approaches, and graph-based recommendations by 16 {\%}. Other recommendation concepts included stereotyping, item-centric recommendations, and hybrid recommendations. The content-based filtering approaches mainly utilized papers that the users had authored, tagged, browsed, or downloaded. TF-IDF was the most frequently applied weighting scheme. In addition to simple terms, n-grams, topics, and citations were utilized to model users' information needs. Our review revealed some shortcomings of the current research. First, it remains unclear which recommendation concepts and approaches are the most promising. For instance, researchers reported different results on the performance of content-based and collaborative filtering. Sometimes content-based filtering performed better than collaborative filtering and sometimes it performed worse. We identified three potential reasons for the ambiguity of the results. (A) Several evaluations had limitations. They were based on strongly pruned datasets, few participants in user studies, or did not use appropriate baselines. (B) Some authors provided little information about their algorithms, which makes it difficult to re-implement the approaches. Consequently, researchers use different implementations of the same recommendations approaches, which might lead to variations in the results. (C) We speculated that minor variations in datasets, algorithms, or user populations inevitably lead to strong variations in the performance of the approaches. Hence, finding the most promising approaches is a challenge. As a second limitation, we noted that many authors neglected to take into account factors other than accuracy, for example overall user satisfaction. In addition, most approaches (81 {\%}) neglected the user-modeling process and did not infer information automatically but let users provide keywords, text snippets, or a single paper as input. Information on runtime was provided for 10 {\%} of the approaches. Finally, few research papers had an impact on research-paper recommender systems in practice. We also identified a lack of authority and long-term research interest in the field: 73 {\%} of the authors published no more than one paper on research-paper recommender systems, and there was little cooperation among different co-author groups. We concluded that several actions could improve the research landscape: developing a common evaluation framework, agreement on the information to include in research papers, a stronger focus on non-accuracy aspects and user modeling, a platform for researchers to exchange information, and an open-source framework that bundles the available recommendation approaches.},
  day      = {01},
  doi      = {10.1007/s00799-015-0156-0},
  review   = {- large survey},
  url      = {https://doi.org/10.1007/s00799-015-0156-0},
}

@Article{Beel2016a,
  author   = {Beel, Joeran and Breitinger, Corinna and Langer, Stefan and Lommatzsch, Andreas and Gipp, Bela},
  title    = {Towards reproducibility in recommender-systems research},
  journal  = {User Modeling and User-Adapted Interaction},
  year     = {2016},
  volume   = {26},
  number   = {1},
  pages    = {69--101},
  month    = {Mar},
  issn     = {1573-1391},
  abstract = {Numerous recommendation approaches are in use today. However, comparing their effectiveness is a challenging task because evaluation results are rarely reproducible. In this article, we examine the challenge of reproducibility in recommender-system research. We conduct experiments using Plista's news recommender system, and Docear's research-paper recommender system. The experiments show that there are large discrepancies in the effectiveness of identical recommendation approaches in only slightly different scenarios, as well as large discrepancies for slightly different approaches in identical scenarios. For example, in one news-recommendation scenario, the performance of a content-based filtering approach was twice as high as the second-best approach, while in another scenario the same content-based filtering approach was the worst performing approach. We found several determinants that may contribute to the large discrepancies observed in recommendation effectiveness. Determinants we examined include user characteristics (gender and age), datasets, weighting schemes, the time at which recommendations were shown, and user-model size. Some of the determinants have interdependencies. For instance, the optimal size of an algorithms' user model depended on users' age. Since minor variations in approaches and scenarios can lead to significant changes in a recommendation approach's performance, ensuring reproducibility of experimental results is difficult. We discuss these findings and conclude that to ensure reproducibility, the recommender-system community needs to (1) survey other research fields and learn from them, (2) find a common understanding of reproducibility, (3) identify and understand the determinants that affect reproducibility, (4) conduct more comprehensive experiments, (5) modernize publication practices, (6) foster the development and use of recommendation frameworks, and (7) establish best-practice guidelines for recommender-systems research.},
  day      = {01},
  doi      = {10.1007/s11257-016-9174-x},
  url      = {https://doi.org/10.1007/s11257-016-9174-x},
}

@Article{Beel2017,
  author        = {J{\"{o}}ran Beel and Siddharth Dinesh},
  title         = {Real-World Recommender Systems for Academia: The Pain and Gain in Building, Operating, and Researching them [Long Version]},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1704.00156},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/BeelD17},
  eprint        = {1704.00156},
  timestamp     = {Mon, 13 Aug 2018 16:48:46 +0200},
  url           = {http://arxiv.org/abs/1704.00156},
}

@Article{Beel2017a,
  author        = {J{\"{o}}ran Beel},
  title         = {It's Time to Consider "Time" when Evaluating Recommender-System Algorithms [Proposal]},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1708.08447},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1708-08447},
  eprint        = {1708.08447},
  review        = {- probably woth mentioning when talking about evaluation of proposed approaches},
  timestamp     = {Mon, 13 Aug 2018 16:48:19 +0200},
  url           = {http://arxiv.org/abs/1708.08447},
}

@Article{Yokoi2009,
  author        = {Yokoi, Keisuke and Aizawa, Akiko},
  title         = {An approach to similarity search for mathematical expressions using MathML},
  journal       = {Towards a Digital Mathematics Library. Grand Bend, Ontario, Canada, July 8-9th, 2009},
  year          = {2009},
  pages         = {27--35},
  __markedentry = {[tarek:]},
  publisher     = {Masaryk University Press},
  review        = {- relevant with regards to filtering out formula environments
also see
2016: https://link.springer.com/article/10.1007/s11786-016-0274-0
2014: https://link.springer.com/chapter/10.1007/978-3-319-08434-3_29
2012: https://www.jstage.jst.go.jp/article/jsik/22/3/22_22_253/_article/-char/ja/},
}

@Comment{jabref-meta: databaseType:bibtex;}
