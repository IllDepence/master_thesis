\chapter{Introduction}\label{chap:introduction}
\section{Motivation}
Citations are a central building block of scholarly discourse. They are the means by which scholars relate their research to existing work---be it in backing up claims, criticising, naming examples or engaging in any other form. Citing in a meaningful way requires an author to be aware of publications relevant to their work.
Here, the ever increasing amount of new reseach publications per year poses a serious challenge. Even with academic search engines like Goolge Scholar and CiteSeer\textsuperscript{x} at our disposal, identifying publications that are worthwhile to examine and appropriate to reference remains a time consuming task.

It is therefore not suprising that methods to aid researchers in these tasks have been and still are being actively researched. While diverse in nature, the common core of these efforts is the goal to utilize the automated processing of publications. This can be achieved by either extracting information from publications~\cite{Nasar2018,Beel2016}, or by adding semantic annotations or derivate representations to facilitate automated processing~\cite{BuckinghamShum2000,Peroni2012,Huh2014,Jaradeh2019}. % TODO: mby distinguish between metadata and annotation/representation of content here (for the latter mby name JATS)
Once processed, a prevalent method is to harvest human made citations, analyze them~\cite{Abujbara2013,Teufel2006a} and use them for example to recommend papers~\cite{Beel2016} or aid in document exploration~\cite{Berger2016}. Although systems like this have existed for over 20 years~\cite{Bollacker1998,Beel2016}, there is not a lot of work looking into the use of explicit semantic representations for the recommendation of papers.
% ,Kitamoto2015 (after BuckinghamShum and Schneider)
% TODO: mby add sth about what the prospective benefits of semantic representations would be (e.g. if rather advanced model: search for citations for certain claims instead of just keyword based)
This is why this thesis will investiage their application. More specifically, we will concentrate on the task of recommending papers for citation---as opposed to, for example, discovery. What this encompasses will be described in more detail in the following section.

%Teufel2006b

% Systems for recommending papers have existed since 1998~\cite{Bollacker1998,Beel2016}. The closely related field of citation analysis has an even longer history that spans multiple disciplices including applied linguistics, history and sociology of sience and information science~\cite{Swales1986,White2004}.

% also thereâ€™s different approaches to it (collab.fil. / content based fil.(input=paper / input=sentence / input=sentence+one citation\cite{Kobayashi2018} / input=abstract\cite{Ayala-Gomez2018}) / graph based)

\section{Problem setting}\label{sec:problemsetting}
In the boradest sense, recommending papers for citation means given an input text, suggest publications that can be referred to from within that text. In scale this can vary from specific recommendations for a section of a sentence (\emph{local} or \emph{context-based}), to general recommendations for a whole input document (\emph{global}). The task can also include deciding whether or not the input contains parts that would justify inserting a citation in the first place~\cite{He2011}. In this thesis, we will focus on local citation recommendation with the assumption that the input always allows for/requires a citation to be put in.

Another distinction to be made is between personalized and general citation recommendation. Some approaches make use of user specific information such as an author's prior citations. Collaborative filtering approaches by nature include a user model and therfore fall into this category. While personalization can improve recommendation, it limits the approach to users that are willing to share personal information. % also if prior own publications would be needed (to see what authors, venues, fields of study one cites) there would be an interesting version of the could start problem. worth mentioning?
We therefore limit ourselves to purely content based filtering approaches.
% TODO: add argument for only using contexts to describe cited docs and not also title, abstract, metadata (mby cite \cite{Elkiss2008}) (possible argument: first only investigate semantic representations of citation contexts, then (future work) look into combining this information with other data)

A last clarification has to be made concerning the term \emph{explicit semantic representations}. This is to be understood as a differentiation from the mere use of unstructured text. A most prominent example for explicit semantic representations would be the structure of the Semantic Web~\cite{Berners-Lee2001}. In the context of citation recommendation as briefly outlined above, this means representing citations in a semantically meaningful way as opposed to just relying on syntactical information like n-grams or bag-of-words representations.

The problem setting can be summarized as follows. To investigate is, the applicability of and requirements for the use of explicit semantic representations for content based, local citation recommendation. The following section will outline how this investigation is performed.

\section{Method}\label{sec:method}
In order to assess if and how explicit semantic representations can benefit citation recommendation, we define several models that encode different semantic aspects of citations, and evaluate them against a non-semantic baseline. For the development and evaluation of these models we generate a data set with precise and accurate citation information, that is also large enough for a realistic assessment. As far as possible, the models are furthermore evaluated on existing data sets.

% In order to assess if and how explicit semantic representations can benefit citation recommendation, we investigate the use of named entities as well as claim structures. This is done by developing representational models based on named entities and claim structures and comparing their performance against a baseline model. For an evaluation in a realistic setting, we generate a large data set with precise citation marker positions and accurate interlinking of documents. To ensure wide applicability and increase comparability with other approaches, we also perform evaluations on existing data sets as far as possible. To gain further insight into the performance of our models as well as the nature of our evaluation data, we also conduct a user study where recommendations are directly judged by human participants.
Our models are based on named entities as well as claim structures contained in citation contexts, and are evaluated against a bag-of-words baseline. The assesment encompasses an offline evaluation on several data sets as well as a user study. Evaluating over several data sets ensures wide applicability and increases comparability with other approaches. The additional user study gives further insight into the data used, and provides reliable judgement of performance, albeit on a smaller scale. Viewing our evaluation results through a selection of differently natured metrics, we can draw conclusions on the applicability of our models for various tasks.

\section{Contributions}\label{sec:contributions}
The data set: accurate citation information, variable context length allowing for experiments with that, several fields of study enabling comparative analysis

Entity and claim based models

Insights into open problems with building claim models around citations (b/c of non-syntactic citation styles)

\section{Document structure}\label{sec:documentstructure}
foo bar
