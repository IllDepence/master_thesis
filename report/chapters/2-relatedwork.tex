\chapter{Related Work}\label{chap:relatedwork}

\section{Semantic approaches to recommendation}

At a point in time where publishing research papers online was an emerging trend, Middleton et al.~\cite{Middleton2001} propose a system for paper recommendation making use of a topic ontology. Based on classifying papers into topics and recording which papers a researcher would access on the web, they employ content-based filtering, collaborative filtering and a feedback mechanism to suggest papers from new topics to users. Comparing the topic ontology to a flat list of topics in two user studies, they report 7--15\% more user satisfaction for the ontology case.
% \cite{Middleton2004} reports on new/extended system using CORA as ontology base (evaluation is somewhat convoluted though)

In a similar vein, Zhang et al.~\cite{Zhang2008} propose a hybrid recommender system for papers based on semantic concept similarity. They derive concepts from CiteULike\footnote{See \url{http://citeulike.org/}.} tags and use these to measure the semantic similarity of papers and users' interest. In their evaluation they compare different settings of the approach but do not compare to other work or alternative techniques.

Jiang et al.~\cite{Jiang2012} use CiteULike tags as academic concepts to build a topic model applied to paper abstracts. In a content-based recommendation setting they let volunteers judge the relevance of recommendations for a test set of 30 papers. The evaluation includes a TFIDF baseline, latent Dirichlet allocation (LDA) and an approach combining LDA with their concept model. The reported MAP@5 and nDCG@5 values are best for the LDA+concept method.

In \cite{Zarrinkalam2012} Zarrinkalam et al. enrich their metadata on research papers using multiple Linked Open Data (LOD) sources to drive a hybrid recommender system. They compare a purely content-based method using only text similarity with a second method additionally utilizing collaborative filtering and a third method furthermore using the LOD enriched data. They report recall, co-cited probability and nDCG values for various cut-off values for which the LOD enriched method consistently achieves the best performance.

With SemCiR~\cite{Zarrinkalam2013} Zarrinkalam et al. introduce a content-based, global citation recommendation approach that utilizes a semantic distance measure between papers. They furthermore introcude a method for extending the measure to determine the semantic distance between an input text and a paper, which is achieved by representing the input by textually similar papers. The distance measure suggested builds on six different relational features including shared authors, venue, and overlapping in- and outgoing citations. The approach is evaluated on a 12,500 paper subset of CiteSeerX~\cite{Caragea2014} in a citation re-prediction setting, using as input a paper's title, abstract and contexts in other papers where it was referred to. An evaluation of different scenarios measuring recall, co-cited probability and nDCG leads the authors to conclude that recommendation results can be improved by using their semantic distance measure and including citation contexts in the measurement textual similarity.

% non paper/cit:

% Using NEL + dependency trees for music recommendation\cite{Sordo2015}

% Leveraging Semantic Annotations to Link Wikipedia and News Archives\cite{Mishra2016}

% DiNoia\cite{DiNoia2012}

\section{Local citation recommendation}

Probably one of the first investigations into local citation recommendation is the work of He et al.~\cite{He2010}. They propose a two-step system that first identifies recommendation candidates and then re-ranks them by concept similarity. While also discussing global citation recommendation in detail, for the local case they compare recommending for a single context and recommending for all contexts within a document simultaneously. In an evaluation on the CiteSeerX data set measured by recall, co-cited probability and nDCG they find that the single context task is harder, but also, that their approach to the all contexts task achieves results comparable to and even better than some global citation recommendation methods.

In a follow-up work Huang et al.~\cite{Huang2014} build upon above work by swapping out the computationally complex concept based re-ranking method with a translational model. In this model citation contexts are treated as the source language and cited papers as words in the target language. The resulting system, RefSeer, is evaluated on two smaller data sets (CiteULike and a CiteSeer subset) and one large one (all of CiteSeer). The authors report precision, recall, Bpref and MRR values for the two smaller data sets and conclude that their system can give correct recommendations in a realistic setting---such as when only the top 10 recommendations are shown.

Huang et al. improve RefSeer with a neural probabilistic model that learns distributed representations of words and documents in \cite{Huang2015}. They evaluate their model for local citation recommendation on the whole of CiteSeer, splitting between train and test set at the year 2011 (9M contexts train, 1.5M contexts test). Measuring MAP, MRR and nDCG they show that their model outperforms 4 different state-of-the-art approaches. An analysis on the influence of papers' citation counts on recommendation performance shows that their approach especially exceeds other work in case of lesser cited papers (<100 citations).

In \cite{Duma2014} Duma et al. test the effectiveness of using a variety of document internal and external text inputs to a TF-IDF model. Their data set is built from the ACL Anthology and contains 5446 citations. In a re-prediction setting the authors measure how reliably their models rank the correct paper at the top position. % @"how reliably" construct: authors say "accuracy" but is the term used correctly?
They conclude that a mixture of internal and external inputs outperforms either of the aforementioned used on their own.

The work presented in \cite{Duma2016} by Duma et al. focusses on the rethorical function of sentences. The authors classify sentences using the Core Scientific Concepts (CoreSC) scheme and investigate how their distinction can be used to improve recommendation. Evaluating on one million papers from the PubMed Central Open Access Subset they measure NDCG values and find that for several classes of input sentences significant gains in recommendation quality can be made by focussing on certain rethoric passages of candidate documents when ranking text similarity.

Ebesu~\cite{Ebesu2017} see Jabref notes

Kobayashi~\cite{Kobayashi2018}
