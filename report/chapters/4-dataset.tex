\chapter{Data set}\label{chap:dataset}
Recommender systems rely on data for their development, training and evaluation. It is therefore important to properly assess potential data sets in terms of their strengths and shortcomings---especially with regards to the task at hand. In citation recommendation, the goal is to identify papers relevant to a user input. Because of the large amount of available research, this means a recommender has to be able to find relevant publications in a large set of possible candidates in order to be considered fit for the task. As a consequence, evaluation results are likely to be more meaningful when a large data set is used. Apart from the size, the quality of data is also crucial. For local citation recommendation, this means that a clean citation context, precise position of citation markers and valid citation information are desirable. With these criteria in mind we assessed existing data sets and come to the conclusion that, for the relatively new task of local citation recommendation, the creation of a dedicated data set will bring considerable benefits.

The following sections describe the details of our assessment as well as the creation process and evaluation of our new data set.

\section{Existing data sets}

Table~\ref{tab:datasets} gives an overview of relevant existing data sets. While various recommendation domains have established quasi standard data sets, this is not yet the case in citation recommendation. CiteSeer\textsuperscript{x} is currently the most used in the field~\cite{Beel2016}. It is comparatively large, but many approaches only use subsets and generate them with varying filtering criteria. It includes pre-extracted citation contexts of 400 characters in length, whereby references are resolved to an internal set of identifiers. Unfortunately there are several quality issues with the data set. The main ones being inaccurate citation information, noisy citation contexts and cut off words at the borders of citation contexts~\cite{Roy2016}.

\begin{table}
\centering
    \caption[Overview of existing data sets.]{Overview of existing data sets.\\Listed are the number of papers, nature of citation contexts, covered disciplines of citing papers and the type of global reference identifiers.\\(\emph{extractable*} is to indicate that extraction might be error-prone due to papers only being available in PDF format.)}
    \label{tab:datasets}
\begin{center}
    % \caption{Overview of existing data sets.}
    % \label{tab:existing-data-sets}
    \begin{tabular}{lllll}
    \toprule
    Data set & \#Papers & Cit. context & Disciplines & Ref. IDs \\
    \midrule
    CiteSeer\textsuperscript{x}~\cite{Caragea2014} / RefSeer~\cite{Huang2014} & 5M & 400 chars & (unrestricted) & internal \\
    PubMed Central OAS\footnote{See \url{https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/}.} & 2.3M & extractable & Biomed./Life Sci. & mixed \\
    Public Library of Science\footnote{See \url{https://www.plos.org/text-and-data-mining}.} & 200k & extractable & mostly Biol./Med. & - \\
    arXiv CS~\cite{Faerber2018} &  90K & 1 sentence & CS & DBLP \\
    Scholarly~\cite{Sugiyama2013}  & 100k & extractable* & CS & - \\
    ACL-AAN~\cite{Radev2013} & 18K & extractable* & CS/Comp. Ling. & -  \\
    ACL-ARC~\cite{Bird2008} & 11K & extractable* & CS/Comp. Ling. & - \\
    \bottomrule
    \end{tabular}
\end{center}
\end{table}


The PubMed Central Open Access Subset (PMC-OAS) is another large data set that has been used for citation based tasks~\cite{Duma2016,Gipp2015,Galke2018,Bhagavatula2018}. Contained publications are already processed and available in the JATS\cite{Huh2014} XML format. While the data set overall is comparatively clean, heterogeneous annotation of citations within the text and mixed usage of global reference identifiers (PubMed, MEDLINE, DOI, ...) make it difficult to retrieve high quality citation interlinkings of documents from the data set\footnote{To be more precise, the heterogeneity makes the usage of the data set \emph{as is} problematic. Resolving references retrospectively would be an option but comparatively challenging in the case of PubMed because of the frequent usage of special notation in publication titles; see also: \url{http://www.sciplore.org/files/citrec/CITREC_Parser_Documentation.pdf}.}~\cite{Gipp2015}.

Similar to the PMC-OAS, albeit smaller, the Public Library of Science (PLOS) offers publications in the same XML format and has seen application for citation based tasks~\cite{Bertin2016,Bertin2018}. In contrast to the PMC-OAS, references are in general not resolved to any global identifier.

Consistent global reference identifiers are given in the arXiv CS data set in the form of DBLP IDs. Linking to an existing repository of publication (meta) data has the advantage that information about cited papers in readily available. The choice of DBLP restricts resolved references to the field of computer science though. Citations to, for example, publications in maths or statistics can not be resolved to a DBLP ID. A strength of the data set is that it was generated from \LaTeX{} source files, which makes it possible to get very clean data.

For the remaining the data sets---Scholarly, ACL-AAN and ACL-ARC---citing papers are only available in PDF format and references are not resolved. The two ACL sets have the additional drawback of being very small.

Above observations lead us to the conclusion that it would be worthwhile to tackle the creation of a data set that is large (in the order of CiteSeer\textsuperscript{x}/RefSeer/PMC-OAS), clean (like the PMC-OAS and arXiv CS) and also offers consistent global reference IDs that don't restrict the data set to citations within the same discipline. The creation and evaluation of this data set is described in the following sections.

% TODO: mby add MAG use/analysis: \cite{Herrmannova2016,Paszcza2016,Hug2017}

% TODO: add: evaluation of reference string parsers\cite{Tkaczyk2018}, a dataset for reference string parsing\cite{Anzaroot2013}

\section{Data set creation}\label{sec:data-set-creation}
Scientific publications are usually distributed in formats targeted at human consumption (e.g. PDF) or, in cases like arXiv, also as source files \emph{for} the aforementioned (e.g. \LaTeX{} sources for generating PDFs). Citation-based tasks, such as local citation recommendation, in contrast, require automated processing of the publications' textual contents as well as the documents' interlinking through citations. The creation of a data set for such tasks therefore encompasses two main steps: extraction of plain text and resolution of references. In the following we will describe how we approached these two steps using arXiv publication sources and the Microsoft Academic Graph (MAG)~\cite{Sinha2015}.

\subsection{Used data sets}

The following two resources are the basis of the data set creation process.

\paragraph{arXiv.org} hosts over 1.4 million submissions from August 1991 onward~\cite{Ginsparg1994}. They are available not only as PDF, but (in most cases) also as \LaTeX{} source files. The discipline most prominently represented is physics, followed by mathematics, with computer science seeing a continued increase in percentage of submissions ranking third (see Figure~\ref{fig:sankey}). The availability of \LaTeX{} sources makes arXiv submissions particularly well suited for extracting high quality plain text and accurate citation information. So much so, that it has been used to generate ground truths for the evaluation of PDF to text conversion tools~\cite{Bast2017}. Approaches to automatically extract citation interlinks from arXiv sources by parsing \LaTeX{} files have existed for over 20 years \cite{Nanba1998}. Nevertheless we are not aware of any existing data sets for citation based tasks generated from the whole of arXiv.

\paragraph{Microsoft Academic Graph (MAG)} is a very large\footnote{At the time of writing the MAG contains data on over 200 million publications.}, automatically generated data set on publications, related entities (authors, venues, etc.) and their interconnections through citation. While citation contexts of varying length are available to some degree, full text documents are not. The size of the MAG makes it a good target for matching reference strings against it, especially given that arXiv spans several fields of study.

\subsection{Pipeline overview}

As depicted in Figure~\ref{fig:datagen}, we start out with arXiv sources to create the data set. From these we generate, per publication, a plain text file with the document's textual contents and a set of database entries reflecting the document's reference section. Association between references and citations in the text are preserved by placing citation markers in the text. In a second step, we then iterate through all references in the database and match them against paper metadata records in the MAG. The result of this process are MAG paper records associated with one or more references, which in turn are associated with citation contexts in the plain text files. In other words, we end up with cited documents described by their MAG metadata and a distributed description of the document, consisting of citation contexts over many citing documents.

\begin{figure}
  \centering
    \includegraphics[scale=0.2]{figures/dataset/data_set_generation_schema.pdf}
  \caption{Schematic representation of the data set generation process.}
  \label{fig:datagen}
\end{figure}

\subsection{\LaTeX{} Parsing}
In the following we will describe the tools considered for parsing \LaTeX{}, the challenges we faced in general and with regard to arXiv sources in particular, and our resulting approach.

\begin{table}[bh]
\centering
  \caption{Comparison of tools for parsing \LaTeX{}.}
  \label{tbl:tools}
\begin{tabular}{llll}
\toprule
    Tool & Output & Robust & Usable w/o modification \\
   \midrule
    plastex\footnote{See \url{https://github.com/tiarno/plastex}.} & DOM & no & yes\\
    TexSoup\footnote{See \url{https://github.com/alvinwan/texsoup}.} & document tree & no & yes\\
    opendetex\footnote{See \url{https://github.com/pkubowicz/opendetex}.}/detex\footnote{See \url{https://www.freebsd.org/cgi/man.cgi?query=detex}.} & plain text & no & yes\\
    GrabCite\cite{Faerber2018} & plain\hphantom{ }text\hphantom{ }+ resolved ref. & yes & no\\
    LaTeXML\footnote{See \url{https://github.com/brucemiller/LaTeXML}.} & XML & yes & yes\\
    Tralics\footnote{See \url{https://www-sop.inria.fr/marelle/tralics/}.} & XML & yes & yes\\
  \bottomrule
\end{tabular}
\end{table}

\subsubsection{Tools}
We took several tools for the conversion from \LaTeX{} to plain text or to intermediate formats into consideration and evaluated them. Table \ref{tbl:tools} gives an overview of our results. Half of the tools failed to produce any output for a large amount of arXiv submissions we used as test input and were therefore deemed not robust enough. \textit{GrabCite} is able to parse 78.5\% of arXiv CS submissions but integrates resolving references against DBLP into the parsing process and therefore would require significant modification to fit our system architecture. \textit{LaTeXML} and \textit{Tralics} are both robust and can be used as \LaTeX{} conversion tools as is. On subsequent tests we note that \textit{LaTeXML} needs on average 7.7 seconds (3.3 if formula environments are heuristically removed beforehand) to parse an arXiv submission while \textit{Tralics} needs 0.09. Because the quality of their output seemed comparable we chose to use \textit{Tralics}.


\subsubsection{Challenges}
Apart from the general difficulty of parsing \LaTeX{} due to its feature richness and people's free-spirited use of it, we especially note difficulty in dealing with extra packages not included in submissions' sources\footnote{arXiv specifically suggest the omission of such (see \url{https://arxiv.org/help/submit\_tex\#wegotem}).}. While \textit{Tralics} is supposed to for example deal with \textit{natbib} citations\footnote{See \url{https://www-sop.inria.fr/marelle/tralics/packages.html\#natbib}.}, normalization of such citations lead to a decrease of citation markers not being able to be matched to an entry in the document's reference section from 30\% to 5\% in a sample of 565,613 citations we tested.

% \vfill % in case there's ugly large spaces between section title and text

\subsubsection{Resulting approach}
Our \LaTeX{} parsing solution consists of two steps. First, we flatten each arXiv submission's sources to a single \LaTeX{} file using \textit{latexpand}\footnote{See \url{https://ctan.org/pkg/latexpand}.}\textsuperscript{,}\footnote{We also tested flatex (\url{https://ctan.org/pkg/flatex}) and flap (\url{https://github.com/fchauvel/flap}) but got the best results with \texttt{latexpand}.} and normalize \texttt{\textbackslash cite} commands to prevent parsing problems later on. In the second step, we then generate an XML representation of the \LaTeX{} document using \textit{Tralics}, replace formulas, figures, tables and intra-document references with replacement tokens and extract the plain text. Furthermore, each entry in the document's reference section is assigned a unique identifier, the reference string stored in a database and corresponding citation markers are placed in the plain text.

\subsection{Reference resolution}\label{sec:refresol}
Resolving references to globally consistent identifiers (e.g. detecting that the references (1), (2), and (3) in Listing~\ref{lst:refitems} all reference the same document) is a challenging and still unsolved task~\cite{Nasar2018}. Given it is, by itself, the most distinctive part of a publication, we base our reference resolution on the title of the cited work and use other pieces of information (e.g. authors' names) only in secondary steps. In the following we will describe the challenges we faced, matching arXiv submissions' reference strings against MAG paper records and how we approached the task.

\begin{lstlisting}[caption={Examples of reference strings.},label={lst:refitems}]
(1) V. N. Senoguz and Q. Shafi, arXiv:hep-ph/0412102
(2) V.N. Senoguz and Q. Shafi, Phys. Rev. D 71 (2005) 043514.
(3) V. N. Şenoğuz and Q. Shafi, ``Reheat temperature in supersymmetric hybrid inflation models,'' Phys. Rev. D 71, 043514 (2005) [hep-ph/0412102].
(4) V.Sauli, JHEP 02, 001 (2003).
(5) Aaij, Roel, et al. "Search for the $B^{0}_{s} \to \eta^{\prime}\phi$ decay" Journal of High Energy Physics 2017.5 (2017): 158.
(6) According to the numerous discussions with my colleagues <removed> and <removed> an experimental verification of our theoretical predictions is feasible.
\end{lstlisting} % (*$B^{0}_{s} \to \eta^{\prime}\phi$*)

\subsubsection{Challenges}
Reference resolution can be challenging when reference strings contain only minimal amounts of information, when formulas are used in titles or when they refer to non publications (e.g. Listing~\ref{lst:refitems} (4)-(6)). A further problem we encountered was noise in the MAG. Our mechanism matched 13,041 reference strings like \texttt{K. Kondo, hep-th/0303251.} and \texttt{T. Heinzl, hep-th/9812190.} to a MAG paper with the title \textit{``hep-th.''} with one of the author's names being \textit{``He''} (paper ID \texttt{2811252340}).

\subsubsection{Resulting approach}
Our reference resolution procedure can be broken down in two steps: title identification and matching. If possible, title identification is done by arXiv ID or DOI (where we retrieve the title from an arXiv metadata dump or via Crossref\footnote{See \url{https://www.crossref.org/}.}); otherwise we use Neural ParsCit~\cite{Animesh2018}. The identified title is matched against the normalized titles of all publications in the MAG. Resulting candidates are considered, if at least one of the author's names is present in the reference string. If multiple candidates remain, we judge by the citation count given in the MAG. The last step particularly helped mitigate rouge almost-duplicate entries in the MAG that often have few to no citations.

\subsection{Result format}\label{sec:datasetformat}
Listing \ref{lst:formatall} shows some example content from the data set. While the data set in and of itself consists of plain text files and a references database, citation contexts of successfully resolved references are straightforward to extract and use as input for a recommender. The bottom part of Listing~\ref{lst:formatall} shows an example of a 3 sentence context with \texttt{cited doc MAG ID},\hphantom{n}\texttt{MAG IDs of adjacent citations},\hphantom{n}\texttt{citing doc arXiv ID} and \texttt{text} in a CSV file. Citations are deemed adjacent, if they are part of a citation group or are at most 5 characters apart (e.g. \emph{''[27,42]''}, \emph{''[27], [42]''} or \emph{''[27] and [42]''}). % Sentence tokenization is performed with NLTK’s pre-trained PunktSentenceTokenizer.

\begin{lstlisting}[caption={Excerpts from (top to bottom) a plain text file, corresponding data base entries in the references DB, entries in the MAG and extracted citation context CSV.},label={lst:formatall}]
It has over 79 million images stored at the resolution of FORMULA . Each image is labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet{{cite:9ad20b7d-87d1-47f5-aeed-10a1cf89a2e2}}{{cite: 298db7f5-9ebb-4e98-9ecf-0bdda28a42cb}} lexical database.

[uuid]         [in_doc]   [mag_id]    [reference_string]
9ad20b7d-87d1  1412.3684  2081580037  George A. Miller (1995). WordNe
-47f5-aeed-..                         t: A Lexical Database for Eng..
298db7f5-9ebb  1412.3684  2038721957  Christiane Fellbaum (1998), ""W
-4e98-9ecf-..                         ordNet: An Electronic Lexical..

[paperid]   [originaltitle]                           [publisher]  ...
2038721957  WordNet : an electronic lexical database  MIT Press    ...
2081580037  WordNet: a lexical database for English   ACM          ...

2038721957|2081580037|1412.3684|It has over 79 million images stored at the resolution of FORMULA . Each image is labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet CIT MAINCIT lexical database. It has been noted that many of the labels are not reliable CIT .
\end{lstlisting}

\begin{table}[h]
  \caption[Confidence intervals for a sample size of 300 with 297 positive results.]{Confidence intervals for a sample size of 300 with 297 positive results as given by Wilson score interval and Jeffreys interval \cite{Brown2001}.}
  \label{tbl:confvals}
  \centering
\begin{tabular}{c@{\hspace{0.1in}}c@{\hspace{0.1in}}c@{\hspace{0.1in}}c}
\toprule
    Confidence level & Method & Lower limit & Upper limit \\\noalign{\smallskip}
\midrule
    0.99 & Wilson & 0.9613 & 0.9975 \\\noalign{\smallskip}
    \ & Jeffreys & 0.9666 & 0.9983 \\\noalign{\smallskip}
    \hline\noalign{\smallskip}
    0.95 & Wilson & 0.9710 & 0.9966 \\\noalign{\smallskip}
    \ & Jeffreys & 0.9736 & 0.9972 \\\noalign{\smallskip}
    \bottomrule
\end{tabular}
\end{table}

\section{Evaluation of reference resolution}
To evaluate the quality of our reference resolution results, we take a random sample of 300 matched reference strings and manually check if the correct record in the MAG was identified by our method. Given the 300 items, we obtained 3 errors, giving us an accuracy estimate of 96\% at the worst, as shown in Table~\ref{tbl:confvals}.

The three incorrectly identified references were as follows (MAG IDs in square brackets):
\begin{enumerate}
    \item \emph{``Eddy, J.A.: 1983, The maunder minimum - a reappraisal. Solar Phys. 89, 195. ADS.''}
    \begin{itemize}
        \item matched: [\texttt{2024069573}]\\\emph{``The Maunder Minimum''} (John A. Eddy; 1976)
        \item correct: [\texttt{2080336740}]\\\emph{``The Maunder Minimum: A reappraisal''} (John A. Eddy; 1983)
    \end{itemize}
    \item \emph{``J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. 1-norm support vector machines. In Advances in Neural Information Processing Systems (NIPS), volume 16, pages 49–56, 2004.''}
    \begin{itemize}
        \item matched: [\texttt{2249237221}]\\\emph{``Support Vector Machines''} (Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani; 2013)
        \item correct: [\texttt{2130698119}]\\\emph{``1-norm Support Vector Machines''} (Ji Zhu, Saharon Rosset, Robert Tibshirani, Trevor J. Hastie; 2003)
    \end{itemize}
    \item \emph{``D. T. Limmer and D. Chandler. The putative liquid-liquid transition is a liquid-solid transition in atomistic models of water. The Journal of Chemical Physics, 135(13):134503, 2011.''}
    \begin{itemize}
        \item matched: [\texttt{2599889364}]\\\emph{``The Putative Liquid-Liquid Transition is a Liquid-Solid Transition in Atomistic Models of Water''} (David Chandler, David Limmer; 2013)
        \item correct: [\texttt{1977410206}]\\\emph{``The putative liquid-liquid transition is a liquid-solid transition in atomistic models of water. II''} (David T. Limmer, David Chandler; 2011)
    \end{itemize}
\end{enumerate}

In all three cases the misidentified document’s title is contained in the correct document’s title, and there is a large or complete author overlap between correct and actual match. This shows that authors sometimes title follow-up work very similarly, which leads to hard to distinguish cases. Another observation that can be made, is that longer titles are more distinctive. As certain publication titles might be sub-strings of other publications' titles, a matching mechanism should always try to first match a long title before trying shorter candidates. The full details of the evaluation can be found in Appendix~\ref{chap:matcheval}.

\section{Statistics and key figures}

\subsection{Creation process}
We used an arXiv source dump containing all submissions up until the end of 2017 (1,340,770 documents). 100,240 of these were only available in PDF format, leaving 1,240,530 sources. Our pipeline output 1,151,707 (92.8\%) plain text files, 1,018,976 (82.1\%) of which contained citation markers. The number of reference strings identified is 35,053,329, for which 56,077,906 citation markers were placed within the plain text files. This first part of the process took 59 hours to run, unparallelized on a 8 core Intel Core i7-7700 3.60GHz machine with 60 GB of memory.

Of the 35,053,329 reference strings, we were able to match 14,046,239 (40.07\%). For 33.14\% of the reference strings we could neither find an arXiv ID or DOI, nor was Neural ParsCit able to identify a title. For the remaining 26.79\% a title was identified but could not be matched with the MAG. Of the matched 14 million items' titles, 50.67\% were identified via Neural ParsCit, 29.67\% by DOI and 19.66\% by arXiv ID. Of the identified DOIs 26.8\% were found as is while 73.2\% were heuristically determined\footnote{This was possible because the DOIs of articles in journals of the American Physical Society follow predictable patterns.}. The matching process took 103 hours, run in 10 parallel processes on a 64 core Intel Xeon Gold 6130 2.10GHz machine with 500 GB of memory.

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/dataset/citing_docs_per_cited_doc.pdf}
  \caption{Number of citing documents per cited document.}
  \label{fig:numcitdoc}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/dataset/citation_contexts_per_reference.pdf}
  \caption{Number of citation contexts per reference.}
  \label{fig:numcontref}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=\textwidth]{figures/dataset/citation_relation_sankey.pdf}
  \caption[Citation flow by field of study for 13.3 million references.]{Citation flow by field of study for 13.3 million references. The number of citing and cited documents per field of study are plotted on the sides.}
  \label{fig:sankey}
\end{figure}

\subsection{Resulting data set}
The resulting data set consists of \emph{2,343,585 cited papers, 926,644 citing papers, 13,303,373 references and 24,558,560 citation contexts}. Note that references with no citation markers (due to parsing errors) are not counted here.

Figure \ref{fig:numcitdoc} shows the number of citing documents for all cited documents. There is one document with close to 10,000 citations, another three with more than 5,000 and another ten with more than 3,000. 1,262,861 (53.89\%) of the documents have at least two citations, 547,036 (23.34\%) have at least five. The mean number of citations is 5.68 (SD 26.82). Figure \ref{fig:numcontref} shows the number of citation context per reference. 8,722,795 (65.57\%) references have only one citation context, the maximum is 278, the mean 1.85 (SD 2.02). This means a cited document is described by on average $1.85 \times 5.68 \approx 10.5$ citation contexts.

Figure \ref{fig:sankey} depicts the flow of citations by field of study for all 13.3 million matched references. Fields of study with very small numbers of references are combined to \emph{other} for legibility reasons. For the citing document's side, these are economics, electrical engineering and systems science, quantitative biology, quantitative finance and statistics. Combined on the cited document's side are chemistry, biology, engineering, materials science, economics, geology, psychology, medicine, business, geography, sociology, political science, philosophy, environmental science and art. To no surprise, publications in each field are cited the most from within the field itself. Notable is, however, that the incoming citations in mathematics are the most varied (physics and computer science combined make up 38\% of the citations).

By generating our data set from \LaTeX{} sources we were able to ensure clean text output as well as accurate citation information and exact citation marker positions. In terms of size it is closer to CiteSeer\textsuperscript{x} and PMC-OAS than the smaller data sets available. The fact that the data set spans multiple disciplines also allows for comparisons in citing behaviour between these disciplines. Because references are already resolved to MAG IDs, the data set is readily usable for recommendation tasks and allows for the use of rich metadata on both the citing and cited document side. Lastly, the embedding of citation markers in the full plain text of papers instead of pre-extraction enables users of the data set to choose and compare citation context lengths and variations at will.
