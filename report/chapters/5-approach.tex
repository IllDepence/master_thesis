\chapter{Approach}\label{chap:approach}
In order to investigate the use of explicit semantic representations for the task of local citation recommendation we first need to decide which kinds of semantic constructs we want to model. As a starting point for this we looked into the field of citation context analysis~\cite{HERNANDEZ-ALVAREZ2016}. A common task in this area is the classification of citation contexts by their polarity (positive/neutral/negative) and function (often based on the four dimensions identified by Moravcsik et al.~\cite{Moravcsik1975}, conceptual/operational, evolutionary/juxtapositional, organic/perfunctory, conformative/negational). Such approaches are primarily concerned with the \emph{intent} of the author rather than the \emph{content} of what is being cited. We can therefore not expect to derive types of semantic constructs directly from citation functions. Starting from an established typology of citation functions will, however, ensure that we consider a wide range of different citations rather than cherry picking those that fit our preconceptions.

\begin{table}[]
\centering
    \caption{Semantic constructs in citation contexts from a range of citation functions used in the field of citation context analysis.}
    \label{tab:citfunctions}
\begin{center}
    \begin{tabular}{m{2.7cm}lm{8.5cm}}
    \toprule
    Function & Construct & Examples (semantic construct \emph{highlighted})\\
    \midrule
    Attribution & claim & ``Berners-Lee et al.~\cite{Berners-Lee2001} argue that \emph{structured collections of information and sets of inference rules are prerequesites for the semantic web to function}.'' \\\noalign{\smallskip}
    \  & NE & ``A variation of this task is `\emph{context-based co-citation recommendation}'~\cite{Kobayashi2018}.'' \\\noalign{\smallskip}
    \  & - & ``In \cite{Duma2014} Duma et al. test the effectiveness of using a variety of document internal and external text inputs to a TF-IDF model.'' \\\noalign{\medskip}
    Exemplification & NE & ``We looked into approaches to \emph{local citation recommendation} such as~\cite{He2010,Huang2014,Huang2015,Duma2014,Duma2016,Ebesu2017,Kobayashi2018,Jeong2019} for our investigation.'' \\\noalign{\medskip}
    Further reference & - & ``See \cite{Niklaus2018} for a comprehensive overview.'' \\\noalign{\medskip}
    Statement of use & NE & ``We use \emph{CiteSeerX}~\cite{Caragea2014} for our evaluation.'' \\\noalign{\medskip}
    Application & NE & ``Using this mechanism we perform `\emph{context-based co-citation recommendation}'~\cite{Kobayashi2018}.'' \\\noalign{\medskip}
    Evaluation & - & ``The use of DBLP in \cite{Faerber2018} restricts their data set to the field of computer science.'' \\\noalign{\medskip}
    Establishing links between sources& claim & ``A common motivation brought forward for research on citation recommendation is that \emph{finding proper citations is a time consuming task} \cite{He2010,He2011,Ebesu2017,Kobayashi2018}.'' \\\noalign{\smallskip}
    \  & - & ``Lamers et al.~\cite{Lamers2018} base their definition on the author's name whereas Thompson~\cite{Thompson2001} focusses on the grammatical role of the citation marker.'' \\\noalign{\medskip}
    Comparison of own work with sources& claim & ``Like \cite{Faerber2018} we find that, albeit written in a structured language, \emph{parsing \LaTeX{} sources is a non trivial task}.'' \\
    \bottomrule
    \end{tabular}
\end{center}
\end{table}

Table~\ref{tab:citfunctions} lists categories of citation functions along with the kinds of semantic constructs that can be found in such citation contexts. The list of citation functions is taken from \cite{Petric2007} (and therein built upon \cite{Thompson2001}). This study was selected because it gives an overview of previous attempts to classify citations, presents their new typology with extensive explanation as well as example contexts, and does not mix polarity into its function categories. Examining contexts from each of the eight functions we identify two types of semantic constructs: named entities (NE) and claims (or statements). The rationale behind these two is as follows. Named entities can identify reference publications for a certain data set/tool/concept (see \emph{Attribution}, \emph{Statement of use} and \emph{Application} in Table~\ref{tab:citfunctions}) as well as a method/field of study common to a selection of publications (see \emph{Exemplification} in Table~\ref{tab:citfunctions}). Claims can identify publications that can be cited to back or support the very claim contained in a citation context. Note that the example contexts listed to have no construct (``-'' in the \emph{Construct} column) may contain named entities and claims as well (e.g. ``DBLP'' or ``Lamers et al. base their definition of the author's name''), but these are (in the case of NEs) not representative of the cited work or (in the case of claims) just statements \emph{about} a publication rather than statements being backed by the cited work.
A third semantic construct that can be considered, but would require considering a larger citation context, is argumentative structures. To keep the scope of this thesis at a reasonable level we will, however, limit our investigation to named entities and claims.

The following sections will describe our investigation of entity based and claim based models for local citation recommendation.

\section{Entity based recommendation}
The intuition behind an entity based approach is, that there exists a reference publication for a named entity. Examples would be a data set (``CiteSeerX~\cite{Caragea2014}''), a tool (``Neural ParsCit~\cite{Animesh2018}'') or a concept (``Semantic Web~\cite{Berners-Lee2001}''). In a more loose sense this can also include publications being referred to as examples (``approaches to local citation recommendation~\cite{He2010,Huang2014,Huang2015,Duma2014,Duma2016,Ebesu2017,Kobayashi2018,Jeong2019}'').
% mention DBpedia Spotlight tests? (sufficient data available?)
For the identification of such named entities we take two approaches. A more strict one based on the fields of study given in the MAG, and a more loose one based on noun phrases.

\subsection{Fields of study in the MAG}
Along with papers, authors, venues etc. the MAG data schema also includes fields of study (FoS) that are associated with papers and interlinked in a child-parent manner. At the time of writing there are 229,716 FoS at 6 levels of granularity. They range from the most coarse level 0 (example entries being \emph{mathematics}, \emph{sociology},  \emph{computer science}) to more and more fine grained entities (\emph{information retrieval}$_{(1)}$→\emph{search engine}$_{(2)}$→\emph{web search query}$_{(3)}$→\emph{ranking (information retrieval)}$_{(4)}$→\emph{Okapi BM25}$_{(5)}$). The levels of granularity don't seem to follow a globally consistent pattern though. \emph{WordNet}, for example, being a particular piece of data in the same way \emph{Okapi BM25} is a particular function, is not at level 5 but level 2 (\emph{computer science}$_{(0)}$→\emph{artificial intelligence}$_{(1)}$→\emph{WordNet}$_{(2)}$). Another noteworthy aspect is that a FoS can have multiple parents (\emph{search engine} for example has a second parent in \emph{World Wide Web}).

\paragraph{Model} The entity based representation of a citation context is the set of FoS that appear within the context. Formally, let $\mathcal{F}$ denote the set of FoS; then the entity based representation of a citation context $c$ is the set of terms $t$ defined as ${R_{\text{FoS}}(c) = \{t|t\text{ appears in }c \land t\in \mathcal{F}\}}$. Because of the hierarchical structure of FoS we experiment with augmenting the set by including the set members' parents into the representation. We find that his leads to worse results, presumably because a context's description becomes more vague which is detrimental to identifying reference publications or exemplifications. We furthermore look into only using a FoS when it directly precedes the citation marker---as it might be more relevant to the citation then---, but notice that such cases are too rare. In a preliminary test with 900k citation contexts, only 0.14\% of our 180k test set items have a FoS in the required position matching any of the representations learned from the training set.

\paragraph{Recommendation} For recommending documents based on $R_{\text{FoS}}$ we use the Jaccard similarity between the input citation context and the aggregrated citation contexts describing each candidate document. Formally, let $c_i$ denote the input citation context and $\mathcal{D}$ be a set of documents with members $d\in \mathcal{D}$. Furthermore let $\varrho(d)$ be the set of citation contexts referencing $d$; ${\varrho(d)=\{c|c\text{ references } d\}}$. The Jaccard similarity then is defined as ${J(A,B)=\frac{|A\cap B|}{|A\cup B|}}$, where $A = R_{\text{FoS}}(c_i)$ and $B=\bigcup\limits_{c \in \varrho(d)} R_{\text{FoS}}(c)$.

\subsection{Noun phrases}
For our second entity based model we take a more loose approach and treat noun phrases extracted from the arXiv data set as named entities. By filtering out items that appear only once we end up with 2,835,929 noun phrases (NPs).

\paragraph{Model} Similar to the FoS representation, we look at the NPs appearing within a citation context. To ensure a high descriptiveness, we only take into account maximally long matches. A context \emph{``This has been done for language model training [27]''}, for example, would have ``language model training'' in its representation, but not ``language model''. Formally we can define ${R_{\text{NP}}(c) = \{t|t\text{ appears in }c \land t\in \mathcal{P} \land t^{+pre} \notin \mathcal{P}\land t^{+suc} \notin \mathcal{P}\}}$ where $\mathcal{P}$ is our set of NPs while $t^{+pre}$ and $t^{+suc}$ denote an extension of $t$ using its preceding or succeeding word respectively. As an alternative representation we furthermore define ${R_{\text{NPmrk}}^{2+}(c)}$ as a subset of ${R_{\text{NP}}(c)}$ containing, if present, the NP of minimum word length 2 directly preceding the citation marker in $c$ that a prediction is to be made for. Formally, ${R_{\text{NPmrk}}^{2+}(c) = \{t|t\in R_{\text{NP}}(c)\land len(t)\geq 2 \land t\text{ directly precedes } m\}}$ where $m$ is the citation marker in $c$ that a prediction is to be made for.

\paragraph{Recommendation} Recommending documents based on ${R_{\text{NP}}}$ and ${R_{\text{NPmrk}}^{2+}}$ is done using a vector space model (VSM) in which NP representations, treated as a bag of word (BoW), are compared by their cosine similarities. Representations of candidate documents are, likewise to the FoS based model, aggregrates over all contexts referencing the document. Formally, the vector representation of a context is given by $V(R(c)) = (t_{1,j}, t_{2,j}, ..., t_{|\mathcal{P}|,j})$ where $\mathcal{P}$ is the set of all NPs % technically wrong b/c sets are not ordered
and $t_{i,j}$ is a non-negative integer representing a quantity with regards to the $i$th term in $\mathcal{P}$. Aggregated context representations for candidate documents are caluclated by adding up all vector representations of the contexts referring to a document. I.e., let $\varrho(d)$ be the set of citation contexts referencing $d$, then $d$'s vector representation is $\sum\limits_{c \in \varrho(d)} V(R(c))$. The similarity between an input context $c_i$ and a candidate document $d\in \mathcal{D}$ can then be calculated as the cosine $\theta$ between the two vector representations ${\mathrm{cos}(\theta)=\frac{A\cdot B}{\|A\| \|B\|}}$ where  $A=V(R(c_i))$ and $B=\sum\limits_{c \in \varrho(d)} V(R(c))$

\section{Claim based recommendation}
For the introduction of a claim based model we first need to make a few observations on how citations interact with the text they're placed in. By convention, citations are constructed by placing a type of marker, which identifies an entry in the document's reference section, in the text. These markers can, depending on discipline, journal, etc. take different forms. Some examples are numbers in square brackets (``In [27] ...''), alphanumeric identifiers in square brackets (``In [Bol98] ...''), a year in parentheses succeeding an author's name (``Swales (1990) has argued ...'') and an author's name with a year in parentheses (``It has been argued (Swaled, 1990) ...''). Named entities can stand in a grammatical relation to such a marker. In ``By using CiteSeer, [Bol98], we ...'', for example, the named entitiy \emph{CiteSeer} and the citation marker \emph{[Bol98]} are in a grammatical relation called \emph{apposition}. A citation marker can, however, reasonably assumend to never be \emph{part of} the named entity. This is different in the case of the statements within a citaiton context. Looking at the sentence ``In \cite{Bollacker1998} Bollacker et al. introduce Citeseer.'' we can see that the marker itself is part of what is being said, while this is not the case in for example ``Bollacker et al. introduced Citeseer in 1998~\cite{Bollacker1998}.''. This distiction will briefly be explained in the following section.

\subsection{Integral and non-integral citations}
The term ``integral''---in the adjectival sense close in meaning to ``essential'' or ``inherent'', not what we denote in caluclus with $\int$---is used to describe citation markers that are part of a sentence. There seems to be no consensus on the exact definition of \emph{``part of''} though. While some \cite{} others \cite{}

how citations are embedded in sentences (integral/non-integral\cite{Swales1990,Hyland1999,Thompson2001,Okamura2008,Lamers2018})

actually look into automatic classification \cite{Whidby2011,Abujbara2012}

\begin{table}[]
\centering
    \caption[Differences in definition of integral and non-integral citations.]{Differences in definition of integral and non-integral citations. i=integral, n=non~integral.}
    \label{tab:integral}
\begin{center}
    \begin{tabular}{llllllll}%m{8cm}
    \toprule
    Citation type & \rotatebox{90}{Swales~\cite{Swales1990}} & \rotatebox{90}{Hyland~\cite{Hyland1999}} & \rotatebox{90}{Thompson~\cite{Thompson2001}} & \rotatebox{90}{Okamura~\cite{Okamura2008}} & \rotatebox{90}{Whidby et al.~\cite{Whidby2011}} & \rotatebox{90}{Abujbara et al.~\cite{Abujbara2012}} & \rotatebox{90}{Lamers et al.~\cite{Lamers2018}} \\
    \midrule
    ``Swales has argued that ... [42].''          & x & x & ? & x & x & x & x \\
    ``Swales (1990) has argued that ...''         & x & x & i & x & x & x & x \\
    ``It has been argued (Swales 1990) that ...'' & x & x & n & x & x & x & x \\
    ``[42] argues that ...''                      & x & x & i & x & x & x & x \\
    ``foo [42]''                                  & x & x & x & x & x & x & x \\
    \bottomrule
    \end{tabular}
\end{center}
\end{table}

\subsection{Tools for extracting claims}
tools tools

also: Survey on open information extraction\cite{Niklaus2018}

context specific claim detection\cite{Levy2014}

if only papers where semantically annotated as proposed in \cite{BuckinghamShum2000}
\subsection{A predicate-argument model}
predpatt\cite{White2016,Zhang2017}

unfeasibility of use of PredPatt output as is

loosened predicate:parameter model

predicates could be grouped/clustered to represent functions as in \cite{Gabor2018}

alternative view: model gives a selective citation context derived from claim structure (cf. concept of reference scope as sub part of citation context sentence\cite{Abujbara2012,RAHUL2017}
