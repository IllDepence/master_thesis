\chapter{Future work}\label{chap:todo}
% As a first step towards semantic citation recommendation, the work presented not only presents insights, but also lays bare new questions.
Several parts of the work presented merit continued development, others lay bare questions yet to be answered. In closing, we want to briefly highlight these with regards to the newly created data set, the entity as well as claim based models, and finally semantic citation recommendation in general.

The flexible nature of our data set makes it suitable for the development and evaluation of various citation based tasks, not just the one presented here. Because arXiv is an established resource in active use, new publications are added to it every day. These new submissions can be a valuable addition to our data set. In parallel to the development and evaluation of our models, we already extended the data with all publications from 2018~\cite{Saier2019}. Apart from the addition of new papers, the inclusion of mathematical formulas in some way, shape, or form (instead of replacing them with a substitution token) would presumably improve recommendation, as they can contain key information and are abundant in mathematics and physics papers. Such improvements can be guided by existing arXiv based efforts in mathematical information retrieval~\cite{Aizawa2014,Zanibbi2016}.

Because the arXiv data set spans several disciplines, a comparative analysis of the performance of our models could reveal interdisciplinary differences. As shown in our offline evaluation, the size and composition of a data set can have a large impact on the results. A comparative analysis would therefore require the adjustment of the data set's math and physics subsets in terms of number of recommendation candidates and number of citing papers describing those.

Concerning our NPmarker model, an evaluation on more than just the arXiv data is needed. Because the model relies on the exact position of the citation marker, which is not given in the other data sets used, a heuristic for the marker's identification could be used. Another possibility would be the use of PMC-OAS data, as their JATS XML files also provide exact citation marker positions. Our Claim model in its current state uses the lemmatized form of predicates. These could be further generalized into a number of semantic relation types as done in~\cite{Gabor2018}. Given the models current \emph{predicate-argument} structure, it might be possible to implement a basic semantic citation search on top of it, allowing users to, for example, search for publications showing that something \emph{``is:NP-hard''} or where the authors \emph{``improve:local citation recommendation''}. A last point concerning the claim based model is the realization of grammatical citation marker awareness. This requires proper handling of non-syntactic citations, which, judging by our user study results, make up the vast majority of citations. The algorithm presented in \cite{Abujbara2012} can be used as a starting point for identifying non-syntactic citations, but would have to be properly evaluated. The same holds true for their method of using the head of the nearest noun phrase as the new position of the marker.

Generally speaking, we think a more thorough and systematic examination of citation types---that is, structural types with respect to the grammatical and character-level location of information most descriptive of the cited document---should be a first step towards refining our existing models and creating new, citation type specific approaches.

In the long run, models revolving around claims could furthermore benefit from an assessment of credibility~\cite{Popat2016}; and beyond the level of claims, the modelling of argumentative structures, informed by existing work in the field of argumentation mining~\cite{Stab2016,Lippi2016,Habernal2017}, could enable even more refined recommendation results.
