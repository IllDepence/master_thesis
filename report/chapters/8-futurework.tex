\chapter{Future work}\label{chap:todo}
% As a first step towards semantic citation recommendation, the work presented not only presents insights, but also lays bare new questions.
Several parts of the work presented merit continued development, others lay bare questions yet to be answererd. In closing, we want to briefly highlight these with regards to the newly created data set, the entity as well as claim based models, and finally semantic citation recommendation in general.

The flexible nature of our data set makes it suitable for the development and evaluation of various citation based tasks, not just the one presented here. Because arXiv is an established resource in active use, new publications are added to it every day. These new submissions can be a valuable addition to our data set. In parallel to the development and evaluation of our models, we already extended the data with all publications from 2018~\cite{Saier2019}. Apart from the addition of new papers, the inclusion of mathematical formulas in some way, shape, or form (instead of replacing them with a substitution token) would presumably improve recommendation, as they can contain key information and are abundant in mathematics and physics papers. Such improvements can be guided by existing arXiv based efforts in mathematical information retrieval~\cite{Aizawa2014,Zanibbi2016}.

Because the arXiv data set spans several disciplines, a comparative analysis of the performance of our models could reveal interdisciplinary differences. As shown in our offline evaluation, the size and composition of a data set can have a large impact on the results. A comparative analysis would therefore require the adjustment of the data set's math and physics subsets in terms of number of recommendation candidates and number of citing papers describing those.

Concerning our NPmarker model, an evaluation on more than just the arXiv data is needed. Because the model relies on the exact position of the citation marker, which is not given in the other data sets used, a heuristic for the marker's identification could be used. Another possibility would be the use of PMC-OAS data, as their JATS XML files also provide exact citation marker positions. Our Claim model in its current state uses the lemmatized form of predicates. These could be further generalized into a number of semantic relation types as done in~\cite{Gabor2018}. Given the models \emph{predicate-argument} strucutre, it might already be possible to implement a basic semantic citation search on top of it, allowing users to, for example, search for publications showing that something \emph{``is:NP-hard''} or where the authors \emph{``improve:local citation recommendation''}. A last point concerning the claim based model is the realization of grammatical citation marker awareness. This requires proper handling of non-syntactic citations, which, judging by our user study results, make up the vast majority of citations.

in general:
As a first step identify types of citations more systematically.Chapter~\ref{chap:conclusion}
For different types, different models.

far future:
assessing credibility of claims\cite{Popat2016}
Argumentative structures. (Argumentation mining\cite{Stab2016,Lippi2016,Habernal2017})
